title,summary,authors,published,updated,url,pdf_url,matched_query,year
Learning Multi-Stage Pick-and-Place with a Legged Mobile Manipulator,"Quadruped-based mobile manipulation presents significant challenges in
robotics due to the diversity of required skills, the extended task horizon,
and partial observability. After presenting a multi-stage pick-and-place task
as a succinct yet sufficiently rich setup that captures key desiderata for
quadruped-based mobile manipulation, we propose an approach that can train a
visuo-motor policy entirely in simulation, and achieve nearly 80\% success in
the real world. The policy efficiently performs search, approach, grasp,
transport, and drop into actions, with emerged behaviors such as re-grasping
and task chaining. We conduct an extensive set of real-world experiments with
ablation studies highlighting key techniques for efficient training and
effective sim-to-real transfer. Additional experiments demonstrate deployment
across a variety of indoor and outdoor environments. Demo videos and additional
resources are available on the project page:
https://horizonrobotics.github.io/gail/SLIM.","Haichao Zhang, Haonan Yu, Le Zhao, Andrew Choi, Qinxun Bai, Yiqing Yang, Wei Xu",2025-09-04T03:36:07Z,2025-09-04T03:36:07Z,http://arxiv.org/abs/2509.03859v1,http://arxiv.org/pdf/2509.03859v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Self-Organizing Aerial Swarm Robotics for Resilient Load Transportation
  : A Table-Mechanics-Inspired Approach","In comparison with existing approaches, which struggle with scalability,
communication dependency, and robustness against dynamic failures, cooperative
aerial transportation via robot swarms holds transformative potential for
logistics and disaster response. Here, we present a physics-inspired
cooperative transportation approach for flying robot swarms that imitates the
dissipative mechanics of table-leg load distribution. By developing a
decentralized dissipative force model, our approach enables autonomous
formation stabilization and adaptive load allocation without the requirement of
explicit communication. Based on local neighbor robots and the suspended
payload, each robot dynamically adjusts its position. This is similar to
energy-dissipating table leg reactions. The stability of the resultant control
system is rigorously proved. Simulations demonstrate that the tracking errors
of the proposed approach are 20%, 68%, 55.5%, and 21.9% of existing approaches
under the cases of capability variation, cable uncertainty, limited vision, and
payload variation, respectively. In real-world experiments with six flying
robots, the cooperative aerial transportation system achieved a 94% success
rate under single-robot failure, disconnection events, 25% payload variation,
and 40% cable length uncertainty, demonstrating strong robustness under outdoor
winds up to Beaufort scale 4. Overall, this physics-inspired approach bridges
swarm intelligence and mechanical stability principles, offering a scalable
framework for heterogeneous aerial systems to collectively handle complex
transportation tasks in communication-constrained environments.","Quan Quan, Jiwen Xu, Runxiao Liu, Yi Ding, Jiaxing Che, Kai-Yuan Cai",2025-09-03T15:11:41Z,2025-09-03T15:11:41Z,http://arxiv.org/abs/2509.03563v1,http://arxiv.org/pdf/2509.03563v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"FlipWalker: Jacob's Ladder toy-inspired robot for locomotion across
  diverse, complex terrain","This paper introduces FlipWalker, a novel underactuated robot locomotion
system inspired by Jacob's Ladder illusion toy, designed to traverse
challenging terrains where wheeled robots often struggle. Like the Jacob's
Ladder toy, FlipWalker features two interconnected segments joined by flexible
cables, enabling it to pivot and flip around singularities in a manner
reminiscent of the toy's cascading motion. Actuation is provided by
motor-driven legs within each segment that push off either the ground or the
opposing segment, depending on the robot's current configuration. A
physics-based model of the underactuated flipping dynamics is formulated to
elucidate the critical design parameters governing forward motion and obstacle
clearance or climbing. The untethered prototype weighs 0.78 kg, achieves a
maximum flipping speed of 0.2 body lengths per second. Experimental trials on
artificial grass, river rocks, and snow demonstrate that FlipWalker's flipping
strategy, which relies on ground reaction forces applied normal to the surface,
offers a promising alternative to traditional locomotion for navigating
irregular outdoor terrain.","Diancheng Li, Nia Ralston, Bastiaan Hagen, Phoebe Tan, Matthew A. Robertson",2025-08-26T19:16:21Z,2025-08-26T19:16:21Z,http://arxiv.org/abs/2508.19380v1,http://arxiv.org/pdf/2508.19380v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
LSD-3D: Large-Scale 3D Driving Scene Generation with Geometry Grounding,"Large-scale scene data is essential for training and testing in robot
learning. Neural reconstruction methods have promised the capability of
reconstructing large physically-grounded outdoor scenes from captured sensor
data. However, these methods have baked-in static environments and only allow
for limited scene control -- they are functionally constrained in scene and
trajectory diversity by the captures from which they are reconstructed. In
contrast, generating driving data with recent image or video diffusion models
offers control, however, at the cost of geometry grounding and causality. In
this work, we aim to bridge this gap and present a method that directly
generates large-scale 3D driving scenes with accurate geometry, allowing for
causal novel view synthesis with object permanence and explicit 3D geometry
estimation. The proposed method combines the generation of a proxy geometry and
environment representation with score distillation from learned 2D image
priors. We find that this approach allows for high controllability, enabling
the prompt-guided geometry and high-fidelity texture and structure that can be
conditioned on map layouts -- producing realistic and geometrically consistent
3D generations of complex driving scenes.","Julian Ost, Andrea Ramazzina, Amogh Joshi, Maximilian Bömer, Mario Bijelic, Felix Heide",2025-08-26T17:04:49Z,2025-08-26T17:04:49Z,http://arxiv.org/abs/2508.19204v1,http://arxiv.org/pdf/2508.19204v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"ZeST: an LLM-based Zero-Shot Traversability Navigation for Unknown
  Environments","The advancement of robotics and autonomous navigation systems hinges on the
ability to accurately predict terrain traversability. Traditional methods for
generating datasets to train these prediction models often involve putting
robots into potentially hazardous environments, posing risks to equipment and
safety. To solve this problem, we present ZeST, a novel approach leveraging
visual reasoning capabilities of Large Language Models (LLMs) to create a
traversability map in real-time without exposing robots to danger. Our approach
not only performs zero-shot traversability and mitigates the risks associated
with real-world data collection but also accelerates the development of
advanced navigation systems, offering a cost-effective and scalable solution.
To support our findings, we present navigation results, in both controlled
indoor and unstructured outdoor environments. As shown in the experiments, our
method provides safer navigation when compared to other state-of-the-art
methods, constantly reaching the final goal.","Shreya Gummadi, Mateus V. Gasparino, Gianluca Capezzuto, Marcelo Becker, Girish Chowdhary",2025-08-26T15:30:19Z,2025-08-26T15:30:19Z,http://arxiv.org/abs/2508.19131v1,http://arxiv.org/pdf/2508.19131v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Finding Outliers in a Haystack: Anomaly Detection for Large Pointcloud
  Scenes","LiDAR scanning in outdoor scenes acquires accurate distance measurements over
wide areas, producing large-scale point clouds. Application examples for this
data include robotics, automotive vehicles, and land surveillance. During such
applications, outlier objects from outside the training data will inevitably
appear. Our research contributes a novel approach to open-set segmentation,
leveraging the learnings of object defect-detection research. We also draw on
the Mamba architecture's strong performance in utilising long-range
dependencies and scalability to large data. Combining both, we create a
reconstruction based approach for the task of outdoor scene open-set
segmentation. We show that our approach improves performance not only when
applied to our our own open-set segmentation method, but also when applied to
existing methods. Furthermore we contribute a Mamba based architecture which is
competitive with existing voxel-convolution based methods on challenging,
large-scale pointclouds.","Ryan Faulkner, Luke Haub, Simon Ratcliffe, Tat-Jun Chin",2025-08-25T03:47:33Z,2025-08-26T07:01:44Z,http://arxiv.org/abs/2508.17634v2,http://arxiv.org/pdf/2508.17634v2.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Lang2Lift: A Framework for Language-Guided Pallet Detection and Pose
  Estimation Integrated in Autonomous Outdoor Forklift Operation","The logistics and construction industries face persistent challenges in
automating pallet handling, especially in outdoor environments with variable
payloads, inconsistencies in pallet quality and dimensions, and unstructured
surroundings. In this paper, we tackle automation of a critical step in pallet
transport: the pallet pick-up operation. Our work is motivated by labor
shortages, safety concerns, and inefficiencies in manually locating and
retrieving pallets under such conditions. We present Lang2Lift, a framework
that leverages foundation models for natural language-guided pallet detection
and 6D pose estimation, enabling operators to specify targets through intuitive
commands such as ""pick up the steel beam pallet near the crane."" The perception
pipeline integrates Florence-2 and SAM-2 for language-grounded segmentation
with FoundationPose for robust pose estimation in cluttered, multi-pallet
outdoor scenes under variable lighting. The resulting poses feed into a motion
planning module for fully autonomous forklift operation. We validate Lang2Lift
on the ADAPT autonomous forklift platform, achieving 0.76 mIoU pallet
segmentation accuracy on a real-world test dataset. Timing and error analysis
demonstrate the system's robustness and confirm its feasibility for deployment
in operational logistics and construction environments. Video demonstrations
are available at https://eric-nguyen1402.github.io/lang2lift.github.io/","Huy Hoang Nguyen, Johannes Huemer, Markus Murschitz, Tobias Glueck, Minh Nhat Vu, Andreas Kugi",2025-08-21T10:28:39Z,2025-08-21T10:28:39Z,http://arxiv.org/abs/2508.15427v1,http://arxiv.org/pdf/2508.15427v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Virtual Community: An Open World for Humans, Robots, and Society","The rapid progress in AI and Robotics may lead to a profound societal
transformation, as humans and robots begin to coexist within shared
communities, introducing both opportunities and challenges. To explore this
future, we present Virtual Community-an open-world platform for humans, robots,
and society-built on a universal physics engine and grounded in real-world 3D
scenes. With Virtual Community, we aim to study embodied social intelligence at
scale: 1) How robots can intelligently cooperate or compete; 2) How humans
develop social relations and build community; 3) More importantly, how
intelligent robots and humans can co-exist in an open world. To support these,
Virtual Community features: 1) An open-source multi-agent physics simulator
that supports robots, humans, and their interactions within a society; 2) A
large-scale, real-world aligned community generation pipeline, including vast
outdoor space, diverse indoor scenes, and a community of grounded agents with
rich characters and appearances. Leveraging Virtual Community, we propose two
novel challenges. The Community Planning Challenge evaluates multi-agent
reasoning and planning ability in open-world settings, such as cooperating to
help agents with daily activities and efficiently connecting other agents. The
Community Robot Challenge requires multiple heterogeneous robots to collaborate
in solving complex open-world tasks. We evaluate various baselines on these
tasks and demonstrate the challenges in both high-level open-world task
planning and low-level cooperation controls. We hope that Virtual Community
will unlock further study of human-robot coexistence within open-world
environments.","Qinhong Zhou, Hongxin Zhang, Xiangye Lin, Zheyuan Zhang, Yutian Chen, Wenjun Liu, Zunzhe Zhang, Sunli Chen, Lixing Fang, Qiushi Lyu, Xinyu Sun, Jincheng Yang, Zeyuan Wang, Bao Chi Dang, Zhehuan Chen, Daksha Ladia, Jiageng Liu, Chuang Gan",2025-08-20T17:59:32Z,2025-08-20T17:59:32Z,http://arxiv.org/abs/2508.14893v1,http://arxiv.org/pdf/2508.14893v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"EAROL: Environmental Augmented Perception-Aware Planning and Robust
  Odometry via Downward-Mounted Tilted LiDAR","To address the challenges of localization drift and perception-planning
coupling in unmanned aerial vehicles (UAVs) operating in open-top scenarios
(e.g., collapsed buildings, roofless mazes), this paper proposes EAROL, a novel
framework with a downward-mounted tilted LiDAR configuration (20{\deg}
inclination), integrating a LiDAR-Inertial Odometry (LIO) system and a
hierarchical trajectory-yaw optimization algorithm. The hardware innovation
enables constraint enhancement via dense ground point cloud acquisition and
forward environmental awareness for dynamic obstacle detection. A
tightly-coupled LIO system, empowered by an Iterative Error-State Kalman Filter
(IESKF) with dynamic motion compensation, achieves high level 6-DoF
localization accuracy in feature-sparse environments. The planner, augmented by
environment, balancing environmental exploration, target tracking precision,
and energy efficiency. Physical experiments demonstrate 81% tracking error
reduction, 22% improvement in perceptual coverage, and near-zero vertical drift
across indoor maze and 60-meter-scale outdoor scenarios. This work proposes a
hardware-algorithm co-design paradigm, offering a robust solution for UAV
autonomy in post-disaster search and rescue missions. We will release our
software and hardware as an open-source package for the community. Video:
https://youtu.be/7av2ueLSiYw.","Xinkai Liang, Yigu Ge, Yangxi Shi, Haoyu Yang, Xu Cao, Hao Fang",2025-08-20T09:16:29Z,2025-08-20T09:16:29Z,http://arxiv.org/abs/2508.14554v1,http://arxiv.org/pdf/2508.14554v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"D$^2$-LIO: Enhanced Optimization for LiDAR-IMU Odometry Considering
  Directional Degeneracy","LiDAR-inertial odometry (LIO) plays a vital role in achieving accurate
localization and mapping, especially in complex environments. However, the
presence of LiDAR feature degeneracy poses a major challenge to reliable state
estimation. To overcome this issue, we propose an enhanced LIO framework that
integrates adaptive outlier-tolerant correspondence with a scan-to-submap
registration strategy. The core contribution lies in an adaptive outlier
removal threshold, which dynamically adjusts based on point-to-sensor distance
and the motion amplitude of platform. This mechanism improves the robustness of
feature matching in varying conditions. Moreover, we introduce a flexible
scan-to-submap registration method that leverages IMU data to refine pose
estimation, particularly in degenerate geometric configurations. To further
enhance localization accuracy, we design a novel weighting matrix that fuses
IMU preintegration covariance with a degeneration metric derived from the
scan-to-submap process. Extensive experiments conducted in both indoor and
outdoor environments-characterized by sparse or degenerate features-demonstrate
that our method consistently outperforms state-of-the-art approaches in terms
of both robustness and accuracy.","Guodong Yao, Hao Wang, Qing Chang",2025-08-20T01:51:53Z,2025-08-20T01:51:53Z,http://arxiv.org/abs/2508.14355v1,http://arxiv.org/pdf/2508.14355v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
ROVR-Open-Dataset: A Large-Scale Depth Dataset for Autonomous Driving,"Depth estimation is a fundamental task for 3D scene understanding in
autonomous driving, robotics, and augmented reality. Existing depth datasets,
such as KITTI, nuScenes, and DDAD, have advanced the field but suffer from
limitations in diversity and scalability. As benchmark performance on these
datasets approaches saturation, there is an increasing need for a new
generation of large-scale, diverse, and cost-efficient datasets to support the
era of foundation models and multi-modal learning. To address these challenges,
we introduce a large-scale, diverse, frame-wise continuous dataset for depth
estimation in dynamic outdoor driving environments, comprising 20K video frames
to evaluate existing methods. Our lightweight acquisition pipeline ensures
broad scene coverage at low cost, while sparse yet statistically sufficient
ground truth enables robust training. Compared to existing datasets, ours
presents greater diversity in driving scenarios and lower depth density,
creating new challenges for generalization. Benchmark experiments with standard
monocular depth estimation models validate the dataset's utility and highlight
substantial performance gaps in challenging conditions, establishing a new
platform for advancing depth estimation research.","Xianda Guo, Ruijun Zhang, Yiqun Duan, Ruilin Wang, Keyuan Zhou, Wenzhao Zheng, Wenke Huang, Gangwei Xu, Mike Horton, Yuan Si, Hao Zhao, Long Chen",2025-08-19T16:13:49Z,2025-08-19T16:13:49Z,http://arxiv.org/abs/2508.13977v1,http://arxiv.org/pdf/2508.13977v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"CAST: Counterfactual Labels Improve Instruction Following in
  Vision-Language-Action Models","Generalist robots should be able to understand and follow user instructions,
but current vision-language-action (VLA) models struggle with following
fine-grained commands despite providing a powerful architecture for mapping
open-vocabulary natural language instructions to robot actions. One cause for
this is a lack of semantic diversity and language grounding in existing robot
datasets and, specifically, a lack of fine-grained task diversity for similar
observations. To address this, we present a novel method to augment existing
robot datasets by leveraging vision language models to create counterfactual
labels. Our method improves the language-following capabilities of VLAs by
increasing the diversity and granularity of language grounding for robot
datasets by generating counterfactual language and actions. We evaluate the
resulting model's ability to follow language instructions, ranging from simple
object-centric commands to complex referential tasks, by conducting visual
language navigation experiments in 3 different indoor and outdoor environments.
Our experiments demonstrate that counterfactual relabeling, without any
additional data collection, significantly improves instruction-following in VLA
policies, making them competitive with state-of-the-art methods and increasing
success rate by 27% on navigation tasks.","Catherine Glossop, William Chen, Arjun Bhorkar, Dhruv Shah, Sergey Levine",2025-08-19T02:01:06Z,2025-08-19T02:01:06Z,http://arxiv.org/abs/2508.13446v1,http://arxiv.org/pdf/2508.13446v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Towards Fully Onboard State Estimation and Trajectory Tracking for UAVs
  with Suspended Payloads","This paper addresses the problem of tracking the position of a
cable-suspended payload carried by an unmanned aerial vehicle, with a focus on
real-world deployment and minimal hardware requirements. In contrast to many
existing approaches that rely on motion-capture systems, additional onboard
cameras, or instrumented payloads, we propose a framework that uses only
standard onboard sensors--specifically, real-time kinematic global navigation
satellite system measurements and data from the onboard inertial measurement
unit--to estimate and control the payload's position. The system models the
full coupled dynamics of the aerial vehicle and payload, and integrates a
linear Kalman filter for state estimation, a model predictive contouring
control planner, and an incremental model predictive controller. The control
architecture is designed to remain effective despite sensing limitations and
estimation uncertainty. Extensive simulations demonstrate that the proposed
system achieves performance comparable to control based on ground-truth
measurements, with only minor degradation (< 6%). The system also shows strong
robustness to variations in payload parameters. Field experiments further
validate the framework, confirming its practical applicability and reliable
performance in outdoor environments using only off-the-shelf aerial vehicle
hardware.","Martin Jiroušek, Tomáš Báča, Martin Saska",2025-08-15T15:48:42Z,2025-08-15T15:48:42Z,http://arxiv.org/abs/2508.11547v1,http://arxiv.org/pdf/2508.11547v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"i2Nav-Robot: A Large-Scale Indoor-Outdoor Robot Dataset for Multi-Sensor
  Fusion Navigation and Mapping","Accurate and reliable navigation is crucial for autonomous unmanned ground
vehicle (UGV). However, current UGV datasets fall short in meeting the demands
for advancing navigation and mapping techniques due to limitations in sensor
configuration, time synchronization, ground truth, and scenario diversity. To
address these challenges, we present i2Nav-Robot, a large-scale dataset
designed for multi-sensor fusion navigation and mapping in indoor-outdoor
environments. We integrate multi-modal sensors, including the newest front-view
and 360-degree solid-state LiDARs, 4-dimensional (4D) radar, stereo cameras,
odometer, global navigation satellite system (GNSS) receiver, and inertial
measurement units (IMU) on an omnidirectional wheeled robot. Accurate
timestamps are obtained through both online hardware synchronization and
offline calibration for all sensors. The dataset includes ten larger-scale
sequences covering diverse UGV operating scenarios, such as outdoor streets,
and indoor parking lots, with a total length of about 17060 meters.
High-frequency ground truth, with centimeter-level accuracy for position, is
derived from post-processing integrated navigation methods using a
navigation-grade IMU. The proposed i2Nav-Robot dataset is evaluated by more
than ten open-sourced multi-sensor fusion systems, and it has proven to have
superior data quality.","Hailiang Tang, Tisheng Zhang, Liqiang Wang, Xin Ding, Man Yuan, Zhiyu Xiang, Jujin Chen, Yuhan Bian, Shuangyan Liu, Yuqing Wang, Guan Wang, Xiaoji Niu",2025-08-15T13:59:08Z,2025-08-27T12:00:54Z,http://arxiv.org/abs/2508.11485v2,http://arxiv.org/pdf/2508.11485v2.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action
  Navigation Model","Existing vision-and-language navigation models often deviate from the correct
trajectory when executing instructions. However, these models lack effective
error correction capability, hindering their recovery from errors. To address
this challenge, we propose Self-correction Flywheel, a novel post-training
paradigm. Instead of considering the model's error trajectories on the training
set as a drawback, our paradigm emphasizes their significance as a valuable
data source. We have developed a method to identify deviations in these error
trajectories and devised innovative techniques to automatically generate
self-correction data for perception and action. These self-correction data
serve as fuel to power the model's continued training. The brilliance of our
paradigm is revealed when we re-evaluate the model on the training set,
uncovering new error trajectories. At this time, the self-correction flywheel
begins to spin. Through multiple flywheel iterations, we progressively enhance
our monocular RGB-based VLA navigation model CorrectNav. Experiments on R2R-CE
and RxR-CE benchmarks show CorrectNav achieves new state-of-the-art success
rates of 65.1% and 69.3%, surpassing prior best VLA navigation models by 8.2%
and 16.4%. Real robot tests in various indoor and outdoor environments
demonstrate \method's superior capability of error correction, dynamic obstacle
avoidance, and long instruction following.","Zhuoyuan Yu, Yuxing Long, Zihan Yang, Chengyan Zeng, Hongwei Fan, Jiyao Zhang, Hao Dong",2025-08-14T07:39:26Z,2025-08-14T07:39:26Z,http://arxiv.org/abs/2508.10416v1,http://arxiv.org/pdf/2508.10416v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for
  Long-Horizon Tasks","Language-guided long-horizon mobile manipulation has long been a grand
challenge in embodied semantic reasoning, generalizable manipulation, and
adaptive locomotion. Three fundamental limitations hinder progress: First,
although large language models have improved spatial reasoning and task
planning through semantic priors, existing implementations remain confined to
tabletop scenarios, failing to address the constrained perception and limited
actuation ranges of mobile platforms. Second, current manipulation strategies
exhibit insufficient generalization when confronted with the diverse object
configurations encountered in open-world environments. Third, while crucial for
practical deployment, the dual requirement of maintaining high platform
maneuverability alongside precise end-effector control in unstructured settings
remains understudied.
  In this work, we present ODYSSEY, a unified mobile manipulation framework for
agile quadruped robots equipped with manipulators, which seamlessly integrates
high-level task planning with low-level whole-body control. To address the
challenge of egocentric perception in language-conditioned tasks, we introduce
a hierarchical planner powered by a vision-language model, enabling
long-horizon instruction decomposition and precise action execution. At the
control level, our novel whole-body policy achieves robust coordination across
challenging terrains. We further present the first benchmark for long-horizon
mobile manipulation, evaluating diverse indoor and outdoor scenarios. Through
successful sim-to-real transfer, we demonstrate the system's generalization and
robustness in real-world deployments, underscoring the practicality of legged
manipulators in unstructured environments. Our work advances the feasibility of
generalized robotic assistants capable of complex, dynamic tasks. Our project
page: https://kaijwang.github.io/odyssey.github.io/","Kaijun Wang, Liqin Lu, Mingyu Liu, Jianuo Jiang, Zeju Li, Bolin Zhang, Wancai Zheng, Xinyi Yu, Hao Chen, Chunhua Shen",2025-08-11T17:54:31Z,2025-08-11T17:54:31Z,http://arxiv.org/abs/2508.08240v1,http://arxiv.org/pdf/2508.08240v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Multi-view Normal and Distance Guidance Gaussian Splatting for Surface
  Reconstruction","3D Gaussian Splatting (3DGS) achieves remarkable results in the field of
surface reconstruction. However, when Gaussian normal vectors are aligned
within the single-view projection plane, while the geometry appears reasonable
in the current view, biases may emerge upon switching to nearby views. To
address the distance and global matching challenges in multi-view scenes, we
design multi-view normal and distance-guided Gaussian splatting. This method
achieves geometric depth unification and high-accuracy reconstruction by
constraining nearby depth maps and aligning 3D normals. Specifically, for the
reconstruction of small indoor and outdoor scenes, we propose a multi-view
distance reprojection regularization module that achieves multi-view Gaussian
alignment by computing the distance loss between two nearby views and the same
Gaussian surface. Additionally, we develop a multi-view normal enhancement
module, which ensures consistency across views by matching the normals of pixel
points in nearby views and calculating the loss. Extensive experimental results
demonstrate that our method outperforms the baseline in both quantitative and
qualitative evaluations, significantly enhancing the surface reconstruction
capability of 3DGS. Our code will be made publicly available at
(https://github.com/Bistu3DV/MND-GS/).","Bo Jia, Yanan Guo, Ying Chang, Benkui Zhang, Ying Xie, Kangning Du, Lin Cao",2025-08-11T07:25:13Z,2025-08-13T15:51:51Z,http://arxiv.org/abs/2508.07701v2,http://arxiv.org/pdf/2508.07701v2.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"ReNiL: Relative Neural Inertial Locator with Any-Scale Bayesian
  Inference","Pedestrian inertial localization is key for mobile and IoT services because
it provides infrastructure-free positioning. Yet most learning-based methods
depend on fixed sliding-window integration, struggle to adapt to diverse motion
scales and cadences, and yield inconsistent uncertainty, limiting real-world
use. We present ReNiL, a Bayesian deep-learning framework for accurate,
efficient, and uncertainty-aware pedestrian localization. ReNiL introduces
Inertial Positioning Demand Points (IPDPs) to estimate motion at contextually
meaningful waypoints instead of dense tracking, and supports inference on IMU
sequences at any scale so cadence can match application needs. It couples a
motion-aware orientation filter with an Any-Scale Laplace Estimator (ASLE), a
dual-task network that blends patch-based self-supervision with Bayesian
regression. By modeling displacements with a Laplace distribution, ReNiL
provides homogeneous Euclidean uncertainty that integrates cleanly with other
sensors. A Bayesian inference chain links successive IPDPs into consistent
trajectories. On RoNIN-ds and a new WUDataset covering indoor and outdoor
motion from 28 participants, ReNiL achieves state-of-the-art displacement
accuracy and uncertainty consistency, outperforming TLIO, CTIN, iMoT, and RoNIN
variants while reducing computation. Application studies further show
robustness and practicality for mobile and IoT localization, making ReNiL a
scalable, uncertainty-aware foundation for next-generation positioning.","Kaixuan Wu, Yuanzhuo Xu, Zejun Zhang, Weiping Zhu, Steve Drew, Xiaoguang Niu",2025-08-08T06:21:23Z,2025-08-12T11:18:57Z,http://arxiv.org/abs/2508.06053v2,http://arxiv.org/pdf/2508.06053v2.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"mmWave Radar-Based Non-Line-of-Sight Pedestrian Localization at
  T-Junctions Utilizing Road Layout Extraction via Camera","Pedestrians Localization in Non-Line-of-Sight (NLoS) regions within urban
environments poses a significant challenge for autonomous driving systems.
While mmWave radar has demonstrated potential for detecting objects in such
scenarios, the 2D radar point cloud (PCD) data is susceptible to distortions
caused by multipath reflections, making accurate spatial inference difficult.
Additionally, although camera images provide high-resolution visual
information, they lack depth perception and cannot directly observe objects in
NLoS regions. In this paper, we propose a novel framework that interprets radar
PCD through road layout inferred from camera for localization of NLoS
pedestrians. The proposed method leverages visual information from the camera
to interpret 2D radar PCD, enabling spatial scene reconstruction. The
effectiveness of the proposed approach is validated through experiments
conducted using a radar-camera system mounted on a real vehicle. The
localization performance is evaluated using a dataset collected in outdoor NLoS
driving environments, demonstrating the practical applicability of the method.","Byeonggyu Park, Hee-Yeun Kim, Byonghyok Choi, Hansang Cho, Byungkwan Kim, Soomok Lee, Mingu Jeon, Seong-Woo Kim",2025-08-04T12:31:11Z,2025-08-04T12:31:11Z,http://arxiv.org/abs/2508.02348v1,http://arxiv.org/pdf/2508.02348v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding,"Visual grounding aims to identify objects or regions in a scene based on
natural language descriptions, essential for spatially aware perception in
autonomous driving. However, existing visual grounding tasks typically depend
on bounding boxes that often fail to capture fine-grained details. Not all
voxels within a bounding box are occupied, resulting in inaccurate object
representations. To address this, we introduce a benchmark for 3D occupancy
grounding in challenging outdoor scenes. Built on the nuScenes dataset, it
integrates natural language with voxel-level occupancy annotations, offering
more precise object perception compared to the traditional grounding task.
Moreover, we propose GroundingOcc, an end-to-end model designed for 3D
occupancy grounding through multi-modal learning. It combines visual, textual,
and point cloud features to predict object location and occupancy information
from coarse to fine. Specifically, GroundingOcc comprises a multimodal encoder
for feature extraction, an occupancy head for voxel-wise predictions, and a
grounding head to refine localization. Additionally, a 2D grounding module and
a depth estimation module enhance geometric understanding, thereby boosting
model performance. Extensive experiments on the benchmark demonstrate that our
method outperforms existing baselines on 3D occupancy grounding. The dataset is
available at https://github.com/RONINGOD/GroundingOcc.","Zhan Shi, Song Wang, Junbo Chen, Jianke Zhu",2025-08-02T05:05:50Z,2025-09-03T12:05:59Z,http://arxiv.org/abs/2508.01197v2,http://arxiv.org/pdf/2508.01197v2.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes,"3D Gaussian Splatting (3DGS) has recently gained popularity in SLAM
applications due to its fast rendering and high-fidelity representation.
However, existing 3DGS-SLAM systems have predominantly focused on indoor
environments and relied on active depth sensors, leaving a gap for large-scale
outdoor applications. We present BGS-SLAM, the first binocular 3D Gaussian
Splatting SLAM system designed for outdoor scenarios. Our approach uses only
RGB stereo pairs without requiring LiDAR or active sensors. BGS-SLAM leverages
depth estimates from pre-trained deep stereo networks to guide 3D Gaussian
optimization with a multi-loss strategy enhancing both geometric consistency
and visual quality. Experiments on multiple datasets demonstrate that BGS-SLAM
achieves superior tracking accuracy and mapping performance compared to other
3DGS-based solutions in complex outdoor environments.","Xiaohan Li, Ziren Gong, Fabio Tosi, Matteo Poggi, Stefano Mattoccia, Dong Liu, Jun Wu",2025-07-31T15:54:51Z,2025-07-31T15:54:51Z,http://arxiv.org/abs/2507.23677v1,http://arxiv.org/pdf/2507.23677v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic
  Expansive Scenarios","LiDAR-based localization serves as a critical component in autonomous
systems, yet existing approaches face persistent challenges in balancing
repeatability, accuracy, and environmental adaptability. Traditional point
cloud registration methods relying solely on offline maps often exhibit limited
robustness against long-term environmental changes, leading to localization
drift and reliability degradation in dynamic real-world scenarios. To address
these challenges, this paper proposes DuLoc, a robust and accurate localization
method that tightly couples LiDAR-inertial odometry with offline map-based
localization, incorporating a constant-velocity motion model to mitigate
outlier noise in real-world scenarios. Specifically, we develop a LiDAR-based
localization framework that seamlessly integrates a prior global map with
dynamic real-time local maps, enabling robust localization in unbounded and
changing environments. Extensive real-world experiments in ultra unbounded port
that involve 2,856 hours of operational data across 32 Intelligent Guided
Vehicles (IGVs) are conducted and reported in this study. The results attained
demonstrate that our system outperforms other state-of-the-art LiDAR
localization systems in large-scale changing outdoor environments.","Haoxuan Jiang, Peicong Qian, Yusen Xie, Xiaocong Li, Ming Liu, Jun Ma",2025-07-31T15:38:50Z,2025-07-31T15:38:50Z,http://arxiv.org/abs/2507.23660v1,http://arxiv.org/pdf/2507.23660v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Vision-Language Fusion for Real-Time Autonomous Driving: Goal-Centered
  Cross-Attention of Camera, HD-Map, & Waypoints","Autonomous cars need geometric accuracy and semantic understanding to
navigate complex environments, yet most stacks handle them separately. We
present XYZ-Drive, a single vision-language model that reads a front-camera
frame, a 25m $\times$ 25m overhead map, and the next waypoint, then outputs
steering and speed. A lightweight goal-centered cross-attention layer lets
waypoint tokens highlight relevant image and map patches, supporting both
action and textual explanations, before the fused tokens enter a partially
fine-tuned LLaMA-3.2 11B model.
  On the MD-NEX Outdoor-Driving benchmark XYZ-Drive attains 95% success and
0.80 Success weighted by Path Length (SPL), surpassing PhysNav-DG by 15%. and
halving collisions, all while significantly improving efficiency by using only
a single branch. Sixteen ablations explain the gains. Removing any modality
(vision, waypoint, map) drops success by up to 11%, confirming their
complementary roles and rich connections. Replacing goal-centered attention
with simple concatenation cuts 3% in performance, showing query-based fusion
injects map knowledge more effectively. Keeping the transformer frozen loses
5%, showing the importance of fine-tuning when applying VLMs for specific tasks
such as autonomous driving. Coarsening map resolution from 10 cm to 40 cm blurs
lane edges and raises crash rate.
  Overall, these results demonstrate that early, token-level fusion of intent
and map layout enables accurate, transparent, real-time driving.","Santosh Patapati, Trisanth Srinivasan, Murari Ambati",2025-07-30T19:51:23Z,2025-08-05T02:56:37Z,http://arxiv.org/abs/2507.23064v2,http://arxiv.org/pdf/2507.23064v2.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Early Goal-Guided Multi-Scale Fusion for Real-Time Vision-Language
  Driving","Autonomous vehicles must react in milliseconds while reasoning about road
geometry and traffic intent to navigate complex situations. We introduce
NovaDrive, a single-branch vision-language architecture that processes
front-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a
single branch. A lightweight, two-stage cross-attention block first aligns
waypoint tokens with the HD map, then refines attention over fine-grained image
and depth patches. Coupled with a novel smoothness loss that discourages abrupt
steering and speed changes, this design eliminates the need for recurrent
memory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language
backbone, enabling real-time inference. On the nuScenes / Waymo subset of the
MD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts
path-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from
2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations
confirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention
fusion each contribute the most to these gains. Beyond safety, NovaDrive's
shorter routes (resulting from the novel smoothness loss) translate to lower
fuel or battery usage, pointing toward leaner, more easily updated driving
stacks. NovaDrive can be extended to other embodied-AI domains as well.","Santosh Patapati, Trisanth Srinivasan",2025-07-30T19:12:42Z,2025-07-30T19:12:42Z,http://arxiv.org/abs/2507.23042v1,http://arxiv.org/pdf/2507.23042v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Sound Source Localization for Human-Robot Interaction in Outdoor
  Environments","This paper presents a sound source localization strategy that relies on a
microphone array embedded in an unmanned ground vehicle and an asynchronous
close-talking microphone near the operator. A signal coarse alignment strategy
is combined with a time-domain acoustic echo cancellation algorithm to estimate
a time-frequency ideal ratio mask to isolate the target speech from
interferences and environmental noise. This allows selective sound source
localization, and provides the robot with the direction of arrival of sound
from the active operator, which enables rich interaction in noisy scenarios.
Results demonstrate an average angle error of 4 degrees and an accuracy within
5 degrees of 95\% at a signal-to-noise ratio of 1dB, which is significantly
superior to the state-of-the-art localization methods.","Victor Liu, Timothy Du, Jordy Sehn, Jack Collier, François Grondin",2025-07-29T01:53:27Z,2025-07-29T01:53:27Z,http://arxiv.org/abs/2507.21431v1,http://arxiv.org/pdf/2507.21431v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
OpenNav: Open-World Navigation with Multimodal Large Language Models,"Pre-trained large language models (LLMs) have demonstrated strong
common-sense reasoning abilities, making them promising for robotic navigation
and planning tasks. However, despite recent progress, bridging the gap between
language descriptions and actual robot actions in the open-world, beyond merely
invoking limited predefined motion primitives, remains an open challenge. In
this work, we aim to enable robots to interpret and decompose complex language
instructions, ultimately synthesizing a sequence of trajectory points to
complete diverse navigation tasks given open-set instructions and open-set
objects. We observe that multi-modal large language models (MLLMs) exhibit
strong cross-modal understanding when processing free-form language
instructions, demonstrating robust scene comprehension. More importantly,
leveraging their code-generation capability, MLLMs can interact with
vision-language perception models to generate compositional 2D bird-eye-view
value maps, effectively integrating semantic knowledge from MLLMs with spatial
information from maps to reinforce the robot's spatial understanding. To
further validate our approach, we effectively leverage large-scale autonomous
vehicle datasets (AVDs) to validate our proposed zero-shot vision-language
navigation framework in outdoor navigation tasks, demonstrating its capability
to execute a diverse range of free-form natural language navigation
instructions while maintaining robustness against object detection errors and
linguistic ambiguities. Furthermore, we validate our system on a Husky robot in
both indoor and outdoor scenes, demonstrating its real-world robustness and
applicability. Supplementary videos are available at
https://trailab.github.io/OpenNav-website/","Mingfeng Yuan, Letian Wang, Steven L. Waslander",2025-07-24T02:05:28Z,2025-07-24T02:05:28Z,http://arxiv.org/abs/2507.18033v1,http://arxiv.org/pdf/2507.18033v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Monocular Semantic Scene Completion via Masked Recurrent Networks,"Monocular Semantic Scene Completion (MSSC) aims to predict the voxel-wise
occupancy and semantic category from a single-view RGB image. Existing methods
adopt a single-stage framework that aims to simultaneously achieve visible
region segmentation and occluded region hallucination, while also being
affected by inaccurate depth estimation. Such methods often achieve suboptimal
performance, especially in complex scenes. We propose a novel two-stage
framework that decomposes MSSC into coarse MSSC followed by the Masked
Recurrent Network. Specifically, we propose the Masked Sparse Gated Recurrent
Unit (MS-GRU) which concentrates on the occupied regions by the proposed mask
updating mechanism, and a sparse GRU design is proposed to reduce the
computation cost. Additionally, we propose the distance attention projection to
reduce projection errors by assigning different attention scores according to
the distance to the observed surface. Experimental results demonstrate that our
proposed unified framework, MonoMRN, effectively supports both indoor and
outdoor scenes and achieves state-of-the-art performance on the NYUv2 and
SemanticKITTI datasets. Furthermore, we conduct robustness analysis under
various disturbances, highlighting the role of the Masked Recurrent Network in
enhancing the model's resilience to such challenges. The source code is
publicly available.","Xuzhi Wang, Xinran Wu, Song Wang, Lingdong Kong, Ziping Zhao",2025-07-23T16:29:45Z,2025-07-23T16:29:45Z,http://arxiv.org/abs/2507.17661v1,http://arxiv.org/pdf/2507.17661v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"When and Where Localization Fails: An Analysis of the Iterative Closest
  Point in Evolving Environment","Robust relocalization in dynamic outdoor environments remains a key challenge
for autonomous systems relying on 3D lidar. While long-term localization has
been widely studied, short-term environmental changes, occurring over days or
weeks, remain underexplored despite their practical significance. To address
this gap, we present a highresolution, short-term multi-temporal dataset
collected weekly from February to April 2025 across natural and semi-urban
settings. Each session includes high-density point cloud maps, 360 deg
panoramic images, and trajectory data. Projected lidar scans, derived from the
point cloud maps and modeled with sensor-accurate occlusions, are used to
evaluate alignment accuracy against the ground truth using two Iterative
Closest Point (ICP) variants: Point-to-Point and Point-to-Plane. Results show
that Point-to-Plane offers significantly more stable and accurate registration,
particularly in areas with sparse features or dense vegetation. This study
provides a structured dataset for evaluating short-term localization
robustness, a reproducible framework for analyzing scan-to-map alignment under
noise, and a comparative evaluation of ICP performance in evolving outdoor
environments. Our analysis underscores how local geometry and environmental
variability affect localization success, offering insights for designing more
resilient robotic systems.","Abdel-Raouf Dannaoui, Johann Laconte, Christophe Debain, Francois Pomerleau, Paul Checchin",2025-07-23T14:10:48Z,2025-07-23T14:10:48Z,http://arxiv.org/abs/2507.17531v1,http://arxiv.org/pdf/2507.17531v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"MARSCalib: Multi-robot, Automatic, Robust, Spherical Target-based
  Extrinsic Calibration in Field and Extraterrestrial Environments","This paper presents a novel spherical target-based LiDAR-camera extrinsic
calibration method designed for outdoor environments with multi-robot systems,
considering both target and sensor corruption. The method extracts the 2D
ellipse center from the image and the 3D sphere center from the pointcloud,
which are then paired to compute the transformation matrix. Specifically, the
image is first decomposed using the Segment Anything Model (SAM). Then, a novel
algorithm extracts an ellipse from a potentially corrupted sphere, and the
extracted center of ellipse is corrected for errors caused by the perspective
projection model. For the LiDAR pointcloud, points on the sphere tend to be
highly noisy due to the absence of flat regions. To accurately extract the
sphere from these noisy measurements, we apply a hierarchical weighted sum to
the accumulated pointcloud. Through experiments, we demonstrated that the
sphere can be robustly detected even under both types of corruption,
outperforming other targets. We evaluated our method using three different
types of LiDARs (spinning, solid-state, and non-repetitive) with cameras
positioned in three different locations. Furthermore, we validated the
robustness of our method to target corruption by experimenting with spheres
subjected to various types of degradation. These experiments were conducted in
both a planetary test and a field environment. Our code is available at
https://github.com/sparolab/MARSCalib.","Seokhwan Jeong, Hogyun Kim, Younggun Cho",2025-07-23T02:11:12Z,2025-07-23T02:11:12Z,http://arxiv.org/abs/2507.17130v1,http://arxiv.org/pdf/2507.17130v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"DINO-VO: A Feature-based Visual Odometry Leveraging a Visual Foundation
  Model","Learning-based monocular visual odometry (VO) poses robustness,
generalization, and efficiency challenges in robotics. Recent advances in
visual foundation models, such as DINOv2, have improved robustness and
generalization in various vision tasks, yet their integration in VO remains
limited due to coarse feature granularity. In this paper, we present DINO-VO, a
feature-based VO system leveraging DINOv2 visual foundation model for its
sparse feature matching. To address the integration challenge, we propose a
salient keypoints detector tailored to DINOv2's coarse features. Furthermore,
we complement DINOv2's robust-semantic features with fine-grained geometric
features, resulting in more localizable representations. Finally, a
transformer-based matcher and differentiable pose estimation layer enable
precise camera motion estimation by learning good matches. Against prior
detector-descriptor networks like SuperPoint, DINO-VO demonstrates greater
robustness in challenging environments. Furthermore, we show superior accuracy
and generalization of the proposed feature descriptors against standalone
DINOv2 coarse features. DINO-VO outperforms prior frame-to-frame VO methods on
the TartanAir and KITTI datasets and is competitive on EuRoC dataset, while
running efficiently at 72 FPS with less than 1GB of memory usage on a single
GPU. Moreover, it performs competitively against Visual SLAM systems on outdoor
driving scenarios, showcasing its generalization capabilities.","Maulana Bisyir Azhari, David Hyunchul Shim",2025-07-17T14:09:34Z,2025-07-17T14:09:34Z,http://arxiv.org/abs/2507.13145v1,http://arxiv.org/pdf/2507.13145v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"CoNav Chair: Development and Evaluation of a Shared Control based
  Wheelchair for the Built Environment","As the global population of people with disabilities (PWD) continues to grow,
so will the need for mobility solutions that promote independent living and
social integration. Wheelchairs are vital for the mobility of PWD in both
indoor and outdoor environments. The current SOTA in powered wheelchairs is
based on either manually controlled or fully autonomous modes of operation,
offering limited flexibility and often proving difficult to navigate in
spatially constrained environments. Moreover, research on robotic wheelchairs
has focused predominantly on complete autonomy or improved manual control;
approaches that can compromise efficiency and user trust. To overcome these
challenges, this paper introduces the CoNav Chair, a smart wheelchair based on
the Robot Operating System (ROS) and featuring shared control navigation and
obstacle avoidance capabilities that are intended to enhance navigational
efficiency, safety, and ease of use for the user. The paper outlines the CoNav
Chair's design and presents a preliminary usability evaluation comparing three
distinct navigation modes, namely, manual, shared, and fully autonomous,
conducted with 21 healthy, unimpaired participants traversing an indoor
building environment. Study findings indicated that the shared control
navigation framework had significantly fewer collisions and performed
comparably, if not superior to the autonomous and manual modes, on task
completion time, trajectory length, and smoothness; and was perceived as being
safer and more efficient based on user reported subjective assessments of
usability. Overall, the CoNav system demonstrated acceptable safety and
performance, laying the foundation for subsequent usability testing with end
users, namely, PWDs who rely on a powered wheelchair for mobility.","Yifan Xu, Qianwei Wang, Jordan Lillie, Vineet Kamat, Carol Menassa, Clive D'Souza",2025-07-15T20:34:42Z,2025-07-15T20:34:42Z,http://arxiv.org/abs/2507.11716v1,http://arxiv.org/pdf/2507.11716v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Uniting the World by Dividing it: Federated Maps to Enable Spatial
  Applications","The emergence of the Spatial Web -- the Web where content is tied to
real-world locations has the potential to improve and enable many applications
such as augmented reality, navigation, robotics, and more. The Spatial Web is
missing a key ingredient that is impeding its growth -- a spatial naming system
to resolve real-world locations to names. Today's spatial naming systems are
digital maps such as Google and Apple maps. These maps and the location-based
services provided on top of these maps are primarily controlled by a few large
corporations and mostly cover outdoor public spaces. Emerging classes of
applications, such as persistent world-scale augmented reality, require
detailed maps of both outdoor and indoor spaces. Existing centralized mapping
infrastructures are proving insufficient for such applications because of the
scale of cartography efforts required and the privacy of indoor map data.
  In this paper, we present a case for a federated spatial naming system, or in
other words, a federated mapping infrastructure. This enables disparate parties
to manage and serve their own maps of physical regions and unlocks scalability
of map management, isolation and privacy of maps. Map-related services such as
address-to-location mapping, location-based search, and routing needs
re-architecting to work on federated maps. We discuss some essential services
and practicalities of enabling these services.","Sagar Bharadwaj, Srinivasan Seshan, Anthony Rowe",2025-07-15T16:00:37Z,2025-07-15T16:00:37Z,http://arxiv.org/abs/2507.11437v1,http://arxiv.org/pdf/2507.11437v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Visual Homing in Outdoor Robots Using Mushroom Body Circuits and
  Learning Walks","Ants achieve robust visual homing with minimal sensory input and only a few
learning walks, inspiring biomimetic solutions for autonomous navigation. While
Mushroom Body (MB) models have been used in robotic route following, they have
not yet been applied to visual homing. We present the first real-world
implementation of a lateralized MB architecture for visual homing onboard a
compact autonomous car-like robot. We test whether the sign of the angular path
integration (PI) signal can categorize panoramic views, acquired during
learning walks and encoded in the MB, into ""goal on the left"" and ""goal on the
right"" memory banks, enabling robust homing in natural outdoor settings. We
validate this approach through four incremental experiments: (1) simulation
showing attractor-like nest dynamics; (2) real-world homing after decoupled
learning walks, producing nest search behavior; (3) homing after random walks
using noisy PI emulated with GPS-RTK; and (4) precise stopping-at-the-goal
behavior enabled by a fifth MB Output Neuron (MBON) encoding goal-views to
control velocity. This mimics the accurate homing behavior of ants and
functionally resembles waypoint-based position control in robotics, despite
relying solely on visual input. Operating at 8 Hz on a Raspberry Pi 4 with
32x32 pixel views and a memory footprint under 9 kB, our system offers a
biologically grounded, resource-efficient solution for autonomous visual
homing.","Gabriel G. Gattaux, Julien R. Serres, Franck Ruffier, Antoine Wystrach",2025-07-13T17:54:01Z,2025-07-13T17:54:01Z,http://arxiv.org/abs/2507.09725v1,http://arxiv.org/pdf/2507.09725v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
OTAS: Open-vocabulary Token Alignment for Outdoor Segmentation,"Understanding open-world semantics is critical for robotic planning and
control, particularly in unstructured outdoor environments. Current
vision-language mapping approaches rely on object-centric segmentation priors,
which often fail outdoors due to semantic ambiguities and indistinct semantic
class boundaries. We propose OTAS - an Open-vocabulary Token Alignment method
for Outdoor Segmentation. OTAS overcomes the limitations of open-vocabulary
segmentation models by extracting semantic structure directly from the output
tokens of pretrained vision models. By clustering semantically similar
structures across single and multiple views and grounding them in language,
OTAS reconstructs a geometrically consistent feature field that supports
open-vocabulary segmentation queries. Our method operates zero-shot, without
scene-specific fine-tuning, and runs at up to ~17 fps. OTAS provides a minor
IoU improvement over fine-tuned and open-vocabulary 2D segmentation methods on
the Off-Road Freespace Detection dataset. Our model achieves up to a 151% IoU
improvement over open-vocabulary mapping methods in 3D segmentation on
TartanAir. Real-world reconstructions demonstrate OTAS' applicability to
robotic applications. The code and ROS node will be made publicly available
upon paper acceptance.","Simon Schwaiger, Stefan Thalhammer, Wilfried Wöber, Gerald Steinbauer-Wagner",2025-07-08T22:49:03Z,2025-07-08T22:49:03Z,http://arxiv.org/abs/2507.08851v1,http://arxiv.org/pdf/2507.08851v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"3DGS_LSR:Large_Scale Relocation for Autonomous Driving Based on 3D
  Gaussian Splatting","In autonomous robotic systems, precise localization is a prerequisite for
safe navigation. However, in complex urban environments, GNSS positioning often
suffers from signal occlusion and multipath effects, leading to unreliable
absolute positioning. Traditional mapping approaches are constrained by storage
requirements and computational inefficiency, limiting their applicability to
resource-constrained robotic platforms. To address these challenges, we propose
3DGS-LSR: a large-scale relocalization framework leveraging 3D Gaussian
Splatting (3DGS), enabling centimeter-level positioning using only a single
monocular RGB image on the client side. We combine multi-sensor data to
construct high-accuracy 3DGS maps in large outdoor scenes, while the robot-side
localization requires just a standard camera input. Using SuperPoint and
SuperGlue for feature extraction and matching, our core innovation is an
iterative optimization strategy that refines localization results through
step-by-step rendering, making it suitable for real-time autonomous navigation.
Experimental validation on the KITTI dataset demonstrates our 3DGS-LSR achieves
average positioning accuracies of 0.026m, 0.029m, and 0.081m in town roads,
boulevard roads, and traffic-dense highways respectively, significantly
outperforming other representative methods while requiring only monocular RGB
input. This approach provides autonomous robots with reliable localization
capabilities even in challenging urban environments where GNSS fails.","Haitao Lu, Haijier Chen, Haoze Liu, Shoujian Zhang, Bo Xu, Ziao Liu",2025-07-08T04:43:46Z,2025-07-08T04:43:46Z,http://arxiv.org/abs/2507.05661v1,http://arxiv.org/pdf/2507.05661v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"MOSU: Autonomous Long-range Robot Navigation with Multi-modal Scene
  Understanding","We present MOSU, a novel autonomous long-range navigation system that
enhances global navigation for mobile robots through multimodal perception and
on-road scene understanding. MOSU addresses the outdoor robot navigation
challenge by integrating geometric, semantic, and contextual information to
ensure comprehensive scene understanding. The system combines GPS and QGIS
map-based routing for high-level global path planning and multi-modal
trajectory generation for local navigation refinement. For trajectory
generation, MOSU leverages multi-modalities: LiDAR-based geometric data for
precise obstacle avoidance, image-based semantic segmentation for
traversability assessment, and Vision-Language Models (VLMs) to capture social
context and enable the robot to adhere to social norms in complex environments.
This multi-modal integration improves scene understanding and enhances
traversability, allowing the robot to adapt to diverse outdoor conditions. We
evaluate our system in real-world on-road environments and benchmark it on the
GND dataset, achieving a 10% improvement in traversability on navigable
terrains while maintaining a comparable navigation distance to existing global
navigation methods.","Jing Liang, Kasun Weerakoon, Daeun Song, Senthurbavan Kirubaharan, Xuesu Xiao, Dinesh Manocha",2025-07-07T06:08:21Z,2025-07-07T06:08:21Z,http://arxiv.org/abs/2507.04686v1,http://arxiv.org/pdf/2507.04686v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"MVL-Loc: Leveraging Vision-Language Model for Generalizable Multi-Scene
  Camera Relocalization","Camera relocalization, a cornerstone capability of modern computer vision,
accurately determines a camera's position and orientation (6-DoF) from images
and is essential for applications in augmented reality (AR), mixed reality
(MR), autonomous driving, delivery drones, and robotic navigation. Unlike
traditional deep learning-based methods that regress camera pose from images in
a single scene, which often lack generalization and robustness in diverse
environments, we propose MVL-Loc, a novel end-to-end multi-scene 6-DoF camera
relocalization framework. MVL-Loc leverages pretrained world knowledge from
vision-language models (VLMs) and incorporates multimodal data to generalize
across both indoor and outdoor settings. Furthermore, natural language is
employed as a directive tool to guide the multi-scene learning process,
facilitating semantic understanding of complex scenes and capturing spatial
relationships among objects. Extensive experiments on the 7Scenes and Cambridge
Landmarks datasets demonstrate MVL-Loc's robustness and state-of-the-art
performance in real-world multi-scene camera relocalization, with improved
accuracy in both positional and orientational estimates.","Zhendong Xiao, Wu Wei, Shujie Ji, Shan Yang, Changhao Chen",2025-07-06T18:52:16Z,2025-07-06T18:52:16Z,http://arxiv.org/abs/2507.04509v1,http://arxiv.org/pdf/2507.04509v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Lidar Variability: A Novel Dataset and Comparative Study of Solid-State
  and Spinning Lidars","Lidar technology has been widely employed across various applications, such
as robot localization in GNSS-denied environments and 3D reconstruction. Recent
advancements have introduced different lidar types, including cost-effective
solid-state lidars such as the Livox Avia and Mid-360. The Mid-360, with its
dome-like design, is increasingly used in portable mapping and unmanned aerial
vehicle (UAV) applications due to its low cost, compact size, and reliable
performance. However, the lack of datasets that include dome-shaped lidars,
such as the Mid-360, alongside other solid-state and spinning lidars
significantly hinders the comparative evaluation of novel approaches across
platforms. Additionally, performance differences between low-cost solid-state
and high-end spinning lidars (e.g., Ouster OS series) remain insufficiently
examined, particularly without an Inertial Measurement Unit (IMU) in odometry.
  To address this gap, we introduce a novel dataset comprising data from
multiple lidar types, including the low-cost Livox Avia and the dome-shaped
Mid-360, as well as high-end spinning lidars such as the Ouster series.
Notably, to the best of our knowledge, no existing dataset comprehensively
includes dome-shaped lidars such as Mid-360 alongside both other solid-state
and spinning lidars. In addition to the dataset, we provide a benchmark
evaluation of state-of-the-art SLAM algorithms applied to this diverse sensor
data. Furthermore, we present a quantitative analysis of point cloud
registration techniques, specifically point-to-point, point-to-plane, and
hybrid methods, using indoor and outdoor data collected from the included lidar
systems. The outcomes of this study establish a foundational reference for
future research in SLAM and 3D reconstruction across heterogeneous lidar
platforms.","Doumegna Mawuto Koudjo Felix, Xianjia Yu, Jiaqiang Zhang, Sier Ha, Zhuo Zou, Tomi Westerlund",2025-07-06T10:04:14Z,2025-07-06T10:04:14Z,http://arxiv.org/abs/2507.04321v1,http://arxiv.org/pdf/2507.04321v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Multi-robot Aerial Soft Manipulator For Floating Litter Collection,"Removing floating litter from water bodies is crucial to preserving aquatic
ecosystems and preventing environmental pollution. In this work, we present a
multi-robot aerial soft manipulator for floating litter collection, leveraging
the capabilities of aerial robots. The proposed system consists of two aerial
robots connected by a flexible rope manipulator, which collects floating litter
using a hook-based tool. Compared to single-aerial-robot solutions, the use of
two aerial robots increases payload capacity and flight endurance while
reducing the downwash effect at the manipulation point, located at the midpoint
of the rope. Additionally, we employ an optimization-based rope-shape planner
to compute the desired rope shape. The planner incorporates an adaptive
behavior that maximizes grasping capabilities near the litter while minimizing
rope tension when farther away. The computed rope shape trajectory is
controlled by a shape visual servoing controller, which approximates the rope
as a parabola. The complete system is validated in outdoor experiments,
demonstrating successful grasping operations. An ablation study highlights how
the planner's adaptive mechanism improves the success rate of the operation.
Furthermore, real-world tests in a water channel confirm the effectiveness of
our system in floating litter collection. These results demonstrate the
potential of aerial robots for autonomous litter removal in aquatic
environments.","Antonio González-Morgado, Sander Smits, Guillermo Heredia, Anibal Ollero, Alexandre Krupa, François Chaumette, Fabien Spindler, Antonio Franchi, Chiara Gabellieri",2025-07-04T12:15:11Z,2025-07-04T12:15:11Z,http://arxiv.org/abs/2507.03517v1,http://arxiv.org/pdf/2507.03517v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Efficient Collision Detection for Long and Slender Robotic Links in
  Euclidean Distance Fields: Application to a Forestry Crane","Collision-free motion planning in complex outdoor environments relies heavily
on perceiving the surroundings through exteroceptive sensors. A widely used
approach represents the environment as a voxelized Euclidean distance field,
where robots are typically approximated by spheres. However, for large-scale
manipulators such as forestry cranes, which feature long and slender links,
this conventional spherical approximation becomes inefficient and inaccurate.
This work presents a novel collision detection algorithm specifically designed
to exploit the elongated structure of such manipulators, significantly
enhancing the computational efficiency of motion planning algorithms. Unlike
traditional sphere decomposition methods, our approach not only improves
computational efficiency but also naturally eliminates the need to fine-tune
the approximation accuracy as an additional parameter. We validate the
algorithm's effectiveness using real-world LiDAR data from a forestry crane
application, as well as simulated environment data.","Marc-Philip Ecker, Bernhard Bischof, Minh Nhat Vu, Christoph Fröhlich, Tobias Glück, Wolfgang Kemmetmüller",2025-07-02T13:37:08Z,2025-07-02T13:37:08Z,http://arxiv.org/abs/2507.01705v1,http://arxiv.org/pdf/2507.01705v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"I Move Therefore I Learn: Experience-Based Traversability in Outdoor
  Robotics","Accurate traversability estimation is essential for safe and effective
navigation of outdoor robots operating in complex environments. This paper
introduces a novel experience-based method that allows robots to autonomously
learn which terrains are traversable based on prior navigation experience,
without relying on extensive pre-labeled datasets. The approach integrates
elevation and texture data into multi-layered grid maps, which are processed
using a variational autoencoder (VAE) trained on a generic texture dataset.
During an initial teleoperated phase, the robot collects sensory data while
moving around the environment. These experiences are encoded into compact
feature vectors and clustered using the BIRCH algorithm to represent
traversable terrain areas efficiently. In deployment, the robot compares new
terrain patches to its learned feature clusters to assess traversability in
real time. The proposed method does not require training with data from the
targeted scenarios, generalizes across diverse surfaces and platforms, and
dynamically adapts as new terrains are encountered. Extensive evaluations on
both synthetic benchmarks and real-world scenarios with wheeled and legged
robots demonstrate its effectiveness, robustness, and superior adaptability
compared to state-of-the-art approaches.","Miguel Ángel de Miguel, Jorge Beltrán, Juan S. Cely, Francisco Martín, Juan Carlos Manzanares, Alberto García",2025-07-01T15:51:24Z,2025-07-01T15:51:24Z,http://arxiv.org/abs/2507.00882v1,http://arxiv.org/pdf/2507.00882v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Diffusion-Based Image Augmentation for Semantic Segmentation in Outdoor
  Robotics","The performance of leaning-based perception algorithms suffer when deployed
in out-of-distribution and underrepresented environments. Outdoor robots are
particularly susceptible to rapid changes in visual scene appearance due to
dynamic lighting, seasonality and weather effects that lead to scenes
underrepresented in the training data of the learning-based perception system.
In this conceptual paper, we focus on preparing our autonomous vehicle for
deployment in snow-filled environments. We propose a novel method for
diffusion-based image augmentation to more closely represent the deployment
environment in our training data. Diffusion-based image augmentations rely on
the public availability of vision foundation models learned on internet-scale
datasets. The diffusion-based image augmentations allow us to take control over
the semantic distribution of the ground surfaces in the training data and to
fine-tune our model for its deployment environment. We employ open vocabulary
semantic segmentation models to filter out augmentation candidates that contain
hallucinations. We believe that diffusion-based image augmentations can be
extended to many other environments apart from snow surfaces, like sandy
environments and volcanic terrains.","Peter Mortimer, Mirko Maehlisch",2025-06-30T18:06:27Z,2025-06-30T18:06:27Z,http://arxiv.org/abs/2507.00153v1,http://arxiv.org/pdf/2507.00153v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Towards foundational LiDAR world models with efficient latent flow
  matching","LiDAR-based world models offer more structured and geometry-aware
representations than their image-based counterparts. However, existing LiDAR
world models are narrowly trained; each model excels only in the domain for
which it was built. Can we develop LiDAR world models that exhibit strong
transferability across multiple domains? We conduct the first systematic domain
transfer study across three demanding scenarios: (i) outdoor to indoor
generalization, (ii) sparse-beam \& dense-beam adaptation, and (iii)
non-semantic to semantic transfer. Given different amounts of fine-tuning data,
our experiments show that a single pre-trained model can achieve up to 11%
absolute improvement (83\% relative) over training from scratch and outperforms
training from scratch in 30/36 of our comparisons. This transferability of
dynamic learning significantly reduces the reliance on manually annotated data
for semantic occupancy forecasting: our method exceed the previous semantic
occupancy forecasting models with only 5% of the labeled training data required
by prior models. We also observed inefficiencies of current LiDAR world models,
mainly through their under-compression of LiDAR data and inefficient training
objectives. To address this, we propose a latent conditional flow matching
(CFM)-based frameworks that achieves state-of-the-art reconstruction accuracy
using only half the training data and a compression ratio 6 times higher than
that of prior methods. Our model achieves SOTA performance on
future-trajectory-conditioned semantic occupancy forecasting while being 23x
more computationally efficient (a 28x FPS speedup); and achieves SOTA
performance on semantic occupancy forecasting while being 2x more
computationally efficient (a 1.1x FPS speedup).","Tianran Liu, Shengwen Zhao, Nicholas Rhinehart",2025-06-30T00:16:55Z,2025-06-30T00:16:55Z,http://arxiv.org/abs/2506.23434v1,http://arxiv.org/pdf/2506.23434v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Minimizing Acoustic Noise: Enhancing Quiet Locomotion for Quadruped
  Robots in Indoor Applications","Recent advancements in quadruped robot research have significantly improved
their ability to traverse complex and unstructured outdoor environments.
However, the issue of noise generated during locomotion is generally
overlooked, which is critically important in noise-sensitive indoor
environments, such as service and healthcare settings, where maintaining low
noise levels is essential. This study aims to optimize the acoustic noise
generated by quadruped robots during locomotion through the development of
advanced motion control algorithms. To achieve this, we propose a novel
approach that minimizes noise emissions by integrating optimized gait design
with tailored control strategies. This method achieves an average noise
reduction of approximately 8 dBA during movement, thereby enhancing the
suitability of quadruped robots for deployment in noise-sensitive indoor
environments. Experimental results demonstrate the effectiveness of this
approach across various indoor settings, highlighting the potential of
quadruped robots for quiet operation in noise-sensitive environments.","Zhanxiang Cao, Buqing Nie, Yang Zhang, Yue Gao",2025-06-29T06:38:54Z,2025-06-29T06:38:54Z,http://arxiv.org/abs/2506.23114v1,http://arxiv.org/pdf/2506.23114v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Cooperative Circumnavigation for Multi-Quadrotor Systems via Onboard
  Sensing","A cooperative circumnavigation framework is proposed for multi-quadrotor
systems to enclose and track a moving target without reliance on external
localization systems. The distinct relationships between quadrotor-quadrotor
and quadrotor-target interactions are evaluated using a heterogeneous
perception strategy and corresponding state estimation algorithms. A modified
Kalman filter is developed to fuse visual-inertial odometry with range
measurements to enhance the accuracy of inter-quadrotor relative localization.
An event-triggered distributed Kalman filter is designed to achieve robust
target state estimation under visual occlusion by incorporating neighbor
measurements and estimated inter-quadrotor relative positions. Using the
estimation results, a cooperative circumnavigation controller is constructed,
leveraging an oscillator-based autonomous formation flight strategy. We conduct
extensive indoor and outdoor experiments to validate the efficiency of the
proposed circumnavigation framework in occluded environments. Furthermore, a
quadrotor failure experiment highlights the inherent fault tolerance property
of the proposed framework, underscoring its potential for deployment in
search-and-rescue operations.","Xueming Liu, Lin Li, Xiang Zhou, Qingrui Zhang, Tianjiang Hu",2025-06-26T02:41:55Z,2025-06-26T02:41:55Z,http://arxiv.org/abs/2506.20954v1,http://arxiv.org/pdf/2506.20954v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"TOMD: A Trail-based Off-road Multimodal Dataset for Traversable Pathway
  Segmentation under Challenging Illumination Conditions","Detecting traversable pathways in unstructured outdoor environments remains a
significant challenge for autonomous robots, especially in critical
applications such as wide-area search and rescue, as well as incident
management scenarios like forest fires. Existing datasets and models primarily
target urban settings or wide, vehicle-traversable off-road tracks, leaving a
substantial gap in addressing the complexity of narrow, trail-like off-road
scenarios. To address this, we introduce the Trail-based Off-road Multimodal
Dataset (TOMD), a comprehensive dataset specifically designed for such
environments. TOMD features high-fidelity multimodal sensor data -- including
128-channel LiDAR, stereo imagery, GNSS, IMU, and illumination measurements --
collected through repeated traversals under diverse conditions. We also propose
a dynamic multiscale data fusion model for accurate traversable pathway
prediction. The study analyzes the performance of early, cross, and mixed
fusion strategies under varying illumination levels. Results demonstrate the
effectiveness of our approach and the relevance of illumination in segmentation
performance. We publicly release TOMD at https://github.com/yyyxs1125/TMOD to
support future research in trail-based off-road navigation.","Yixin Sun, Li Li, Wenke E, Amir Atapour-Abarghouei, Toby P. Breckon",2025-06-24T23:58:44Z,2025-06-24T23:58:44Z,http://arxiv.org/abs/2506.21630v1,http://arxiv.org/pdf/2506.21630v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Look to Locate: Vision-Based Multisensory Navigation with 3-D Digital
  Maps for GNSS-Challenged Environments","In Global Navigation Satellite System (GNSS)-denied environments such as
indoor parking structures or dense urban canyons, achieving accurate and robust
vehicle positioning remains a significant challenge. This paper proposes a
cost-effective, vision-based multi-sensor navigation system that integrates
monocular depth estimation, semantic filtering, and visual map registration
(VMR) with 3-D digital maps. Extensive testing in real-world indoor and outdoor
driving scenarios demonstrates the effectiveness of the proposed system,
achieving sub-meter accuracy of 92% indoors and more than 80% outdoors, with
consistent horizontal positioning and heading average root mean-square errors
of approximately 0.98 m and 1.25 {\deg}, respectively. Compared to the
baselines examined, the proposed solution significantly reduced drift and
improved robustness under various conditions, achieving positioning accuracy
improvements of approximately 88% on average. This work highlights the
potential of cost-effective monocular vision systems combined with 3D maps for
scalable, GNSS-independent navigation in land vehicles.","Ola Elmaghraby, Eslam Mounier, Paulo Ricardo Marques de Araujo, Aboelmagd Noureldin",2025-06-24T17:44:03Z,2025-06-24T17:44:03Z,http://arxiv.org/abs/2506.19827v1,http://arxiv.org/pdf/2506.19827v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale
  Multi-Agent Gaussian SLAM","3D Gaussian splatting has emerged as an expressive scene representation for
RGB-D visual SLAM, but its application to large-scale, multi-agent outdoor
environments remains unexplored. Multi-agent Gaussian SLAM is a promising
approach to rapid exploration and reconstruction of environments, offering
scalable environment representations, but existing approaches are limited to
small-scale, indoor environments. To that end, we propose Gaussian
Reconstruction via Multi-Agent Dense SLAM, or GRAND-SLAM, a collaborative
Gaussian splatting SLAM method that integrates i) an implicit tracking module
based on local optimization over submaps and ii) an approach to inter- and
intra-robot loop closure integrated into a pose-graph optimization framework.
Experiments show that GRAND-SLAM provides state-of-the-art tracking performance
and 28% higher PSNR than existing methods on the Replica indoor dataset, as
well as 91% lower multi-agent tracking error and improved rendering over
existing multi-agent methods on the large-scale, outdoor Kimera-Multi dataset.","Annika Thomas, Aneesa Sonawalla, Alex Rose, Jonathan P. How",2025-06-23T17:55:42Z,2025-06-23T17:55:42Z,http://arxiv.org/abs/2506.18885v1,http://arxiv.org/pdf/2506.18885v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit
  Neural Scene Representation","Neural implicit scene representations have recently shown promising results
in dense visual SLAM. However, existing implicit SLAM algorithms are
constrained to single-agent scenarios, and fall difficulties in large-scale
scenes and long sequences. Existing NeRF-based multi-agent SLAM frameworks
cannot meet the constraints of communication bandwidth. To this end, we propose
the first distributed multi-agent collaborative neural SLAM framework with
hybrid scene representation, distributed camera tracking, intra-to-inter loop
closure, and online distillation for multiple submap fusion. A novel
triplane-grid joint scene representation method is proposed to improve scene
reconstruction. A novel intra-to-inter loop closure method is designed to
achieve local (single-agent) and global (multi-agent) consistency. We also
design a novel online distillation method to fuse the information of different
submaps to achieve global consistency. Furthermore, to the best of our
knowledge, there is no real-world dataset for NeRF-based/GS-based SLAM that
provides both continuous-time trajectories groundtruth and high-accuracy 3D
meshes groundtruth. To this end, we propose the first real-world Dense slam
(DES) dataset covering both single-agent and multi-agent scenarios, ranging
from small rooms to large-scale outdoor scenes, with high-accuracy ground truth
for both 3D mesh and continuous-time camera trajectory. This dataset can
advance the development of the research in both SLAM, 3D reconstruction, and
visual foundation model. Experiments on various datasets demonstrate the
superiority of the proposed method in both mapping, tracking, and
communication. The dataset and code will open-source on
https://github.com/dtc111111/mcnslam.","Tianchen Deng, Guole Shen, Xun Chen, Shenghai Yuan, Hongming Shen, Guohao Peng, Zhenyu Wu, Jingchuan Wang, Lihua Xie, Danwei Wang, Hesheng Wang, Weidong Chen",2025-06-23T14:22:29Z,2025-08-19T06:02:44Z,http://arxiv.org/abs/2506.18678v2,http://arxiv.org/pdf/2506.18678v2.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Crowdsourcing Ubiquitous Indoor Localization with Non-Cooperative Wi-Fi
  Ranging","Indoor localization opens the path to potentially transformative
applications. Although many indoor localization methods have been proposed over
the years, they remain too impractical for widespread deployment in the real
world. In this paper, we introduce PeepLoc, a deployable and scalable
Wi-Fi-based solution for indoor localization that relies only on pre-existing
devices and infrastructure. Specifically, PeepLoc works on any mobile device
with an unmodified Wi-Fi transceiver and in any indoor environment with a
sufficient number of Wi-Fi access points (APs) and pedestrian traffic. At the
core of PeepLoc is (a) a mechanism which allows any Wi-Fi device to obtain
non-cooperative time-of-flight (ToF) to any Wi-Fi AP and (b) a novel
bootstrapping mechanism that relies on pedestrian dead reckoning (PDR) and
crowdsourcing to opportunistically initialize pre-existing APs as anchor points
within an environment. We implement PeepLoc using commodity hardware and
evaluate it extensively across 4 campus buildings. We show PeepLoc leads to a
mean and median positional error of 3.41 m and 3.06 m respectively, which is
superior to existing deployed indoor localization systems and is competitive
with commodity GPS in outdoor environments.","Emerson Sie, Enguang Fan, Federico Cifuentes-Urtubey, Deepak Vasisht",2025-06-23T06:04:45Z,2025-07-05T07:44:19Z,http://arxiv.org/abs/2506.18317v2,http://arxiv.org/pdf/2506.18317v2.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
GeNIE: A Generalizable Navigation System for In-the-Wild Environments,"Reliable navigation in unstructured, real-world environments remains a
significant challenge for embodied agents, especially when operating across
diverse terrains, weather conditions, and sensor configurations. In this paper,
we introduce GeNIE (Generalizable Navigation System for In-the-Wild
Environments), a robust navigation framework designed for global deployment.
GeNIE integrates a generalizable traversability prediction model built on SAM2
with a novel path fusion strategy that enhances planning stability in noisy and
ambiguous settings. We deployed GeNIE in the Earth Rover Challenge (ERC) at
ICRA 2025, where it was evaluated across six countries spanning three
continents. GeNIE took first place and achieved 79% of the maximum possible
score, outperforming the second-best team by 17%, and completed the entire
competition without a single human intervention. These results set a new
benchmark for robust, generalizable outdoor robot navigation. We will release
the codebase, pretrained model weights, and newly curated datasets to support
future research in real-world navigation.","Jiaming Wang, Diwen Liu, Jizhuo Chen, Jiaxuan Da, Nuowen Qian, Tram Minh Man, Harold Soh",2025-06-22T09:36:05Z,2025-06-22T09:36:05Z,http://arxiv.org/abs/2506.17960v1,http://arxiv.org/pdf/2506.17960v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Distilling On-device Language Models for Robot Planning with Minimal
  Human Intervention","Large language models (LLMs) provide robots with powerful contextual
reasoning abilities and a natural human interface. Yet, current LLM-enabled
robots typically depend on cloud-hosted models, limiting their usability in
environments with unreliable communication infrastructure, such as outdoor or
industrial settings. We present PRISM, a framework for distilling small
language model (SLM)-enabled robot planners that run on-device with minimal
human supervision. Starting from an existing LLM-enabled planner, PRISM
automatically synthesizes diverse tasks and environments, elicits plans from
the LLM, and uses this synthetic dataset to distill a compact SLM as a drop-in
replacement of the source model. We apply PRISM to three LLM-enabled planners
for mapping and exploration, manipulation, and household assistance, and we
demonstrate that PRISM improves the performance of Llama-3.2-3B from 10-20% of
GPT-4o's performance to over 93% - using only synthetic data. We further
demonstrate that the distilled planners generalize across heterogeneous robotic
platforms (ground and aerial) and diverse environments (indoor and outdoor). We
release all software, trained models, and datasets at
https://zacravichandran.github.io/PRISM.","Zachary Ravichandran, Ignacio Hounie, Fernando Cladera, Alejandro Ribeiro, George J. Pappas, Vijay Kumar",2025-06-20T21:44:27Z,2025-06-20T21:44:27Z,http://arxiv.org/abs/2506.17486v1,http://arxiv.org/pdf/2506.17486v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"AnyTraverse: An off-road traversability framework with VLM and human
  operator in the loop","Off-road traversability segmentation enables autonomous navigation with
applications in search-and-rescue, military operations, wildlife exploration,
and agriculture. Current frameworks struggle due to significant variations in
unstructured environments and uncertain scene changes, and are not adaptive to
be used for different robot types. We present AnyTraverse, a framework
combining natural language-based prompts with human-operator assistance to
determine navigable regions for diverse robotic vehicles. The system segments
scenes for a given set of prompts and calls the operator only when encountering
previously unexplored scenery or unknown class not part of the prompt in its
region-of-interest, thus reducing active supervision load while adapting to
varying outdoor scenes. Our zero-shot learning approach eliminates the need for
extensive data collection or retraining. Our experimental validation includes
testing on RELLIS-3D, Freiburg Forest, and RUGD datasets and demonstrate
real-world deployment on multiple robot platforms. The results show that
AnyTraverse performs better than GA-NAV and Off-seg while offering a
vehicle-agnostic approach to off-road traversability that balances automation
with targeted human supervision.","Sattwik Sahu, Agamdeep Singh, Karthik Nambiar, Srikanth Saripalli, P. B. Sujit",2025-06-20T08:31:13Z,2025-06-20T08:31:13Z,http://arxiv.org/abs/2506.16826v1,http://arxiv.org/pdf/2506.16826v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Self-supervised Feature Extraction for Enhanced Ball Detection on Soccer
  Robots","Robust and accurate ball detection is a critical component for autonomous
humanoid soccer robots, particularly in dynamic and challenging environments
such as RoboCup outdoor fields. However, traditional supervised approaches
require extensive manual annotation, which is costly and time-intensive. To
overcome this problem, we present a self-supervised learning framework for
domain-adaptive feature extraction to enhance ball detection performance. The
proposed approach leverages a general-purpose pretrained model to generate
pseudo-labels, which are then used in a suite of self-supervised pretext tasks
-- including colorization, edge detection, and triplet loss -- to learn robust
visual features without relying on manual annotations. Additionally, a
model-agnostic meta-learning (MAML) strategy is incorporated to ensure rapid
adaptation to new deployment scenarios with minimal supervision. A new dataset
comprising 10,000 labeled images from outdoor RoboCup SPL matches is
introduced, used to validate the method, and made available to the community.
Experimental results demonstrate that the proposed pipeline outperforms
baseline models in terms of accuracy, F1 score, and IoU, while also exhibiting
faster convergence.","Can Lin, Daniele Affinita, Marco E. P. Zimmatore, Daniele Nardi, Domenico D. Bloisi, Vincenzo Suriani",2025-06-20T08:21:34Z,2025-06-20T08:21:34Z,http://arxiv.org/abs/2506.16821v1,http://arxiv.org/pdf/2506.16821v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Experimental Setup and Software Pipeline to Evaluate Optimization based
  Autonomous Multi-Robot Search Algorithms","Signal source localization has been a problem of interest in the multi-robot
systems domain given its applications in search & rescue and hazard
localization in various industrial and outdoor settings. A variety of
multi-robot search algorithms exist that usually formulate and solve the
associated autonomous motion planning problem as a heuristic model-free or
belief model-based optimization process. Most of these algorithms however
remains tested only in simulation, thereby losing the opportunity to generate
knowledge about how such algorithms would compare/contrast in a real physical
setting in terms of search performance and real-time computing performance. To
address this gap, this paper presents a new lab-scale physical setup and
associated open-source software pipeline to evaluate and benchmark multi-robot
search algorithms. The presented physical setup innovatively uses an acoustic
source (that is safe and inexpensive) and small ground robots (e-pucks)
operating in a standard motion-capture environment. This setup can be easily
recreated and used by most robotics researchers. The acoustic source also
presents interesting uncertainty in terms of its noise-to-signal ratio, which
is useful to assess sim-to-real gaps. The overall software pipeline is designed
to readily interface with any multi-robot search algorithm with minimal effort
and is executable in parallel asynchronous form. This pipeline includes a
framework for distributed implementation of multi-robot or swarm search
algorithms, integrated with a ROS (Robotics Operating System)-based software
stack for motion capture supported localization. The utility of this novel
setup is demonstrated by using it to evaluate two state-of-the-art multi-robot
search algorithms, based on swarm optimization and batch-Bayesian Optimization
(called Bayes-Swarm), as well as a random walk baseline.","Aditya Bhatt, Mary Katherine Corra, Franklin Merlo, Prajit KrisshnaKumar, Souma Chowdhury",2025-06-20T03:06:43Z,2025-07-11T17:47:54Z,http://arxiv.org/abs/2506.16710v3,http://arxiv.org/pdf/2506.16710v3.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Embodied Web Agents: Bridging Physical-Digital Realms for Integrated
  Agent Intelligence","AI agents today are mostly siloed - they either retrieve and reason over vast
amount of digital information and knowledge obtained online; or interact with
the physical world through embodied perception, planning and action - but
rarely both. This separation limits their ability to solve tasks that require
integrated physical and digital intelligence, such as cooking from online
recipes, navigating with dynamic map data, or interpreting real-world landmarks
using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI
agents that fluidly bridge embodiment and web-scale reasoning. To
operationalize this concept, we first develop the Embodied Web Agents task
environments, a unified simulation platform that tightly integrates realistic
3D indoor and outdoor environments with functional web interfaces. Building
upon this platform, we construct and release the Embodied Web Agents Benchmark,
which encompasses a diverse suite of tasks including cooking, navigation,
shopping, tourism, and geolocation - all requiring coordinated reasoning across
physical and digital realms for systematic assessment of cross-domain
intelligence. Experimental results reveal significant performance gaps between
state-of-the-art AI systems and human capabilities, establishing both
challenges and opportunities at the intersection of embodied cognition and
web-scale knowledge access. All datasets, codes and websites are publicly
available at our project page https://embodied-web-agent.github.io/.","Yining Hong, Rui Sun, Bingxuan Li, Xingcheng Yao, Maxine Wu, Alexander Chien, Da Yin, Ying Nian Wu, Zhecan James Wang, Kai-Wei Chang",2025-06-18T17:58:17Z,2025-07-29T22:40:49Z,http://arxiv.org/abs/2506.15677v3,http://arxiv.org/pdf/2506.15677v3.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
MCOO-SLAM: A Multi-Camera Omnidirectional Object SLAM System,"Object-level SLAM offers structured and semantically meaningful environment
representations, making it more interpretable and suitable for high-level
robotic tasks. However, most existing approaches rely on RGB-D sensors or
monocular views, which suffer from narrow fields of view, occlusion
sensitivity, and limited depth perception-especially in large-scale or outdoor
environments. These limitations often restrict the system to observing only
partial views of objects from limited perspectives, leading to inaccurate
object modeling and unreliable data association. In this work, we propose
MCOO-SLAM, a novel Multi-Camera Omnidirectional Object SLAM system that fully
leverages surround-view camera configurations to achieve robust, consistent,
and semantically enriched mapping in complex outdoor scenarios. Our approach
integrates point features and object-level landmarks enhanced with
open-vocabulary semantics. A semantic-geometric-temporal fusion strategy is
introduced for robust object association across multiple views, leading to
improved consistency and accurate object modeling, and an omnidirectional loop
closure module is designed to enable viewpoint-invariant place recognition
using scene-level descriptors. Furthermore, the constructed map is abstracted
into a hierarchical 3D scene graph to support downstream reasoning tasks.
Extensive experiments in real-world demonstrate that MCOO-SLAM achieves
accurate localization and scalable object-level mapping with improved
robustness to occlusion, pose variation, and environmental complexity.","Miaoxin Pan, Jinnan Li, Yaowen Zhang, Yi Yang, Yufeng Yue",2025-06-18T12:20:34Z,2025-06-18T12:20:34Z,http://arxiv.org/abs/2506.15402v1,http://arxiv.org/pdf/2506.15402v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Time-Optimized Safe Navigation in Unstructured Environments through
  Learning Based Depth Completion","Quadrotors hold significant promise for several applications such as
agriculture, search and rescue, and infrastructure inspection. Achieving
autonomous operation requires systems to navigate safely through complex and
unfamiliar environments. This level of autonomy is particularly challenging due
to the complexity of such environments and the need for real-time decision
making especially for platforms constrained by size, weight, and power (SWaP),
which limits flight time and precludes the use of bulky sensors like Light
Detection and Ranging (LiDAR) for mapping. Furthermore, computing globally
optimal, collision-free paths and translating them into time-optimized, safe
trajectories in real time adds significant computational complexity. To address
these challenges, we present a fully onboard, real-time navigation system that
relies solely on lightweight onboard sensors. Our system constructs a dense 3D
map of the environment using a novel visual depth estimation approach that
fuses stereo and monocular learning-based depth, yielding longer-range, denser,
and less noisy depth maps than conventional stereo methods. Building on this
map, we introduce a novel planning and trajectory generation framework capable
of rapidly computing time-optimal global trajectories. As the map is
incrementally updated with new depth information, our system continuously
refines the trajectory to maintain safety and optimality. Both our planner and
trajectory generator outperforms state-of-the-art methods in terms of
computational efficiency and guarantee obstacle-free trajectories. We validate
our system through robust autonomous flight experiments in diverse indoor and
outdoor environments, demonstrating its effectiveness for safe navigation in
previously unknown settings.","Jeffrey Mao, Raghuram Cauligi Srinivas, Steven Nogar, Giuseppe Loianno",2025-06-17T21:01:05Z,2025-06-17T21:01:05Z,http://arxiv.org/abs/2506.14975v1,http://arxiv.org/pdf/2506.14975v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Towards Perception-based Collision Avoidance for UAVs when Guiding the
  Visually Impaired","Autonomous navigation by drones using onboard sensors combined with machine
learning and computer vision algorithms is impacting a number of domains,
including agriculture, logistics, and disaster management. In this paper, we
examine the use of drones for assisting visually impaired people (VIPs) in
navigating through outdoor urban environments. Specifically, we present a
perception-based path planning system for local planning around the
neighborhood of the VIP, integrated with a global planner based on GPS and maps
for coarse planning. We represent the problem using a geometric formulation and
propose a multi DNN based framework for obstacle avoidance of the UAV as well
as the VIP. Our evaluations conducted on a drone human system in a university
campus environment verifies the feasibility of our algorithms in three
scenarios; when the VIP walks on a footpath, near parked vehicles, and in a
crowded street.","Suman Raj, Swapnil Padhi, Ruchi Bhoot, Prince Modi, Yogesh Simmhan",2025-06-17T09:08:30Z,2025-06-17T09:08:30Z,http://arxiv.org/abs/2506.14857v1,http://arxiv.org/pdf/2506.14857v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Whole-Body Control Framework for Humanoid Robots with Heavy Limbs: A
  Model-Based Approach","Humanoid robots often face significant balance issues due to the motion of
their heavy limbs. These challenges are particularly pronounced when attempting
dynamic motion or operating in environments with irregular terrain. To address
this challenge, this manuscript proposes a whole-body control framework for
humanoid robots with heavy limbs, using a model-based approach that combines a
kino-dynamics planner and a hierarchical optimization problem. The
kino-dynamics planner is designed as a model predictive control (MPC) scheme to
account for the impact of heavy limbs on mass and inertia distribution. By
simplifying the robot's system dynamics and constraints, the planner enables
real-time planning of motion and contact forces. The hierarchical optimization
problem is formulated using Hierarchical Quadratic Programming (HQP) to
minimize limb control errors and ensure compliance with the policy generated by
the kino-dynamics planner. Experimental validation of the proposed framework
demonstrates its effectiveness. The humanoid robot with heavy limbs controlled
by the proposed framework can achieve dynamic walking speeds of up to 1.2~m/s,
respond to external disturbances of up to 60~N, and maintain balance on
challenging terrains such as uneven surfaces, and outdoor environments.","Tianlin Zhang, Linzhu Yue, Hongbo Zhang, Lingwei Zhang, Xuanqi Zeng, Zhitao Song, Yun-Hui Liu",2025-06-17T07:45:09Z,2025-06-17T07:45:09Z,http://arxiv.org/abs/2506.14278v1,http://arxiv.org/pdf/2506.14278v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Leader360V: The Large-scale, Real-world 360 Video Dataset for Multi-task
  Learning in Diverse Environment","360 video captures the complete surrounding scenes with the ultra-large field
of view of 360X180. This makes 360 scene understanding tasks, eg, segmentation
and tracking, crucial for appications, such as autonomous driving, robotics.
With the recent emergence of foundation models, the community is, however,
impeded by the lack of large-scale, labelled real-world datasets. This is
caused by the inherent spherical properties, eg, severe distortion in polar
regions, and content discontinuities, rendering the annotation costly yet
complex. This paper introduces Leader360V, the first large-scale, labeled
real-world 360 video datasets for instance segmentation and tracking. Our
datasets enjoy high scene diversity, ranging from indoor and urban settings to
natural and dynamic outdoor scenes. To automate annotation, we design an
automatic labeling pipeline, which subtly coordinates pre-trained 2D segmentors
and large language models to facilitate the labeling. The pipeline operates in
three novel stages. Specifically, in the Initial Annotation Phase, we introduce
a Semantic- and Distortion-aware Refinement module, which combines object mask
proposals from multiple 2D segmentors with LLM-verified semantic labels. These
are then converted into mask prompts to guide SAM2 in generating
distortion-aware masks for subsequent frames. In the Auto-Refine Annotation
Phase, missing or incomplete regions are corrected either by applying the SDR
again or resolving the discontinuities near the horizontal borders. The Manual
Revision Phase finally incorporates LLMs and human annotators to further refine
and validate the annotations. Extensive user studies and evaluations
demonstrate the effectiveness of our labeling pipeline. Meanwhile, experiments
confirm that Leader360V significantly enhances model performance for 360 video
segmentation and tracking, paving the way for more scalable 360 scene
understanding.","Weiming Zhang, Dingwen Xiao, Aobotao Dai, Yexin Liu, Tianbo Pan, Shiqi Wen, Lei Chen, Lin Wang",2025-06-17T07:37:08Z,2025-06-17T07:37:08Z,http://arxiv.org/abs/2506.14271v1,http://arxiv.org/pdf/2506.14271v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Grounded Vision-Language Navigation for UAVs with Open-Vocabulary Goal
  Understanding","Vision-and-language navigation (VLN) is a long-standing challenge in
autonomous robotics, aiming to empower agents with the ability to follow human
instructions while navigating complex environments. Two key bottlenecks remain
in this field: generalization to out-of-distribution environments and reliance
on fixed discrete action spaces. To address these challenges, we propose
Vision-Language Fly (VLFly), a framework tailored for Unmanned Aerial Vehicles
(UAVs) to execute language-guided flight. Without the requirement for
localization or active ranging sensors, VLFly outputs continuous velocity
commands purely from egocentric observations captured by an onboard monocular
camera. The VLFly integrates three modules: an instruction encoder based on a
large language model (LLM) that reformulates high-level language into
structured prompts, a goal retriever powered by a vision-language model (VLM)
that matches these prompts to goal images via vision-language similarity, and a
waypoint planner that generates executable trajectories for real-time UAV
control. VLFly is evaluated across diverse simulation environments without
additional fine-tuning and consistently outperforms all baselines. Moreover,
real-world VLN tasks in indoor and outdoor environments under direct and
indirect instructions demonstrate that VLFly achieves robust open-vocabulary
goal understanding and generalized navigation capabilities, even in the
presence of abstract language input.","Yuhang Zhang, Haosheng Yu, Jiaping Xiao, Mir Feroskhan",2025-06-12T14:40:50Z,2025-06-12T14:40:50Z,http://arxiv.org/abs/2506.10756v1,http://arxiv.org/pdf/2506.10756v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Attention-Based Map Encoding for Learning Generalized Legged Locomotion,"Dynamic locomotion of legged robots is a critical yet challenging topic in
expanding the operational range of mobile robots. It requires precise planning
when possible footholds are sparse, robustness against uncertainties and
disturbances, and generalizability across diverse terrains. While traditional
model-based controllers excel at planning on complex terrains, they struggle
with real-world uncertainties. Learning-based controllers offer robustness to
such uncertainties but often lack precision on terrains with sparse steppable
areas. Hybrid methods achieve enhanced robustness on sparse terrains by
combining both methods but are computationally demanding and constrained by the
inherent limitations of model-based planners. To achieve generalized legged
locomotion on diverse terrains while preserving the robustness of
learning-based controllers, this paper proposes to learn an attention-based map
encoding conditioned on robot proprioception, which is trained as part of the
end-to-end controller using reinforcement learning. We show that the network
learns to focus on steppable areas for future footholds when the robot
dynamically navigates diverse and challenging terrains. We synthesize behaviors
that exhibit robustness against uncertainties while enabling precise and agile
traversal of sparse terrains. Additionally, our method offers a way to
interpret the topographical perception of a neural network. We have trained two
controllers for a 12-DoF quadrupedal robot and a 23-DoF humanoid robot
respectively and tested the resulting controllers in the real world under
various challenging indoor and outdoor scenarios, including ones unseen during
training.","Junzhe He, Chong Zhang, Fabian Jenelten, Ruben Grandia, Moritz BÄcher, Marco Hutter",2025-06-11T10:38:59Z,2025-06-11T10:38:59Z,http://arxiv.org/abs/2506.09588v1,http://arxiv.org/pdf/2506.09588v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
VAULT: A Mobile Mapping System for ROS 2-based Autonomous Robots,"Localization plays a crucial role in the navigation capabilities of
autonomous robots, and while indoor environments can rely on wheel odometry and
2D LiDAR-based mapping, outdoor settings such as agriculture and forestry,
present unique challenges that necessitate real-time localization and
consistent mapping. Addressing this need, this paper introduces the VAULT
prototype, a ROS 2-based mobile mapping system (MMS) that combines various
sensors to enable robust outdoor and indoor localization. The proposed solution
harnesses the power of Global Navigation Satellite System (GNSS) data,
visual-inertial odometry (VIO), inertial measurement unit (IMU) data, and the
Extended Kalman Filter (EKF) to generate reliable 3D odometry. To further
enhance the localization accuracy, Visual SLAM (VSLAM) is employed, resulting
in the creation of a comprehensive 3D point cloud map. By leveraging these
sensor technologies and advanced algorithms, the prototype offers a
comprehensive solution for outdoor localization in autonomous mobile robots,
enabling them to navigate and map their surroundings with confidence and
precision.","Miguel Á. González-Santamarta, Francisco J. Rodríguez-Lera, Vicente Matellán-Olivera",2025-06-11T10:26:32Z,2025-06-11T10:26:32Z,http://arxiv.org/abs/2506.09583v1,http://arxiv.org/pdf/2506.09583v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Deploying SICNav in the Field: Safe and Interactive Crowd Navigation
  using MPC and Bilevel Optimization","Safe and efficient navigation in crowded environments remains a critical
challenge for robots that provide a variety of service tasks such as food
delivery or autonomous wheelchair mobility. Classical robot crowd navigation
methods decouple human motion prediction from robot motion planning, which
neglects the closed-loop interactions between humans and robots. This lack of a
model for human reactions to the robot plan (e.g. moving out of the way) can
cause the robot to get stuck. Our proposed Safe and Interactive Crowd
Navigation (SICNav) method is a bilevel Model Predictive Control (MPC)
framework that combines prediction and planning into one optimization problem,
explicitly modeling interactions among agents. In this paper, we present a
systems overview of the crowd navigation platform we use to deploy SICNav in
previously unseen indoor and outdoor environments. We provide a preliminary
analysis of the system's operation over the course of nearly 7 km of autonomous
navigation over two hours in both indoor and outdoor environments.","Sepehr Samavi, Garvish Bhutani, Florian Shkurti, Angela P. Schoellig",2025-06-10T14:40:48Z,2025-06-10T14:40:48Z,http://arxiv.org/abs/2506.08851v1,http://arxiv.org/pdf/2506.08851v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"LogoSP: Local-global Grouping of Superpoints for Unsupervised Semantic
  Segmentation of 3D Point Clouds","We study the problem of unsupervised 3D semantic segmentation on raw point
clouds without needing human labels in training. Existing methods usually
formulate this problem into learning per-point local features followed by a
simple grouping strategy, lacking the ability to discover additional and
possibly richer semantic priors beyond local features. In this paper, we
introduce LogoSP to learn 3D semantics from both local and global point
features. The key to our approach is to discover 3D semantic information by
grouping superpoints according to their global patterns in the frequency
domain, thus generating highly accurate semantic pseudo-labels for training a
segmentation network. Extensive experiments on two indoor and an outdoor
datasets show that our LogoSP surpasses all existing unsupervised methods by
large margins, achieving the state-of-the-art performance for unsupervised 3D
semantic segmentation. Notably, our investigation into the learned global
patterns reveals that they truly represent meaningful 3D semantics in the
absence of human labels during training.","Zihui Zhang, Weisheng Dai, Hongtao Wen, Bo Yang",2025-06-09T15:21:37Z,2025-06-09T15:21:37Z,http://arxiv.org/abs/2506.07857v1,http://arxiv.org/pdf/2506.07857v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Language-Grounded Hierarchical Planning and Execution with Multi-Robot
  3D Scene Graphs","In this paper, we introduce a multi-robot system that integrates mapping,
localization, and task and motion planning (TAMP) enabled by 3D scene graphs to
execute complex instructions expressed in natural language. Our system builds a
shared 3D scene graph incorporating an open-set object-based map, which is
leveraged for multi-robot 3D scene graph fusion. This representation supports
real-time, view-invariant relocalization (via the object-based map) and
planning (via the 3D scene graph), allowing a team of robots to reason about
their surroundings and execute complex tasks. Additionally, we introduce a
planning approach that translates operator intent into Planning Domain
Definition Language (PDDL) goals using a Large Language Model (LLM) by
leveraging context from the shared 3D scene graph and robot capabilities. We
provide an experimental assessment of the performance of our system on
real-world tasks in large-scale, outdoor environments. A supplementary video is
available at https://youtu.be/8xbGGOLfLAY.","Jared Strader, Aaron Ray, Jacob Arkin, Mason B. Peterson, Yun Chang, Nathan Hughes, Christopher Bradley, Yi Xuan Jia, Carlos Nieto-Granda, Rajat Talak, Chuchu Fan, Luca Carlone, Jonathan P. How, Nicholas Roy",2025-06-09T06:02:34Z,2025-07-10T23:26:07Z,http://arxiv.org/abs/2506.07454v2,http://arxiv.org/pdf/2506.07454v2.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"UNO: Unified Self-Supervised Monocular Odometry for Platform-Agnostic
  Deployment","This work presents UNO, a unified monocular visual odometry framework that
enables robust and adaptable pose estimation across diverse environments,
platforms, and motion patterns. Unlike traditional methods that rely on
deployment-specific tuning or predefined motion priors, our approach
generalizes effectively across a wide range of real-world scenarios, including
autonomous vehicles, aerial drones, mobile robots, and handheld devices. To
this end, we introduce a Mixture-of-Experts strategy for local state
estimation, with several specialized decoders that each handle a distinct class
of ego-motion patterns. Moreover, we introduce a fully differentiable
Gumbel-Softmax module that constructs a robust inter-frame correlation graph,
selects the optimal expert decoder, and prunes erroneous estimates. These cues
are then fed into a unified back-end that combines pre-trained,
scale-independent depth priors with a lightweight bundling adjustment to
enforce geometric consistency. We extensively evaluate our method on three
major benchmark datasets: KITTI (outdoor/autonomous driving), EuRoC-MAV
(indoor/aerial drones), and TUM-RGBD (indoor/handheld), demonstrating
state-of-the-art performance.","Wentao Zhao, Yihe Niu, Yanbo Wang, Tianchen Deng, Shenghai Yuan, Zhenli Wang, Rui Guo, Jingchuan Wang",2025-06-08T06:30:37Z,2025-06-08T06:30:37Z,http://arxiv.org/abs/2506.07013v1,http://arxiv.org/pdf/2506.07013v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Technical Report for ICRA 2025 GOOSE 3D Semantic Segmentation Challenge:
  Adaptive Point Cloud Understanding for Heterogeneous Robotic Systems","This technical report presents the implementation details of the winning
solution for the ICRA 2025 GOOSE 3D Semantic Segmentation Challenge. This
challenge focuses on semantic segmentation of 3D point clouds from diverse
unstructured outdoor environments collected from multiple robotic platforms.
This problem was addressed by implementing Point Prompt Tuning (PPT) integrated
with Point Transformer v3 (PTv3) backbone, enabling adaptive processing of
heterogeneous LiDAR data through platform-specific conditioning and
cross-dataset class alignment strategies. The model is trained without
requiring additional external data. As a result, this approach achieved
substantial performance improvements with mIoU increases of up to 22.59% on
challenging platforms compared to the baseline PTv3 model, demonstrating the
effectiveness of adaptive point cloud understanding for field robotics
applications.",Xiaoya Zhang,2025-06-08T04:55:44Z,2025-06-08T04:55:44Z,http://arxiv.org/abs/2506.06995v1,http://arxiv.org/pdf/2506.06995v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Towards Terrain-Aware Task-Driven 3D Scene Graph Generation in Outdoor
  Environments","High-level autonomous operations depend on a robot's ability to construct a
sufficiently expressive model of its environment. Traditional three-dimensional
(3D) scene representations, such as point clouds and occupancy grids, provide
detailed geometric information but lack the structured, semantic organization
needed for high-level reasoning. 3D scene graphs (3DSGs) address this
limitation by integrating geometric, topological, and semantic relationships
into a multi-level graph-based representation. By capturing hierarchical
abstractions of objects and spatial layouts, 3DSGs enable robots to reason
about environments in a structured manner, improving context-aware
decision-making and adaptive planning. Although most recent work has focused on
indoor 3DSGs, this paper investigates their construction and utility in outdoor
environments. We present a method for generating a task-agnostic
metric-semantic point cloud for large outdoor settings and propose
modifications to existing indoor 3DSG generation techniques for outdoor
applicability. Our preliminary qualitative results demonstrate the feasibility
of outdoor 3DSGs and highlight their potential for future deployment in
real-world field robotic applications.","Chad R Samuelson, Timothy W McLain, Joshua G Mangelson",2025-06-06T22:19:41Z,2025-06-06T22:19:41Z,http://arxiv.org/abs/2506.06562v1,http://arxiv.org/pdf/2506.06562v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Whole-Body Constrained Learning for Legged Locomotion via Hierarchical
  Optimization","Reinforcement learning (RL) has demonstrated impressive performance in legged
locomotion over various challenging environments. However, due to the
sim-to-real gap and lack of explainability, unconstrained RL policies deployed
in the real world still suffer from inevitable safety issues, such as joint
collisions, excessive torque, or foot slippage in low-friction environments.
These problems limit its usage in missions with strict safety requirements,
such as planetary exploration, nuclear facility inspection, and deep-sea
operations. In this paper, we design a hierarchical optimization-based
whole-body follower, which integrates both hard and soft constraints into RL
framework to make the robot move with better safety guarantees. Leveraging the
advantages of model-based control, our approach allows for the definition of
various types of hard and soft constraints during training or deployment, which
allows for policy fine-tuning and mitigates the challenges of sim-to-real
transfer. Meanwhile, it preserves the robustness of RL when dealing with
locomotion in complex unstructured environments. The trained policy with
introduced constraints was deployed in a hexapod robot and tested in various
outdoor environments, including snow-covered slopes and stairs, demonstrating
the great traversability and safety of our approach.","Haoyu Wang, Ruyi Zhou, Liang Ding, Tie Liu, Zhelin Zhang, Peng Xu, Haibo Gao, Zongquan Deng",2025-06-05T15:00:27Z,2025-06-05T15:00:27Z,http://arxiv.org/abs/2506.05115v1,http://arxiv.org/pdf/2506.05115v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
FlySearch: Exploring how vision-language models explore,"The real world is messy and unstructured. Uncovering critical information
often requires active, goal-driven exploration. It remains to be seen whether
Vision-Language Models (VLMs), which recently emerged as a popular zero-shot
tool in many difficult tasks, can operate effectively in such conditions. In
this paper, we answer this question by introducing FlySearch, a 3D, outdoor,
photorealistic environment for searching and navigating to objects in complex
scenes. We define three sets of scenarios with varying difficulty and observe
that state-of-the-art VLMs cannot reliably solve even the simplest exploration
tasks, with the gap to human performance increasing as the tasks get harder. We
identify a set of central causes, ranging from vision hallucination, through
context misunderstanding, to task planning failures, and we show that some of
them can be addressed by finetuning. We publicly release the benchmark,
scenarios, and the underlying codebase.","Adam Pardyl, Dominik Matuszek, Mateusz Przebieracz, Marek Cygan, Bartosz Zieliński, Maciej Wołczyk",2025-06-03T14:03:42Z,2025-06-04T09:32:09Z,http://arxiv.org/abs/2506.02896v2,http://arxiv.org/pdf/2506.02896v2.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
ADEPT: Adaptive Diffusion Environment for Policy Transfer Sim-to-Real,"Model-free reinforcement learning has emerged as a powerful method for
developing robust robot control policies capable of navigating through complex
and unstructured environments. The effectiveness of these methods hinges on two
essential elements: (1) the use of massively parallel physics simulations to
expedite policy training, and (2) an environment generator tasked with crafting
sufficiently challenging yet attainable environments to facilitate continuous
policy improvement. Existing methods of outdoor environment generation often
rely on heuristics constrained by a set of parameters, limiting the diversity
and realism. In this work, we introduce ADEPT, a novel \textbf{A}daptive
\textbf{D}iffusion \textbf{E}nvironment for \textbf{P}olicy \textbf{T}ransfer
in the zero-shot sim-to-real fashion that leverages Denoising Diffusion
Probabilistic Models to dynamically expand existing training environments by
adding more diverse and complex environments adaptive to the current policy.
ADEPT guides the diffusion model's generation process through initial noise
optimization, blending noise-corrupted environments from existing training
environments weighted by the policy's performance in each corresponding
environment. By manipulating the noise corruption level, ADEPT seamlessly
transitions between generating similar environments for policy fine-tuning and
novel ones to expand training diversity. To benchmark ADEPT in off-road
navigation, we propose a fast and effective multi-layer map representation for
wild environment generation. Our experiments show that the policy trained by
ADEPT outperforms both procedural generated and natural environments, along
with popular navigation methods.","Youwei Yu, Junhong Xu, Lantao Liu",2025-06-02T15:07:12Z,2025-06-05T02:23:13Z,http://arxiv.org/abs/2506.01759v2,http://arxiv.org/pdf/2506.01759v2.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Lazy Heuristic Search for Solving POMDPs with Expensive-to-Compute
  Belief Transitions","Heuristic search solvers like RTDP-Bel and LAO* have proven effective for
computing optimal and bounded sub-optimal solutions for Partially Observable
Markov Decision Processes (POMDPs), which are typically formulated as belief
MDPs. A belief represents a probability distribution over possible system
states. Given a parent belief and an action, computing belief state transitions
involves Bayesian updates that combine the transition and observation models of
the POMDP to determine successor beliefs and their transition probabilities.
However, there is a class of problems, specifically in robotics, where
computing these transitions can be prohibitively expensive due to costly
physics simulations, raycasting, or expensive collision checks required by the
underlying transition and observation models, leading to long planning times.
To address this challenge, we propose Lazy RTDP-Bel and Lazy LAO*, which defer
computing expensive belief state transitions by leveraging Q-value estimation,
significantly reducing planning time. We demonstrate the superior performance
of the proposed lazy planners in domains such as contact-rich manipulation for
pose estimation, outdoor navigation in rough terrain, and indoor navigation
with a 1-D LiDAR sensor. Additionally, we discuss practical Q-value estimation
techniques for commonly encountered problem classes that our lazy planners can
leverage. Our results show that lazy heuristic search methods dramatically
improve planning speed by postponing expensive belief transition evaluations
while maintaining solution quality.","Muhammad Suhail Saleem, Rishi Veerapaneni, Maxim Likhachev",2025-05-30T22:26:26Z,2025-05-30T22:26:26Z,http://arxiv.org/abs/2506.00285v1,http://arxiv.org/pdf/2506.00285v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"AniTrack: A Power-Efficient, Time-Slotted and Robust UWB Localization
  System for Animal Tracking in a Controlled Setting","Accurate localization is essential for a wide range of applications,
including asset tracking, smart agriculture, and animal monitoring. While
traditional localization methods, such as Global Navigation Satellite System
(GNSS), Wi-Fi, and Bluetooth Low Energy (BLE), offer varying levels of accuracy
and coverage, they have drawbacks regarding power consumption, infrastructure
requirements, and deployment flexibility. Ultra-Wideband (UWB) is emerging as
an alternative, offering centimeter-level accuracy and energy efficiency,
especially suitable for medium to large field monitoring with capabilities to
work indoors and outdoors. However, existing UWB localization systems require
infrastructure with mains power to supply the anchors, which impedes their
scalability and ease of deployment. This underscores the need for a fully
battery-powered and energy-efficient localization system. This paper presents
an energy-optimized, battery-operated UWB localization system that leverages
Long Range Wide Area Network (LoRaWAN) for data transmission to a server
backend. By employing single-sided two-way ranging (SS-TWR) in a time-slotted
localization approach, the power consumption both on the anchor and the tag is
reduced, while maintaining high accuracy. With a low average power consumption
of 20.44 mW per anchor and 7.19 mW per tag, the system allows fully
battery-powered operation for up to 25 days, achieving average accuracy of
13.96 cm with self-localizing anchors on a 600 m2 testing ground. To validate
its effectiveness and ease of installation in a challenging application
scenario, ten anchors and two tags were successfully deployed in a tropical
zoological biome where they could be used to track Aldabra Giant Tortoises
(Aldabrachelys gigantea).","Victor Luder, Lukas Schulthess, Silvano Cortesi, Leyla Rivero Davis, Michele Magno",2025-05-30T20:42:40Z,2025-05-30T20:42:40Z,http://arxiv.org/abs/2506.00216v1,http://arxiv.org/pdf/2506.00216v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Exploiting Euclidean Distance Field Properties for Fast and Safe 3D
  planning with a modified Lazy Theta*","Graph search planners have been widely used for 3D path planning in the
literature, and Euclidean Distance Fields (EDFs) are increasingly being used as
a representation of the environment. However, to the best of our knowledge, the
integration of EDFs into heuristic planning has been carried out in a loosely
coupled fashion, dismissing EDF properties that can be used to
accelerate/improve the planning process and enhance the safety margins of the
resultant trajectories. This paper presents a fast graph search planner based
on a modified Lazy Theta* planning algorithm for aerial robots in challenging
3D environments that exploits the EDF properties. The proposed planner
outperforms classic graph search planners in terms of path smoothness and
safety. It integrates EDFs as environment representation and directly generates
fast and smooth paths avoiding the use of post-processing methods; it also
considers the analytical properties of EDFs to obtain an approximation of the
EDF cost along the line-of-sight segments and to reduce the number of
visibility neighbours, which directly impacts the computation time. Moreover,
we demonstrate that the proposed EDF-based cost function satisfies the triangle
inequality, which reduces calculations during exploration and, hence,
computation time. Many experiments and comparatives are carried out in 3D
challenging indoor and outdoor simulation environments to evaluate and validate
the proposed planner. The results show an efficient and safe planner in these
environments.","Jose A. Cobano, L. Merino, F. Caballero",2025-05-29T21:51:02Z,2025-05-29T21:51:02Z,http://arxiv.org/abs/2505.24024v1,http://arxiv.org/pdf/2505.24024v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
LiDAR Based Semantic Perception for Forklifts in Outdoor Environments,"In this study, we present a novel LiDAR-based semantic segmentation framework
tailored for autonomous forklifts operating in complex outdoor environments.
Central to our approach is the integration of a dual LiDAR system, which
combines forward-facing and downward-angled LiDAR sensors to enable
comprehensive scene understanding, specifically tailored for industrial
material handling tasks. The dual configuration improves the detection and
segmentation of dynamic and static obstacles with high spatial precision. Using
high-resolution 3D point clouds captured from two sensors, our method employs a
lightweight yet robust approach that segments the point clouds into
safety-critical instance classes such as pedestrians, vehicles, and forklifts,
as well as environmental classes such as driveable ground, lanes, and
buildings. Experimental validation demonstrates that our approach achieves high
segmentation accuracy while satisfying strict runtime requirements,
establishing its viability for safety-aware, fully autonomous forklift
navigation in dynamic warehouse and yard environments.","Benjamin Serfling, Hannes Reichert, Lorenzo Bayerlein, Konrad Doll, Kati Radkhah-Lens",2025-05-28T11:45:14Z,2025-05-28T11:45:14Z,http://arxiv.org/abs/2505.22258v1,http://arxiv.org/pdf/2505.22258v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Real-World Deployment of Cloud Autonomous Mobility System Using 5G
  Networks for Outdoor and Indoor Environments","The growing complexity of both outdoor and indoor mobility systems demands
scalable, cost-effective, and reliable perception and communication frameworks.
This work presents the real-world deployment and evaluation of a Cloud
Autonomous Mobility (CAM) system that leverages distributed sensor nodes
connected via 5G networks, which integrates LiDAR- and camera-based perception
at infrastructure units, cloud computing for global information fusion, and
Ultra-Reliable Low Latency Communications (URLLC) to enable real-time
situational awareness and autonomous operation. The CAM system is deployed in
two distinct environments: a dense urban roundabout and a narrow indoor
hospital corridor. Field experiments show improved traffic monitoring, hazard
detection, and asset management capabilities. The paper also discusses
practical deployment challenges and shares key insights for scaling CAM
systems. The results highlight the potential of cloud-based infrastructure
perception to advance both outdoor and indoor intelligent transportation
systems.","Yufeng Yang, Minghao Ning, Keqi Shu, Aladdin Saleh, Ehsan Hashemi, Amir Khajepour",2025-05-27T18:51:45Z,2025-05-27T18:51:45Z,http://arxiv.org/abs/2505.21676v1,http://arxiv.org/pdf/2505.21676v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
EgoWalk: A Multimodal Dataset for Robot Navigation in the Wild,"Data-driven navigation algorithms are critically dependent on large-scale,
high-quality real-world data collection for successful training and robust
performance in realistic and uncontrolled conditions. To enhance the growing
family of navigation-related real-world datasets, we introduce EgoWalk - a
dataset of 50 hours of human navigation in a diverse set of indoor/outdoor,
varied seasons, and location environments. Along with the raw and Imitation
Learning-ready data, we introduce several pipelines to automatically create
subsidiary datasets for other navigation-related tasks, namely natural language
goal annotations and traversability segmentation masks. Diversity studies, use
cases, and benchmarks for the proposed dataset are provided to demonstrate its
practical applicability.
  We openly release all data processing pipelines and the description of the
hardware platform used for data collection to support future research and
development in robot navigation systems.","Timur Akhtyamov, Mohamad Al Mdfaa, Javier Antonio Ramirez, Sergey Bakulin, German Devchich, Denis Fatykhov, Alexander Mazurov, Kristina Zipa, Malik Mohrat, Pavel Kolesnik, Ivan Sosin, Gonzalo Ferrer",2025-05-27T14:51:34Z,2025-05-27T14:51:34Z,http://arxiv.org/abs/2505.21282v1,http://arxiv.org/pdf/2505.21282v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"GET: Goal-directed Exploration and Targeting for Large-Scale Unknown
  Environments","Object search in large-scale, unstructured environments remains a fundamental
challenge in robotics, particularly in dynamic or expansive settings such as
outdoor autonomous exploration. This task requires robust spatial reasoning and
the ability to leverage prior experiences. While Large Language Models (LLMs)
offer strong semantic capabilities, their application in embodied contexts is
limited by a grounding gap in spatial reasoning and insufficient mechanisms for
memory integration and decision consistency.To address these challenges, we
propose GET (Goal-directed Exploration and Targeting), a framework that
enhances object search by combining LLM-based reasoning with experience-guided
exploration. At its core is DoUT (Diagram of Unified Thought), a reasoning
module that facilitates real-time decision-making through a role-based feedback
loop, integrating task-specific criteria and external memory. For repeated
tasks, GET maintains a probabilistic task map based on a Gaussian Mixture
Model, allowing for continual updates to object-location priors as environments
evolve.Experiments conducted in real-world, large-scale environments
demonstrate that GET improves search efficiency and robustness across multiple
LLMs and task settings, significantly outperforming heuristic and LLM-only
baselines. These results suggest that structured LLM integration provides a
scalable and generalizable approach to embodied decision-making in complex
environments.","Lanxiang Zheng, Ruidong Mei, Mingxin Wei, Hao Ren, Hui Cheng",2025-05-27T07:40:22Z,2025-05-28T10:29:18Z,http://arxiv.org/abs/2505.20828v2,http://arxiv.org/pdf/2505.20828v2.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"From Single Images to Motion Policies via Video-Generation Environment
  Representations","Autonomous robots typically need to construct representations of their
surroundings and adapt their motions to the geometry of their environment.
Here, we tackle the problem of constructing a policy model for collision-free
motion generation, consistent with the environment, from a single input RGB
image. Extracting 3D structures from a single image often involves monocular
depth estimation. Developments in depth estimation have given rise to large
pre-trained models such as DepthAnything. However, using outputs of these
models for downstream motion generation is challenging due to frustum-shaped
errors that arise. Instead, we propose a framework known as Video-Generation
Environment Representation (VGER), which leverages the advances of large-scale
video generation models to generate a moving camera video conditioned on the
input image. Frames of this video, which form a multiview dataset, are then
input into a pre-trained 3D foundation model to produce a dense point cloud. We
then introduce a multi-scale noise approach to train an implicit representation
of the environment structure and build a motion generation model that complies
with the geometry of the representation. We extensively evaluate VGER over a
diverse set of indoor and outdoor environments. We demonstrate its ability to
produce smooth motions that account for the captured geometry of a scene, all
from a single RGB input image.","Weiming Zhi, Ziyong Ma, Tianyi Zhang, Matthew Johnson-Roberson",2025-05-25T20:30:25Z,2025-05-25T20:30:25Z,http://arxiv.org/abs/2505.19306v1,http://arxiv.org/pdf/2505.19306v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"A Coarse to Fine 3D LiDAR Localization with Deep Local Features for Long
  Term Robot Navigation in Large Environments","The location of a robot is a key aspect in the field of mobile robotics. This
problem is particularly complex when the initial pose of the robot is unknown.
In order to find a solution, it is necessary to perform a global localization.
In this paper, we propose a method that addresses this problem using a
coarse-to-fine solution. The coarse localization relies on a probabilistic
approach of the Monte Carlo Localization (MCL) method, with the contribution of
a robust deep learning model, the MinkUNeXt neural network, to produce a robust
description of point clouds of a 3D LiDAR within the observation model. For
fine localization, global point cloud registration has been implemented.
MinkUNeXt aids this by exploiting the outputs of its intermediate layers to
produce deep local features for each point in a scan. These features facilitate
precise alignment between the current sensor observation and one of the point
clouds on the map. The proposed MCL method incorporating Deep Local Features
for fine localization is termed MCL-DLF. Alternatively, a classical ICP method
has been implemented for this precise localization aiming at comparison
purposes. This method is termed MCL-ICP. In order to validate the performance
of MCL-DLF method, it has been tested on publicly available datasets such as
the NCLT dataset, which provides seasonal large-scale environments.
Additionally, tests have been also performed with own data (UMH) that also
includes seasonal variations on large indoor/outdoor scenarios. The results,
which were compared with established state-of-the-art methodologies,
demonstrate that the MCL-DLF method obtains an accurate estimate of the robot
localization in dynamic environments despite changes in environmental
conditions. For reproducibility purposes, the code is publicly available at
https://github.com/miriammaximo/MCL-DLF.git","Míriam Máximo, Antonio Santo, Arturo Gil, Mónica Ballesta, David Valiente",2025-05-23T19:53:46Z,2025-05-23T19:53:46Z,http://arxiv.org/abs/2505.18340v1,http://arxiv.org/pdf/2505.18340v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Distance Estimation in Outdoor Driving Environments Using Phase-only
  Correlation Method with Event Cameras","With the growing adoption of autonomous driving, the advancement of sensor
technology is crucial for ensuring safety and reliable operation. Sensor fusion
techniques that combine multiple sensors such as LiDAR, radar, and cameras have
proven effective, but the integration of multiple devices increases both
hardware complexity and cost. Therefore, developing a single sensor capable of
performing multiple roles is highly desirable for cost-efficient and scalable
autonomous driving systems.
  Event cameras have emerged as a promising solution due to their unique
characteristics, including high dynamic range, low latency, and high temporal
resolution. These features enable them to perform well in challenging lighting
conditions, such as low-light or backlit environments. Moreover, their ability
to detect fine-grained motion events makes them suitable for applications like
pedestrian detection and vehicle-to-infrastructure communication via visible
light.
  In this study, we present a method for distance estimation using a monocular
event camera and a roadside LED bar. By applying a phase-only correlation
technique to the event data, we achieve sub-pixel precision in detecting the
spatial shift between two light sources. This enables accurate
triangulation-based distance estimation without requiring stereo vision. Field
experiments conducted in outdoor driving scenarios demonstrated that the
proposed approach achieves over 90% success rate with less than 0.5-meter error
for distances ranging from 20 to 60 meters.
  Future work includes extending this method to full position estimation by
leveraging infrastructure such as smart poles equipped with LEDs, enabling
event-camera-based vehicles to determine their own position in real time. This
advancement could significantly enhance navigation accuracy, route
optimization, and integration into intelligent transportation systems.","Masataka Kobayashi, Shintaro Shiba, Quan Kong, Norimasa Kobori, Tsukasa Shimizu, Shan Lu, Takaya Yamazato",2025-05-23T07:44:33Z,2025-05-23T07:44:33Z,http://arxiv.org/abs/2505.17582v1,http://arxiv.org/pdf/2505.17582v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Evaluation of Mobile Environment for Vehicular Visible Light
  Communication Using Multiple LEDs and Event Cameras","In the fields of Advanced Driver Assistance Systems (ADAS) and Autonomous
Driving (AD), sensors that serve as the ``eyes'' for sensing the vehicle's
surrounding environment are essential. Traditionally, image sensors and LiDAR
have played this role. However, a new type of vision sensor, event cameras, has
recently attracted attention. Event cameras respond to changes in the
surrounding environment (e.g., motion), exhibit strong robustness against
motion blur, and perform well in high dynamic range environments, which are
desirable in robotics applications. Furthermore, the asynchronous and
low-latency principles of data acquisition make event cameras suitable for
optical communication. By adding communication functionality to event cameras,
it becomes possible to utilize I2V communication to immediately share
information about forward collisions, sudden braking, and road conditions,
thereby contributing to hazard avoidance. Additionally, receiving information
such as signal timing and traffic volume enables speed adjustment and optimal
route selection, facilitating more efficient driving. In this study, we
construct a vehicle visible light communication system where event cameras are
receivers, and multiple LEDs are transmitters. In driving scenes, the system
tracks the transmitter positions and separates densely packed LED light sources
using pilot sequences based on Walsh-Hadamard codes. As a result, outdoor
vehicle experiments demonstrate error-free communication under conditions where
the transmitter-receiver distance was within 40 meters and the vehicle's
driving speed was 30 km/h (8.3 m/s).","Ryota Soga, Shintaro Shiba, Quan Kong, Norimasa Kobori, Tsukasa Shimizu, Shan Lu, Takaya Yamazato",2025-05-21T11:54:56Z,2025-05-21T11:54:56Z,http://arxiv.org/abs/2505.15412v1,http://arxiv.org/pdf/2505.15412v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"R3GS: Gaussian Splatting for Robust Reconstruction and Relocalization in
  Unconstrained Image Collections","We propose R3GS, a robust reconstruction and relocalization framework
tailored for unconstrained datasets. Our method uses a hybrid representation
during training. Each anchor combines a global feature from a convolutional
neural network (CNN) with a local feature encoded by the multiresolution hash
grids [2]. Subsequently, several shallow multi-layer perceptrons (MLPs) predict
the attributes of each Gaussians, including color, opacity, and covariance. To
mitigate the adverse effects of transient objects on the reconstruction
process, we ffne-tune a lightweight human detection network. Once ffne-tuned,
this network generates a visibility map that efffciently generalizes to other
transient objects (such as posters, banners, and cars) with minimal need for
further adaptation. Additionally, to address the challenges posed by sky
regions in outdoor scenes, we propose an effective sky-handling technique that
incorporates a depth prior as a constraint. This allows the inffnitely distant
sky to be represented on the surface of a large-radius sky sphere,
signiffcantly reducing ffoaters caused by errors in sky reconstruction.
Furthermore, we introduce a novel relocalization method that remains robust to
changes in lighting conditions while estimating the camera pose of a given
image within the reconstructed 3DGS scene. As a result, R3GS significantly
enhances rendering ffdelity, improves both training and rendering efffciency,
and reduces storage requirements. Our method achieves state-of-the-art
performance compared to baseline methods on in-the-wild datasets. The code will
be made open-source following the acceptance of the paper.","Xu yan, Zhaohui Wang, Rong Wei, Jingbo Yu, Dong Li, Xiangde Liu",2025-05-21T09:25:22Z,2025-05-21T09:25:22Z,http://arxiv.org/abs/2505.15294v1,http://arxiv.org/pdf/2505.15294v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"RadarRGBD A Multi-Sensor Fusion Dataset for Perception with RGB-D and
  mmWave Radar","Multi-sensor fusion has significant potential in perception tasks for both
indoor and outdoor environments. Especially under challenging conditions such
as adverse weather and low-light environments, the combined use of
millimeter-wave radar and RGB-D sensors has shown distinct advantages. However,
existing multi-sensor datasets in the fields of autonomous driving and robotics
often lack high-quality millimeter-wave radar data. To address this gap, we
present a new multi-sensor dataset:RadarRGBD. This dataset includes RGB-D data,
millimeter-wave radar point clouds, and raw radar matrices, covering various
indoor and outdoor scenes, as well as low-light environments. Compared to
existing datasets, RadarRGBD employs higher-resolution millimeter-wave radar
and provides raw data, offering a new research foundation for the fusion of
millimeter-wave radar and visual sensors. Furthermore, to tackle the noise and
gaps in depth maps captured by Kinect V2 due to occlusions and mismatches, we
fine-tune an open-source relative depth estimation framework, incorporating the
absolute depth information from the dataset for depth supervision. We also
introduce pseudo-relative depth scale information to further optimize the
global depth scale estimation. Experimental results demonstrate that the
proposed method effectively fills in missing regions in sensor data. Our
dataset and related documentation will be publicly available at:
https://github.com/song4399/RadarRGBD.","Tieshuai Song, Jiandong Ye, Ao Guo, Guidong He, Bin Yang",2025-05-21T05:30:04Z,2025-05-21T05:30:04Z,http://arxiv.org/abs/2505.15860v1,http://arxiv.org/pdf/2505.15860v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Robotic Monitoring of Colorimetric Leaf Sensors for Precision
  Agriculture","Common remote sensing modalities (RGB, multispectral, hyperspectral imaging
or LiDAR) are often used to indirectly measure crop health and do not directly
capture plant stress indicators. Commercially available direct leaf sensors are
bulky, powered electronics that are expensive and interfere with crop growth.
In contrast, low-cost, passive and bio-degradable leaf sensors offer an
opportunity to advance real-time monitoring as they directly interface with the
crop surface while not interfering with crop growth. To this end, we co-design
a sensor-detector system, where the sensor is a passive colorimetric leaf
sensor that directly measures crop health in a precision agriculture setting,
and the detector autonomously obtains optical signals from these leaf sensors.
The detector comprises a low size weight and power (SWaP) mobile ground robot
with an onboard monocular RGB camera and object detector to localize each leaf
sensor, as well as a hyperspectral camera with a motorized mirror and halogen
light to acquire hyperspectral images. The sensor's crop health-dependent
optical signals can be extracted from the hyperspectral images. The
proof-of-concept system is demonstrated in row-crop environments both indoors
and outdoors where it is able to autonomously navigate, locate and obtain a
hyperspectral image of all leaf sensors present, and acquire interpretable
spectral resonance with 80 $\%$ accuracy within a required retrieval distance
from the sensor.","Malakhi Hopkins, Alice Kate Li, Shobhita Kramadhati, Jackson Arnold, Akhila Mallavarapu, Chavez Lawrence, Varun Murali, Sanjeev J. Koppal, Cherie R. Kagan, Vijay Kumar",2025-05-20T04:26:45Z,2025-07-18T06:12:42Z,http://arxiv.org/abs/2505.13916v2,http://arxiv.org/pdf/2505.13916v2.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Toward Real-World Cooperative and Competitive Soccer with Quadrupedal
  Robot Teams","Achieving coordinated teamwork among legged robots requires both fine-grained
locomotion control and long-horizon strategic decision-making. Robot soccer
offers a compelling testbed for this challenge, combining dynamic, competitive,
and multi-agent interactions. In this work, we present a hierarchical
multi-agent reinforcement learning (MARL) framework that enables fully
autonomous and decentralized quadruped robot soccer. First, a set of highly
dynamic low-level skills is trained for legged locomotion and ball
manipulation, such as walking, dribbling, and kicking. On top of these, a
high-level strategic planning policy is trained with Multi-Agent Proximal
Policy Optimization (MAPPO) via Fictitious Self-Play (FSP). This learning
framework allows agents to adapt to diverse opponent strategies and gives rise
to sophisticated team behaviors, including coordinated passing, interception,
and dynamic role allocation. With an extensive ablation study, the proposed
learning method shows significant advantages in the cooperative and competitive
multi-agent soccer game. We deploy the learned policies to real quadruped
robots relying solely on onboard proprioception and decentralized localization,
with the resulting system supporting autonomous robot-robot and robot-human
soccer matches on indoor and outdoor soccer courts.","Zhi Su, Yuman Gao, Emily Lukas, Yunfei Li, Jiaze Cai, Faris Tulbah, Fei Gao, Chao Yu, Zhongyu Li, Yi Wu, Koushil Sreenath",2025-05-20T02:20:54Z,2025-08-30T02:08:59Z,http://arxiv.org/abs/2505.13834v2,http://arxiv.org/pdf/2505.13834v2.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Depth Transfer: Learning to See Like a Simulator for Real-World Drone
  Navigation","Sim-to-real transfer is a fundamental challenge in robot reinforcement
learning. Discrepancies between simulation and reality can significantly impair
policy performance, especially if it receives high-dimensional inputs such as
dense depth estimates from vision. We propose a novel depth transfer method
based on domain adaptation to bridge the visual gap between simulated and
real-world depth data. A Variational Autoencoder (VAE) is first trained to
encode ground-truth depth images from simulation into a latent space, which
serves as input to a reinforcement learning (RL) policy. During deployment, the
encoder is refined to align stereo depth images with this latent space,
enabling direct policy transfer without fine-tuning. We apply our method to the
task of autonomous drone navigation through cluttered environments. Experiments
in IsaacGym show that our method nearly doubles the obstacle avoidance success
rate when switching from ground-truth to stereo depth input. Furthermore, we
demonstrate successful transfer to the photo-realistic simulator AvoidBench
using only IsaacGym-generated stereo data, achieving superior performance
compared to state-of-the-art baselines. Real-world evaluations in both indoor
and outdoor environments confirm the effectiveness of our approach, enabling
robust and generalizable depth-based navigation across diverse domains.","Hang Yu, Christophe De Wagter, Guido C. H. E de Croon",2025-05-18T13:53:53Z,2025-05-18T13:53:53Z,http://arxiv.org/abs/2505.12428v1,http://arxiv.org/pdf/2505.12428v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Real-Time Spatial Reasoning by Mobile Robots for Reconstruction and
  Navigation in Dynamic LiDAR Scenes","Our brain has an inner global positioning system which enables us to sense
and navigate 3D spaces in real time. Can mobile robots replicate such a
biological feat in a dynamic environment? We introduce the first spatial
reasoning framework for real-time surface reconstruction and navigation that is
designed for outdoor LiDAR scanning data captured by ground mobile robots and
capable of handling moving objects such as pedestrians. Our
reconstruction-based approach is well aligned with the critical cellular
functions performed by the border vector cells (BVCs) over all layers of the
medial entorhinal cortex (MEC) for surface sensing and tracking. To address the
challenges arising from blurred boundaries resulting from sparse single-frame
LiDAR points and outdated data due to object movements, we integrate real-time
single-frame mesh reconstruction, via visibility reasoning, with robot
navigation assistance through on-the-fly 3D free space determination. This
enables continuous and incremental updates of the scene and free space across
multiple frames. Key to our method is the utilization of line-of-sight (LoS)
vectors from LiDAR, which enable real-time surface normal estimation, as well
as robust and instantaneous per-voxel free space updates. We showcase two
practical applications: real-time 3D scene reconstruction and autonomous
outdoor robot navigation in real-world conditions. Comprehensive experiments on
both synthetic and real scenes highlight our method's superiority in speed and
quality over existing real-time LiDAR processing approaches.","Pengdi Huang, Mingyang Wang, Huan Tian, Minglun Gong, Hao Zhang, Hui Huang",2025-05-18T07:12:51Z,2025-05-18T07:12:51Z,http://arxiv.org/abs/2505.12267v1,http://arxiv.org/pdf/2505.12267v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Gaussian Splatting as a Unified Representation for Autonomy in
  Unstructured Environments","In this work, we argue that Gaussian splatting is a suitable unified
representation for autonomous robot navigation in large-scale unstructured
outdoor environments. Such environments require representations that can
capture complex structures while remaining computationally tractable for
real-time navigation. We demonstrate that the dense geometric and photometric
information provided by a Gaussian splatting representation is useful for
navigation in unstructured environments. Additionally, semantic information can
be embedded in the Gaussian map to enable large-scale task-driven navigation.
From the lessons learned through our experiments, we highlight several
challenges and opportunities arising from the use of such a representation for
robot autonomy.","Dexter Ong, Yuezhan Tao, Varun Murali, Igor Spasojevic, Vijay Kumar, Pratik Chaudhari",2025-05-17T02:49:13Z,2025-05-17T02:49:13Z,http://arxiv.org/abs/2505.11794v1,http://arxiv.org/pdf/2505.11794v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Technical Report for ICRA 2025 GOOSE 2D Semantic Segmentation Challenge:
  Boosting Off-Road Segmentation via Photometric Distortion and Exponential
  Moving Average","We report on the application of a high-capacity semantic segmentation
pipeline to the GOOSE 2D Semantic Segmentation Challenge for unstructured
off-road environments. Using a FlashInternImage-B backbone together with a
UPerNet decoder, we adapt established techniques, rather than designing new
ones, to the distinctive conditions of off-road scenes. Our training recipe
couples strong photometric distortion augmentation (to emulate the wide
lighting variations of outdoor terrain) with an Exponential Moving Average
(EMA) of weights for better generalization. Using only the GOOSE training
dataset, we achieve 88.8\% mIoU on the validation set.","Wonjune Kim, Lae-kyoung Lee, Su-Yong An",2025-05-17T00:29:17Z,2025-05-17T00:29:17Z,http://arxiv.org/abs/2505.11769v1,http://arxiv.org/pdf/2505.11769v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
SHIELD: Safety on Humanoids via CBFs In Expectation on Learned Dynamics,"Robot learning has produced remarkably effective ``black-box'' controllers
for complex tasks such as dynamic locomotion on humanoids. Yet ensuring dynamic
safety, i.e., constraint satisfaction, remains challenging for such policies.
Reinforcement learning (RL) embeds constraints heuristically through reward
engineering, and adding or modifying constraints requires retraining.
Model-based approaches, like control barrier functions (CBFs), enable runtime
constraint specification with formal guarantees but require accurate dynamics
models. This paper presents SHIELD, a layered safety framework that bridges
this gap by: (1) training a generative, stochastic dynamics residual model
using real-world data from hardware rollouts of the nominal controller,
capturing system behavior and uncertainties; and (2) adding a safety layer on
top of the nominal (learned locomotion) controller that leverages this model
via a stochastic discrete-time CBF formulation enforcing safety constraints in
probability. The result is a minimally-invasive safety layer that can be added
to the existing autonomy stack to give probabilistic guarantees of safety that
balance risk and performance. In hardware experiments on an Unitree G1
humanoid, SHIELD enables safe navigation (obstacle avoidance) through varied
indoor and outdoor environments using a nominal (unknown) RL controller and
onboard perception.","Lizhi Yang, Blake Werner, Ryan K. Cosner, David Fridovich-Keil, Preston Culbertson, Aaron D. Ames",2025-05-16T17:57:03Z,2025-08-01T07:24:37Z,http://arxiv.org/abs/2505.11494v2,http://arxiv.org/pdf/2505.11494v2.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"GRoQ-LoCO: Generalist and Robot-agnostic Quadruped Locomotion Control
  using Offline Datasets","Recent advancements in large-scale offline training have demonstrated the
potential of generalist policy learning for complex robotic tasks. However,
applying these principles to legged locomotion remains a challenge due to
continuous dynamics and the need for real-time adaptation across diverse
terrains and robot morphologies. In this work, we propose GRoQ-LoCO, a
scalable, attention-based framework that learns a single generalist locomotion
policy across multiple quadruped robots and terrains, relying solely on offline
datasets. Our approach leverages expert demonstrations from two distinct
locomotion behaviors - stair traversal (non-periodic gaits) and flat terrain
traversal (periodic gaits) - collected across multiple quadruped robots, to
train a generalist model that enables behavior fusion. Crucially, our framework
operates solely on proprioceptive data from all robots without incorporating
any robot-specific encodings. The policy is directly deployable on an Intel i7
nuc, producing low-latency control outputs without any test-time optimization.
Our extensive experiments demonstrate zero-shot transfer across highly diverse
quadruped robots and terrains, including hardware deployment on the Unitree
Go1, a commercially available 12kg robot. Notably, we evaluate challenging
cross-robot training setups where different locomotion skills are unevenly
distributed across robots, yet observe successful transfer of both flat walking
and stair traversal behaviors to all robots at test time. We also show
preliminary walking on Stoch 5, a 70kg quadruped, on flat and outdoor terrains
without requiring any fine tuning. These results demonstrate the potential of
offline, data-driven learning to generalize locomotion across diverse quadruped
morphologies and behaviors.","Narayanan PP, Sarvesh Prasanth Venkatesan, Srinivas Kantha Reddy, Shishir Kolathaya",2025-05-16T08:17:01Z,2025-05-24T18:28:59Z,http://arxiv.org/abs/2505.10973v3,http://arxiv.org/pdf/2505.10973v3.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Robust 2D lidar-based SLAM in arboreal environments without IMU/GNSS,"Simultaneous localization and mapping (SLAM) approaches for mobile robots
remains challenging in forest or arboreal fruit farming environments, where
tree canopies obstruct Global Navigation Satellite Systems (GNSS) signals.
Unlike indoor settings, these agricultural environments possess additional
challenges due to outdoor variables such as foliage motion and illumination
variability. This paper proposes a solution based on 2D lidar measurements,
which requires less processing and storage, and is more cost-effective, than
approaches that employ 3D lidars. Utilizing the modified Hausdorff distance
(MHD) metric, the method can solve the scan matching robustly and with high
accuracy without needing sophisticated feature extraction. The method's
robustness was validated using public datasets and considering various metrics,
facilitating meaningful comparisons for future research. Comparative
evaluations against state-of-the-art algorithms, particularly A-LOAM, show that
the proposed approach achieves lower positional and angular errors while
maintaining higher accuracy and resilience in GNSS-denied settings. This work
contributes to the advancement of precision agriculture by enabling reliable
and autonomous navigation in challenging outdoor environments.","Paola Nazate-Burgos, Miguel Torres-Torriti, Sergio Aguilera-Marinovic, Tito Arévalo, Shoudong Huang, Fernando Auat Cheein",2025-05-16T04:39:48Z,2025-05-16T04:39:48Z,http://arxiv.org/abs/2505.10847v1,http://arxiv.org/pdf/2505.10847v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Large-Scale Gaussian Splatting SLAM,"The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian
Splatting (3DGS) have shown encouraging and impressive results for visual SLAM.
However, most representative methods require RGBD sensors and are only
available for indoor environments. The robustness of reconstruction in
large-scale outdoor scenarios remains unexplored. This paper introduces a
large-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The
proposed LSG-SLAM employs a multi-modality strategy to estimate prior poses
under large view changes. In tracking, we introduce feature-alignment warping
constraints to alleviate the adverse effects of appearance similarity in
rendering losses. For the scalability of large-scale scenarios, we introduce
continuous Gaussian Splatting submaps to tackle unbounded scenes with limited
memory. Loops are detected between GS submaps by place recognition and the
relative pose between looped keyframes is optimized utilizing rendering and
feature warping losses. After the global optimization of camera poses and
Gaussian points, a structure refinement module enhances the reconstruction
quality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM
achieves superior performance over existing Neural, 3DGS-based, and even
traditional approaches. Project page: https://lsg-slam.github.io.","Zhe Xin, Chenyang Wu, Penghui Huang, Yanyong Zhang, Yinian Mao, Guoquan Huang",2025-05-15T03:00:32Z,2025-05-15T03:00:32Z,http://arxiv.org/abs/2505.09915v1,http://arxiv.org/pdf/2505.09915v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged
  Information Guidance","Learning navigation in dynamic open-world environments is an important yet
challenging skill for robots. Most previous methods rely on precise
localization and mapping or learn from expensive real-world demonstrations. In
this paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end
framework trained solely in simulation and can zero-shot transfer to different
embodiments in diverse real-world environments. The key ingredient of NavDP's
network is the combination of diffusion-based trajectory generation and a
critic function for trajectory selection, which are conditioned on only local
observation tokens encoded from a shared policy transformer. Given the
privileged information of the global environment in simulation, we scale up the
demonstrations of good quality to train the diffusion policy and formulate the
critic value function targets with contrastive negative samples. Our
demonstration generation approach achieves about 2,500 trajectories/GPU per
day, 20$\times$ more efficient than real-world data collection, and results in
a large-scale navigation dataset with 363.2km trajectories across 1244 scenes.
Trained with this simulation dataset, NavDP achieves state-of-the-art
performance and consistently outstanding generalization capability on
quadruped, wheeled, and humanoid robots in diverse indoor and outdoor
environments. In addition, we present a preliminary attempt at using Gaussian
Splatting to make in-domain real-to-sim fine-tuning to further bridge the
sim-to-real gap. Experiments show that adding such real-to-sim data can improve
the success rate by 30\% without hurting its generalization capability.","Wenzhe Cai, Jiaqi Peng, Yuqiang Yang, Yujian Zhang, Meng Wei, Hanqing Wang, Yilun Chen, Tai Wang, Jiangmiao Pang",2025-05-13T16:20:28Z,2025-05-15T18:11:03Z,http://arxiv.org/abs/2505.08712v2,http://arxiv.org/pdf/2505.08712v2.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"TPT-Bench: A Large-Scale, Long-Term and Robot-Egocentric Dataset for
  Benchmarking Target Person Tracking","Tracking a target person from robot-egocentric views is crucial for
developing autonomous robots that provide continuous personalized assistance or
collaboration in Human-Robot Interaction (HRI) and Embodied AI. However, most
existing target person tracking (TPT) benchmarks are limited to controlled
laboratory environments with few distractions, clean backgrounds, and
short-term occlusions. In this paper, we introduce a large-scale dataset
designed for TPT in crowded and unstructured environments, demonstrated through
a robot-person following task. The dataset is collected by a human pushing a
sensor-equipped cart while following a target person, capturing human-like
following behavior and emphasizing long-term tracking challenges, including
frequent occlusions and the need for re-identification from numerous
pedestrians. It includes multi-modal data streams, including odometry, 3D
LiDAR, IMU, panoramic images, and RGB-D images, along with exhaustively
annotated 2D bounding boxes of the target person across 48 sequences, both
indoors and outdoors. Using this dataset and visual annotations, we perform
extensive experiments with existing SOTA TPT methods, offering a thorough
analysis of their limitations and suggesting future research directions.","Hanjing Ye, Yu Zhan, Weixi Situ, Guangcheng Chen, Jingwen Yu, Ziqi Zhao, Kuanqi Cai, Arash Ajoudani, Hong Zhang",2025-05-12T11:10:24Z,2025-07-09T13:47:58Z,http://arxiv.org/abs/2505.07446v2,http://arxiv.org/pdf/2505.07446v2.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"CityNavAgent: Aerial Vision-and-Language Navigation with Hierarchical
  Semantic Planning and Global Memory","Aerial vision-and-language navigation (VLN), requiring drones to interpret
natural language instructions and navigate complex urban environments, emerges
as a critical embodied AI challenge that bridges human-robot interaction, 3D
spatial reasoning, and real-world deployment. Although existing ground VLN
agents achieved notable results in indoor and outdoor settings, they struggle
in aerial VLN due to the absence of predefined navigation graphs and the
exponentially expanding action space in long-horizon exploration. In this work,
we propose \textbf{CityNavAgent}, a large language model (LLM)-empowered agent
that significantly reduces the navigation complexity for urban aerial VLN.
Specifically, we design a hierarchical semantic planning module (HSPM) that
decomposes the long-horizon task into sub-goals with different semantic levels.
The agent reaches the target progressively by achieving sub-goals with
different capacities of the LLM. Additionally, a global memory module storing
historical trajectories into a topological graph is developed to simplify
navigation for visited targets. Extensive benchmark experiments show that our
method achieves state-of-the-art performance with significant improvement.
Further experiments demonstrate the effectiveness of different modules of
CityNavAgent for aerial VLN in continuous city environments. The code is
available at \href{https://github.com/VinceOuti/CityNavAgent}{link}.","Weichen Zhang, Chen Gao, Shiquan Yu, Ruiying Peng, Baining Zhao, Qian Zhang, Jinqiang Cui, Xinlei Chen, Yong Li",2025-05-08T20:01:35Z,2025-05-08T20:01:35Z,http://arxiv.org/abs/2505.05622v1,http://arxiv.org/pdf/2505.05622v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Learning to Drive Anywhere with Model-Based Reannotation,"Developing broadly generalizable visual navigation policies for robots is a
significant challenge, primarily constrained by the availability of
large-scale, diverse training data. While curated datasets collected by
researchers offer high quality, their limited size restricts policy
generalization. To overcome this, we explore leveraging abundant, passively
collected data sources, including large volumes of crowd-sourced teleoperation
data and unlabeled YouTube videos, despite their potential for lower quality or
missing action labels. We propose Model-Based ReAnnotation (MBRA), a framework
that utilizes a learned short-horizon, model-based expert model to relabel or
generate high-quality actions for these passive datasets. This relabeled data
is then distilled into LogoNav, a long-horizon navigation policy conditioned on
visual goals or GPS waypoints. We demonstrate that LogoNav, trained using
MBRA-processed data, achieves state-of-the-art performance, enabling robust
navigation over distances exceeding 300 meters in previously unseen indoor and
outdoor environments. Our extensive real-world evaluations, conducted across a
fleet of robots (including quadrupeds) in six cities on three continents,
validate the policy's ability to generalize and navigate effectively even
amidst pedestrians in crowded settings.","Noriaki Hirose, Lydia Ignatova, Kyle Stachowicz, Catherine Glossop, Sergey Levine, Dhruv Shah",2025-05-08T18:43:39Z,2025-05-12T01:50:31Z,http://arxiv.org/abs/2505.05592v2,http://arxiv.org/pdf/2505.05592v2.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"One Patch to Rule Them All: Transforming Static Patches into Dynamic
  Attacks in the Physical World","Numerous methods have been proposed to generate physical adversarial patches
(PAPs) against real-world machine learning systems. However, each existing PAP
typically supports only a single, fixed attack goal, and switching to a
different objective requires re-generating and re-deploying a new PAP. This
rigidity limits their practicality in dynamic environments like autonomous
driving, where traffic conditions and attack goals can change rapidly. For
example, if no obstacles are present around the target vehicle, the attack may
fail to cause meaningful consequences.
  To overcome this limitation, we propose SwitchPatch, a novel PAP that is
static yet enables dynamic and controllable attack outcomes based on real-time
scenarios. Attackers can alter pre-defined conditions, e.g., by projecting
different natural-color lights onto SwitchPatch to seamlessly switch between
attack goals. Unlike prior work, SwitchPatch does not require re-generation or
re-deployment for different objectives, significantly reducing cost and
complexity. Furthermore, SwitchPatch remains benign when the enabling
conditions are absent, enhancing its stealth.
  We evaluate SwitchPatch on two key tasks: traffic sign recognition
(classification and detection) and depth estimation. First, we conduct
theoretical analysis and empirical studies to demonstrate the feasibility of
SwitchPatch and explore how many goals it can support using techniques like
color light projection and occlusion. Second, we perform simulation-based
experiments and ablation studies to verify its effectiveness and
transferability. Third, we conduct outdoor tests using a Unmanned Ground
Vehicle (UGV) to confirm its robustness in the physical world. Overall,
SwitchPatch introduces a flexible and practical adversarial strategy that can
be adapted to diverse tasks and real-world conditions.","Xingshuo Han, Chen Ling, Shiyi Yao, Haozhao Wang, Hangcheng Liu, Yutong Wu, Shengmin Xu, Changhai Ou, Xinyi Huang, Tianwei Zhang",2025-06-10T06:12:21Z,2025-06-10T06:12:21Z,http://arxiv.org/abs/2506.08482v1,http://arxiv.org/pdf/2506.08482v1.pdf,all:outdoor AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2025
Collection: Datasets from AFAR Challenge,"This paper presents a comprehensive real-world and Digital Twin (DT) dataset
collected as part of the Find A Rover (AFAR) Challenge, organized by the NSF
Aerial Experimentation and Research Platform for Advanced Wireless (AERPAW)
testbed and hosted at the Lake Wheeler Field in Raleigh, North Carolina. The
AFAR Challenge was a competition involving five finalist university teams,
focused on promoting innovation in UAV-assisted radio frequency (RF) source
localization. Participating teams were tasked with designing UAV flight
trajectories and localization algorithms to detect the position of a hidden
unmanned ground vehicle (UGV), also referred to as a rover, emitting wireless
probe signals generated by GNU Radio. The competition was structured to
evaluate solutions in a DT environment first, followed by deployment and
testing in AERPAW's outdoor wireless testbed. For each team, the UGV was placed
at three different positions, resulting in a total of 30 datasets, 15 collected
in a DT simulation environment and 15 in a physical outdoor testbed. Each
dataset contains time-synchronized measurements of received signal strength
(RSS), received signal quality (RSQ), GPS coordinates, UAV velocity, and UAV
orientation (roll, pitch, and yaw). Data is organized into structured folders
by team, environment (DT and real-world), and UGV location. The dataset
supports research in UAV-assisted RF source localization, air-to-ground (A2G)
wireless propagation modeling, trajectory optimization, signal prediction,
autonomous navigation, and DT validation. With approximately 300k
time-synchronized samples collected from real-world experiments, the dataset
provides a substantial foundation for training and evaluating deep learning
(DL) models. Overall, the AFAR dataset serves as a valuable resource for
advancing robust, real-world solutions in UAV-enabled wireless communications
and sensing systems.","Saad Masrur, Ozgur Ozdemir, Anil Gurses, Ismail Guvenc, Mihail L. Sichitiu, Rudra Dutta, Magreth Mushi, homas Zajkowski, Cole Dickerson, Gautham Reddy, Sergio Vargas Villar, Chau-Wai Wong, Baisakhi Chatterjee, Sonali Chaudhari, Zhizhen Li, Yuchen Liu, Paul Kudyba, Haijian Sun, Jaya Sravani Mandapaka, Kamesh Namuduri, Weijie Wang, Fraida Fund",2025-05-11T03:22:48Z,2025-05-11T03:22:48Z,http://arxiv.org/abs/2505.06823v1,http://arxiv.org/pdf/2505.06823v1.pdf,all:outdoor AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2025
"ASAP-MO:Advanced Situational Awareness and Perception for
  Mission-critical Operations","Deploying robotic missions can be challenging due to the complexity of
controlling robots with multiple degrees of freedom, fusing diverse sensory
inputs, and managing communication delays and interferences. In nuclear
inspection, robots can be crucial in assessing environments where human
presence is limited, requiring precise teleoperation and coordination.
Teleoperation requires extensive training, as operators must process multiple
outputs while ensuring safe interaction with critical assets. These challenges
are amplified when operating a fleet of heterogeneous robots across multiple
environments, as each robot may have distinct control interfaces, sensory
systems, and operational constraints. Efficient coordination in such settings
remains an open problem. This paper presents a field report on how we
integrated robot fleet capabilities - including mapping, localization, and
telecommunication - toward a joint mission. We simulated a nuclear inspection
scenario for exposed areas, using lights to represent a radiation source. We
deployed two Unmanned Ground Vehicles (UGVs) tasked with mapping indoor and
outdoor environments while remotely controlled from a single base station.
Despite having distinct operational goals, the robots produced a unified map
output, demonstrating the feasibility of coordinated multi-robot missions. Our
results highlight key operational challenges and provide insights into
improving adaptability and situational awareness in remote robotic deployments.","Veronica Vannini, William Dubois, Olivier Gamache, Jean-Michel Fortin, Nicolas Samson, Effie Daum, François Pomerleau, Edith Brotherton",2025-05-02T19:19:40Z,2025-06-20T15:42:53Z,http://arxiv.org/abs/2505.01547v2,http://arxiv.org/pdf/2505.01547v2.pdf,all:outdoor AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2025
"ShanghaiTech Mapping Robot is All You Need: Robot System for Collecting
  Universal Ground Vehicle Datasets","This paper presents the ShanghaiTech Mapping Robot, a state-of-the-art
unmanned ground vehicle (UGV) designed for collecting comprehensive
multi-sensor datasets to support research in robotics, Simultaneous
Localization and Mapping (SLAM), computer vision, and autonomous driving. The
robot is equipped with a wide array of sensors including RGB cameras, RGB-D
cameras, event-based cameras, IR cameras, LiDARs, mmWave radars, IMUs,
ultrasonic range finders, and a GNSS RTK receiver. The sensor suite is
integrated onto a specially designed mechanical structure with a centralized
power system and a synchronization mechanism to ensure spatial and temporal
alignment of the sensor data. A 16-node on-board computing cluster handles
sensor control, data collection, and storage. We describe the hardware and
software architecture of the robot in detail and discuss the calibration
procedures for the various sensors and investigate the interference for LiDAR
and RGB-D sensors. The capabilities of the platform are demonstrated through an
extensive outdoor dataset collected in a diverse campus environment.
Experiments with two LiDAR-based and two RGB-based SLAM approaches showcase the
potential of the dataset to support development and benchmarking for robotics.
To facilitate research, we make the dataset publicly available along with the
associated robot sensor calibration data:
https://slam-hive.net/wiki/ShanghaiTech_Datasets","Bowen Xu, Xiting Zhao, Delin Feng, Yuanyuan Yang, Sören Schwertfeger",2024-06-24T15:15:25Z,2024-11-14T16:12:56Z,http://arxiv.org/abs/2406.16713v4,http://arxiv.org/pdf/2406.16713v4.pdf,all:outdoor AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2024
"FIT-SLAM -- Fisher Information and Traversability estimation-based
  Active SLAM for exploration in 3D environments","Active visual SLAM finds a wide array of applications in GNSS-Denied
sub-terrain environments and outdoor environments for ground robots. To achieve
robust localization and mapping accuracy, it is imperative to incorporate the
perception considerations in the goal selection and path planning towards the
goal during an exploration mission. Through this work, we propose FIT-SLAM
(Fisher Information and Traversability estimation-based Active SLAM), a new
exploration method tailored for unmanned ground vehicles (UGVs) to explore 3D
environments. This approach is devised with the dual objectives of sustaining
an efficient exploration rate while optimizing SLAM accuracy. Initially, an
estimation of a global traversability map is conducted, which accounts for the
environmental constraints pertaining to traversability. Subsequently, we
propose a goal candidate selection approach along with a path planning method
towards this goal that takes into account the information provided by the
landmarks used by the SLAM backend to achieve robust localization and
successful path execution . The entire algorithm is tested and evaluated first
in a simulated 3D world, followed by a real-world environment and is compared
to pre-existing exploration methods. The results obtained during this
evaluation demonstrate a significant increase in the exploration rate while
effectively minimizing the localization covariance.","Suchetan Saravanan, Corentin Chauffaut, Caroline Chanel, Damien Vivet",2024-01-17T16:46:38Z,2024-01-17T16:46:38Z,http://arxiv.org/abs/2401.09322v1,http://arxiv.org/pdf/2401.09322v1.pdf,all:outdoor AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2024
Haris: an Advanced Autonomous Mobile Robot for Smart Parking Assistance,"This paper presents Haris, an advanced autonomous mobile robot system for
tracking the location of vehicles in crowded car parks using license plate
recognition. The system employs simultaneous localization and mapping (SLAM)
for autonomous navigation and precise mapping of the parking area, eliminating
the need for GPS dependency. In addition, the system utilizes a sophisticated
framework using computer vision techniques for object detection and automatic
license plate recognition (ALPR) for reading and associating license plate
numbers with location data. This information is subsequently synchronized with
a back-end service and made accessible to users via a user-friendly mobile app,
offering effortless vehicle location and alleviating congestion within the
parking facility. The proposed system has the potential to improve the
management of short-term large outdoor parking areas in crowded places such as
sports stadiums. The demo of the robot can be found on
https://youtu.be/ZkTCM35fxa0?si=QjggJuN7M1o3oifx.","Layth Hamad, Muhammad Asif Khan, Hamid Menouar, Fethi Filali, Amr Mohamed",2024-01-31T11:00:26Z,2024-01-31T11:00:26Z,http://arxiv.org/abs/2401.17741v1,http://arxiv.org/pdf/2401.17741v1.pdf,all:outdoor AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"AERIAL-CORE: AI-Powered Aerial Robots for Inspection and Maintenance of
  Electrical Power Infrastructures","Large-scale infrastructures are prone to deterioration due to age,
environmental influences, and heavy usage. Ensuring their safety through
regular inspections and maintenance is crucial to prevent incidents that can
significantly affect public safety and the environment. This is especially
pertinent in the context of electrical power networks, which, while essential
for energy provision, can also be sources of forest fires. Intelligent drones
have the potential to revolutionize inspection and maintenance, eliminating the
risks for human operators, increasing productivity, reducing inspection time,
and improving data collection quality. However, most of the current methods and
technologies in aerial robotics have been trialed primarily in indoor testbeds
or outdoor settings under strictly controlled conditions, always within the
line of sight of human operators. Additionally, these methods and technologies
have typically been evaluated in isolation, lacking comprehensive integration.
This paper introduces the first autonomous system that combines various
innovative aerial robots. This system is designed for extended-range
inspections beyond the visual line of sight, features aerial manipulators for
maintenance tasks, and includes support mechanisms for human operators working
at elevated heights. The paper further discusses the successful validation of
this system on numerous electrical power lines, with aerial robots executing
flights over 10 kilometers away from their ground control stations.","Anibal Ollero, Alejandro Suarez, Christos Papaioannidis, Ioannis Pitas, Juan M. Marredo, Viet Duong, Emad Ebeid, Vit Kratky, Martin Saska, Chloe Hanoune, Amr Afifi, Antonio Franchi, Charalampos Vourtsis, Dario Floreano, Goran Vasiljevic, Stjepan Bogdan, Alvaro Caballero, Fabio Ruggiero, Vincenzo Lippiello, Carlos Matilla, Giovanni Cioffi, Davide Scaramuzza, Jose R. Martinez-de-Dios, Begona C. Arrue, Carlos Martin, Krzysztof Zurad, Carlos Gaitan, Jacob Rodriguez, Antonio Munoz, Antidio Viguria",2024-01-04T16:34:27Z,2024-01-04T16:34:27Z,http://arxiv.org/abs/2401.02343v1,http://arxiv.org/pdf/2401.02343v1.pdf,all:outdoor AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"Near-Driven Autonomous Rover Navigation in Complex Environments:
  Extensions to Urban Search-and-Rescue and Industrial Inspection","This paper explores the use of an extended neuroevolutionary approach, based
on NeuroEvolution of Augmenting Topologies (NEAT), for autonomous robots in
dynamic environments associated with hazardous tasks like firefighting, urban
search-and-rescue (USAR), and industrial inspections. Building on previous
research, it expands the simulation environment to larger and more complex
settings, demonstrating NEAT's adaptability across different applications. By
integrating recent advancements in NEAT and reinforcement learning, the study
uses modern simulation frameworks for realism and hybrid algorithms for
optimization. Experimental results show that NEAT-evolved controllers achieve
success rates comparable to state-of-the-art deep reinforcement learning
methods, with superior structural adaptability. The agents reached ~80% success
in outdoor tests, surpassing baseline models. The paper also highlights the
benefits of transfer learning among tasks and evaluates the effectiveness of
NEAT in complex 3D navigation. Contributions include evaluating NEAT for
diverse autonomous applications and discussing real-world deployment
considerations, emphasizing the approach's potential as an alternative or
complement to deep reinforcement learning in autonomous navigation tasks.","Dhadkan Shrestha, Lincoln Bhattarai",2025-04-11T20:00:23Z,2025-04-11T20:00:23Z,http://arxiv.org/abs/2504.17794v1,http://arxiv.org/pdf/2504.17794v1.pdf,all:outdoor AND all:rover AND submittedDate:[202309062228 TO 202509052228],2025
"Singularity-Free Guiding Vector Field over Bézier's Curves Applied to
  Rovers Path Planning and Path Following","This paper presents a guidance algorithm for solving the problem of following
parametric paths, as well as a curvature-varying speed setpoint for land-based
car-type wheeled mobile robots (WMRs). The guidance algorithm relies on
Singularity-Free Guiding Vector Fields SF-GVF. This novel GVF approach expands
the desired robot path and the Guiding vector field to a higher dimensional
space, in which an angular control function can be found to ensure global
asymptotic convergence to the desired parametric path while avoiding field
singularities. In SF-GVF, paths should follow a parametric definition. This
feature makes using Bezier's curves attractive to define the robot's desired
patch. The curvature-varying speed setpoint, combined with the guidance
algorithm, eases the convergence to the path when physical restrictions exist,
such as minimal turning radius or maximal lateral acceleration. We provide
theoretical results, simulations, and outdoor experiments using a WMR platform
assembled with off-the-shelf components.","Alfredo González-Calvin, Lía García-Pérez, Juan Jiménez",2024-12-17T15:56:40Z,2024-12-17T15:56:40Z,http://arxiv.org/abs/2412.13033v1,http://arxiv.org/pdf/2412.13033v1.pdf,all:outdoor AND all:rover AND submittedDate:[202309062228 TO 202509052228],2024
ROVER: A Multi-Season Dataset for Visual SLAM,"Robust SLAM is a crucial enabler for autonomous navigation in natural,
semi-structured environments such as parks and gardens. However, these
environments present unique challenges for SLAM due to frequent seasonal
changes, varying light conditions, and dense vegetation. These factors often
degrade the performance of visual SLAM algorithms originally developed for
structured urban environments. To address this gap, we present ROVER, a
comprehensive benchmark dataset tailored for evaluating visual SLAM algorithms
under diverse environmental conditions and spatial configurations. We captured
the dataset with a robotic platform equipped with monocular, stereo, and RGBD
cameras, as well as inertial sensors. It covers 39 recordings across five
outdoor locations, collected through all seasons and various lighting
scenarios, i.e., day, dusk, and night with and without external lighting. With
this novel dataset, we evaluate several traditional and deep learning-based
SLAM methods and study their performance in diverse challenging conditions. The
results demonstrate that while stereo-inertial and RGBD configurations
generally perform better under favorable lighting and moderate vegetation, most
SLAM systems perform poorly in low-light and high-vegetation scenarios,
particularly during summer and autumn. Our analysis highlights the need for
improved adaptability in visual SLAM algorithms for outdoor applications, as
current systems struggle with dynamic environmental factors affecting scale,
feature extraction, and trajectory consistency. This dataset provides a solid
foundation for advancing visual SLAM research in real-world, semi-structured
environments, fostering the development of more resilient SLAM systems for
long-term outdoor localization and mapping. The dataset and the code of the
benchmark are available under https://iis-esslingen.github.io/rover.","Fabian Schmidt, Julian Daubermann, Marcel Mitschke, Constantin Blessing, Stefan Meyer, Markus Enzweiler, Abhinav Valada",2024-12-03T15:34:00Z,2025-07-09T10:26:20Z,http://arxiv.org/abs/2412.02506v3,http://arxiv.org/pdf/2412.02506v3.pdf,all:outdoor AND all:rover AND submittedDate:[202309062228 TO 202509052228],2024
"Modeling of Terrain Deformation by a Grouser Wheel for Lunar Rover
  Simulation","Simulation of vehicle motion in planetary environments is challenging. This
is due to the modeling of complex terrain, optical conditions, and
terrain-aware vehicle dynamics. One of the critical issues of typical
simulators is that they assume terrain is a rigid body, which limits their
ability to render wheel traces and compute the wheel-terrain interactions. This
prevents, for example, the use of wheel traces as landmarks for localization,
as well as the accurate simulation of motion. In the context of lunar regolith,
the surface is not rigid but granular. As such, there are differences in the
rover's motion, such as sinkage and slippage, and a clear wheel trace left
behind the rover, compared to that on a rigid terrain. This study presents a
novel approach to integrating a terramechanics-aware terrain deformation engine
to simulate a realistic wheel trace in a digital lunar environment. By
leveraging Discrete Element Method simulation results alongside experimental
single-wheel test data, we construct a regression model to derive deformation
height as a function of contact normal force. The region of interest in a
height map is retrieved from the wheel poses. The elevation values of
corresponding pixels are subsequently modified using contact normal forces and
the regression model. Finally, we apply the determined elevation change to each
mesh vertex to render wheel traces during runtime. The deformation engine is
integrated into our ongoing development of a lunar simulator based on NVIDIA's
Omniverse IsaacSim. We hypothesize that our work will be crucial to testing
perception and downstream navigation systems under conditions similar to
outdoor or terrestrial fields. A demonstration video is available here:
https://www.youtube.com/watch?v=TpzD0h-5hv4","Junnosuke Kamohara, Vinicius Ares, James Hurrell, Keisuke Takehana, Antoine Richard, Shreya Santra, Kentaro Uno, Eric Rohmer, Kazuya Yoshida",2024-08-24T05:09:09Z,2024-08-24T05:09:09Z,http://arxiv.org/abs/2408.13468v1,http://arxiv.org/pdf/2408.13468v1.pdf,all:outdoor AND all:rover AND submittedDate:[202309062228 TO 202509052228],2024
"Risk-Aware Coverage Path Planning for Lunar Micro-Rovers Leveraging
  Global and Local Environmental Data","This paper presents a novel 3D myopic coverage path planning algorithm for
lunar micro-rovers that can explore unknown environments with limited sensing
and computational capabilities. The algorithm expands upon traditional
non-graph path planning methods to accommodate the complexities of lunar
terrain, utilizing global data with local topographic features into motion cost
calculations. The algorithm also integrates localization and mapping to update
the rover's pose and map the environment. The resulting environment map's
accuracy is evaluated and tested in a 3D simulator. Outdoor field tests were
conducted to validate the algorithm's efficacy in sim-to-real scenarios. The
results showed that the algorithm could achieve high coverage with low energy
consumption and computational cost, while incrementally exploring the terrain
and avoiding obstacles. This study contributes to the advancement of path
planning methodologies for space exploration, paving the way for efficient,
scalable and autonomous exploration of lunar environments by small rovers.","Shreya Santra, Kentaro Uno, Gen Kudo, Kazuya Yoshida",2024-04-29T14:10:13Z,2024-04-29T14:10:13Z,http://arxiv.org/abs/2404.18721v1,http://arxiv.org/pdf/2404.18721v1.pdf,all:outdoor AND all:rover AND submittedDate:[202309062228 TO 202509052228],2024
One Flight Over the Gap: A Survey from Perspective to Panoramic Vision,"Driven by the demand for spatial intelligence and holistic scene perception,
omnidirectional images (ODIs), which provide a complete 360\textdegree{} field
of view, are receiving growing attention across diverse applications such as
virtual reality, autonomous driving, and embodied robotics. Despite their
unique characteristics, ODIs exhibit remarkable differences from perspective
images in geometric projection, spatial distribution, and boundary continuity,
making it challenging for direct domain adaption from perspective methods. This
survey reviews recent panoramic vision techniques with a particular emphasis on
the perspective-to-panorama adaptation. We first revisit the panoramic imaging
pipeline and projection methods to build the prior knowledge required for
analyzing the structural disparities. Then, we summarize three challenges of
domain adaptation: severe geometric distortions near the poles, non-uniform
sampling in Equirectangular Projection (ERP), and periodic boundary continuity.
Building on this, we cover 20+ representative tasks drawn from more than 300
research papers in two dimensions. On one hand, we present a cross-method
analysis of representative strategies for addressing panoramic specific
challenges across different tasks. On the other hand, we conduct a cross-task
comparison and classify panoramic vision into four major categories: visual
quality enhancement and assessment, visual understanding, multimodal
understanding, and visual generation. In addition, we discuss open challenges
and future directions in data, models, and applications that will drive the
advancement of panoramic vision research. We hope that our work can provide new
insight and forward looking perspectives to advance the development of
panoramic vision technologies. Our project page is
https://insta360-research-team.github.io/Survey-of-Panorama","Xin Lin, Xian Ge, Dizhe Zhang, Zhaoliang Wan, Xianshun Wang, Xiangtai Li, Wenjie Jiang, Bo Du, Dacheng Tao, Ming-Hsuan Yang, Lu Qi",2025-09-04T17:59:10Z,2025-09-04T17:59:10Z,http://arxiv.org/abs/2509.04444v1,http://arxiv.org/pdf/2509.04444v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Privacy Perceptions in Robot-Assisted Well-Being Coaching: Examining the
  Roles of Information Transparency, User Control, and Proactivity","Social robots are increasingly recognized as valuable supporters in the field
of well-being coaching. They can function as independent coaches or provide
support alongside human coaches, and healthcare professionals. In coaching
interactions, these robots often handle sensitive information shared by users,
making privacy a relevant issue. Despite this, little is known about the
factors that shape users' privacy perceptions. This research aims to examine
three key factors systematically: (1) the transparency about information usage,
(2) the level of specific user control over how the robot uses their
information, and (3) the robot's behavioral approach - whether it acts
proactively or only responds on demand. Our results from an online study (N =
200) show that even when users grant the robot general access to personal data,
they additionally expect the ability to explicitly control how that information
is interpreted and shared during sessions. Experimental conditions that
provided such control received significantly higher ratings for perceived
privacy appropriateness and trust. Compared to user control, the effects of
transparency and proactivity on privacy appropriateness perception were low,
and we found no significant impact. The results suggest that merely informing
users or proactive sharing is insufficient without accompanying user control.
These insights underscore the need for further research on mechanisms that
allow users to manage robots' information processing and sharing, especially
when social robots take on more proactive roles alongside humans.","Atikkhan Faridkhan Nilgar, Manuel Dietrich, Kristof Van Laerhoven",2025-09-04T16:19:24Z,2025-09-04T16:19:24Z,http://arxiv.org/abs/2509.04358v1,http://arxiv.org/pdf/2509.04358v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Real Time FPGA Based CNNs for Detection, Classification, and Tracking in
  Autonomous Systems: State of the Art Designs and Optimizations","This paper presents a comprehensive review of recent advances in deploying
convolutional neural networks (CNNs) for object detection, classification, and
tracking on Field Programmable Gate Arrays (FPGAs). With the increasing demand
for real-time computer vision applications in domains such as autonomous
vehicles, robotics, and surveillance, FPGAs have emerged as a powerful
alternative to GPUs and ASICs due to their reconfigurability, low power
consumption, and deterministic latency. We critically examine state-of-the-art
FPGA implementations of CNN-based vision tasks, covering algorithmic
innovations, hardware acceleration techniques, and the integration of
optimization strategies like pruning, quantization, and sparsity-aware methods
to maximize performance within hardware constraints. This survey also explores
the landscape of modern FPGA platforms, including classical LUT-DSP based
architectures, System-on-Chip (SoC) FPGAs, and Adaptive Compute Acceleration
Platforms (ACAPs), comparing their capabilities in handling deep learning
workloads. Furthermore, we review available software development tools such as
Vitis AI, FINN, and Intel FPGA AI Suite, which significantly streamline the
design and deployment of AI models on FPGAs. The paper uniquely discusses
hybrid architecture that combine GPUs and FPGAs for collaborative acceleration
of AI inference, addressing challenges related to energy efficiency and
throughput. Additionally, we highlight hardware-software co-design practices,
dataflow optimizations, and pipelined processing techniques essential for
real-time inference on resource-constrained devices. Through this survey,
researchers and engineers are equipped with insights to develop
next-generation, power-efficient, and high-performance vision systems optimized
for FPGA deployment in edge and embedded applications.","Safa Mohammed Sali, Mahmoud Meribout, Ashiyana Abdul Majeed",2025-09-04T12:28:36Z,2025-09-04T12:28:36Z,http://arxiv.org/abs/2509.04153v1,http://arxiv.org/pdf/2509.04153v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Object-Reconstruction-Aware Whole-body Control of Mobile Manipulators,"Object reconstruction and inspection tasks play a crucial role in various
robotics applications. Identifying paths that reveal the most unknown areas of
the object becomes paramount in this context, as it directly affects
efficiency, and this problem is known as the view path planning problem.
Current methods often use sampling-based path planning techniques, evaluating
potential views along the path to enhance reconstruction performance. However,
these methods are computationally expensive as they require evaluating several
candidate views on the path. To this end, we propose a computationally
efficient solution that relies on calculating a focus point in the most
informative (unknown) region and having the robot maintain this point in the
camera field of view along the path. We incorporated this strategy into the
whole-body control of a mobile manipulator employing a visibility constraint
without the need for an additional path planner. We conducted comprehensive and
realistic simulations using a large dataset of 114 diverse objects of varying
sizes from 57 categories to compare our method with a sampling-based planning
strategy using Bayesian data analysis. Furthermore, we performed real-world
experiments with an 8-DoF mobile manipulator to demonstrate the proposed
method's performance in practice. Our results suggest that there is no
significant difference in object coverage and entropy. In contrast, our method
is approximately nine times faster than the baseline sampling-based method in
terms of the average time the robot spends between views.","Fatih Dursun, Bruno Vilhena Adorno, Simon Watson, Wei Pan",2025-09-04T10:52:27Z,2025-09-04T10:52:27Z,http://arxiv.org/abs/2509.04094v1,http://arxiv.org/pdf/2509.04094v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Harnessing modal fields retrieved from speckle for multi-dimensional
  metrology","Although speckle is a powerful tool for high-precision metrology, large
datasets and cumbersome training are always required to learn from the encoded
speckle patterns, which is unfavorable for rapid deployment and
multi-dimensional metrology. To enable high accuracy and fast training,
physics-informed machine learning enforces physical laws to address
high-dimensional problems. Here, we harness the modal fields in a few-mode
fiber, which follow the law of beam propagation, to enable high-accuracy and
fast-training parameter estimation. Anti-noise fast mode decomposition is
implemented to retrieve the modal fields from the speckles. The accuracy is
enhanced since the modal fields enable parameter estimation at random points in
the continuous space-time domain. Artificial tactile perception and
multi-dimensional metrology are achieved with high accuracy because the modal
fields respond diversely to different parameters. Meanwhile, the number of
specklegrams for training is reduced by around 5 times. The training time of
machine learning is significantly reduced by 800 times, from 9 hours and 45
minutes to 40 seconds. Therefore, harnessing the modal fields paves a new way
for the speckle-based metrology to develop efficient, low-cost,
multi-dimensional sensors, making it suitable for intelligent wearable devices,
industrial robots and healthcare applications.","Qingbo Liu, Zhongyang Xu, Guangkui Tao, Xiuyuan Sun, Min Xue, Weihao Yuan, Shilong Pan",2025-09-04T08:02:38Z,2025-09-04T08:02:38Z,http://arxiv.org/abs/2509.03976v1,http://arxiv.org/pdf/2509.03976v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Cost-Optimized Systems Engineering for IoT-Enabled Robot Nurse in
  Infectious Pandemic Management","The utilization of robotic technology has gained traction in healthcare
facilities due to progress in the field that enables time and cost savings,
minimizes waste, and improves patient care. Digital healthcare technologies
that leverage automation, such as robotics and artificial intelligence, have
the potential to enhance the sustainability and profitability of healthcare
systems in the long run. However, the recent COVID-19 pandemic has amplified
the need for cyber-physical robots to automate check-ups and medication
administration. A robot nurse is controlled by the Internet of Things (IoT) and
can serve as an automated medical assistant while also allowing supervisory
control based on custom commands. This system helps reduce infection risk and
improves outcomes in pandemic settings. This research presents a test case with
a nurse robot that can assess a patient's health status and take action
accordingly. We also evaluate the system's performance in medication
administration, health-status monitoring, and life-cycle considerations.","Md Mhamud Hussen Sifat, Md Maruf, Md Rokunuzzaman",2025-09-03T16:06:39Z,2025-09-03T16:06:39Z,http://arxiv.org/abs/2509.03436v1,http://arxiv.org/pdf/2509.03436v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"The Role of Embodiment in Intuitive Whole-Body Teleoperation for Mobile
  Manipulation","Intuitive Teleoperation interfaces are essential for mobile manipulation
robots to ensure high quality data collection while reducing operator workload.
A strong sense of embodiment combined with minimal physical and cognitive
demands not only enhances the user experience during large-scale data
collection, but also helps maintain data quality over extended periods. This
becomes especially crucial for challenging long-horizon mobile manipulation
tasks that require whole-body coordination. We compare two distinct robot
control paradigms: a coupled embodiment integrating arm manipulation and base
navigation functions, and a decoupled embodiment treating these systems as
separate control entities. Additionally, we evaluate two visual feedback
mechanisms: immersive virtual reality and conventional screen-based
visualization of the robot's field of view. These configurations were
systematically assessed across a complex, multi-stage task sequence requiring
integrated planning and execution. Our results show that the use of VR as a
feedback modality increases task completion time, cognitive workload, and
perceived effort of the teleoperator. Coupling manipulation and navigation
leads to a comparable workload on the user as decoupling the embodiments, while
preliminary experiments suggest that data acquired by coupled teleoperation
leads to better imitation learning performance. Our holistic view on intuitive
teleoperation interfaces provides valuable insight into collecting
high-quality, high-dimensional mobile manipulation data at scale with the human
operator in mind. Project
website:https://sophiamoyen.github.io/role-embodiment-wbc-moma-teleop/","Sophia Bianchi Moyen, Rickmer Krohn, Sophie Lueth, Kay Pompetzki, Jan Peters, Vignesh Prasad, Georgia Chalvatzaki",2025-09-03T11:25:36Z,2025-09-03T11:25:36Z,http://arxiv.org/abs/2509.03222v1,http://arxiv.org/pdf/2509.03222v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Population-aware Online Mirror Descent for Mean-Field Games with Common
  Noise by Deep Reinforcement Learning","Mean Field Games (MFGs) offer a powerful framework for studying large-scale
multi-agent systems. Yet, learning Nash equilibria in MFGs remains a
challenging problem, particularly when the initial distribution is unknown or
when the population is subject to common noise. In this paper, we introduce an
efficient deep reinforcement learning (DRL) algorithm designed to achieve
population-dependent Nash equilibria without relying on averaging or historical
sampling, inspired by Munchausen RL and Online Mirror Descent. The resulting
policy is adaptable to various initial distributions and sources of common
noise. Through numerical experiments on seven canonical examples, we
demonstrate that our algorithm exhibits superior convergence properties
compared to state-of-the-art algorithms, particularly a DRL version of
Fictitious Play for population-dependent policies. The performance in the
presence of common noise underscores the robustness and adaptability of our
approach.","Zida Wu, Mathieu Lauriere, Matthieu Geist, Olivier Pietquin, Ankur Mehta",2025-09-03T05:33:46Z,2025-09-03T05:33:46Z,http://arxiv.org/abs/2509.03030v1,http://arxiv.org/pdf/2509.03030v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Sem-RaDiff: Diffusion-Based 3D Radar Semantic Perception in Cluttered
  Agricultural Environments","Accurate and robust environmental perception is crucial for robot autonomous
navigation. While current methods typically adopt optical sensors (e.g.,
camera, LiDAR) as primary sensing modalities, their susceptibility to visual
occlusion often leads to degraded performance or complete system failure. In
this paper, we focus on agricultural scenarios where robots are exposed to the
risk of onboard sensor contamination. Leveraging radar's strong penetration
capability, we introduce a radar-based 3D environmental perception framework as
a viable alternative. It comprises three core modules designed for dense and
accurate semantic perception: 1) Parallel frame accumulation to enhance
signal-to-noise ratio of radar raw data. 2) A diffusion model-based
hierarchical learning framework that first filters radar sidelobe artifacts
then generates fine-grained 3D semantic point clouds. 3) A specifically
designed sparse 3D network optimized for processing large-scale radar raw data.
We conducted extensive benchmark comparisons and experimental evaluations on a
self-built dataset collected in real-world agricultural field scenes. Results
demonstrate that our method achieves superior structural and semantic
prediction performance compared to existing methods, while simultaneously
reducing computational and memory costs by 51.3% and 27.5%, respectively.
Furthermore, our approach achieves complete reconstruction and accurate
classification of thin structures such as poles and wires-which existing
methods struggle to perceive-highlighting its potential for dense and accurate
3D radar perception.","Ruibin Zhang, Fei Gao",2025-09-02T13:07:02Z,2025-09-03T05:06:33Z,http://arxiv.org/abs/2509.02283v2,http://arxiv.org/pdf/2509.02283v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"MIRAGE: Multimodal Intention Recognition and Admittance-Guided
  Enhancement in VR-based Multi-object Teleoperation","Effective human-robot interaction (HRI) in multi-object teleoperation tasks
faces significant challenges due to perceptual ambiguities in virtual reality
(VR) environments and the limitations of single-modality intention recognition.
This paper proposes a shared control framework that combines a virtual
admittance (VA) model with a Multimodal-CNN-based Human Intention Perception
Network (MMIPN) to enhance teleoperation performance and user experience. The
VA model employs artificial potential fields to guide operators toward target
objects by adjusting admittance force and optimizing motion trajectories. MMIPN
processes multimodal inputs, including gaze movement, robot motions, and
environmental context, to estimate human grasping intentions, helping to
overcome depth perception challenges in VR. Our user study evaluated four
conditions across two factors, and the results showed that MMIPN significantly
improved grasp success rates, while the VA model enhanced movement efficiency
by reducing path lengths. Gaze data emerged as the most crucial input modality.
These findings demonstrate the effectiveness of combining multimodal cues with
implicit guidance in VR-based teleoperation, providing a robust solution for
multi-object grasping tasks and enabling more natural interactions across
various applications in the future.","Chi Sun, Xian Wang, Abhishek Kumar, Chengbin Cui, Lik-Hang Lee",2025-09-02T06:20:28Z,2025-09-02T06:20:28Z,http://arxiv.org/abs/2509.01996v1,http://arxiv.org/pdf/2509.01996v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Geometric Control of Mechanical Systems with Symmetries Based on Sliding
  Modes","In this paper, we propose a framework for designing sliding mode controllers
for a class of mechanical systems with symmetry, both unconstrained and
constrained, that evolve on principal fiber bundles. Control laws are developed
based on the reduced motion equations by exploring symmetries, leading to a
sliding mode control strategy where the reaching stage is executed on the base
space, and the sliding stage is performed on the structure group. Thus, design
complexity is reduced, and difficult choices for coordinate representations
when working with a particular Lie group are avoided. For this purpose, a
sliding subgroup is constructed on the structure group based on a kinematic
controller, and the sliding variable will converge to the identity of the state
manifold upon reaching the sliding subgroup. A reaching law based on a general
sliding vector field is then designed on the base space using the local form of
the mechanical connection to drive the sliding variable to the sliding
subgroup, and its time evolution is given according to the appropriate
covariant derivative. Almost global asymptotic stability and local exponential
stability are demonstrated using a Lyapunov analysis. We apply the results to a
fully actuated system (a rigid spacecraft actuated by reaction wheels) and a
subactuated nonholonomic system (unicycle mobile robot actuated by wheels),
which is also simulated for illustration.","Eduardo Espindola, Yu Tang",2025-09-02T06:06:21Z,2025-09-02T06:06:21Z,http://arxiv.org/abs/2509.01985v1,http://arxiv.org/pdf/2509.01985v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Hybrid Autonomy Framework for a Future Mars Science Helicopter,"Autonomous aerial vehicles, such as NASA's Ingenuity, enable rapid planetary
surface exploration beyond the reach of ground-based robots. Thus, NASA is
studying a Mars Science Helicopter (MSH), an advanced concept capable of
performing long-range science missions and autonomously navigating challenging
Martian terrain. Given significant Earth-Mars communication delays and mission
complexity, an advanced autonomy framework is required to ensure safe and
efficient operation by continuously adapting behavior based on mission
objectives and real-time conditions, without human intervention. This study
presents a deterministic high-level control framework for aerial exploration,
integrating a Finite State Machine (FSM) with Behavior Trees (BTs) to achieve a
scalable, robust, and computationally efficient autonomy solution for critical
scenarios like deep space exploration. In this paper we outline key
capabilities of a possible MSH and detail the FSM-BT hybrid autonomy framework
which orchestrates them to achieve the desired objectives. Monte Carlo
simulations and real field tests validate the framework, demonstrating its
robustness and adaptability to both discrete events and real-time system
feedback. These inputs trigger state transitions or dynamically adjust behavior
execution, enabling reactive and context-aware responses. The framework is
middleware-agnostic, supporting integration with systems like F-Prime and
extending beyond aerial robotics.","Luca Di Pierno, Robert Hewitt, Stephan Weiss, Roland Brockers",2025-09-02T05:47:14Z,2025-09-02T05:47:14Z,http://arxiv.org/abs/2509.01980v1,http://arxiv.org/pdf/2509.01980v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Speculative Design of Equitable Robotics: Queer Fictions and Futures,"This paper examines the speculative topic of equitable robots through an
exploratory essay format. It focuses specifically on robots by and for LGBTQ+
populations. It aims to provoke thought and conversations in the field about
what aspirational queer robotics futures may look like, both in the arts and
sciences. First, it briefly reviews the state-of-the-art of queer robotics in
fiction and science, drawing together threads from each. Then, it discusses
queering robots through three speculative design proposals for queer robot
roles: 1) reflecting the queerness of their ''in-group'' queer users, building
and celebrating ''in-group'' identity, 2) a new kind of queer activism by
implementing queer robot identity performance to interact with ''out-group''
users, with a goal of reducing bigotry through familiarisation, and 3) a
network of queer-owned robots, through which the community could reach each
other, and distribute and access important resources. The paper then questions
whether robots should be queered, and what ethical implications this raises.
Finally, the paper makes suggestions for what aspirational queer robotics
futures may look like, and what would be required to get there.",Minja Axelsson,2025-09-01T17:36:43Z,2025-09-01T17:36:43Z,http://arxiv.org/abs/2509.01643v1,http://arxiv.org/pdf/2509.01643v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"TransForSeg: A Multitask Stereo ViT for Joint Stereo Segmentation and 3D
  Force Estimation in Catheterization","Recently, the emergence of multitask deep learning models has enhanced
catheterization procedures by providing tactile and visual perception data
through an end-to-end architecture. This information is derived from a
segmentation and force estimation head, which localizes the catheter in X-ray
images and estimates the applied pressure based on its deflection within the
image. These stereo vision architectures incorporate a CNN-based
encoder-decoder that captures the dependencies between X-ray images from two
viewpoints, enabling simultaneous 3D force estimation and stereo segmentation
of the catheter. With these tasks in mind, this work approaches the problem
from a new perspective. We propose a novel encoder-decoder Vision Transformer
model that processes two input X-ray images as separate sequences. Given
sequences of X-ray patches from two perspectives, the transformer captures
long-range dependencies without the need to gradually expand the receptive
field for either image. The embeddings generated by both the encoder and
decoder are fed into two shared segmentation heads, while a regression head
employs the fused information from the decoder for 3D force estimation. The
proposed model is a stereo Vision Transformer capable of simultaneously
segmenting the catheter from two angles while estimating the generated forces
at its tip in 3D. This model has undergone extensive experiments on synthetic
X-ray images with various noise levels and has been compared against
state-of-the-art pure segmentation models, vision-based catheter force
estimation methods, and a multitask catheter segmentation and force estimation
approach. It outperforms existing models, setting a new state-of-the-art in
both catheter segmentation and force estimation.","Pedram Fekri, Mehrdad Zadeh, Javad Dargahi",2025-09-01T16:36:23Z,2025-09-01T16:36:23Z,http://arxiv.org/abs/2509.01605v1,http://arxiv.org/pdf/2509.01605v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"FGO-SLAM: Enhancing Gaussian SLAM with Globally Consistent Opacity
  Radiance Field","Visual SLAM has regained attention due to its ability to provide perceptual
capabilities and simulation test data for Embodied AI. However, traditional
SLAM methods struggle to meet the demands of high-quality scene reconstruction,
and Gaussian SLAM systems, despite their rapid rendering and high-quality
mapping capabilities, lack effective pose optimization methods and face
challenges in geometric reconstruction. To address these issues, we introduce
FGO-SLAM, a Gaussian SLAM system that employs an opacity radiance field as the
scene representation to enhance geometric mapping performance. After initial
pose estimation, we apply global adjustment to optimize camera poses and sparse
point cloud, ensuring robust tracking of our approach. Additionally, we
maintain a globally consistent opacity radiance field based on 3D Gaussians and
introduce depth distortion and normal consistency terms to refine the scene
representation. Furthermore, after constructing tetrahedral grids, we identify
level sets to directly extract surfaces from 3D Gaussians. Results across
various real-world and large-scale synthetic datasets demonstrate that our
method achieves state-of-the-art tracking accuracy and mapping performance.","Fan Zhu, Yifan Zhao, Ziyu Chen, Biao Yu, Hui Zhu",2025-09-01T15:20:41Z,2025-09-01T15:20:41Z,http://arxiv.org/abs/2509.01547v1,http://arxiv.org/pdf/2509.01547v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"A Reactive Grasping Framework for Multi-DoF Grippers via Task Space
  Velocity Fields and Joint Space QP","We present a fast and reactive grasping framework for multi-DoF grippers that
combines task-space velocity fields with a joint-space Quadratic Program (QP)
in a hierarchical structure. Reactive, collision-free global motion planning is
particularly challenging for high-DoF systems, since simultaneous increases in
state dimensionality and planning horizon trigger a combinatorial explosion of
the search space, making real-time planning intractable. To address this, we
plan globally in a lower-dimensional task space, such as fingertip positions,
and track locally in the full joint space while enforcing all constraints. This
approach is realized by constructing velocity fields in multiple task-space
coordinates (or in some cases a subset of joint coordinates) and solving a
weighted joint-space QP to compute joint velocities that track these fields
with appropriately assigned priorities. Through simulation experiments with
privileged knowledge and real-world tests using the recent pose-tracking
algorithm FoundationPose, we verify that our method enables high-DoF arm-hand
systems to perform real-time, collision-free reaching motions while adapting to
dynamic environments and external disturbances.","Yonghyeon Lee, Tzu-Yuan Lin, Alexander Alexiev, Sangbae Kim",2025-09-01T00:52:01Z,2025-09-01T00:52:01Z,http://arxiv.org/abs/2509.01044v1,http://arxiv.org/pdf/2509.01044v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Enhanced Mean Field Game for Interactive Decision-Making with Varied
  Stylish Multi-Vehicles","This paper presents an MFG-based decision-making framework for autonomous
driving in heterogeneous traffic. To capture diverse human behaviors, we
propose a quantitative driving style representation that maps abstract traits
to parameters such as speed, safety factors, and reaction time. These
parameters are embedded into the MFG through a spatial influence field model.
To ensure safe operation in dense traffic, we introduce a safety-critical
lane-changing algorithm that leverages dynamic safety margins,
time-to-collision analysis, and multi-layered constraints. Real-world NGSIM
data is employed for style calibration and empirical validation. Experimental
results demonstrate zero collisions across six style combinations, two
15-vehicle scenarios, and NGSIM-based trials, consistently outperforming
conventional game-theoretic baselines. Overall, our approach provides a
scalable, interpretable, and behavior-aware planning framework for real-world
autonomous driving applications.","Liancheng Zheng, Zhen Tian, Yangfan He, Shuo Liu, Ke Gong, Huilin Chen, Zhihao Lin",2025-08-31T20:24:53Z,2025-08-31T20:24:53Z,http://arxiv.org/abs/2509.00981v1,http://arxiv.org/pdf/2509.00981v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"One-Step Model Predictive Path Integral for Manipulator Motion Planning
  Using Configuration Space Distance Fields","Motion planning for robotic manipulators is a fundamental problem in
robotics. Classical optimization-based methods typically rely on the gradients
of signed distance fields (SDFs) to impose collision-avoidance constraints.
However, these methods are susceptible to local minima and may fail when the
SDF gradients vanish. Recently, Configuration Space Distance Fields (CDFs) have
been introduced, which directly model distances in the robot's configuration
space. Unlike workspace SDFs, CDFs are differentiable almost everywhere and
thus provide reliable gradient information. On the other hand, gradient-free
approaches such as Model Predictive Path Integral (MPPI) control leverage
long-horizon rollouts to achieve collision avoidance. While effective, these
methods are computationally expensive due to the large number of trajectory
samples, repeated collision checks, and the difficulty of designing cost
functions with heterogeneous physical units. In this paper, we propose a
framework that integrates CDFs with MPPI to enable direct navigation in the
robot's configuration space. Leveraging CDF gradients, we unify the MPPI cost
in joint-space and reduce the horizon to one step, substantially cutting
computation while preserving collision avoidance in practice. We demonstrate
that our approach achieves nearly 100% success rates in 2D environments and
consistently high success rates in challenging 7-DOF Franka manipulator
simulations with complex obstacles. Furthermore, our method attains control
frequencies exceeding 750 Hz, substantially outperforming both
optimization-based and standard MPPI baselines. These results highlight the
effectiveness and efficiency of the proposed CDF-MPPI framework for
high-dimensional motion planning.","Yulin Li, Tetsuro Miyazaki, Kenji Kawashima",2025-08-31T13:09:24Z,2025-08-31T13:09:24Z,http://arxiv.org/abs/2509.00836v1,http://arxiv.org/pdf/2509.00836v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"CARIS: A Context-Adaptable Robot Interface System for Personalized and
  Scalable Human-Robot Interaction","The human-robot interaction (HRI) field has traditionally used Wizard-of-Oz
(WoZ) controlled robots to explore navigation, conversational dynamics,
human-in-the-loop interactions, and more to explore appropriate robot behaviors
in everyday settings. However, existing WoZ tools are often limited to one
context, making them less adaptable across different settings, users, and
robotic platforms. To mitigate these issues, we introduce a Context-Adaptable
Robot Interface System (CARIS) that combines advanced robotic capabilities such
teleoperation, human perception, human-robot dialogue, and multimodal data
recording. Through pilot studies, we demonstrate the potential of CARIS to WoZ
control a robot in two contexts: 1) mental health companion and as a 2) tour
guide. Furthermore, we identified areas of improvement for CARIS, including
smoother integration between movement and communication, clearer functionality
separation, recommended prompts, and one-click communication options to enhance
the usability wizard control of CARIS. This project offers a publicly
available, context-adaptable tool for the HRI community, enabling researchers
to streamline data-driven approaches to intelligent robot behavior.","Felipe Arias-Russi, Yuanchen Bai, Angelique Taylor",2025-08-31T02:11:18Z,2025-08-31T02:11:18Z,http://arxiv.org/abs/2509.00660v1,http://arxiv.org/pdf/2509.00660v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"A Risk-aware Spatial-temporal Trajectory Planning Framework for
  Autonomous Vehicles Using QP-MPC and Dynamic Hazard Fields","Trajectory planning is a critical component in ensuring the safety,
stability, and efficiency of autonomous vehicles. While existing trajectory
planning methods have achieved progress, they often suffer from high
computational costs, unstable performance in dynamic environments, and limited
validation across diverse scenarios. To overcome these challenges, we propose
an enhanced QP-MPC-based framework that incorporates three key innovations: (i)
a novel cost function designed with a dynamic hazard field, which explicitly
balances safety, efficiency, and comfort; (ii) seamless integration of this
cost function into the QP-MPC formulation, enabling direct optimization of
desired driving behaviors; and (iii) extensive validation of the proposed
framework across complex tasks. The spatial safe planning is guided by a
dynamic hazard field (DHF) for risk assessment, while temporal safe planning is
based on a space-time graph. Besides, the quintic polynomial sampling and
sub-reward of comforts are used to ensure comforts during lane-changing. The
sub-reward of efficiency is used to maintain driving efficiency. Finally, the
proposed DHF-enhanced objective function integrates multiple objectives,
providing a proper optimization tasks for QP-MPC. Extensive simulations
demonstrate that the proposed framework outperforms benchmark optimization
methods in terms of efficiency, stability, and comfort across a variety of
scenarios likes lane-changing, overtaking, and crossing intersections.","Zhen Tian, Zhihao Lin, Dezong Zhao, Christos Anagnostopoulos, Qiyuan Wang, Wenjing Zhao, Xiaodan Wang, Chongfeng Wei",2025-08-31T00:35:55Z,2025-08-31T00:35:55Z,http://arxiv.org/abs/2509.00643v1,http://arxiv.org/pdf/2509.00643v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Vehicle-in-Virtual-Environment (VVE) Method for Developing and
  Evaluating VRU Safety of Connected and Autonomous Driving with Focus on
  Bicyclist Safety","Extensive research has already been conducted in the autonomous driving field
to help vehicles navigate safely and efficiently. At the same time, plenty of
current research on vulnerable road user (VRU) safety is performed which
largely concentrates on perception, localization, or trajectory prediction of
VRUs. However, existing research still exhibits several gaps, including the
lack of a unified planning and collision avoidance system for autonomous
vehicles, limited investigation into delay tolerant control strategies, and the
absence of an efficient and standardized testing methodology. Ensuring VRU
safety remains one of the most pressing challenges in autonomous driving,
particularly in dynamic and unpredictable environments. In this two year
project, we focused on applying the Vehicle in Virtual Environment (VVE) method
to develop, evaluate, and demonstrate safety functions for Vulnerable Road
Users (VRUs) using automated steering and braking of ADS. In this current
second year project report, our primary focus was on enhancing the previous
year results while also considering bicyclist safety.","Haochong Chen, Xincheng Cao, Bilin Aksun-Guvenc, Levent Guvenc",2025-08-30T22:43:14Z,2025-08-30T22:43:14Z,http://arxiv.org/abs/2509.00624v1,http://arxiv.org/pdf/2509.00624v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Reinforcement Learning of Dolly-In Filming Using a Ground-Based Robot,"Free-roaming dollies enhance filmmaking with dynamic movement, but challenges
in automated camera control remain unresolved. Our study advances this field by
applying Reinforcement Learning (RL) to automate dolly-in shots using
free-roaming ground-based filming robots, overcoming traditional control
hurdles. We demonstrate the effectiveness of combined control for precise film
tasks by comparing it to independent control strategies. Our robust RL pipeline
surpasses traditional Proportional-Derivative controller performance in
simulation and proves its efficacy in real-world tests on a modified ROSBot 2.0
platform equipped with a camera turret. This validates our approach's
practicality and sets the stage for further research in complex filming
scenarios, contributing significantly to the fusion of technology with
cinematic creativity. This work presents a leap forward in the field and opens
new avenues for research and development, effectively bridging the gap between
technological advancement and creative filmmaking.","Philip Lorimer, Jack Saunders, Alan Hunter, Wenbin Li",2025-08-30T17:14:11Z,2025-08-30T17:14:11Z,http://arxiv.org/abs/2509.00564v1,http://arxiv.org/pdf/2509.00564v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Embodied Spatial Intelligence: from Implicit Scene Modeling to Spatial
  Reasoning","This thesis introduces ""Embodied Spatial Intelligence"" to address the
challenge of creating robots that can perceive and act in the real world based
on natural language instructions. To bridge the gap between Large Language
Models (LLMs) and physical embodiment, we present contributions on two fronts:
scene representation and spatial reasoning. For perception, we develop robust,
scalable, and accurate scene representations using implicit neural models, with
contributions in self-supervised camera calibration, high-fidelity depth field
generation, and large-scale reconstruction. For spatial reasoning, we enhance
the spatial capabilities of LLMs by introducing a novel navigation benchmark, a
method for grounding language in 3D, and a state-feedback mechanism to improve
long-horizon decision-making. This work lays a foundation for robots that can
robustly perceive their surroundings and intelligently act upon complex,
language-based commands.",Jiading Fang,2025-08-30T11:42:26Z,2025-08-30T11:42:26Z,http://arxiv.org/abs/2509.00465v1,http://arxiv.org/pdf/2509.00465v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Domain Adaptation-Based Crossmodal Knowledge Distillation for 3D
  Semantic Segmentation","Semantic segmentation of 3D LiDAR data plays a pivotal role in autonomous
driving. Traditional approaches rely on extensive annotated data for point
cloud analysis, incurring high costs and time investments. In contrast,
realworld image datasets offer abundant availability and substantial scale. To
mitigate the burden of annotating 3D LiDAR point clouds, we propose two
crossmodal knowledge distillation methods: Unsupervised Domain Adaptation
Knowledge Distillation (UDAKD) and Feature and Semantic-based Knowledge
Distillation (FSKD). Leveraging readily available spatio-temporally
synchronized data from cameras and LiDARs in autonomous driving scenarios, we
directly apply a pretrained 2D image model to unlabeled 2D data. Through
crossmodal knowledge distillation with known 2D-3D correspondence, we actively
align the output of the 3D network with the corresponding points of the 2D
network, thereby obviating the necessity for 3D annotations. Our focus is on
preserving modality-general information while filtering out modality-specific
details during crossmodal distillation. To achieve this, we deploy
self-calibrated convolution on 3D point clouds as the foundation of our domain
adaptation module. Rigorous experimentation validates the effectiveness of our
proposed methods, consistently surpassing the performance of state-of-the-art
approaches in the field.","Jialiang Kang, Jiawen Wang, Dingsheng Luo",2025-08-30T06:34:39Z,2025-08-30T06:34:39Z,http://arxiv.org/abs/2509.00379v1,http://arxiv.org/pdf/2509.00379v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Neural Network Acceleration on MPSoC board: Integrating SLAC's SNL,
  Rogue Software and Auto-SNL","The LCLS-II Free Electron Laser (FEL) will generate X-ray pulses for beamline
experiments at rates of up to 1~MHz, with detectors producing data throughputs
exceeding 1 TB/s. Managing such massive data streams presents significant
challenges, as transmission and storage infrastructures become prohibitively
expensive. Machine learning (ML) offers a promising solution for real-time data
reduction, but conventional implementations introduce excessive latency, making
them unsuitable for high-speed experimental environments. To address these
challenges, SLAC developed the SLAC Neural Network Library (SNL), a specialized
framework designed to deploy real-time ML inference models on
Field-Programmable Gate Arrays (FPGA). SNL's key feature is the ability to
dynamically update model weights without requiring FPGA resynthesis, enhancing
flexibility for adaptive learning applications. To further enhance usability
and accessibility, we introduce Auto-SNL, a Python extension that streamlines
the process of converting Python-based neural network models into
SNL-compatible high-level synthesis code. This paper presents a benchmark
comparison against hls4ml, the current state-of-the-art tool, across multiple
neural network architectures, fixed-point precisions, and synthesis
configurations targeting a Xilinx ZCU102 FPGA. The results showed that SNL
achieves competitive or superior latency in most tested architectures, while in
some cases also offering FPGA resource savings. This adaptation demonstrates
SNL's versatility, opening new opportunities for researchers and academics in
fields such as high-energy physics, medical imaging, robotics, and many more.","Hamza Ezzaoui Rahali, Abhilasha Dave, Larry Ruckman, Mohammad Mehdi Rahimifar, Audrey C. Therrien, James J. Russel, Ryan T. Herbst",2025-08-29T16:04:15Z,2025-08-29T16:04:15Z,http://arxiv.org/abs/2508.21739v1,http://arxiv.org/pdf/2508.21739v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics,"We present a multi-modal dataset collected in a soybean crop field,
comprising over two hours of recorded data from sensors such as stereo infrared
camera, color camera, accelerometer, gyroscope, magnetometer, GNSS (Single
Point Positioning, Real-Time Kinematic and Post-Processed Kinematic), and wheel
odometry. This dataset captures key challenges inherent to robotics in
agricultural environments, including variations in natural lighting, motion
blur, rough terrain, and long, perceptually aliased sequences. By addressing
these complexities, the dataset aims to support the development and
benchmarking of advanced algorithms for localization, mapping, perception, and
navigation in agricultural robotics. The platform and data collection system is
designed to meet the key requirements for evaluating multi-modal SLAM systems,
including hardware synchronization of sensors, 6-DOF ground truth and loops on
long trajectories.
  We run multimodal state-of-the art SLAM methods on the dataset, showcasing
the existing limitations in their application on agricultural settings. The
dataset and utilities to work with it are released on
https://cifasis.github.io/rosariov2/.","Nicolas Soncini, Javier Cremona, Erica Vidal, Maximiliano García, Gastón Castro, Taihú Pire",2025-08-29T13:58:55Z,2025-08-29T13:58:55Z,http://arxiv.org/abs/2508.21635v1,http://arxiv.org/pdf/2508.21635v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Cooperative Sensing Enhanced UAV Path-Following and Obstacle Avoidance
  with Variable Formation","The high mobility of unmanned aerial vehicles (UAVs) enables them to be used
in various civilian fields, such as rescue and cargo transport. Path-following
is a crucial way to perform these tasks while sensing and collision avoidance
are essential for safe flight. In this paper, we investigate how to efficiently
and accurately achieve path-following, obstacle sensing and avoidance subtasks,
as well as their conflict-free fusion scheduling. Firstly, a high precision
deep reinforcement learning (DRL)-based UAV formation path-following model is
developed, and the reward function with adaptive weights is designed from the
perspective of distance and velocity errors. Then, we use integrated sensing
and communication (ISAC) signals to detect the obstacle and derive the
Cramer-Rao lower bound (CRLB) for obstacle sensing by information-level fusion,
based on which we propose the variable formation enhanced obstacle position
estimation (VFEO) algorithm. In addition, an online obstacle avoidance scheme
without pretraining is designed to solve the sparse reward. Finally, with the
aid of null space based (NSB) behavioral method, we present a hierarchical
subtasks fusion strategy. Simulation results demonstrate the effectiveness and
superiority of the subtask algorithms and the hierarchical fusion strategy.","Changheng Wang, Zhiqing Wei, Wangjun Jiang, Haoyue Jiang, Zhiyong Feng",2025-08-29T02:48:41Z,2025-08-29T02:48:41Z,http://arxiv.org/abs/2508.21316v1,http://arxiv.org/pdf/2508.21316v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Embodied AI: Emerging Risks and Opportunities for Policy Action,"The field of embodied AI (EAI) is rapidly advancing. Unlike virtual AI, EAI
systems can exist in, learn from, reason about, and act in the physical world.
With recent advances in AI models and hardware, EAI systems are becoming
increasingly capable across wider operational domains. While EAI systems can
offer many benefits, they also pose significant risks, including physical harm
from malicious use, mass surveillance, as well as economic and societal
disruption. These risks require urgent attention from policymakers, as existing
policies governing industrial robots and autonomous vehicles are insufficient
to address the full range of concerns EAI systems present. To help address this
issue, this paper makes three contributions. First, we provide a taxonomy of
the physical, informational, economic, and social risks EAI systems pose.
Second, we analyze policies in the US, EU, and UK to assess how existing
frameworks address these risks and to identify critical gaps. We conclude by
offering policy recommendations for the safe and beneficial deployment of EAI
systems, such as mandatory testing and certification schemes, clarified
liability frameworks, and strategies to manage EAI's potentially transformative
economic and societal impacts.","Jared Perlo, Alexander Robey, Fazl Barez, Luciano Floridi, Jakob Mökander",2025-08-28T17:59:07Z,2025-09-03T17:55:11Z,http://arxiv.org/abs/2509.00117v2,http://arxiv.org/pdf/2509.00117v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Model-Free Hovering and Source Seeking via Extremum Seeking Control:
  Experimental Demonstration","In a recent effort, we successfully proposed a categorically novel approach
to mimic the phenomenoa of hovering and source seeking by flapping insects and
hummingbirds using a new extremum seeking control (ESC) approach. Said ESC
approach was shown capable of characterizing the physics of hovering and source
seeking by flapping systems, providing at the same time uniquely novel
opportunity for a model-free, real-time biomimicry control design. In this
paper, we experimentally test and verify, for the first time in the literature,
the potential of ESC in flapping robots to achieve model-free, real-time
controlled hovering and source seeking. The results of this paper, while being
restricted to 1D, confirm the premise of introducing ESC as a natural control
method and biomimicry mechanism to the field of flapping flight and robotics.","Ahmed A. Elgohary, Rohan Palanikumar, Sameh A. Eisa",2025-08-28T14:29:07Z,2025-08-28T14:29:07Z,http://arxiv.org/abs/2508.20836v1,http://arxiv.org/pdf/2508.20836v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"First observation of the Josephson-Anderson relation in experiments on
  hydrodynamic drag","We verify a recent prediction (Eq. 3.50 in G. L. Eyink, Phys. Rev. X 11,
031054 (2021)) for the drag on an object moving through a fluid. In this
prediction the velocity field is decomposed into a nonvortical (potential) and
vortical contribution, and so is the associated drag force. In the
Josephson-Anderson relation the vortical contribution of the drag force follows
from the flux of vorticity traversing the streamlines of the corresponding
potential flow. The potential component is directly determined by the plate
acceleration and its added mass. The Josephson-Anderson relation is derived
from the quantum description of superfluids, but remarkably applies to the
classical fluid in our experiment. In our experiment a flat plate is
accelerated through water using a robotic arm. This geometry is simple enough
to allow analytic potential flow streamlines. The monitored plate position
shows an oscillatory component of the acceleration, which adds an additional
test of the Josephson-Anderson relation. The instantaneous velocity field is
measured using particle image velocimetry. It enables us to evaluate Eq. 3.50
from [1] and compare its prediction to the measured drag force. We find
excellent agreement, and, most remarkably find that the added mass contribution
to the drag force still stands out after the flow has turned vortical. We
finally comment on the requirements on the experimental techniques for
evaluating the Josephson-Anderson relation.","Nicola Savelli, Ali R Khojasteh, Abel-John Buchner, Jerry Westerweel, Willem van de Water",2025-08-27T12:19:43Z,2025-08-27T12:19:43Z,http://arxiv.org/abs/2508.19824v1,http://arxiv.org/pdf/2508.19824v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Generalizing Monocular 3D Object Detection,"Monocular 3D object detection (Mono3D) is a fundamental computer vision task
that estimates an object's class, 3D position, dimensions, and orientation from
a single image. Its applications, including autonomous driving, augmented
reality, and robotics, critically rely on accurate 3D environmental
understanding. This thesis addresses the challenge of generalizing Mono3D
models to diverse scenarios, including occlusions, datasets, object sizes, and
camera parameters. To enhance occlusion robustness, we propose a mathematically
differentiable NMS (GrooMeD-NMS). To improve generalization to new datasets, we
explore depth equivariant (DEVIANT) backbones. We address the issue of large
object detection, demonstrating that it's not solely a data imbalance or
receptive field problem but also a noise sensitivity issue. To mitigate this,
we introduce a segmentation-based approach in bird's-eye view with dice loss
(SeaBird). Finally, we mathematically analyze the extrapolation of Mono3D
models to unseen camera heights and improve Mono3D generalization in such
out-of-distribution settings.",Abhinav Kumar,2025-08-27T06:06:18Z,2025-08-27T06:06:18Z,http://arxiv.org/abs/2508.19593v1,http://arxiv.org/pdf/2508.19593v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"DATR: Diffusion-based 3D Apple Tree Reconstruction Framework with
  Sparse-View","Digital twin applications offered transformative potential by enabling
real-time monitoring and robotic simulation through accurate virtual replicas
of physical assets. The key to these systems is 3D reconstruction with high
geometrical fidelity. However, existing methods struggled under field
conditions, especially with sparse and occluded views. This study developed a
two-stage framework (DATR) for the reconstruction of apple trees from sparse
views. The first stage leverages onboard sensors and foundation models to
semi-automatically generate tree masks from complex field images. Tree masks
are used to filter out background information in multi-modal data for the
single-image-to-3D reconstruction at the second stage. This stage consists of a
diffusion model and a large reconstruction model for respective multi view and
implicit neural field generation. The training of the diffusion model and LRM
was achieved by using realistic synthetic apple trees generated by a Real2Sim
data generator. The framework was evaluated on both field and synthetic
datasets. The field dataset includes six apple trees with field-measured ground
truth, while the synthetic dataset featured structurally diverse trees.
Evaluation results showed that our DATR framework outperformed existing 3D
reconstruction methods across both datasets and achieved domain-trait
estimation comparable to industrial-grade stationary laser scanners while
improving the throughput by $\sim$360 times, demonstrating strong potential for
scalable agricultural digital twin systems.","Tian Qiu, Alan Zoubi, Yiyuan Lin, Ruiming Du, Lailiang Cheng, Yu Jiang",2025-08-27T01:45:54Z,2025-08-27T01:45:54Z,http://arxiv.org/abs/2508.19508v1,http://arxiv.org/pdf/2508.19508v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"VisionSafeEnhanced VPC: Cautious Predictive Control with Visibility
  Constraints under Uncertainty for Autonomous Robotic Surgery","Autonomous control of the laparoscope in robot-assisted Minimally Invasive
Surgery (MIS) has received considerable research interest due to its potential
to improve surgical safety. Despite progress in pixel-level Image-Based Visual
Servoing (IBVS) control, the requirement of continuous visibility and the
existence of complex disturbances, such as parameterization error, measurement
noise, and uncertainties of payloads, could degrade the surgeon's visual
experience and compromise procedural safety. To address these limitations, this
paper proposes VisionSafeEnhanced Visual Predictive Control (VPC), a robust and
uncertainty-adaptive framework for autonomous laparoscope control that
guarantees Field of View (FoV) safety under uncertainty. Firstly, Gaussian
Process Regression (GPR) is utilized to perform hybrid (deterministic +
stochastic) quantification of operational uncertainties including residual
model uncertainties, stochastic uncertainties, and external disturbances. Based
on uncertainty quantification, a novel safety aware trajectory optimization
framework with probabilistic guarantees is proposed, where a
uncertainty-adaptive safety Control Barrier Function (CBF) condition is given
based on uncertainty propagation, and chance constraints are simultaneously
formulated based on probabilistic approximation. This uncertainty aware
formulation enables adaptive control effort allocation, minimizing unnecessary
camera motion while maintaining robustness. The proposed method is validated
through comparative simulations and experiments on a commercial surgical robot
platform (MicroPort MedBot Toumai) performing a sequential multi-target lymph
node dissection. Compared with baseline methods, the framework maintains
near-perfect target visibility (>99.9%), reduces tracking e","Wang Jiayin, Wei Yanran, Jiang Lei, Guo Xiaoyu, Zheng Ayong, Zhao Weidong, Li Zhongkui",2025-08-26T11:24:20Z,2025-08-26T11:24:20Z,http://arxiv.org/abs/2508.18937v1,http://arxiv.org/pdf/2508.18937v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Enhancing Video-Based Robot Failure Detection Using Task Knowledge,"Robust robotic task execution hinges on the reliable detection of execution
failures in order to trigger safe operation modes, recovery strategies, or task
replanning. However, many failure detection methods struggle to provide
meaningful performance when applied to a variety of real-world scenarios. In
this paper, we propose a video-based failure detection approach that uses
spatio-temporal knowledge in the form of the actions the robot performs and
task-relevant objects within the field of view. Both pieces of information are
available in most robotic scenarios and can thus be readily obtained. We
demonstrate the effectiveness of our approach on three datasets that we amend,
in part, with additional annotations of the aforementioned task-relevant
knowledge. In light of the results, we also propose a data augmentation method
that improves performance by applying variable frame rates to different parts
of the video. We observe an improvement from 77.9 to 80.0 in F1 score on the
ARMBench dataset without additional computational expense and an additional
increase to 81.4 with test-time augmentation. The results emphasize the
importance of spatio-temporal information during failure detection and suggest
further investigation of suitable heuristics in future implementations. Code
and annotations are available.","Santosh Thoduka, Sebastian Houben, Juergen Gall, Paul G. Plöger",2025-08-26T06:10:46Z,2025-08-26T06:10:46Z,http://arxiv.org/abs/2508.18705v1,http://arxiv.org/pdf/2508.18705v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"AgriChrono: A Multi-modal Dataset Capturing Crop Growth and Lighting
  Variability with a Field Robot","Existing datasets for precision agriculture have primarily been collected in
static or controlled environments such as indoor labs or greenhouses, often
with limited sensor diversity and restricted temporal span. These conditions
fail to reflect the dynamic nature of real farmland, including illumination
changes, crop growth variation, and natural disturbances. As a result, models
trained on such data often lack robustness and generalization when applied to
real-world field scenarios. In this paper, we present AgriChrono, a novel
robotic data collection platform and multi-modal dataset designed to capture
the dynamic conditions of real-world agricultural environments. Our platform
integrates multiple sensors and enables remote, time-synchronized acquisition
of RGB, Depth, LiDAR, and IMU data, supporting efficient and repeatable
long-term data collection across varying illumination and crop growth stages.
We benchmark a range of state-of-the-art 3D reconstruction models on the
AgriChrono dataset, highlighting the difficulty of reconstruction in real-world
field environments and demonstrating its value as a research asset for
advancing model generalization under dynamic conditions. The code and dataset
are publicly available at: https://github.com/StructuresComp/agri-chrono","Jaehwan Jeong, Tuan-Anh Vu, Mohammad Jony, Shahab Ahmad, Md. Mukhlesur Rahman, Sangpil Kim, M. Khalid Jawed",2025-08-26T05:39:47Z,2025-08-26T05:39:47Z,http://arxiv.org/abs/2508.18694v1,http://arxiv.org/pdf/2508.18694v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Mimicking associative learning of rats via a neuromorphic robot in open
  field maze using spatial cell models","Data-driven Artificial Intelligence (AI) approaches have exhibited remarkable
prowess across various cognitive tasks using extensive training data. However,
the reliance on large datasets and neural networks presents challenges such as
highpower consumption and limited adaptability, particularly in
SWaP-constrained applications like planetary exploration. To address these
issues, we propose enhancing the autonomous capabilities of intelligent robots
by emulating the associative learning observed in animals. Associative learning
enables animals to adapt to their environment by memorizing concurrent events.
By replicating this mechanism, neuromorphic robots can navigate dynamic
environments autonomously, learning from interactions to optimize performance.
This paper explores the emulation of associative learning in rodents using
neuromorphic robots within open-field maze environments, leveraging insights
from spatial cells such as place and grid cells. By integrating these models,
we aim to enable online associative learning for spatial tasks in real-time
scenarios, bridging the gap between biological spatial cognition and robotics
for advancements in autonomous systems.","Tianze Liu, Md Abu Bakr Siddique, Hongyu An",2025-08-25T20:17:38Z,2025-08-25T20:17:38Z,http://arxiv.org/abs/2508.18460v1,http://arxiv.org/pdf/2508.18460v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Efficient task and path planning for maintenance automation using a
  robot system","The research and development of intelligent automation solutions is a
ground-breaking point for the factory of the future. A promising and
challenging mission is the use of autonomous robot systems to automate tasks in
the field of maintenance. For this purpose, the robot system must be able to
plan autonomously the different manipulation tasks and the corresponding paths.
Basic requirements are the development of algorithms with a low computational
complexity and the possibility to deal with environmental uncertainties. In
this work, an approach is presented, which is especially suited to solve the
problem of maintenance automation. For this purpose, offline data from CAD is
combined with online data from an RGBD vision system via a probabilistic
filter, to compensate uncertainties from offline data. For planning the
different tasks, a method is explained, which use a symbolic description,
founded on a novel sampling-based method to compute the disassembly space. For
path planning we use global state-of-the art algorithms with a method that
allows the adaption of the exploration stepsize in order to reduce the planning
time. Every method is experimentally validated and discussed.","Christian Friedrich, Akos Csiszar, Armin Lechler, Alexander Verl",2025-08-25T18:40:27Z,2025-08-25T18:40:27Z,http://arxiv.org/abs/2508.18400v1,http://arxiv.org/pdf/2508.18400v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Object Detection with Multimodal Large Vision-Language Models: An
  In-depth Review","The fusion of language and vision in large vision-language models (LVLMs) has
revolutionized deep learning-based object detection by enhancing adaptability,
contextual reasoning, and generalization beyond traditional architectures. This
in-depth review presents a structured exploration of the state-of-the-art in
LVLMs, systematically organized through a three-step research review process.
First, we discuss the functioning of vision language models (VLMs) for object
detection, describing how these models harness natural language processing
(NLP) and computer vision (CV) techniques to revolutionize object detection and
localization. We then explain the architectural innovations, training
paradigms, and output flexibility of recent LVLMs for object detection,
highlighting how they achieve advanced contextual understanding for object
detection. The review thoroughly examines the approaches used in integration of
visual and textual information, demonstrating the progress made in object
detection using VLMs that facilitate more sophisticated object detection and
localization strategies. This review presents comprehensive visualizations
demonstrating LVLMs' effectiveness in diverse scenarios including localization
and segmentation, and then compares their real-time performance, adaptability,
and complexity to traditional deep learning systems. Based on the review, its
is expected that LVLMs will soon meet or surpass the performance of
conventional methods in object detection. The review also identifies a few
major limitations of the current LVLM modes, proposes solutions to address
those challenges, and presents a clear roadmap for the future advancement in
this field. We conclude, based on this study, that the recent advancement in
LVLMs have made and will continue to make a transformative impact on object
detection and robotic applications in the future.","Ranjan Sapkota, Manoj Karkee",2025-08-25T17:21:00Z,2025-08-25T17:21:00Z,http://arxiv.org/abs/2508.19294v1,http://arxiv.org/pdf/2508.19294v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"BirdRecorder's AI on Sky: Safeguarding birds of prey by detection and
  classification of tiny objects around wind turbines","The urgent need for renewable energy expansion, particularly wind power, is
hindered by conflicts with wildlife conservation. To address this, we developed
BirdRecorder, an advanced AI-based anti-collision system to protect endangered
birds, especially the red kite (Milvus milvus). Integrating robotics,
telemetry, and high-performance AI algorithms, BirdRecorder aims to detect,
track, and classify avian species within a range of 800 m to minimize
bird-turbine collisions.
  BirdRecorder integrates advanced AI methods with optimized hardware and
software architectures to enable real-time image processing. Leveraging Single
Shot Detector (SSD) for detection, combined with specialized hardware
acceleration and tracking algorithms, our system achieves high detection
precision while maintaining the speed necessary for real-time decision-making.
By combining these components, BirdRecorder outperforms existing approaches in
both accuracy and efficiency.
  In this paper, we summarize results on field tests and performance of the
BirdRecorder system. By bridging the gap between renewable energy expansion and
wildlife conservation, BirdRecorder contributes to a more sustainable
coexistence of technology and nature.","Nico Klar, Nizam Gifary, Felix P. G. Ziegler, Frank Sehnke, Anton Kaifel, Eric Price, Aamir Ahmad",2025-08-25T15:41:36Z,2025-08-25T15:41:36Z,http://arxiv.org/abs/2508.18136v1,http://arxiv.org/pdf/2508.18136v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Physical Embodiment Enables Information Processing Beyond Explicit
  Sensing in Active Matter","Living microorganisms have evolved dedicated sensory machinery to detect
environmental perturbations, processing these signals through biochemical
networks to guide behavior. Replicating such capabilities in synthetic active
matter remains a fundamental challenge. Here, we demonstrate that synthetic
active particles can adapt to hidden hydrodynamic perturbations through
physical embodiment alone, without explicit sensing mechanisms. Using
reinforcement learning to control self-thermophoretic particles, we show that
they learn navigation strategies to counteract unobserved flow fields by
exploiting information encoded in their physical dynamics. Remarkably,
particles successfully navigate perturbations that are not included in their
state inputs, revealing that embodied dynamics can serve as an implicit sensing
mechanism. This discovery establishes physical embodiment as a computational
resource for information processing in active matter, with implications for
autonomous microrobotic systems and bio-inspired computation.","Diptabrata Paul, Nikola Milosevic, Nico Scherf, Frank Cichos",2025-08-25T11:39:36Z,2025-08-25T11:39:36Z,http://arxiv.org/abs/2508.17921v1,http://arxiv.org/pdf/2508.17921v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Robotic Manipulation via Imitation Learning: Taxonomy, Evolution,
  Benchmark, and Challenges","Robotic Manipulation (RM) is central to the advancement of autonomous robots,
enabling them to interact with and manipulate objects in real-world
environments. This survey focuses on RM methodologies that leverage imitation
learning, a powerful technique that allows robots to learn complex manipulation
skills by mimicking human demonstrations. We identify and analyze the most
influential studies in this domain, selected based on community impact and
intrinsic quality. For each paper, we provide a structured summary, covering
the research purpose, technical implementation, hierarchical classification,
input formats, key priors, strengths and limitations, and citation metrics.
Additionally, we trace the chronological development of imitation learning
techniques within RM policy (RMP), offering a timeline of key technological
advancements. Where available, we report benchmark results and perform
quantitative evaluations to compare existing methods. By synthesizing these
insights, this review provides a comprehensive resource for researchers and
practitioners, highlighting both the state of the art and the challenges that
lie ahead in the field of robotic manipulation through imitation learning.","Zezeng Li, Alexandre Chapin, Enda Xiang, Rui Yang, Bruno Machado, Na Lei, Emmanuel Dellandrea, Di Huang, Liming Chen",2025-08-24T17:01:15Z,2025-09-04T16:46:34Z,http://arxiv.org/abs/2508.17449v2,http://arxiv.org/pdf/2508.17449v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
A Survey of Deep Learning-based Point Cloud Denoising,"Accurate 3D geometry acquisition is essential for a wide range of
applications, such as computer graphics, autonomous driving, robotics, and
augmented reality. However, raw point clouds acquired in real-world
environments are often corrupted with noise due to various factors such as
sensor, lighting, material, environment etc, which reduces geometric fidelity
and degrades downstream performance. Point cloud denoising is a fundamental
problem, aiming to recover clean point sets while preserving underlying
structures. Classical optimization-based methods, guided by hand-crafted
filters or geometric priors, have been extensively studied but struggle to
handle diverse and complex noise patterns. Recent deep learning approaches
leverage neural network architectures to learn distinctive representations and
demonstrate strong outcomes, particularly on complex and large-scale point
clouds. Provided these significant advances, this survey provides a
comprehensive and up-to-date review of deep learning-based point cloud
denoising methods up to August 2025. We organize the literature from two
perspectives: (1) supervision level (supervised vs. unsupervised), and (2)
modeling perspective, proposing a functional taxonomy that unifies diverse
approaches by their denoising principles. We further analyze architectural
trends both structurally and chronologically, establish a unified benchmark
with consistent training settings, and evaluate methods in terms of denoising
quality, surface fidelity, point distribution, and computational efficiency.
Finally, we discuss open challenges and outline directions for future research
in this rapidly evolving field.","Jinxi Wang, Ben Fei, Dasith de Silva Edirimuni, Zheng Liu, Ying He, Xuequan Lu",2025-08-23T12:53:24Z,2025-08-23T12:53:24Z,http://arxiv.org/abs/2508.17011v1,http://arxiv.org/pdf/2508.17011v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
COSMO-Bench: A Benchmark for Collaborative SLAM Optimization,"Recent years have seen a focus on research into distributed optimization
algorithms for multi-robot Collaborative Simultaneous Localization and Mapping
(C-SLAM). Research in this domain, however, is made difficult by a lack of
standard benchmark datasets. Such datasets have been used to great effect in
the field of single-robot SLAM, and researchers focused on multi-robot problems
would benefit greatly from dedicated benchmark datasets. To address this gap,
we design and release the Collaborative Open-Source Multi-robot Optimization
Benchmark (COSMO-Bench) -- a suite of 24 datasets derived from a
state-of-the-art C-SLAM front-end and real-world LiDAR data. Data DOI:
https://doi.org/10.1184/R1/29652158","Daniel McGann, Easton R. Potokar, Michael Kaess",2025-08-22T18:13:03Z,2025-08-22T18:13:03Z,http://arxiv.org/abs/2508.16731v1,http://arxiv.org/pdf/2508.16731v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Terrain Classification for the Spot Quadrupedal Mobile Robot Using Only
  Proprioceptive Sensing","Quadrupedal mobile robots can traverse a wider range of terrain types than
their wheeled counterparts but do not perform the same on all terrain types.
These robots are prone to undesirable behaviours like sinking and slipping on
challenging terrains. To combat this issue, we propose a terrain classifier
that provides information on terrain type that can be used in robotic systems
to create a traversability map to plan safer paths for the robot to navigate.
The work presented here is a terrain classifier developed for a Boston Dynamics
Spot robot. Spot provides over 100 measured proprioceptive signals describing
the motions of the robot and its four legs (e.g., foot penetration, forces,
joint angles, etc.). The developed terrain classifier combines dimensionality
reduction techniques to extract relevant information from the signals and then
applies a classification technique to differentiate terrain based on
traversability. In representative field testing, the resulting terrain
classifier was able to identify three different terrain types with an accuracy
of approximately 97%","Sophie Villemure, Jefferson Silveira, Joshua A. Marshall",2025-08-22T16:29:11Z,2025-08-22T16:29:11Z,http://arxiv.org/abs/2508.16504v1,http://arxiv.org/pdf/2508.16504v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Take That for Me: Multimodal Exophora Resolution with Interactive
  Questioning for Ambiguous Out-of-View Instructions","Daily life support robots must interpret ambiguous verbal instructions
involving demonstratives such as ``Bring me that cup,'' even when objects or
users are out of the robot's view. Existing approaches to exophora resolution
primarily rely on visual data and thus fail in real-world scenarios where the
object or user is not visible. We propose Multimodal Interactive Exophora
resolution with user Localization (MIEL), which is a multimodal exophora
resolution framework leveraging sound source localization (SSL), semantic
mapping, visual-language models (VLMs), and interactive questioning with
GPT-4o. Our approach first constructs a semantic map of the environment and
estimates candidate objects from a linguistic query with the user's skeletal
data. SSL is utilized to orient the robot toward users who are initially
outside its visual field, enabling accurate identification of user gestures and
pointing directions. When ambiguities remain, the robot proactively interacts
with the user, employing GPT-4o to formulate clarifying questions. Experiments
in a real-world environment showed results that were approximately 1.3 times
better when the user was visible to the robot and 2.0 times better when the
user was not visible to the robot, compared to the methods without SSL and
interactive questioning. The project website is
https://emergentsystemlabstudent.github.io/MIEL/.","Akira Oyama, Shoichi Hasegawa, Akira Taniguchi, Yoshinobu Hagiwara, Tadahiro Taniguchi",2025-08-22T07:09:06Z,2025-08-22T07:09:06Z,http://arxiv.org/abs/2508.16143v1,http://arxiv.org/pdf/2508.16143v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"UnPose: Uncertainty-Guided Diffusion Priors for Zero-Shot Pose
  Estimation","Estimating the 6D pose of novel objects is a fundamental yet challenging
problem in robotics, often relying on access to object CAD models. However,
acquiring such models can be costly and impractical. Recent approaches aim to
bypass this requirement by leveraging strong priors from foundation models to
reconstruct objects from single or multi-view images, but typically require
additional training or produce hallucinated geometry. To this end, we propose
UnPose, a novel framework for zero-shot, model-free 6D object pose estimation
and reconstruction that exploits 3D priors and uncertainty estimates from a
pre-trained diffusion model. Specifically, starting from a single-view RGB-D
frame, UnPose uses a multi-view diffusion model to estimate an initial 3D model
using 3D Gaussian Splatting (3DGS) representation, along with pixel-wise
epistemic uncertainty estimates. As additional observations become available,
we incrementally refine the 3DGS model by fusing new views guided by the
diffusion model's uncertainty, thereby continuously improving the pose
estimation accuracy and 3D reconstruction quality. To ensure global
consistency, the diffusion prior-generated views and subsequent observations
are further integrated in a pose graph and jointly optimized into a coherent
3DGS field. Extensive experiments demonstrate that UnPose significantly
outperforms existing approaches in both 6D pose estimation accuracy and 3D
reconstruction quality. We further showcase its practical applicability in
real-world robotic manipulation tasks.","Zhaodong Jiang, Ashish Sinha, Tongtong Cao, Yuan Ren, Bingbing Liu, Binbin Xu",2025-08-21T21:31:04Z,2025-08-21T21:31:04Z,http://arxiv.org/abs/2508.15972v1,http://arxiv.org/pdf/2508.15972v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Mag-Match: Magnetic Vector Field Features for Map Matching and
  Registration","Map matching and registration are essential tasks in robotics for
localisation and integration of multi-session or multi-robot data. Traditional
methods rely on cameras or LiDARs to capture visual or geometric information
but struggle in challenging conditions like smoke or dust. Magnetometers, on
the other hand, detect magnetic fields, revealing features invisible to other
sensors and remaining robust in such environments. In this paper, we introduce
Mag-Match, a novel method for extracting and describing features in 3D magnetic
vector field maps to register different maps of the same area. Our feature
descriptor, based on higher-order derivatives of magnetic field maps, is
invariant to global orientation, eliminating the need for gravity-aligned
mapping. To obtain these higher-order derivatives map-wide given point-wise
magnetometer data, we leverage a physics-informed Gaussian Process to perform
efficient and recursive probabilistic inference of both the magnetic field and
its derivatives. We evaluate Mag-Match in simulated and real-world experiments
against a SIFT-based approach, demonstrating accurate map-to-map, robot-to-map,
and robot-to-robot transformations - even without initial gravitational
alignment.","William McDonald, Cedric Le Gentil, Jennifer Wakulicz, Teresa Vidal-Calleja",2025-08-21T06:41:41Z,2025-08-21T06:41:41Z,http://arxiv.org/abs/2508.15300v1,http://arxiv.org/pdf/2508.15300v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Decentralized Vision-Based Autonomous Aerial Wildlife Monitoring,"Wildlife field operations demand efficient parallel deployment methods to
identify and interact with specific individuals, enabling simultaneous
collective behavioral analysis, and health and safety interventions. Previous
robotics solutions approach the problem from the herd perspective, or are
manually operated and limited in scale. We propose a decentralized vision-based
multi-quadrotor system for wildlife monitoring that is scalable, low-bandwidth,
and sensor-minimal (single onboard RGB camera). Our approach enables robust
identification and tracking of large species in their natural habitat. We
develop novel vision-based coordination and tracking algorithms designed for
dynamic, unstructured environments without reliance on centralized
communication or control. We validate our system through real-world
experiments, demonstrating reliable deployment in diverse field conditions.","Makram Chahine, William Yang, Alaa Maalouf, Justin Siriska, Ninad Jadhav, Daniel Vogt, Stephanie Gil, Robert Wood, Daniela Rus",2025-08-20T20:05:05Z,2025-08-20T20:05:05Z,http://arxiv.org/abs/2508.15038v1,http://arxiv.org/pdf/2508.15038v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"An Informative Planning Framework for Target Tracking and Active Mapping
  in Dynamic Environments with ASVs","Mobile robot platforms are increasingly being used to automate information
gathering tasks such as environmental monitoring. Efficient target tracking in
dynamic environments is critical for applications such as search and rescue and
pollutant cleanups. In this letter, we study active mapping of floating targets
that drift due to environmental disturbances such as wind and currents. This is
a challenging problem as it involves predicting both spatial and temporal
variations in the map due to changing conditions. We propose an informative
path planning framework to map an arbitrary number of moving targets with
initially unknown positions in dynamic environments. A key component of our
approach is a spatiotemporal prediction network that predicts target position
distributions over time. We propose an adaptive planning objective for target
tracking that leverages these predictions. Simulation experiments show that our
proposed planning objective improves target tracking performance compared to
existing methods that consider only entropy reduction as the planning
objective. Finally, we validate our approach in field tests using an autonomous
surface vehicle, showcasing its ability to track targets in real-world
monitoring scenarios.","Sanjeev Ramkumar Sudha, Marija Popović, Erlend M. Coates",2025-08-20T11:44:30Z,2025-08-21T11:04:50Z,http://arxiv.org/abs/2508.14636v2,http://arxiv.org/pdf/2508.14636v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"TRUST-Planner: Topology-guided Robust Trajectory Planner for AAVs with
  Uncertain Obstacle Spatial-temporal Avoidance","Despite extensive developments in motion planning of autonomous aerial
vehicles (AAVs), existing frameworks faces the challenges of local minima and
deadlock in complex dynamic environments, leading to increased collision risks.
To address these challenges, we present TRUST-Planner, a topology-guided
hierarchical planning framework for robust spatial-temporal obstacle avoidance.
In the frontend, a dynamic enhanced visible probabilistic roadmap (DEV-PRM) is
proposed to rapidly explore topological paths for global guidance. The backend
utilizes a uniform terminal-free minimum control polynomial (UTF-MINCO) and
dynamic distance field (DDF) to enable efficient predictive obstacle avoidance
and fast parallel computation. Furthermore, an incremental multi-branch
trajectory management framework is introduced to enable spatio-temporal
topological decision-making, while efficiently leveraging historical
information to reduce replanning time. Simulation results show that
TRUST-Planner outperforms baseline competitors, achieving a 96\% success rate
and millisecond-level computation efficiency in tested complex environments.
Real-world experiments further validate the feasibility and practicality of the
proposed method.","Junzhi Li, Teng Long, Jingliang Sun, Jianxin Zhong",2025-08-20T10:52:28Z,2025-08-20T10:52:28Z,http://arxiv.org/abs/2508.14610v1,http://arxiv.org/pdf/2508.14610v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Dimension-Decomposed Learning for Quadrotor Geometric Attitude Control
  with Almost Global Exponential Convergence on SO(3)","This paper introduces a lightweight and interpretable online learning
approach called Dimension-Decomposed Learning (DiD-L) for disturbance
identification in quadrotor geometric attitude control. As a module instance of
DiD-L, we propose the Sliced Adaptive-Neuro Mapping (SANM). Specifically, to
address underlying underfitting problems, the high-dimensional mapping for
online identification is axially ``sliced"" into multiple low-dimensional
submappings (slices). In this way, the complex high-dimensional problem is
decomposed into a set of simple low-dimensional subtasks addressed by shallow
neural networks and adaptive laws. These neural networks and adaptive laws are
updated online via Lyapunov-based adaptation without the persistent excitation
(PE) condition. To enhance the interpretability of the proposed approach, we
prove that the state solution of the rotational error dynamics exponentially
converges into an arbitrarily small ball within an almost global attraction
domain, despite time-varying disturbances and inertia uncertainties. This
result is novel as it demonstrates exponential convergence without requiring
pre-training for unseen disturbances and specific knowledge of the model. To
our knowledge in the quadrotor control field, DiD-L is the first online
learning approach that is lightweight enough to run in real-time at 400 Hz on
microcontroller units (MCUs) such as STM32, and has been validated through
real-world experiments.","Tianhua Gao, Masashi Izumita, Kohji Tomita, Akiya Kamimura",2025-08-20T04:41:42Z,2025-08-28T11:58:55Z,http://arxiv.org/abs/2508.14422v2,http://arxiv.org/pdf/2508.14422v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Blast Hole Seeking and Dipping -- The Navigation and Perception
  Framework in a Mine Site Inspection Robot","In open-pit mining, holes are drilled into the surface of the excavation site
and detonated with explosives to facilitate digging. These blast holes need to
be inspected internally for investigation of downhole material types and
properties. Knowing these properties can lead to significant savings in
material handling costs in downstream processes. Manual hole inspection is slow
and expensive, with major limitations in revealing the geometric and geological
properties of the holes and their contents. This has been the motivation for
the development of our autonomous mine-site inspection robot - ""DIPPeR"". In
this paper, the automation aspect of the project is explained. We present a
robust blast hole seeking and detection framework that enables target-based
navigation and accurate down-hole sensor positioning. The pipeline first
processes point-cloud data collected by the on-board LiDAR sensors, extracting
the cone-shaped volume of drill-waste above the ground. By projecting the 3D
cone points into a virtual depth image, segmentation is achieved in the 2D
domain, yielding a circular hole at the image centre and a collared cone face.
We then identify the hole centre using a robust detection module while
suppressing non-maximum candidates, ensuring precise sensor placement for
down-hole inspection and avoiding collisions with the cavity wall. To enable
autonomous hole-seeking, the pipeline automatically adjusts its projection
parameters during robot navigation to account for variations in point sparsity
and hole opening size, ensuring a consistent hole appearance in 2D images. This
allows continuous tracking of the target hole as the robot approaches the goal
point. We demonstrate the effectiveness of our navigation and perception system
in both high-fidelity simulation environments and on-site field tests. A
demonstration video is available at
""https://www.youtube.com/watch?v=fRNbcBcaSqE"".","Liyang Liu, Ehsan Mihankhah, Nathan Wallace, Javier Martinez, Andrew J. Hill",2025-08-19T12:40:20Z,2025-08-19T12:40:20Z,http://arxiv.org/abs/2508.13785v1,http://arxiv.org/pdf/2508.13785v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Assessing Pedestrian Behavior Around Autonomous Cleaning Robots in
  Public Spaces: Findings from a Field Observation","As autonomous robots become more common in public spaces, spontaneous
encounters with laypersons are more frequent. For this, robots need to be
equipped with communication strategies that enhance momentary transparency and
reduce the probability of critical situations. Adapting these robotic
strategies requires consideration of robot movements, environmental conditions,
and user characteristics and states. While numerous studies have investigated
the impact of distraction on pedestrians' movement behavior, limited research
has examined this behavior in the presence of autonomous robots. This research
addresses the impact of robot type and robot movement pattern on distracted and
undistracted pedestrians' movement behavior. In a field setting, unaware
pedestrians were videotaped while moving past two working, autonomous cleaning
robots. Out of N=498 observed pedestrians, approximately 8% were distracted by
smartphones. Distracted and undistracted pedestrians did not exhibit
significant differences in their movement behaviors around the robots. Instead,
both the larger sweeping robot and the offset rectangular movement pattern
significantly increased the number of lateral adaptations compared to the
smaller cleaning robot and the circular movement pattern. The offset
rectangular movement pattern also led to significantly more close lateral
adaptations. Depending on the robot type, the movement patterns led to
differences in the distances of lateral adaptations. The study provides initial
insights into pedestrian movement behavior around an autonomous cleaning robot
in public spaces, contributing to the growing field of HRI research.","Maren Raab, Linda Miller, Zhe Zeng, Pascal Jansen, Martin Baumann, Johannes Kraus",2025-08-19T10:05:08Z,2025-08-19T10:05:08Z,http://arxiv.org/abs/2508.13699v1,http://arxiv.org/pdf/2508.13699v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Multi-Robot Navigation in Social Mini-Games: Definitions, Taxonomy, and
  Algorithms","The ``Last Mile Challenge'' has long been considered an important, yet
unsolved, challenge for autonomous vehicles, public service robots, and
delivery robots. A central issue in this challenge is the ability of robots to
navigate constrained and cluttered environments that have high agency (e.g.,
doorways, hallways, corridor intersections), often while competing for space
with other robots and humans. We refer to these environments as ``Social
Mini-Games'' (SMGs). Traditional navigation approaches designed for MRN do not
perform well in SMGs, which has led to focused research on dedicated SMG
solvers. However, publications on SMG navigation research make different
assumptions (on centralized versus decentralized, observability, communication,
cooperation, etc.), and have different objective functions (safety versus
liveness). These assumptions and objectives are sometimes implicitly assumed or
described informally. This makes it difficult to establish appropriate
baselines for comparison in research papers, as well as making it difficult for
practitioners to find the papers relevant to their concrete application. Such
ad-hoc representation of the field also presents a barrier to new researchers
wanting to start research in this area. SMG navigation research requires its
own taxonomy, definitions, and evaluation protocols to guide effective research
moving forward. This survey is the first to catalog SMG solvers using a
well-defined and unified taxonomy and to classify existing methods accordingly.
It also discusses the essential properties of SMG solvers, defines what SMGs
are and how they appear in practice, outlines how to evaluate SMG solvers, and
highlights the differences between SMG solvers and general navigation systems.
The survey concludes with an overview of future directions and open challenges
in the field.","Rohan Chandra, Shubham Singh, Abhishek Jha, Dannon Andrade, Hriday Sainathuni, Katia Sycara",2025-08-19T02:33:15Z,2025-08-20T02:15:48Z,http://arxiv.org/abs/2508.13459v2,http://arxiv.org/pdf/2508.13459v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"2COOOL: 2nd Workshop on the Challenge Of Out-Of-Label Hazards in
  Autonomous Driving","As the computer vision community advances autonomous driving algorithms,
integrating vision-based insights with sensor data remains essential for
improving perception, decision making, planning, prediction, simulation, and
control. Yet we must ask: Why don't we have entirely safe self-driving cars
yet? A key part of the answer lies in addressing novel scenarios, one of the
most critical barriers to real-world deployment. Our 2COOOL workshop provides a
dedicated forum for researchers and industry experts to push the state of the
art in novelty handling, including out-of-distribution hazard detection,
vision-language models for hazard understanding, new benchmarking and
methodologies, and safe autonomous driving practices. The 2nd Workshop on the
Challenge of Out-of-Label Hazards in Autonomous Driving (2COOOL) will be held
at the International Conference on Computer Vision (ICCV) 2025 in Honolulu,
Hawaii, on October 19, 2025. We aim to inspire the development of new
algorithms and systems for hazard avoidance, drawing on ideas from anomaly
detection, open-set recognition, open-vocabulary modeling, domain adaptation,
and related fields. Building on the success of its inaugural edition at the
Winter Conference on Applications of Computer Vision (WACV) 2025, the workshop
will feature a mix of academic and industry participation.","Ali K. AlShami, Ryan Rabinowitz, Maged Shoman, Jianwu Fang, Lukas Picek, Shao-Yuan Lo, Steve Cruz, Khang Nhut Lam, Nachiket Kamod, Lei-Lei Li, Jugal Kalita, Terrance E. Boult",2025-08-18T18:55:54Z,2025-08-18T18:55:54Z,http://arxiv.org/abs/2508.21080v1,http://arxiv.org/pdf/2508.21080v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"IGFuse: Interactive 3D Gaussian Scene Reconstruction via Multi-Scans
  Fusion","Reconstructing complete and interactive 3D scenes remains a fundamental
challenge in computer vision and robotics, particularly due to persistent
object occlusions and limited sensor coverage. Multiview observations from a
single scene scan often fail to capture the full structural details. Existing
approaches typically rely on multi stage pipelines, such as segmentation,
background completion, and inpainting or require per-object dense scanning,
both of which are error-prone, and not easily scalable. We propose IGFuse, a
novel framework that reconstructs interactive Gaussian scene by fusing
observations from multiple scans, where natural object rearrangement between
captures reveal previously occluded regions. Our method constructs segmentation
aware Gaussian fields and enforces bi-directional photometric and semantic
consistency across scans. To handle spatial misalignments, we introduce a
pseudo-intermediate scene state for unified alignment, alongside collaborative
co-pruning strategies to refine geometry. IGFuse enables high fidelity
rendering and object level scene manipulation without dense observations or
complex pipelines. Extensive experiments validate the framework's strong
generalization to novel scene configurations, demonstrating its effectiveness
for real world 3D reconstruction and real-to-simulation transfer. Our project
page is available online.","Wenhao Hu, Zesheng Li, Haonan Zhou, Liu Liu, Xuexiang Wen, Zhizhong Su, Xi Li, Gaoang Wang",2025-08-18T17:59:47Z,2025-08-18T17:59:47Z,http://arxiv.org/abs/2508.13153v1,http://arxiv.org/pdf/2508.13153v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Foundation Model for Skeleton-Based Human Action Understanding,"Human action understanding serves as a foundational pillar in the field of
intelligent motion perception. Skeletons serve as a modality- and
device-agnostic representation for human modeling, and skeleton-based action
understanding has potential applications in humanoid robot control and
interaction. \RED{However, existing works often lack the scalability and
generalization required to handle diverse action understanding tasks. There is
no skeleton foundation model that can be adapted to a wide range of action
understanding tasks}. This paper presents a Unified Skeleton-based Dense
Representation Learning (USDRL) framework, which serves as a foundational model
for skeleton-based human action understanding. USDRL consists of a
Transformer-based Dense Spatio-Temporal Encoder (DSTE), Multi-Grained Feature
Decorrelation (MG-FD), and Multi-Perspective Consistency Training (MPCT). The
DSTE module adopts two parallel streams to learn temporal dynamic and spatial
structure features. The MG-FD module collaboratively performs feature
decorrelation across temporal, spatial, and instance domains to reduce
dimensional redundancy and enhance information extraction. The MPCT module
employs both multi-view and multi-modal self-supervised consistency training.
The former enhances the learning of high-level semantics and mitigates the
impact of low-level discrepancies, while the latter effectively facilitates the
learning of informative multimodal features. We perform extensive experiments
on 25 benchmarks across across 9 skeleton-based action understanding tasks,
covering coarse prediction, dense prediction, and transferred prediction. Our
approach significantly outperforms the current state-of-the-art methods. We
hope that this work would broaden the scope of research in skeleton-based
action understanding and encourage more attention to dense prediction tasks.","Hongsong Wang, Wanjiang Weng, Junbo Wang, Fang Zhao, Guo-Sen Xie, Xin Geng, Liang Wang",2025-08-18T02:42:16Z,2025-08-18T02:42:16Z,http://arxiv.org/abs/2508.12586v1,http://arxiv.org/pdf/2508.12586v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Efficient Environment Design for Multi-Robot Navigation via Continuous
  Control","Multi-robot navigation and path planning in continuous state and action
spaces with uncertain environments remains an open challenge. Deep
Reinforcement Learning (RL) is one of the most popular paradigms for solving
this task, but its real-world application has been limited due to sample
inefficiency and long training periods. Moreover, the existing works using RL
for multi-robot navigation lack formal guarantees while designing the
environment. In this paper, we introduce an efficient and highly customizable
environment for continuous-control multi-robot navigation, where the robots
must visit a set of regions of interest (ROIs) by following the shortest paths.
The task is formally modeled as a Markov Decision Process (MDP). We describe
the multi-robot navigation task as an optimization problem and relate it to
finding an optimal policy for the MDP. We crafted several variations of the
environment and measured the performance using both gradient and non-gradient
based RL methods: A2C, PPO, TRPO, TQC, CrossQ and ARS. To show real-world
applicability, we deployed our environment to a 3-D agricultural field with
uncertainties using the CoppeliaSim robot simulator and measured the robustness
by running inference on the learned models. We believe our work will guide the
researchers on how to develop MDP-based environments that are applicable to
real-world systems and solve them using the existing state-of-the-art RL
methods with limited resources and within reasonable time periods.","Jahid Chowdhury Choton, John Woods, William Hsu",2025-08-17T16:18:07Z,2025-08-17T16:18:07Z,http://arxiv.org/abs/2508.14105v1,http://arxiv.org/pdf/2508.14105v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Bimanual Robot-Assisted Dressing: A Spherical Coordinate-Based Strategy
  for Tight-Fitting Garments","Robot-assisted dressing is a popular but challenging topic in the field of
robotic manipulation, offering significant potential to improve the quality of
life for individuals with mobility limitations. Currently, the majority of
research on robot-assisted dressing focuses on how to put on loose-fitting
clothing, with little attention paid to tight garments. For the former, since
the armscye is larger, a single robotic arm can usually complete the dressing
task successfully. However, for the latter, dressing with a single robotic arm
often fails due to the narrower armscye and the property of diminishing
rigidity in the armscye, which eventually causes the armscye to get stuck. This
paper proposes a bimanual dressing strategy suitable for dressing tight-fitting
clothing. To facilitate the encoding of dressing trajectories that adapt to
different human arm postures, a spherical coordinate system for dressing is
established. We uses the azimuthal angle of the spherical coordinate system as
a task-relevant feature for bimanual manipulation. Based on this new
coordinate, we employ Gaussian Mixture Model (GMM) and Gaussian Mixture
Regression (GMR) for imitation learning of bimanual dressing trajectories,
generating dressing strategies that adapt to different human arm postures. The
effectiveness of the proposed method is validated through various experiments.","Jian Zhao, Yunlong Lian, Andy M Tyrrell, Michael Gienger, Jihong Zhu",2025-08-17T07:45:34Z,2025-08-17T07:45:34Z,http://arxiv.org/abs/2508.12274v1,http://arxiv.org/pdf/2508.12274v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"From Screen to Stage: Kid Cosmo, A Life-Like, Torque-Controlled Humanoid
  for Entertainment Robotics","Humanoid robots represent the cutting edge of robotics research, yet their
potential in entertainment remains largely unexplored. Entertainment as a field
prioritizes visuals and form, a principle that contrasts with the purely
functional designs of most contemporary humanoid robots. Designing
entertainment humanoid robots capable of fluid movement presents a number of
unique challenges. In this paper, we present Kid Cosmo, a research platform
designed for robust locomotion and life-like motion generation while imitating
the look and mannerisms of its namesake character from Netflix's movie The
Electric State. Kid Cosmo is a child-sized humanoid robot, standing 1.45 m tall
and weighing 25 kg. It contains 28 degrees of freedom and primarily uses
proprioceptive actuators, enabling torque-control walking and lifelike motion
generation. Following worldwide showcases as part of the movie's press tour, we
present the system architecture, challenges of a functional entertainment robot
and unique solutions, and our initial findings on stability during simultaneous
upper and lower body movement. We demonstrate the viability of
performance-oriented humanoid robots that prioritize both character embodiment
and technical functionality.","Havel Liu, Mingzhang Zhu, Arturo Moises Flores Alvarez, Yuan Hung Lo, Conrad Ku, Federico Parres, Justin Quan, Colin Togashi, Aditya Navghare, Quanyou Wang, Dennis W. Hong",2025-08-16T03:14:15Z,2025-08-16T03:14:15Z,http://arxiv.org/abs/2508.11884v1,http://arxiv.org/pdf/2508.11884v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Recent Advances in Transformer and Large Language Models for UAV
  Applications","The rapid advancement of Transformer-based models has reshaped the landscape
of uncrewed aerial vehicle (UAV) systems by enhancing perception,
decision-making, and autonomy. This review paper systematically categorizes and
evaluates recent developments in Transformer architectures applied to UAVs,
including attention mechanisms, CNN-Transformer hybrids, reinforcement learning
Transformers, and large language models (LLMs). Unlike previous surveys, this
work presents a unified taxonomy of Transformer-based UAV models, highlights
emerging applications such as precision agriculture and autonomous navigation,
and provides comparative analyses through structured tables and performance
benchmarks. The paper also reviews key datasets, simulators, and evaluation
metrics used in the field. Furthermore, it identifies existing gaps in the
literature, outlines critical challenges in computational efficiency and
real-time deployment, and offers future research directions. This comprehensive
synthesis aims to guide researchers and practitioners in understanding and
advancing Transformer-driven UAV technologies.","Hamza Kheddar, Yassine Habchi, Mohamed Chahine Ghanem, Mustapha Hemis, Dusit Niyato",2025-08-15T22:56:37Z,2025-08-15T22:56:37Z,http://arxiv.org/abs/2508.11834v1,http://arxiv.org/pdf/2508.11834v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Nominal Evaluation Of Automatic Multi-Sections Control Potential In
  Comparison To A Simpler One- Or Two-Sections Alternative With Predictive
  Spray Switching","Automatic Section Control (ASC) is a long-standing trend for spraying in
agriculture. It promises to minimise spray overlap areas. The core idea is to
(i) switch off spray nozzles on areas that have already been sprayed, and (ii)
to dynamically adjust nozzle flow rates along the boom bar that holds the spray
nozzles when velocities of boom sections vary during turn maneuvers. ASC is not
possible without sensors, in particular for accurate positioning data. Spraying
and the movement of modern wide boom bars are highly dynamic processes. In
addition, many uncertainty factors have an effect such as cross wind drift,
boom height, nozzle clogging in open-field conditions, and so forth. In view of
this complexity, the natural question arises if a simpler alternative exist.
Therefore, an Automatic Multi-Sections Control method is compared to a proposed
simpler one- or two-sections alternative that uses predictive spray switching.
The comparison is provided under nominal conditions. Agricultural spraying is
intrinsically linked to area coverage path planning and spray switching logic.
Combinations of two area coverage path planning and switching logics as well as
three sections-setups are compared. The three sections-setups differ by
controlling 48 sections, 2 sections or controlling all nozzles uniformly with
the same control signal as one single section. Methods are evaluated on 10
diverse real-world field examples, including non-convex field contours,
freeform mainfield lanes and multiple obstacle areas. A preferred method is
suggested that (i) minimises area coverage pathlength, (ii) offers intermediate
overlap, (iii) is suitable for manual driving by following a pre-planned
predictive spray switching logic for an area coverage path plan, and (iv) and
in contrast to ASC can be implemented sensor-free and therefore at low cost.",Mogens Plessen,2025-08-15T16:27:44Z,2025-08-15T16:27:44Z,http://arxiv.org/abs/2508.11573v1,http://arxiv.org/pdf/2508.11573v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Tactile Robotics: An Outlook,"Robotics research has long sought to give robots the ability to perceive the
physical world through touch in an analogous manner to many biological systems.
Developing such tactile capabilities is important for numerous emerging
applications that require robots to co-exist and interact closely with humans.
Consequently, there has been growing interest in tactile sensing, leading to
the development of various technologies, including piezoresistive and
piezoelectric sensors, capacitive sensors, magnetic sensors, and optical
tactile sensors. These diverse approaches utilise different transduction
methods and materials to equip robots with distributed sensing capabilities,
enabling more effective physical interactions. These advances have been
supported in recent years by simulation tools that generate large-scale tactile
datasets to support sensor designs and algorithms to interpret and improve the
utility of tactile data. The integration of tactile sensing with other
modalities, such as vision, as well as with action strategies for active
tactile perception highlights the growing scope of this field. To further the
transformative progress in tactile robotics, a holistic approach is essential.
In this outlook article, we examine several challenges associated with the
current state of the art in tactile robotics and explore potential solutions to
inspire innovations across multiple domains, including manufacturing,
healthcare, recycling and agriculture.","Shan Luo, Nathan F. Lepora, Wenzhen Yuan, Kaspar Althoefer, Gordon Cheng, Ravinder Dahiya",2025-08-15T06:55:31Z,2025-08-15T06:55:31Z,http://arxiv.org/abs/2508.11261v1,http://arxiv.org/pdf/2508.11261v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Embodied Edge Intelligence Meets Near Field Communication: Concept,
  Design, and Verification","Realizing embodied artificial intelligence is challenging due to the huge
computation demands of large models (LMs). To support LMs while ensuring
real-time inference, embodied edge intelligence (EEI) is a promising paradigm,
which leverages an LM edge to provide computing powers in close proximity to
embodied robots. Due to embodied data exchange, EEI requires higher spectral
efficiency, enhanced communication security, and reduced inter-user
interference. To meet these requirements, near-field communication (NFC), which
leverages extremely large antenna arrays as its hardware foundation, is an
ideal solution. Therefore, this paper advocates the integration of EEI and NFC,
resulting in a near-field EEI (NEEI) paradigm. However, NEEI also introduces
new challenges that cannot be adequately addressed by isolated EEI or NFC
designs, creating research opportunities for joint optimization of both
functionalities. To this end, we propose radio-friendly embodied planning for
EEI-assisted NFC scenarios and view-guided beam-focusing for NFC-assisted EEI
scenarios. We also elaborate how to realize resource-efficient NEEI through
opportunistic collaborative navigation. Experimental results are provided to
confirm the superiority of the proposed techniques compared with various
benchmarks.","Guoliang Li, Xibin Jin, Yujie Wan, Chenxuan Liu, Tong Zhang, Shuai Wang, Chengzhong Xu",2025-08-15T05:43:41Z,2025-08-15T05:43:41Z,http://arxiv.org/abs/2508.11232v1,http://arxiv.org/pdf/2508.11232v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"An Open-Source User-Friendly Interface for Simulating Magnetic Soft
  Robots using Simulation Open Framework Architecture (SOFA)","Soft robots, particularly magnetic soft robots, require specialized
simulation tools to accurately model their deformation under external magnetic
fields. However, existing platforms often lack dedicated support for magnetic
materials, making them difficult to use for researchers at different expertise
levels. This work introduces an open-source, user-friendly simulation interface
using the Simulation Open Framework Architecture (SOFA), specifically designed
to model magnetic soft robots. The tool enables users to define material
properties, apply magnetic fields, and observe resulting deformations in real
time. By integrating intuitive controls and stress analysis capabilities, it
aims to bridge the gap between theoretical modeling and practical design. Four
benchmark models -- a beam, three- and four-finger grippers, and a butterfly --
demonstrate its functionality. The software's ease of use makes it accessible
to both beginners and advanced researchers. Future improvements will refine
accuracy through experimental validation and comparison with industry-standard
finite element solvers, ensuring realistic and predictive simulations of
magnetic soft robots.","Carla Wehner, Finn Schubert, Heiko Hellkamp, Julius Hahnewald, Kilian Schaefer, Muhammad Bilal Khan, Oliver Gutfleisch",2025-08-14T14:30:31Z,2025-08-15T09:16:48Z,http://arxiv.org/abs/2508.10686v2,http://arxiv.org/pdf/2508.10686v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Why Report Failed Interactions With Robots?! Towards Vignette-based
  Interaction Quality","Although the quality of human-robot interactions has improved with the advent
of LLMs, there are still various factors that cause systems to be sub-optimal
when compared to human-human interactions. The nature and criticality of
failures are often dependent on the context of the interaction and so cannot be
generalized across the wide range of scenarios and experiments which have been
implemented in HRI research. In this work we propose the use of a technique
overlooked in the field of HRI, ethnographic vignettes, to clearly highlight
these failures, particularly those that are rarely documented. We describe the
methodology behind the process of writing vignettes and create our own based on
our personal experiences with failures in HRI systems. We emphasize the
strength of vignettes as the ability to communicate failures from a
multi-disciplinary perspective, promote transparency about the capabilities of
robots, and document unexpected behaviours which would otherwise be omitted
from research reports. We encourage the use of vignettes to augment existing
interaction evaluation methods.","Agnes Axelsson, Merle Reimann, Ronald Cumbal, Hannah Pelikan, Divesh Lala",2025-08-14T12:44:39Z,2025-08-15T16:07:03Z,http://arxiv.org/abs/2508.10603v2,http://arxiv.org/pdf/2508.10603v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Observations of atypical users from a pilot deployment of a public-space
  social robot in a church","Though a goal of HRI is the natural integration of social robots into
everyday public spaces, real-world studies still occur mostly within controlled
environments with predetermined participants. True public spaces present an
environment which is largely unconstrained and unpredictable, frequented by a
diverse range of people whose goals can often conflict with those of the robot.
When combined with the general unfamiliarity most people have with social
robots, this leads to unexpected human-robot interactions in these public
spaces that are rarely discussed or detected in other contexts. In this paper,
we describe atypical users we observed interacting with our robot, and those
who did not, during a three-day pilot deployment within a large working church
and visitor attraction. We then discuss theoretical future advances in the
field that could address these challenges, as well as immediate practical
mitigations and strategies to help improve public space human-robot
interactions in the present. This work contributes empirical insights into the
dynamics of human-robot interaction in public environments and offers
actionable guidance for more effective future deployments for social robot
designers.","Andrew Blair, Peggy Gregory, Mary Ellen Foster",2025-08-14T07:35:11Z,2025-08-14T07:35:11Z,http://arxiv.org/abs/2508.16622v1,http://arxiv.org/pdf/2508.16622v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"RayletDF: Raylet Distance Fields for Generalizable 3D Surface
  Reconstruction from Point Clouds or Gaussians","In this paper, we present a generalizable method for 3D surface
reconstruction from raw point clouds or pre-estimated 3D Gaussians by 3DGS from
RGB images. Unlike existing coordinate-based methods which are often
computationally intensive when rendering explicit surfaces, our proposed
method, named RayletDF, introduces a new technique called raylet distance
field, which aims to directly predict surface points from query rays. Our
pipeline consists of three key modules: a raylet feature extractor, a raylet
distance field predictor, and a multi-raylet blender. These components work
together to extract fine-grained local geometric features, predict raylet
distances, and aggregate multiple predictions to reconstruct precise surface
points. We extensively evaluate our method on multiple public real-world
datasets, demonstrating superior performance in surface reconstruction from
point clouds or 3D Gaussians. Most notably, our method achieves exceptional
generalization ability, successfully recovering 3D surfaces in a single-forward
pass across unseen datasets in testing.","Shenxing Wei, Jinxi Li, Yafei Yang, Siyuan Zhou, Bo Yang",2025-08-13T14:05:21Z,2025-08-13T14:05:21Z,http://arxiv.org/abs/2508.09830v1,http://arxiv.org/pdf/2508.09830v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Surg-InvNeRF: Invertible NeRF for 3D tracking and reconstruction in
  surgical vision","We proposed a novel test-time optimisation (TTO) approach framed by a
NeRF-based architecture for long-term 3D point tracking. Most current methods
in point tracking struggle to obtain consistent motion or are limited to 2D
motion. TTO approaches frame the solution for long-term tracking as optimising
a function that aggregates correspondences from other specialised
state-of-the-art methods. Unlike the state-of-the-art on TTO, we propose
parametrising such a function with our new invertible Neural Radiance Field
(InvNeRF) architecture to perform both 2D and 3D tracking in surgical
scenarios. Our approach allows us to exploit the advantages of a
rendering-based approach by supervising the reprojection of pixel
correspondences. It adapts strategies from recent rendering-based methods to
obtain a bidirectional deformable-canonical mapping, to efficiently handle a
defined workspace, and to guide the rays' density. It also presents our
multi-scale HexPlanes for fast inference and a new algorithm for efficient
pixel sampling and convergence criteria. We present results in the STIR and
SCARE datasets, for evaluating point tracking and testing the integration of
kinematic data in our pipeline, respectively. In 2D point tracking, our
approach surpasses the precision and accuracy of the TTO state-of-the-art
methods by nearly 50% on average precision, while competing with other
approaches. In 3D point tracking, this is the first TTO approach, surpassing
feed-forward methods while incorporating the benefits of a deformable
NeRF-based reconstruction.","Gerardo Loza, Junlei Hu, Dominic Jones, Sharib Ali, Pietro Valdastri",2025-08-13T10:20:24Z,2025-08-13T10:20:24Z,http://arxiv.org/abs/2508.09681v1,http://arxiv.org/pdf/2508.09681v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"ESCoT: An Enhanced Step-based Coordinate Trajectory Planning Method for
  Multiple Car-like Robots","Multi-vehicle trajectory planning (MVTP) is one of the key challenges in
multi-robot systems (MRSs) and has broad applications across various fields.
This paper presents ESCoT, an enhanced step-based coordinate trajectory
planning method for multiple car-like robots. ESCoT incorporates two key
strategies: collaborative planning for local robot groups and replanning for
duplicate configurations. These strategies effectively enhance the performance
of step-based MVTP methods. Through extensive experiments, we show that ESCoT
1) in sparse scenarios, significantly improves solution quality compared to
baseline step-based method, achieving up to 70% improvement in typical conflict
scenarios and 34% in randomly generated scenarios, while maintaining high
solving efficiency; and 2) in dense scenarios, outperforms all baseline
methods, maintains a success rate of over 50% even in the most challenging
configurations. The results demonstrate that ESCoT effectively solves MVTP,
further extending the capabilities of step-based methods. Finally, practical
robot tests validate the algorithm's applicability in real-world scenarios.","Junkai Jiang, Yihe Chen, Yibin Yang, Ruochen Li, Shaobing Xu, Jianqiang Wang",2025-08-13T07:53:29Z,2025-08-13T07:53:29Z,http://arxiv.org/abs/2508.09581v1,http://arxiv.org/pdf/2508.09581v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Distilling LLM Prior to Flow Model for Generalizable Agent's Imagination
  in Object Goal Navigation","The Object Goal Navigation (ObjectNav) task challenges agents to locate a
specified object in an unseen environment by imagining unobserved regions of
the scene. Prior approaches rely on deterministic and discriminative models to
complete semantic maps, overlooking the inherent uncertainty in indoor layouts
and limiting their ability to generalize to unseen environments. In this work,
we propose GOAL, a generative flow-based framework that models the semantic
distribution of indoor environments by bridging observed regions with
LLM-enriched full-scene semantic maps. During training, spatial priors inferred
from large language models (LLMs) are encoded as two-dimensional Gaussian
fields and injected into target maps, distilling rich contextual knowledge into
the flow model and enabling more generalizable completions. Extensive
experiments demonstrate that GOAL achieves state-of-the-art performance on MP3D
and Gibson, and shows strong generalization in transfer settings to HM3D. Codes
and pretrained models are available at https://github.com/Badi-Li/GOAL.","Badi Li, Ren-jie Lu, Yu Zhou, Jingke Meng, Wei-shi Zheng",2025-08-13T01:57:48Z,2025-08-13T01:57:48Z,http://arxiv.org/abs/2508.09423v1,http://arxiv.org/pdf/2508.09423v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Decision-Making-Based Path Planning for Autonomous UAVs: A Survey,"One of the most critical features for the successful operation of autonomous
UAVs is the ability to make decisions based on the information acquired from
their surroundings. Each UAV must be able to make decisions during the flight
in order to deal with uncertainties in its system and the environment, and to
further act upon the information being received. Such decisions influence the
future behavior of the UAV, which is expressed as the path plan. Thus,
decision-making in path planning is an enabling technique for deploying
autonomous UAVs in real-world applications. This survey provides an overview of
existing studies that use aspects of decision-making in path planning,
presenting the research strands for Exploration Path Planning and Informative
Path Planning, and focusing on characteristics of how data have been modeled
and understood. Finally, we highlight the existing challenges for relevant
topics in this field.","Kelen C. Teixeira Vivaldini, Robert Pěnička, Martin Saska",2025-08-12T19:38:33Z,2025-08-12T19:38:33Z,http://arxiv.org/abs/2508.09304v1,http://arxiv.org/pdf/2508.09304v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Nonlinear Symmetry Breaking to Enhance the Sagnac Effect in a
  Microresonator Gyroscope","Optical gyroscopes based on the Sagnac effect have been widely used for
inertial navigation in aircrafts, submarines, satellites and unmanned robotics.
With the rapid progress in the field of ultrahigh-quality whispering gallery
mode and ring resonators in recent years, these devices offer the promise of a
compact alternative to ring-laser gyroscopes (RLGs) and fiber-optic gyroscopes
(FOGs). Yet, successful commercialization of a microresonator gyroscope has
been hindered by the scaling of the Sagnac effect with resonator area. While
several techniques have been proposed to enhance the Sagnac effect in
microresonators, these enhancements also amplify the thermal noise in the
microresonator. Here, we present a novel approach to measuring the Sagnac
signal in chip-scale devices that overcomes this fundamental noise limitation
to achieve unprecedented performance in a 200 {\mu}m optical resonator - the
smallest reported to date. Our proof-of-concept design shows a 10^4 enhancement
of the Sagnac signal while simultaneously suppressing thermal noise by 27 dB
and environmental contributions to noise by 22 dB. We believe this approach
offers a pathway for integrated photonic gyroscopes with sensitivities that
match or exceed RLGs and FOGs.","Thariq Shanavas, Gregory Krueper, Jiangang Zhu, Wounjhang Park, Juliet T. Gopinath",2025-08-12T17:57:24Z,2025-08-12T17:57:24Z,http://arxiv.org/abs/2508.09132v1,http://arxiv.org/pdf/2508.09132v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Towards Safe Imitation Learning via Potential Field-Guided Flow Matching,"Deep generative models, particularly diffusion and flow matching models, have
recently shown remarkable potential in learning complex policies through
imitation learning. However, the safety of generated motions remains
overlooked, particularly in complex environments with inherent obstacles. In
this work, we address this critical gap by proposing Potential Field-Guided
Flow Matching Policy (PF2MP), a novel approach that simultaneously learns task
policies and extracts obstacle-related information, represented as a potential
field, from the same set of successful demonstrations. During inference, PF2MP
modulates the flow matching vector field via the learned potential field,
enabling safe motion generation. By leveraging these complementary fields, our
approach achieves improved safety without compromising task success across
diverse environments, such as navigation tasks and robotic manipulation
scenarios. We evaluate PF2MP in both simulation and real-world settings,
demonstrating its effectiveness in task space and joint space control.
Experimental results demonstrate that PF2MP enhances safety, achieving a
significant reduction of collisions compared to baseline policies. This work
paves the way for safer motion generation in unstructured and obstaclerich
environments.","Haoran Ding, Anqing Duan, Zezhou Sun, Leonel Rozo, Noémie Jaquier, Dezhen Song, Yoshihiko Nakamura",2025-08-12T07:53:53Z,2025-08-12T07:53:53Z,http://arxiv.org/abs/2508.08707v1,http://arxiv.org/pdf/2508.08707v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"PADReg: Physics-Aware Deformable Registration Guided by Contact Force
  for Ultrasound Sequences","Ultrasound deformable registration estimates spatial transformations between
pairs of deformed ultrasound images, which is crucial for capturing
biomechanical properties and enhancing diagnostic accuracy in diseases such as
thyroid nodules and breast cancer. However, ultrasound deformable registration
remains highly challenging, especially under large deformation. The inherently
low contrast, heavy noise and ambiguous tissue boundaries in ultrasound images
severely hinder reliable feature extraction and correspondence matching.
Existing methods often suffer from poor anatomical alignment and lack physical
interpretability. To address the problem, we propose PADReg, a physics-aware
deformable registration framework guided by contact force. PADReg leverages
synchronized contact force measured by robotic ultrasound systems as a physical
prior to constrain the registration. Specifically, instead of directly
predicting deformation fields, we first construct a pixel-wise stiffness map
utilizing the multi-modal information from contact force and ultrasound images.
The stiffness map is then combined with force data to estimate a dense
deformation field, through a lightweight physics-aware module inspired by
Hooke's law. This design enables PADReg to achieve physically plausible
registration with better anatomical alignment than previous methods relying
solely on image similarity. Experiments on in-vivo datasets demonstrate that it
attains a HD95 of 12.90, which is 21.34\% better than state-of-the-art methods.
The source code is available at https://github.com/evelynskip/PADReg.","Yimeng Geng, Mingyang Zhao, Fan Xu, Guanglin Cao, Gaofeng Meng, Hongbin Liu",2025-08-12T07:19:21Z,2025-08-12T07:19:21Z,http://arxiv.org/abs/2508.08685v1,http://arxiv.org/pdf/2508.08685v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Tracking Any Point Methods for Markerless 3D Tissue Tracking in
  Endoscopic Stereo Images","Minimally invasive surgery presents challenges such as dynamic tissue motion
and a limited field of view. Accurate tissue tracking has the potential to
support surgical guidance, improve safety by helping avoid damage to sensitive
structures, and enable context-aware robotic assistance during complex
procedures. In this work, we propose a novel method for markerless 3D tissue
tracking by leveraging 2D Tracking Any Point (TAP) networks. Our method
combines two CoTracker models, one for temporal tracking and one for stereo
matching, to estimate 3D motion from stereo endoscopic images. We evaluate the
system using a clinical laparoscopic setup and a robotic arm simulating tissue
motion, with experiments conducted on a synthetic 3D-printed phantom and a
chicken tissue phantom. Tracking on the chicken tissue phantom yielded more
reliable results, with Euclidean distance errors as low as 1.1 mm at a velocity
of 10 mm/s. These findings highlight the potential of TAP-based models for
accurate, markerless 3D tracking in challenging surgical scenarios.","Konrad Reuter, Suresh Guttikonda, Sarah Latus, Lennart Maack, Christian Betz, Tobias Maurer, Alexander Schlaefer",2025-08-11T11:10:16Z,2025-08-11T11:10:16Z,http://arxiv.org/abs/2508.07851v1,http://arxiv.org/pdf/2508.07851v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"SwarmVLM: VLM-Guided Impedance Control for Autonomous Navigation of
  Heterogeneous Robots in Dynamic Warehousing","With the growing demand for efficient logistics, unmanned aerial vehicles
(UAVs) are increasingly being paired with automated guided vehicles (AGVs).
While UAVs offer the ability to navigate through dense environments and varying
altitudes, they are limited by battery life, payload capacity, and flight
duration, necessitating coordinated ground support.
  Focusing on heterogeneous navigation, SwarmVLM addresses these limitations by
enabling semantic collaboration between UAVs and ground robots through
impedance control. The system leverages the Vision Language Model (VLM) and the
Retrieval-Augmented Generation (RAG) to adjust impedance control parameters in
response to environmental changes. In this framework, the UAV acts as a leader
using Artificial Potential Field (APF) planning for real-time navigation, while
the ground robot follows via virtual impedance links with adaptive link
topology to avoid collisions with short obstacles.
  The system demonstrated a 92% success rate across 12 real-world trials. Under
optimal lighting conditions, the VLM-RAG framework achieved 8% accuracy in
object detection and selection of impedance parameters. The mobile robot
prioritized short obstacle avoidance, occasionally resulting in a lateral
deviation of up to 50 cm from the UAV path, which showcases safe navigation in
a cluttered setting.","Malaika Zafar, Roohan Ahmed Khan, Faryal Batool, Yasheerah Yaqoot, Ziang Guo, Mikhail Litvinov, Aleksey Fedoseev, Dzmitry Tsetserukou",2025-08-11T09:56:33Z,2025-08-11T09:56:33Z,http://arxiv.org/abs/2508.07814v1,http://arxiv.org/pdf/2508.07814v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
LAURON VI: A Six-Legged Robot for Dynamic Walking,"Legged locomotion enables robotic systems to traverse extremely challenging
terrains. In many real-world scenarios, the terrain is not that difficult and
these mixed terrain types introduce the need for flexible use of different
walking strategies to achieve mission goals in a fast, reliable, and
energy-efficient way. Six-legged robots have a high degree of flexibility and
inherent stability that aids them in traversing even some of the most difficult
terrains, such as collapsed buildings. However, their lack of fast walking
gaits for easier surfaces is one reason why they are not commonly applied in
these scenarios.
  This work presents LAURON VI, a six-legged robot platform for research on
dynamic walking gaits as well as on autonomy for complex field missions. The
robot's 18 series elastic joint actuators offer high-frequency interfaces for
Cartesian impedance and pure torque control. We have designed, implemented, and
compared three control approaches: kinematic-based, model-predictive, and
reinforcement-learned controllers. The robot hardware and the different control
approaches were extensively tested in a lab environment as well as on a Mars
analog mission. The introduction of fast locomotion strategies for LAURON VI
makes six-legged robots vastly more suitable for a wide range of real-world
applications.","Christian Eichmann, Sabine Bellmann, Nicolas Hügel, Louis-Elias Enslin, Carsten Plasberg, Georg Heppner, Arne Roennau, Ruediger Dillmann",2025-08-11T07:07:02Z,2025-08-11T07:07:02Z,http://arxiv.org/abs/2508.07689v1,http://arxiv.org/pdf/2508.07689v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Risk Map As Middleware: Towards Interpretable Cooperative End-to-end
  Autonomous Driving for Risk-Aware Planning","End-to-end paradigm has emerged as a promising approach to autonomous
driving. However, existing single-agent end-to-end pipelines are often
constrained by occlusion and limited perception range, resulting in hazardous
driving. Furthermore, their black-box nature prevents the interpretability of
the driving behavior, leading to an untrustworthiness system. To address these
limitations, we introduce Risk Map as Middleware (RiskMM) and propose an
interpretable cooperative end-to-end driving framework. The risk map learns
directly from the driving data and provides an interpretable spatiotemporal
representation of the scenario from the upstream perception and the
interactions between the ego vehicle and the surrounding environment for
downstream planning. RiskMM first constructs a multi-agent spatiotemporal
representation with unified Transformer-based architecture, then derives
risk-aware representations by modeling interactions among surrounding
environments with attention. These representations are subsequently fed into a
learning-based Model Predictive Control (MPC) module. The MPC planner
inherently accommodates physical constraints and different vehicle types and
can provide interpretation by aligning learned parameters with explicit MPC
elements. Evaluations conducted on the real-world V2XPnP-Seq dataset confirm
that RiskMM achieves superior and robust performance in risk-aware trajectory
planning, significantly enhancing the interpretability of the cooperative
end-to-end driving framework. The codebase will be released to facilitate
future research in this field.","Mingyue Lei, Zewei Zhou, Hongchen Li, Jiaqi Ma, Jia Hu",2025-08-11T07:00:52Z,2025-08-11T07:00:52Z,http://arxiv.org/abs/2508.07686v1,http://arxiv.org/pdf/2508.07686v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
A Learning-Based Framework for Collision-Free Motion Planning,"This paper presents a learning-based extension to a Circular Field (CF)-based
motion planner for efficient, collision-free trajectory generation in cluttered
environments. The proposed approach overcomes the limitations of hand-tuned
force field parameters by employing a deep neural network trained to infer
optimal planner gains from a single depth image of the scene. The pipeline
incorporates a CUDA-accelerated perception module, a predictive agent-based
planning strategy, and a dataset generated through Bayesian optimization in
simulation. The resulting framework enables real-time planning without manual
parameter tuning and is validated both in simulation and on a Franka Emika
Panda robot. Experimental results demonstrate successful task completion and
improved generalization compared to classical planners.","Mateus Salomão, Tianyü Ren, Alexander König",2025-08-10T23:14:56Z,2025-08-10T23:14:56Z,http://arxiv.org/abs/2508.07502v1,http://arxiv.org/pdf/2508.07502v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Determining the acceleration field of a rigid body using three
  accelerometers and one gyroscope, with applications in mild traumatic brain
  injury","Mild traumatic brain injury (mTBI) often results from violent head motion or
impact. Most prevention strategies explicitly or implicitly rely on motion- or
deformation-based injury criteria, both of which require accurate measurements
of head motion. We present an algorithm for reconstructing the full
acceleration field of a rigid body from measurements obtained by three
tri-axial accelerometers and one tri-axial gyroscope. Unlike traditional
gyroscope-based methods, which require numerically differentiating noisy
angular velocity data, or gyroscope-free methods, which may impose restrictive
sensor placement or involve nonlinear optimization, the proposed algorithm
recovers angular acceleration and translational acceleration by solving a set
of linear equations derived from rigid body kinematics. In the proposed method,
the only constraint on sensor placement is that the accelerometers must be
non-collinear. We validated the algorithm in controlled soccer heading
experiments, demonstrating accurate prediction of accelerations at unsensed
locations across trials. The proposed algorithm provides a robust, flexible,
and efficient tool for reconstructing rigid body motion, with direct
applications in contact sports, robotics, and biomechanical injury prediction.","Yang Wan, Benjamin E. Grossman-Ponemona, Haneesh Kesari",2025-08-10T19:31:38Z,2025-08-10T19:31:38Z,http://arxiv.org/abs/2508.07464v1,http://arxiv.org/pdf/2508.07464v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Noise-Aware Generative Microscopic Traffic Simulation,"Accurately modeling individual vehicle behavior in microscopic traffic
simulation remains a key challenge in intelligent transportation systems, as it
requires vehicles to realistically generate and respond to complex traffic
phenomena such as phantom traffic jams. While traditional human driver
simulation models offer computational tractability, they do so by abstracting
away the very complexity that defines human driving. On the other hand, recent
advances in infrastructure-mounted camera-based roadway sensing have enabled
the extraction of vehicle trajectory data, presenting an opportunity to shift
toward generative, agent-based models. Yet, a major bottleneck remains: most
existing datasets are either overly sanitized or lack standardization, failing
to reflect the noisy, imperfect nature of real-world sensing. Unlike data from
vehicle-mounted sensors-which can mitigate sensing artifacts like occlusion
through overlapping fields of view and sensor fusion-infrastructure-based
sensors surface a messier, more practical view of challenges that traffic
engineers encounter. To this end, we present the I-24 MOTION Scenario Dataset
(I24-MSD)-a standardized, curated dataset designed to preserve a realistic
level of sensor imperfection, embracing these errors as part of the learning
problem rather than an obstacle to overcome purely from preprocessing. Drawing
from noise-aware learning strategies in computer vision, we further adapt
existing generative models in the autonomous driving community for I24-MSD with
noise-aware loss functions. Our results show that such models not only
outperform traditional baselines in realism but also benefit from explicitly
engaging with, rather than suppressing, data imperfection. We view I24-MSD as a
stepping stone toward a new generation of microscopic traffic simulation that
embraces the real-world challenges and is better aligned with practical needs.","Vindula Jayawardana, Catherine Tang, Junyi Ji, Jonah Philion, Xue Bin Peng, Cathy Wu",2025-08-10T18:41:49Z,2025-08-10T18:41:49Z,http://arxiv.org/abs/2508.07453v1,http://arxiv.org/pdf/2508.07453v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Collision-Free Trajectory Planning and control of Robotic Manipulator
  using Energy-Based Artificial Potential Field (E-APF)","Robotic trajectory planning in dynamic and cluttered environments remains a
critical challenge, particularly when striving for both time efficiency and
motion smoothness under actuation constraints. Traditional path planner, such
as Artificial Potential Field (APF), offer computational efficiency but suffer
from local minima issue due to position-based potential field functions and
oscillatory motion near the obstacles due to Newtonian mechanics. To address
this limitation, an Energy-based Artificial Potential Field (APF) framework is
proposed in this paper that integrates position and velocity-dependent
potential functions. E-APF ensures dynamic adaptability and mitigates local
minima, enabling uninterrupted progression toward the goal. The proposed
framework integrates E-APF with a hybrid trajectory optimizer that jointly
minimizes jerk and execution time under velocity and acceleration constraints,
ensuring geometric smoothness and time efficiency. The entire framework is
validated in simulation using the 7-degree-of-freedom Kinova Gen3 robotic
manipulator. The results demonstrate collision-free, smooth, time-efficient,
and oscillation-free trajectory in the presence of obstacles, highlighting the
efficacy of the combined trajectory optimization and real-time obstacle
avoidance approach. This work lays the foundation for future integration with
reactive control strategies and physical hardware deployment in real-world
manipulation tasks.","Adeetya Uppal, Rakesh Kumar Sahoo, Manoranjan Sinha",2025-08-10T12:34:49Z,2025-08-10T12:34:49Z,http://arxiv.org/abs/2508.07323v1,http://arxiv.org/pdf/2508.07323v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"3D Gaussian Representations with Motion Trajectory Field for Dynamic
  Scene Reconstruction","This paper addresses the challenge of novel-view synthesis and motion
reconstruction of dynamic scenes from monocular video, which is critical for
many robotic applications. Although Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have demonstrated remarkable success in rendering
static scenes, extending them to reconstruct dynamic scenes remains
challenging. In this work, we introduce a novel approach that combines 3DGS
with a motion trajectory field, enabling precise handling of complex object
motions and achieving physically plausible motion trajectories. By decoupling
dynamic objects from static background, our method compactly optimizes the
motion trajectory field. The approach incorporates time-invariant motion
coefficients and shared motion trajectory bases to capture intricate motion
patterns while minimizing optimization complexity. Extensive experiments
demonstrate that our approach achieves state-of-the-art results in both
novel-view synthesis and motion trajectory recovery from monocular video,
advancing the capabilities of dynamic scene reconstruction.","Xuesong Li, Lars Petersson, Vivien Rolland",2025-08-10T05:15:57Z,2025-08-10T05:15:57Z,http://arxiv.org/abs/2508.07182v1,http://arxiv.org/pdf/2508.07182v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Nonlinear Photonic Neuromorphic Chips for Spiking Reinforcement Learning,"Photonic computing chips have made significant progress in accelerating
linear computations, but nonlinear computations are usually implemented in the
digital domain, which introduces additional system latency and power
consumption, and hinders the implementation of fully-functional photonic neural
network chips. Here, we propose and fabricate a 16-channel programmable
incoherent photonic neuromorphic computing chip by co-designing a simplified
MZI mesh and distributed feedback lasers with saturable absorber array using
different materials, enabling implementation of both linear and nonlinear spike
computations in the optical domain. Furthermore, previous studies mainly
focused on supervised learning and simple image classification tasks. Here, we
propose a photonic spiking reinforcement learning (RL) architecture for the
first time, and develop a software-hardware collaborative training-inference
framework to address the challenge of training spiking RL models. We achieve
large-scale, energy-efficient (photonic linear computation: 1.39 TOPS/W,
photonic nonlinear computation: 987.65 GOPS/W) and low-latency (320 ps)
end-to-end deployment of an entire layer of photonic spiking RL. Two RL
benchmarks include the discrete CartPole task and the continuous Pendulum tasks
are demonstrated experimentally based on spiking proximal policy optimization
algorithm. The hardware-software collaborative computing reward value converges
to 200 (-250) for the CartPole tasks, respectively, comparable to that of a
traditional PPO algorithm. This experimental demonstration addresses the
challenge of the absence of large-scale photonic nonlinear spike computation
and spiking RL training difficulty, and presents a high-speed and low-latency
photonic spiking RL solution with promising application prospects in fields
such as real-time decision-making and control for robots and autonomous
driving.","Shuiying Xiang, Yonghang Chen, Haowen Zhao, Shangxuan Shi, Xintao Zeng, Yahui Zhang, Xingxing Guo, Yanan Han, Ye Tian, Yuechun Shi, Yue Hao",2025-08-09T12:19:49Z,2025-08-09T12:19:49Z,http://arxiv.org/abs/2508.06962v1,http://arxiv.org/pdf/2508.06962v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Affordance-R1: Reinforcement Learning for Generalizable Affordance
  Reasoning in Multimodal Large Language Model","Affordance grounding focuses on predicting the specific regions of objects
that are associated with the actions to be performed by robots. It plays a
vital role in the fields of human-robot interaction, human-object interaction,
embodied manipulation, and embodied perception. Existing models often neglect
the affordance shared among different objects because they lack the
Chain-of-Thought(CoT) reasoning abilities, limiting their out-of-domain (OOD)
generalization and explicit reasoning capabilities. To address these
challenges, we propose Affordance-R1, the first unified affordance grounding
framework that integrates cognitive CoT guided Group Relative Policy
Optimization (GRPO) within a reinforcement learning paradigm. Specifically, we
designed a sophisticated affordance function, which contains format,
perception, and cognition rewards to effectively guide optimization directions.
Furthermore, we constructed a high-quality affordance-centric reasoning
dataset, ReasonAff, to support training. Trained exclusively via reinforcement
learning with GRPO and without explicit reasoning data, Affordance-R1 achieves
robust zero-shot generalization and exhibits emergent test-time reasoning
capabilities. Comprehensive experiments demonstrate that our model outperforms
well-established methods and exhibits open-world generalization. To the best of
our knowledge, Affordance-R1 is the first to integrate GRPO-based RL with
reasoning into affordance reasoning. The code of our method and our dataset is
released on https://github.com/hq-King/Affordance-R1.","Hanqing Wang, Shaoyang Wang, Yiming Zhong, Zemin Yang, Jiamin Wang, Zhiqing Cui, Jiahao Yuan, Yifan Han, Mingyu Liu, Yuexin Ma",2025-08-08T10:39:04Z,2025-08-16T13:00:05Z,http://arxiv.org/abs/2508.06206v3,http://arxiv.org/pdf/2508.06206v3.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
PA-HOI: A Physics-Aware Human and Object Interaction Dataset,"The Human-Object Interaction (HOI) task explores the dynamic interactions
between humans and objects in physical environments, providing essential
biomechanical and cognitive-behavioral foundations for fields such as robotics,
virtual reality, and human-computer interaction. However, existing HOI data
sets focus on details of affordance, often neglecting the influence of physical
properties of objects on human long-term motion. To bridge this gap, we
introduce the PA-HOI Motion Capture dataset, which highlights the impact of
objects' physical attributes on human motion dynamics, including human posture,
moving velocity, and other motion characteristics. The dataset comprises 562
motion sequences of human-object interactions, with each sequence performed by
subjects of different genders interacting with 35 3D objects that vary in size,
shape, and weight. This dataset stands out by significantly extending the scope
of existing ones for understanding how the physical attributes of different
objects influence human posture, speed, motion scale, and interacting
strategies. We further demonstrate the applicability of the PA-HOI dataset by
integrating it with existing motion generation methods, validating its capacity
to transfer realistic physical awareness.","Ruiyan Wang, Lin Zuo, Zonghao Lin, Qiang Wang, Zhengxue Cheng, Rong Xie, Jun Ling, Li Song",2025-08-08T10:36:23Z,2025-08-08T10:36:23Z,http://arxiv.org/abs/2508.06205v1,http://arxiv.org/pdf/2508.06205v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Towards Transparent Ethical AI: A Roadmap for Trustworthy Robotic
  Systems","As artificial intelligence (AI) and robotics increasingly permeate society,
ensuring the ethical behavior of these systems has become paramount. This paper
contends that transparency in AI decision-making processes is fundamental to
developing trustworthy and ethically aligned robotic systems. We explore how
transparency facilitates accountability, enables informed consent, and supports
the debugging of ethical algorithms. The paper outlines technical, ethical, and
practical challenges in implementing transparency and proposes novel approaches
to enhance it, including standardized metrics, explainable AI techniques, and
user-friendly interfaces. This paper introduces a framework that connects
technical implementation with ethical considerations in robotic systems,
focusing on the specific challenges of achieving transparency in dynamic,
real-world contexts. We analyze how prioritizing transparency can impact public
trust, regulatory policies, and avenues for future research. By positioning
transparency as a fundamental element in ethical AI system design, we aim to
add to the ongoing discussion on responsible AI and robotics, providing
direction for future advancements in this vital field.","Ahmad Farooq, Kamran Iqbal",2025-08-07T20:49:16Z,2025-08-07T20:49:16Z,http://arxiv.org/abs/2508.05846v1,http://arxiv.org/pdf/2508.05846v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Evaluation of an Autonomous Surface Robot Equipped with a Transformable
  Mobility Mechanism for Efficient Mobility Control","Efficient mobility and power consumption are critical for autonomous water
surface robots in long-term water environmental monitoring. This study develops
and evaluates a transformable mobility mechanism for a water surface robot with
two control modes: station-keeping and traveling to improve energy efficiency
and maneuverability. Field experiments show that, in a round-trip task between
two points, the traveling mode reduces power consumption by 10\% and decreases
the total time required for travel by 5\% compared to the station-keeping mode.
These results confirm the effectiveness of the transformable mobility mechanism
for enhancing operational efficiency in patrolling on water surface.","Yasuyuki Fujii, Dinh Tuan Tran, Joo-Ho Lee",2025-08-07T12:15:59Z,2025-08-07T12:15:59Z,http://arxiv.org/abs/2508.08303v1,http://arxiv.org/pdf/2508.08303v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Towards Embodied Agentic AI: Review and Classification of LLM- and
  VLM-Driven Robot Autonomy and Interaction","Foundation models, including large language models (LLMs) and vision-language
models (VLMs), have recently enabled novel approaches to robot autonomy and
human-robot interfaces. In parallel, vision-language-action models (VLAs) or
large behavior models (LBMs) are increasing the dexterity and capabilities of
robotic systems. This survey paper focuses on those works advancing towards
agentic applications and architectures. This includes initial efforts exploring
GPT-style interfaces to tooling, as well as more complex system where AI agents
are coordinators, planners, perception actors, or generalist interfaces. Such
agentic architectures allow robots to reason over natural language
instructions, invoke APIs, plan task sequences, or assist in operations and
diagnostics. In addition to peer-reviewed research, due to the fast-evolving
nature of the field, we highlight and include community-driven projects, ROS
packages, and industrial frameworks that show emerging trends. We propose a
taxonomy for classifying model integration approaches and present a comparative
analysis of the role that agents play in different solutions in today's
literature.","Sahar Salimpour, Lei Fu, Farhad Keramat, Leonardo Militano, Giovanni Toffetti, Harry Edelman, Jorge Peña Queralta",2025-08-07T11:48:03Z,2025-08-14T12:55:31Z,http://arxiv.org/abs/2508.05294v2,http://arxiv.org/pdf/2508.05294v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"A Study of the Framework and Real-World Applications of Language
  Embedding for 3D Scene Understanding","Gaussian Splatting has rapidly emerged as a transformative technique for
real-time 3D scene representation, offering a highly efficient and expressive
alternative to Neural Radiance Fields (NeRF). Its ability to render complex
scenes with high fidelity has enabled progress across domains such as scene
reconstruction, robotics, and interactive content creation. More recently, the
integration of Large Language Models (LLMs) and language embeddings into
Gaussian Splatting pipelines has opened new possibilities for text-conditioned
generation, editing, and semantic scene understanding. Despite these advances,
a comprehensive overview of this emerging intersection has been lacking. This
survey presents a structured review of current research efforts that combine
language guidance with 3D Gaussian Splatting, detailing theoretical
foundations, integration strategies, and real-world use cases. We highlight key
limitations such as computational bottlenecks, generalizability, and the
scarcity of semantically annotated 3D Gaussian data and outline open challenges
and future directions for advancing language-guided 3D scene understanding
using Gaussian Splatting.","Mahmoud Chick Zaouali, Todd Charter, Yehor Karpichev, Brandon Haworth, Homayoun Najjaran",2025-08-07T06:33:08Z,2025-08-19T00:47:37Z,http://arxiv.org/abs/2508.05064v2,http://arxiv.org/pdf/2508.05064v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"A Vision-Based Collision Sensing Method for Stable Circular Object
  Grasping with A Soft Gripper System","External collisions to robot actuators typically pose risks to grasping
circular objects. This work presents a vision-based sensing module capable of
detecting collisions to maintain stable grasping with a soft gripper system.
The system employs an eye-in-palm camera with a broad field of view to
simultaneously monitor the motion of fingers and the grasped object.
Furthermore, we have developed a collision-rich grasping strategy to ensure the
stability and security of the entire dynamic grasping process. A physical soft
gripper was manufactured and affixed to a collaborative robotic arm to evaluate
the performance of the collision detection mechanism. An experiment regarding
testing the response time of the mechanism confirmed the system has the
capability to react to the collision instantaneously. A dodging test was
conducted to demonstrate the gripper can detect the direction and scale of
external collisions precisely.","Boyang Zhang, Jiahui Zuo, Zeyu Duan, Fumin Zhang",2025-08-07T05:43:45Z,2025-08-07T05:43:45Z,http://arxiv.org/abs/2508.05040v1,http://arxiv.org/pdf/2508.05040v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned
  and Addressed for XR Research","The development of radiance fields (RF), such as 3D Gaussian Splatting (3DGS)
and Neural Radiance Fields (NeRF), has revolutionized interactive
photorealistic view synthesis and presents enormous opportunities for XR
research and applications. However, despite the exponential growth of RF
research, RF-related contributions to the XR community remain sparse. To better
understand this research gap, we performed a systematic survey of current RF
literature to analyze (i) how RF is envisioned for XR applications, (ii) how
they have already been implemented, and (iii) the remaining research gaps. We
collected 365 RF contributions related to XR from computer vision, computer
graphics, robotics, multimedia, human-computer interaction, and XR communities,
seeking to answer the above research questions. Among the 365 papers, we
performed an analysis of 66 papers that already addressed a detailed aspect of
RF research for XR. With this survey, we extended and positioned XR-specific RF
research topics in the broader RF research field and provide a helpful resource
for the XR community to navigate within the rapid development of RF research.","Ke Li, Mana Masuda, Susanne Schmidt, Shohei Mori",2025-08-06T11:14:06Z,2025-08-07T15:24:10Z,http://arxiv.org/abs/2508.04326v2,http://arxiv.org/pdf/2508.04326v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"OmniShape: Zero-Shot Multi-Hypothesis Shape and Pose Estimation in the
  Real World","We would like to estimate the pose and full shape of an object from a single
observation, without assuming known 3D model or category. In this work, we
propose OmniShape, the first method of its kind to enable probabilistic pose
and shape estimation. OmniShape is based on the key insight that shape
completion can be decoupled into two multi-modal distributions: one capturing
how measurements project into a normalized object reference frame defined by
the dataset and the other modelling a prior over object geometries represented
as triplanar neural fields. By training separate conditional diffusion models
for these two distributions, we enable sampling multiple hypotheses from the
joint pose and shape distribution. OmniShape demonstrates compelling
performance on challenging real world datasets. Project website:
https://tri-ml.github.io/omnishape","Katherine Liu, Sergey Zakharov, Dian Chen, Takuya Ikeda, Greg Shakhnarovich, Adrien Gaidon, Rares Ambrus",2025-08-05T17:30:41Z,2025-08-05T17:30:41Z,http://arxiv.org/abs/2508.03669v1,http://arxiv.org/pdf/2508.03669v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Opti-Acoustic Scene Reconstruction in Highly Turbid Underwater
  Environments","Scene reconstruction is an essential capability for underwater robots
navigating in close proximity to structures. Monocular vision-based
reconstruction methods are unreliable in turbid waters and lack depth scale
information. Sonars are robust to turbid water and non-uniform lighting
conditions, however, they have low resolution and elevation ambiguity. This
work proposes a real-time opti-acoustic scene reconstruction method that is
specially optimized to work in turbid water. Our strategy avoids having to
identify point features in visual data and instead identifies regions of
interest in the data. We then match relevant regions in the image to
corresponding sonar data. A reconstruction is obtained by leveraging range data
from the sonar and elevation data from the camera image. Experimental
comparisons against other vision-based and sonar-based approaches at varying
turbidity levels, and field tests conducted in marina environments, validate
the effectiveness of the proposed approach. We have made our code open-source
to facilitate reproducibility and encourage community engagement.","Ivana Collado-Gonzalez, John McConnell, Paul Szenher, Brendan Englot",2025-08-05T12:53:45Z,2025-08-06T16:26:09Z,http://arxiv.org/abs/2508.03408v2,http://arxiv.org/pdf/2508.03408v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Investigation of Air Fluidization during Intruder Penetration in Sand,"Self-burrowing robots navigating through granular media benefit from
airflow-assisted burrowing, which reduces penetration resistance. However, the
mechanisms underlying airflow-granular interactions remain poorly understood.
To address this knowledge gap, we employ a coupled computational fluid dynamics
and discrete element method (CFD-DEM) approach, supplemented by experimental
cone penetration tests (CPT) under varying airflow conditions, to investigate
the effects of aeration on penetration resistance. Experimental results reveal
a nonlinear relationship between penetration resistance reduction and depth,
wherein resistance approaches near-zero values up to a critical depth, beyond
which the effectiveness of fluidization diminishes. Simulations demonstrate
that higher airflow rates enhance the mobilization of overlying grains,
increasing the critical depth. A detailed meso- and micro-scale analysis of
particle motion, contact forces, and fluid pressure fields reveals four
distinct penetration stages: particle ejection and channel formation, channel
sealing, channel refill, and final compaction. These findings contribute to a
deeper understanding of granular aeration mechanisms and their implications for
geotechnical engineering, excavation technologies, and the development of
self-burrowing robotic systems.","Bowen Wang, Yuxing Peng, Alvaro Vergara, Jordan H. Boyle, Raul Fuentes",2025-08-05T11:55:20Z,2025-08-05T11:55:20Z,http://arxiv.org/abs/2508.03350v1,http://arxiv.org/pdf/2508.03350v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"ActionSink: Toward Precise Robot Manipulation with Dynamic Integration
  of Action Flow","Language-instructed robot manipulation has garnered significant interest due
to the potential of learning from collected data. While the challenges in
high-level perception and planning are continually addressed along the progress
of general large pre-trained models, the low precision of low-level action
estimation has emerged as the key limiting factor in manipulation performance.
To this end, this paper introduces a novel robot manipulation framework, i.e.,
ActionSink, to pave the way toward precise action estimations in the field of
learning-based robot manipulation. As the name suggests, ActionSink
reformulates the actions of robots as action-caused optical flows from videos,
called ""action flow"", in a self-supervised manner, which are then used to be
retrieved and integrated to enhance the action estimation. Specifically,
ActionSink incorporates two primary modules. The first module is a
coarse-to-fine action flow matcher, which continuously refines the accuracy of
action flow via iterative retrieval and denoising process. The second module is
a dynamic action flow integrator, which employs a working memory pool that
dynamically and efficiently manages the historical action flows that should be
used to integrate to enhance the current action estimation. In this module, a
multi-layer fusion module is proposed to integrate direct estimation and action
flows from both the current and the working memory, achieving highly accurate
action estimation through a series of estimation-integration processes. Our
ActionSink framework outperformed prior SOTA on the LIBERO benchmark by a 7.9\%
success rate, and obtained nearly an 8\% accuracy gain on the challenging
long-horizon visual task LIBERO-Long.","Shanshan Guo, Xiwen Liang, Junfan Lin, Yuzheng Zhuang, Liang Lin, Xiaodan Liang",2025-08-05T08:46:17Z,2025-08-05T08:46:17Z,http://arxiv.org/abs/2508.03218v1,http://arxiv.org/pdf/2508.03218v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Scaling DRL for Decision Making: A Survey on Data, Network, and Training
  Budget Strategies","In recent years, the expansion of neural network models and training data has
driven remarkable progress in deep learning, particularly in computer vision
and natural language processing. This advancement is underpinned by the concept
of Scaling Laws, which demonstrates that scaling model parameters and training
data enhances learning performance. While these fields have witnessed
breakthroughs, such as the development of large language models like GPT-4 and
advanced vision models like Midjourney, the application of scaling laws in deep
reinforcement learning (DRL) remains relatively unexplored. Despite its
potential to improve performance, the integration of scaling laws into DRL for
decision making has not been fully realized. This review addresses this gap by
systematically analyzing scaling strategies in three dimensions: data, network,
and training budget. In data scaling, we explore methods to optimize data
efficiency through parallel sampling and data generation, examining the
relationship between data volume and learning outcomes. For network scaling, we
investigate architectural enhancements, including monolithic expansions,
ensemble and MoE methods, and agent number scaling techniques, which
collectively enhance model expressivity while posing unique computational
challenges. Lastly, in training budget scaling, we evaluate the impact of
distributed training, high replay ratios, large batch sizes, and auxiliary
training on training efficiency and convergence. By synthesizing these
strategies, this review not only highlights their synergistic roles in
advancing DRL for decision making but also provides a roadmap for future
research. We emphasize the importance of balancing scalability with
computational efficiency and outline promising directions for leveraging
scaling to unlock the full potential of DRL in various tasks such as robot
control, autonomous driving and LLM training.","Yi Ma, Hongyao Tang, Chenjun Xiao, Yaodong Yang, Wei Wei, Jianye Hao, Jiye Liang",2025-08-05T08:03:12Z,2025-08-05T08:03:12Z,http://arxiv.org/abs/2508.03194v1,http://arxiv.org/pdf/2508.03194v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Point2Act: Efficient 3D Distillation of Multimodal LLMs for Zero-Shot
  Context-Aware Grasping","We propose Point2Act, which directly retrieves the 3D action point relevant
for a contextually described task, leveraging Multimodal Large Language Models
(MLLMs). Foundation models opened the possibility for generalist robots that
can perform a zero-shot task following natural language descriptions within an
unseen environment. While the semantics obtained from large-scale image and
language datasets provide contextual understanding in 2D images, the rich yet
nuanced features deduce blurry 2D regions and struggle to find precise 3D
locations for actions. Our proposed 3D relevancy fields bypass the
high-dimensional features and instead efficiently imbue lightweight 2D
point-level guidance tailored to the task-specific action. The multi-view
aggregation effectively compensates for misalignments due to geometric
ambiguities, such as occlusion, or semantic uncertainties inherent in the
language descriptions. The output region is highly localized, reasoning
fine-grained 3D spatial context that can directly transfer to an explicit
position for physical action at the on-the-fly reconstruction of the scene. Our
full-stack pipeline, which includes capturing, MLLM querying, 3D
reconstruction, and grasp pose extraction, generates spatially grounded
responses in under 20 seconds, facilitating practical manipulation tasks.
Project page: https://sangminkim-99.github.io/point2act/","Sang Min Kim, Hyeongjun Heo, Junho Kim, Yonghyeon Lee, Young Min Kim",2025-08-05T05:23:19Z,2025-08-05T05:23:19Z,http://arxiv.org/abs/2508.03099v1,http://arxiv.org/pdf/2508.03099v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Physics-informed Neural Time Fields for Prehensile Object Manipulation,"Object manipulation skills are necessary for robots operating in various
daily-life scenarios, ranging from warehouses to hospitals. They allow the
robots to manipulate the given object to their desired arrangement in the
cluttered environment. The existing approaches to solving object manipulations
are either inefficient sampling based techniques, require expert
demonstrations, or learn by trial and error, making them less ideal for
practical scenarios. In this paper, we propose a novel, multimodal
physics-informed neural network (PINN) for solving object manipulation tasks.
Our approach efficiently learns to solve the Eikonal equation without expert
data and finds object manipulation trajectories fast in complex, cluttered
environments. Our method is multimodal as it also reactively replans the
robot's grasps during manipulation to achieve the desired object poses. We
demonstrate our approach in both simulation and real-world scenarios and
compare it against state-of-the-art baseline methods. The results indicate that
our approach is effective across various objects, has efficient training
compared to previous learning-based methods, and demonstrates high performance
in planning time, trajectory length, and success rates. Our demonstration
videos can be found at https://youtu.be/FaQLkTV9knI.","Hanwen Ren, Ruiqi Ni, Ahmed H. Qureshi",2025-08-05T00:55:28Z,2025-08-05T00:55:28Z,http://arxiv.org/abs/2508.02976v1,http://arxiv.org/pdf/2508.02976v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"A fluid--peridynamic structure model of deformation and damage of
  microchannels","Soft-walled microchannels arise in many applications, ranging from
organ-on-a-chip platforms to soft-robotic actuators. However, despite extensive
research on their static and dynamic response, the potential failure of these
devices has not been addressed. To this end, we explore fluid--structure
interaction in microchannels whose compliant top wall is governed by a nonlocal
mechanical theory capable of simulating both deformation and material failure.
We develop a one-dimensional model by coupling viscous flow under the
lubrication approximation to a state-based peridynamic formulation of an
Euler--Bernoulli beam. The peridynamic formulation enables the wall to be
modeled as a genuinely nonlocal beam, and the integral form of its equation of
motion remains valid whether the deformation field is smooth or contains
discontinuities. Through the proposed computational model, we explore the
steady and time-dependent behaviors of this fluid--peridynamic structure
interaction. We rationalize the wave and damping dynamics observed in the
simulations through a dispersion (linearized) analysis of the coupled system,
finding that, with increasing nonlocal influence, wave propagation exhibits a
clear departure from classical behavior, characterized by a gradual suppression
of the phase velocity. The main contribution of our study is to outline the
potential failure scenarios of the microchannel's soft wall under the
hydrodynamic load of the flow. Specifically, we find a dividing curve in the
space spanned by the dimensionless Strouhal number (quantifying unsteady
inertia of the beam) and the compliance number (quantifying the strength of the
fluid--structure coupling) separating scenarios of potential failure during
transient conditions from potential failure at the steady load.","Ziyu Wang, Ivan C. Christov",2025-08-04T20:08:07Z,2025-08-04T20:08:07Z,http://arxiv.org/abs/2508.02875v1,http://arxiv.org/pdf/2508.02875v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Manip4Care: Robotic Manipulation of Human Limbs for Solving Assistive
  Tasks","Enabling robots to grasp and reposition human limbs can significantly enhance
their ability to provide assistive care to individuals with severe mobility
impairments, particularly in tasks such as robot-assisted bed bathing and
dressing. However, existing assistive robotics solutions often assume that the
human remains static or quasi-static, limiting their effectiveness. To address
this issue, we present Manip4Care, a modular simulation pipeline that enables
robotic manipulators to grasp and reposition human limbs effectively. Our
approach features a physics simulator equipped with built-in techniques for
grasping and repositioning while considering biomechanical and collision
avoidance constraints. Our grasping method employs antipodal sampling with
force closure to grasp limbs, and our repositioning system utilizes the Model
Predictive Path Integral (MPPI) and vector-field-based control method to
generate motion trajectories under collision avoidance and biomechanical
constraints. We evaluate this approach across various limb manipulation tasks
in both supine and sitting positions and compare outcomes for different age
groups with differing shoulder joint limits. Additionally, we demonstrate our
approach for limb manipulation using a real-world mannequin and further
showcase its effectiveness in bed bathing tasks.","Yubin Koh, Ahmed H. Qureshi",2025-08-04T17:41:58Z,2025-08-04T17:41:58Z,http://arxiv.org/abs/2508.02649v1,http://arxiv.org/pdf/2508.02649v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Vision-based Navigation of Unmanned Aerial Vehicles in Orchards: An
  Imitation Learning Approach","Autonomous unmanned aerial vehicle (UAV) navigation in orchards presents
significant challenges due to obstacles and GPS-deprived environments. In this
work, we introduce a learning-based approach to achieve vision-based navigation
of UAVs within orchard rows. Our method employs a variational autoencoder
(VAE)-based controller, trained with an intervention-based learning framework
that allows the UAV to learn a visuomotor policy from human experience. We
validate our approach in real orchard environments with a custom-built
quadrotor platform. Field experiments demonstrate that after only a few
iterations of training, the proposed VAE-based controller can autonomously
navigate the UAV based on a front-mounted camera stream. The controller
exhibits strong obstacle avoidance performance, achieves longer flying
distances with less human assistance, and outperforms existing algorithms.
Furthermore, we show that the policy generalizes effectively to novel
environments and maintains competitive performance across varying conditions
and speeds. This research not only advances UAV autonomy but also holds
significant potential for precision agriculture, improving efficiency in
orchard monitoring and management.","Peng Wei, Prabhash Ragbir, Stavros G. Vougioukas, Zhaodan Kong",2025-08-04T17:06:04Z,2025-08-04T17:06:04Z,http://arxiv.org/abs/2508.02617v1,http://arxiv.org/pdf/2508.02617v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"MUTE-DSS: A Digital-Twin-Based Decision Support System for Minimizing
  Underwater Radiated Noise in Ship Voyage Planning","We present a novel MUTE-DSS, a digital-twin-based decision support system for
minimizing underwater radiated noise (URN) during ship voyage planning. It is a
ROS2-centric framework that integrates state-of-the-art acoustic models
combining a semi-empirical reference spectrum for near-field modeling with 3D
ray tracing for propagation losses for far-field modeling, offering real-time
computation of the ship noise signature, alongside a data-driven Southern
resident killer whale distribution model. The proposed DSS performs a two-stage
optimization pipeline: Batch Informed Trees for collision-free ship routing and
a genetic algorithm for adaptive ship speed profiling under voyage constraints
that minimizes cumulative URN exposure to marine mammals. The effectiveness of
MUTE-DSS is demonstrated through case studies of ships operating between the
Strait of Georgia and the Strait of Juan de Fuca, comparing optimized voyages
against baseline trajectories derived from automatic identification system
data. Results show substantial reductions in noise exposure level, up to 7.14
dB, corresponding to approximately an 80.68% reduction in a simplified
scenario, and an average 4.90 dB reduction, corresponding to approximately a
67.6% reduction in a more realistic dynamic setting. These results illustrate
the adaptability and practical utility of the proposed decision support system.","Akash Venkateshwaran, Indu Kant Deo, Rajeev K. Jaiman",2025-08-03T20:02:56Z,2025-08-03T20:02:56Z,http://arxiv.org/abs/2508.01907v1,http://arxiv.org/pdf/2508.01907v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Exploring Stiffness Gradient Effects in Magnetically Induced Metamorphic
  Materials via Continuum Simulation and Validation","Magnetic soft continuum robots are capable of bending with remote control in
confined space environments, and they have been applied in various
bioengineering contexts. As one type of ferromagnetic soft continuums, the
Magnetically Induced Metamorphic Materials (MIMMs)-based continuum (MC)
exhibits similar bending behaviors. Based on the characteristics of its base
material, MC is flexible in modifying unit stiffness and convenient in molding
fabrication. However, recent studies on magnetic continuum robots have
primarily focused on one or two design parameters, limiting the development of
a comprehensive magnetic continuum bending model. In this work, we constructed
graded-stiffness MCs (GMCs) and developed a numerical model for GMCs' bending
performance, incorporating four key parameters that determine their
performance. The simulated bending results were validated with real bending
experiments in four different categories: varying magnetic field,
cross-section, unit stiffness, and unit length. The graded-stiffness design
strategy applied to GMCs prevents sharp bending at the fixed end and results in
a more circular curvature. We also trained an expansion model for GMCs' bending
performance that is highly efficient and accurate compared to the simulation
process. An extensive library of bending prediction for GMCs was built using
the trained model.","Wentao Shi, Yang Yang, Yiming Huang, Hongliang Ren",2025-08-03T15:50:11Z,2025-08-03T15:50:11Z,http://arxiv.org/abs/2508.01810v1,http://arxiv.org/pdf/2508.01810v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"OpenGS-Fusion: Open-Vocabulary Dense Mapping with Hybrid 3D Gaussian
  Splatting for Refined Object-Level Understanding","Recent advancements in 3D scene understanding have made significant strides
in enabling interaction with scenes using open-vocabulary queries, particularly
for VR/AR and robotic applications. Nevertheless, existing methods are hindered
by rigid offline pipelines and the inability to provide precise 3D object-level
understanding given open-ended queries. In this paper, we present
OpenGS-Fusion, an innovative open-vocabulary dense mapping framework that
improves semantic modeling and refines object-level understanding.
OpenGS-Fusion combines 3D Gaussian representation with a Truncated Signed
Distance Field to facilitate lossless fusion of semantic features on-the-fly.
Furthermore, we introduce a novel multimodal language-guided approach named
MLLM-Assisted Adaptive Thresholding, which refines the segmentation of 3D
objects by adaptively adjusting similarity thresholds, achieving an improvement
17\% in 3D mIoU compared to the fixed threshold strategy. Extensive experiments
demonstrate that our method outperforms existing methods in 3D object
understanding and scene reconstruction quality, as well as showcasing its
effectiveness in language-guided scene interaction. The code is available at
https://young-bit.github.io/opengs-fusion.github.io/ .","Dianyi Yang, Xihan Wang, Yu Gao, Shiyang Liu, Bohan Ren, Yufeng Yue, Yi Yang",2025-08-02T02:22:36Z,2025-08-02T02:22:36Z,http://arxiv.org/abs/2508.01150v1,http://arxiv.org/pdf/2508.01150v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction,
  Forecasting, and Generation","Egocentric human motion generation and forecasting with scene-context is
crucial for enhancing AR/VR experiences, improving human-robot interaction,
advancing assistive technologies, and enabling adaptive healthcare solutions by
accurately predicting and simulating movement from a first-person perspective.
However, existing methods primarily focus on third-person motion synthesis with
structured 3D scene contexts, limiting their effectiveness in real-world
egocentric settings where limited field of view, frequent occlusions, and
dynamic cameras hinder scene perception. To bridge this gap, we introduce
Egocentric Motion Generation and Egocentric Motion Forecasting, two novel tasks
that utilize first-person images for scene-aware motion synthesis without
relying on explicit 3D scene. We propose UniEgoMotion, a unified conditional
motion diffusion model with a novel head-centric motion representation tailored
for egocentric devices. UniEgoMotion's simple yet effective design supports
egocentric motion reconstruction, forecasting, and generation from first-person
visual inputs in a unified framework. Unlike previous works that overlook scene
semantics, our model effectively extracts image-based scene context to infer
plausible 3D motion. To facilitate training, we introduce EE4D-Motion, a
large-scale dataset derived from EgoExo4D, augmented with pseudo-ground-truth
3D motion annotations. UniEgoMotion achieves state-of-the-art performance in
egocentric motion reconstruction and is the first to generate motion from a
single egocentric image. Extensive evaluations demonstrate the effectiveness of
our unified framework, setting a new benchmark for egocentric motion modeling
and unlocking new possibilities for egocentric applications.","Chaitanya Patel, Hiroki Nakamura, Yuta Kyuragi, Kazuki Kozuka, Juan Carlos Niebles, Ehsan Adeli",2025-08-02T00:41:20Z,2025-08-02T00:41:20Z,http://arxiv.org/abs/2508.01126v1,http://arxiv.org/pdf/2508.01126v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Reducing the gap between general purpose data and aerial images in
  concentrated solar power plants","In the context of Concentrated Solar Power (CSP) plants, aerial images
captured by drones present a unique set of challenges. Unlike urban or natural
landscapes commonly found in existing datasets, solar fields contain highly
reflective surfaces, and domain-specific elements that are uncommon in
traditional computer vision benchmarks. As a result, machine learning models
trained on generic datasets struggle to generalize to this setting without
extensive retraining and large volumes of annotated data. However, collecting
and labeling such data is costly and time-consuming, making it impractical for
rapid deployment in industrial applications.
  To address this issue, we propose a novel approach: the creation of
AerialCSP, a virtual dataset that simulates aerial imagery of CSP plants. By
generating synthetic data that closely mimic real-world conditions, our
objective is to facilitate pretraining of models before deployment,
significantly reducing the need for extensive manual labeling. Our main
contributions are threefold: (1) we introduce AerialCSP, a high-quality
synthetic dataset for aerial inspection of CSP plants, providing annotated data
for object detection and image segmentation; (2) we benchmark multiple models
on AerialCSP, establishing a baseline for CSP-related vision tasks; and (3) we
demonstrate that pretraining on AerialCSP significantly improves real-world
fault detection, particularly for rare and small defects, reducing the need for
extensive manual labeling. AerialCSP is made publicly available at
https://mpcutino.github.io/aerialcsp/.","M. A. Pérez-Cutiño, J. Valverde, J. Capitán, J. M. Díaz-Báñez",2025-08-01T08:57:02Z,2025-08-01T08:57:02Z,http://arxiv.org/abs/2508.00440v1,http://arxiv.org/pdf/2508.00440v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Online Estimation of Table-Top Grown Strawberry Mass in Field Conditions
  with Occlusions","Accurate mass estimation of table-top grown strawberries under field
conditions remains challenging due to frequent occlusions and pose variations.
This study proposes a vision-based pipeline integrating RGB-D sensing and deep
learning to enable non-destructive, real-time and online mass estimation. The
method employed YOLOv8-Seg for instance segmentation, Cycle-consistent
generative adversarial network (CycleGAN) for occluded region completion, and
tilt-angle correction to refine frontal projection area calculations. A
polynomial regression model then mapped the geometric features to mass.
Experiments demonstrated mean mass estimation errors of 8.11% for isolated
strawberries and 10.47% for occluded cases. CycleGAN outperformed large mask
inpainting (LaMa) model in occlusion recovery, achieving superior pixel area
ratios (PAR) (mean: 0.978 vs. 1.112) and higher intersection over union (IoU)
scores (92.3% vs. 47.7% in the [0.9-1] range). This approach addresses critical
limitations of traditional methods, offering a robust solution for automated
harvesting and yield monitoring with complex occlusion patterns.","Jinshan Zhen, Yuanyue Ge, Tianxiao Zhu, Hui Zhao, Ya Xiong",2025-07-31T12:10:23Z,2025-07-31T12:10:23Z,http://arxiv.org/abs/2507.23487v1,http://arxiv.org/pdf/2507.23487v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Multi-Waypoint Path Planning and Motion Control for Non-holonomic Mobile
  Robots in Agricultural Applications","There is a growing demand for autonomous mobile robots capable of navigating
unstructured agricultural environments. Tasks such as weed control in meadows
require efficient path planning through an unordered set of coordinates while
minimizing travel distance and adhering to curvature constraints to prevent
soil damage and protect vegetation. This paper presents an integrated
navigation framework combining a global path planner based on the Dubins
Traveling Salesman Problem (DTSP) with a Nonlinear Model Predictive Control
(NMPC) strategy for local path planning and control. The DTSP generates a
minimum-length, curvature-constrained path that efficiently visits all targets,
while the NMPC leverages this path to compute control signals to accurately
reach each waypoint. The system's performance was validated through comparative
simulation analysis on real-world field datasets, demonstrating that the
coupled DTSP-based planner produced smoother and shorter paths, with a
reduction of about 16% in the provided scenario, compared to decoupled methods.
Based thereon, the NMPC controller effectively steered the robot to the desired
waypoints, while locally optimizing the trajectory and ensuring adherence to
constraints. These findings demonstrate the potential of the proposed framework
for efficient autonomous navigation in agricultural environments.","Mahmoud Ghorab, Matthias Lorenzen",2025-07-31T08:56:24Z,2025-07-31T08:56:24Z,http://arxiv.org/abs/2507.23350v1,http://arxiv.org/pdf/2507.23350v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Experimentally-Driven Analysis of Stability in Connected Vehicle
  Platooning: Insights and Control Strategies","This paper presents the development of a tangible platform for demonstrating
the practical implementation of cooperative adaptive cruise control (CACC)
systems, an enhancement to the standard adaptive cruise control (ACC) concept
by means of Vehicle-to-Everything (V2X) communication. It involves a detailed
examination of existing longitudinal controllers and their performance in
homogeneous vehicle platoons. Moreover, extensive tests are conducted using
multiple autonomous experimental vehicle platform topologies to verify the
effectiveness of the controller. The outcomes from both simulations and field
tests affirm the substantial benefits of the proposed CACC platooning approach
in longitudinal vehicle platooning scenarios. This research is crucial due to a
notable gap in the existing literature; while numerous studies focus on
simulated vehicle platooning systems, there is lack of research demonstrating
these controllers on physical vehicle systems or robot platforms. This paper
seeks to fill this gap by providing a practical demonstration of CACC systems
in action, showcasing their potential for real-world application in intelligent
transportation systems.","Niladri Dutta, Elham Abolfazli, Themistoklis Charalambous",2025-07-30T20:23:51Z,2025-07-30T20:23:51Z,http://arxiv.org/abs/2507.23078v1,http://arxiv.org/pdf/2507.23078v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Endoscopic Depth Estimation Based on Deep Learning: A Survey,"Endoscopic depth estimation is a critical technology for improving the safety
and precision of minimally invasive surgery. It has attracted considerable
attention from researchers in medical imaging, computer vision, and robotics.
Over the past decade, a large number of methods have been developed. Despite
the existence of several related surveys, a comprehensive overview focusing on
recent deep learning-based techniques is still limited. This paper endeavors to
bridge this gap by systematically reviewing the state-of-the-art literature.
Specifically, we provide a thorough survey of the field from three key
perspectives: data, methods, and applications, covering a range of methods
including both monocular and stereo approaches. We describe common performance
evaluation metrics and summarize publicly available datasets. Furthermore, this
review analyzes the specific challenges of endoscopic scenes and categorizes
representative techniques based on their supervision strategies and network
architectures. The application of endoscopic depth estimation in the important
area of robot-assisted surgery is also reviewed. Finally, we outline potential
directions for future research, such as domain adaptation, real-time
implementation, and enhanced model generalization, thereby providing a valuable
starting point for researchers to engage with and advance the field.","Ke Niu, Zeyun Liu, Xue Feng, Heng Li, Kaize Shi",2025-07-28T14:34:45Z,2025-07-28T14:34:45Z,http://arxiv.org/abs/2507.20881v1,http://arxiv.org/pdf/2507.20881v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"LanternNet: A Hub-and-Spoke System to Seek and Suppress Spotted
  Lanternfly Populations","The invasive spotted lanternfly (SLF) poses a significant threat to
agriculture and ecosystems, causing widespread damage. Current control methods,
such as egg scraping, pesticides, and quarantines, prove labor-intensive,
environmentally hazardous, and inadequate for long-term SLF suppression. This
research introduces LanternNet, a novel autonomous robotic Hub-and-Spoke system
designed for scalable detection and suppression of SLF populations. A central,
tree-mimicking hub utilizes a YOLOv8 computer vision model for precise SLF
identification. Three specialized robotic spokes perform targeted tasks: pest
neutralization, environmental monitoring, and navigation/mapping. Field
deployment across multiple infested sites over 5 weeks demonstrated
LanternNet's efficacy. Quantitative analysis revealed significant reductions (p
< 0.01, paired t-tests) in SLF populations and corresponding improvements in
tree health indicators across the majority of test sites. Compared to
conventional methods, LanternNet offers substantial cost advantages and
improved scalability. Furthermore, the system's adaptability for enhanced
autonomy and targeting of other invasive species presents significant potential
for broader ecological impact. LanternNet demonstrates the transformative
potential of integrating robotics and AI for advanced invasive species
management and improved environmental outcomes.",Vinil Polepalli,2025-07-28T13:08:33Z,2025-09-03T17:09:49Z,http://arxiv.org/abs/2507.20800v3,http://arxiv.org/pdf/2507.20800v3.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"LLMs-guided adaptive compensator: Bringing Adaptivity to Automatic
  Control Systems with Large Language Models","With rapid advances in code generation, reasoning, and problem-solving, Large
Language Models (LLMs) are increasingly applied in robotics. Most existing work
focuses on high-level tasks such as task decomposition. A few studies have
explored the use of LLMs in feedback controller design; however, these efforts
are restricted to overly simplified systems, fixed-structure gain tuning, and
lack real-world validation. To further investigate LLMs in automatic control,
this work targets a key subfield: adaptive control. Inspired by the framework
of model reference adaptive control (MRAC), we propose an LLM-guided adaptive
compensator framework that avoids designing controllers from scratch. Instead,
the LLMs are prompted using the discrepancies between an unknown system and a
reference system to design a compensator that aligns the response of the
unknown system with that of the reference, thereby achieving adaptivity.
Experiments evaluate five methods: LLM-guided adaptive compensator, LLM-guided
adaptive controller, indirect adaptive control, learning-based adaptive
control, and MRAC, on soft and humanoid robots in both simulated and real-world
environments. Results show that the LLM-guided adaptive compensator outperforms
traditional adaptive controllers and significantly reduces reasoning complexity
compared to the LLM-guided adaptive controller. The Lyapunov-based analysis and
reasoning-path inspection demonstrate that the LLM-guided adaptive compensator
enables a more structured design process by transforming mathematical
derivation into a reasoning task, while exhibiting strong generalizability,
adaptability, and robustness. This study opens a new direction for applying
LLMs in the field of automatic control, offering greater deployability and
practicality compared to vision-language models.","Zhongchao Zhou, Yuxi Lu, Yaonan Zhu, Yifan Zhao, Bin He, Liang He, Wenwen Yu, Yusuke Iwasawa",2025-07-28T04:12:43Z,2025-07-28T04:12:43Z,http://arxiv.org/abs/2507.20509v1,http://arxiv.org/pdf/2507.20509v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
A roadmap for AI in robotics,"AI technologies, including deep learning, large-language models have gone
from one breakthrough to the other. As a result, we are witnessing growing
excitement in robotics at the prospect of leveraging the potential of AI to
tackle some of the outstanding barriers to the full deployment of robots in our
daily lives. However, action and sensing in the physical world pose greater and
different challenges than analysing data in isolation. As the development and
application of AI in robotic products advances, it is important to reflect on
which technologies, among the vast array of network architectures and learning
models now available in the AI field, are most likely to be successfully
applied to robots; how they can be adapted to specific robot designs, tasks,
environments; which challenges must be overcome. This article offers an
assessment of what AI for robotics has achieved since the 1990s and proposes a
short- and medium-term research roadmap listing challenges and promises. These
range from keeping up-to-date large datasets, representatives of a diversity of
tasks robots may have to perform, and of environments they may encounter, to
designing AI algorithms tailored specifically to robotics problems but generic
enough to apply to a wide range of applications and transfer easily to a
variety of robotic platforms. For robots to collaborate effectively with
humans, they must predict human behavior without relying on bias-based
profiling. Explainability and transparency in AI-driven robot control are not
optional but essential for building trust, preventing misuse, and attributing
responsibility in accidents. We close on what we view as the primary long-term
challenges, that is, to design robots capable of lifelong learning, while
guaranteeing safe deployment and usage, and sustainable computational costs.","Aude Billard, Alin Albu-Schaeffer, Michael Beetz, Wolfram Burgard, Peter Corke, Matei Ciocarlie, Ravinder Dahiya, Danica Kragic, Ken Goldberg, Yukie Nagai, Davide Scaramuzza",2025-07-26T15:18:28Z,2025-07-26T15:18:28Z,http://arxiv.org/abs/2507.19975v1,http://arxiv.org/pdf/2507.19975v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Optimizing Spreading Factor Selection for Mobile LoRa Gateways Using
  Single-Channel Hardware","The deployment of mobile LoRa gateways using low-cost single-channel hardware
presents a significant challenge in maintaining reliable communication due to
the lack of dynamic configuration support. In traditional LoRaWAN networks,
Adaptive Data Rate (ADR) mechanisms optimize communication parameters in real
time. However, such features are typically supported only by expensive
multi-channel gateways. This study proposes a cost-effective and
energy-efficient solution by statically selecting the optimal Spreading Factor
(SF) using a two-phase algorithm. The method first applies rule-based exclusion
to eliminate SFs that violate constraints related to distance, data rate, link
margin, and regulatory limits. Remaining candidates are then evaluated using a
weighted scoring model incorporating Time-on-Air, energy consumption, data
rate, and link robustness. The proposed algorithm was validated through
extensive field tests and NS-3 simulations under line-of-sight conditions.
Results demonstrate that the selected SF matched the optimal SF in over 92% of
cases across 672 simulated scenarios, confirming the algorithm's effectiveness.
This approach offers a scalable alternative to dynamic protocols, enabling
reliable mobile LoRa deployments in cost-sensitive environments such as
agriculture and rural sensing applications.",W. A. Sasindu Wijesuriya,2025-07-26T12:54:11Z,2025-07-26T12:54:11Z,http://arxiv.org/abs/2507.19938v1,http://arxiv.org/pdf/2507.19938v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Efficient Lines Detection for Robot Soccer,"Self-localization is essential in robot soccer, where accurate detection of
visual field features, such as lines and boundaries, is critical for reliable
pose estimation. This paper presents a lightweight and efficient method for
detecting soccer field lines using the ELSED algorithm, extended with a
classification step that analyzes RGB color transitions to identify lines
belonging to the field. We introduce a pipeline based on Particle Swarm
Optimization (PSO) for threshold calibration to optimize detection performance,
requiring only a small number of annotated samples. Our approach achieves
accuracy comparable to a state-of-the-art deep learning model while offering
higher processing speed, making it well-suited for real-time applications on
low-power robotic platforms.","João G. Melo, João P. Mafaldo, Edna Barros",2025-07-25T17:54:51Z,2025-07-25T17:54:51Z,http://arxiv.org/abs/2507.19469v1,http://arxiv.org/pdf/2507.19469v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"SmartPNT-MSF: A Multi-Sensor Fusion Dataset for Positioning and
  Navigation Research","High-precision navigation and positioning systems are critical for
applications in autonomous vehicles and mobile mapping, where robust and
continuous localization is essential. To test and enhance the performance of
algorithms, some research institutions and companies have successively
constructed and publicly released datasets. However, existing datasets still
suffer from limitations in sensor diversity and environmental coverage. To
address these shortcomings and advance development in related fields, the
SmartPNT Multisource Integrated Navigation, Positioning, and Attitude Dataset
has been developed. This dataset integrates data from multiple sensors,
including Global Navigation Satellite Systems (GNSS), Inertial Measurement
Units (IMU), optical cameras, and LiDAR, to provide a rich and versatile
resource for research in multi-sensor fusion and high-precision navigation. The
dataset construction process is thoroughly documented, encompassing sensor
configurations, coordinate system definitions, and calibration procedures for
both cameras and LiDAR. A standardized framework for data collection and
processing ensures consistency and scalability, enabling large-scale analysis.
Validation using state-of-the-art Simultaneous Localization and Mapping (SLAM)
algorithms, such as VINS-Mono and LIO-SAM, demonstrates the dataset's
applicability for advanced navigation research. Covering a wide range of
real-world scenarios, including urban areas, campuses, tunnels, and suburban
environments, the dataset offers a valuable tool for advancing navigation
technologies and addressing challenges in complex environments. By providing a
publicly accessible, high-quality dataset, this work aims to bridge gaps in
sensor diversity, data accessibility, and environmental representation,
fostering further innovation in the field.","Feng Zhu, Zihang Zhang, Kangcheng Teng, Abduhelil Yakup, Xiaohong Zhang",2025-07-25T09:06:11Z,2025-07-31T05:59:58Z,http://arxiv.org/abs/2507.19079v2,http://arxiv.org/pdf/2507.19079v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
GMM-Based Time-Varying Coverage Control,"In coverage control problems that involve time-varying density functions, the
coverage control law depends on spatial integrals of the time evolution of the
density function. The latter is often neglected, replaced with an upper bound
or calculated as a numerical approximation of the spatial integrals involved.
In this paper, we consider a special case of time-varying density functions
modeled as Gaussian Mixture Models (GMMs) that evolve with time via a set of
time-varying sources (with known corresponding velocities). By imposing this
structure, we obtain an efficient time-varying coverage controller that fully
incorporates the time evolution of the density function. We show that the
induced trajectories under our control law minimise the overall coverage cost.
We elicit the structure of the proposed controller and compare it with a
classical time-varying coverage controller, against which we benchmark the
coverage performance in simulation. Furthermore, we highlight that the
computationally efficient and distributed nature of the proposed control law
makes it ideal for multi-vehicle robotic applications involving time-varying
coverage control problems. We employ our method in plume monitoring using a
swarm of drones. In an experimental field trial we show that drones guided by
the proposed controller are able to track a simulated time-varying chemical
plume in a distributed manner.","Behzad Zamani, James Kennedy, Airlie Chapman, Peter Dower, Chris Manzie, Simon Crase",2025-07-25T04:19:20Z,2025-07-25T04:19:20Z,http://arxiv.org/abs/2507.18938v1,http://arxiv.org/pdf/2507.18938v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Equivariant Volumetric Grasping,"We propose a new volumetric grasp model that is equivariant to rotations
around the vertical axis, leading to a significant improvement in sample
efficiency. Our model employs a tri-plane volumetric feature representation --
i.e., the projection of 3D features onto three canonical planes. We introduce a
novel tri-plane feature design in which features on the horizontal plane are
equivariant to 90{\deg} rotations, while the sum of features from the other two
planes remains invariant to the same transformations. This design is enabled by
a new deformable steerable convolution, which combines the adaptability of
deformable convolutions with the rotational equivariance of steerable ones.
This allows the receptive field to adapt to local object geometry while
preserving equivariance properties. We further develop equivariant adaptations
of two state-of-the-art volumetric grasp planners, GIGA and IGD. Specifically,
we derive a new equivariant formulation of IGD's deformable attention mechanism
and propose an equivariant generative model of grasp orientations based on flow
matching. We provide a detailed analytical justification of the proposed
equivariance properties and validate our approach through extensive simulated
and real-world experiments. Our results demonstrate that the proposed
projection-based design significantly reduces both computational and memory
costs. Moreover, the equivariant grasp models built on top of our tri-plane
features consistently outperform their non-equivariant counterparts, achieving
higher performance with only a modest computational overhead. Video and code
can be viewed in: https://mousecpn.github.io/evg-page/","Pinhao Song, Yutong Hu, Pengteng Li, Renaud Detry",2025-07-24T23:18:32Z,2025-08-05T12:56:02Z,http://arxiv.org/abs/2507.18847v2,http://arxiv.org/pdf/2507.18847v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
SaLF: Sparse Local Fields for Multi-Sensor Rendering in Real-Time,"High-fidelity sensor simulation of light-based sensors such as cameras and
LiDARs is critical for safe and accurate autonomy testing. Neural radiance
field (NeRF)-based methods that reconstruct sensor observations via ray-casting
of implicit representations have demonstrated accurate simulation of driving
scenes, but are slow to train and render, hampering scale. 3D Gaussian
Splatting (3DGS) has demonstrated faster training and rendering times through
rasterization, but is primarily restricted to pinhole camera sensors,
preventing usage for realistic multi-sensor autonomy evaluation. Moreover, both
NeRF and 3DGS couple the representation with the rendering procedure (implicit
networks for ray-based evaluation, particles for rasterization), preventing
interoperability, which is key for general usage. In this work, we present
Sparse Local Fields (SaLF), a novel volumetric representation that supports
rasterization and raytracing. SaLF represents volumes as a sparse set of 3D
voxel primitives, where each voxel is a local implicit field. SaLF has fast
training (<30 min) and rendering capabilities (50+ FPS for camera and 600+ FPS
LiDAR), has adaptive pruning and densification to easily handle large scenes,
and can support non-pinhole cameras and spinning LiDARs. We demonstrate that
SaLF has similar realism as existing self-driving sensor simulation methods
while improving efficiency and enhancing capabilities, enabling more scalable
simulation. https://waabi.ai/salf/","Yun Chen, Matthew Haines, Jingkang Wang, Krzysztof Baron-Lis, Sivabalan Manivasagam, Ze Yang, Raquel Urtasun",2025-07-24T18:01:22Z,2025-07-24T18:01:22Z,http://arxiv.org/abs/2507.18713v1,http://arxiv.org/pdf/2507.18713v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Evaluation of facial landmark localization performance in a surgical
  setting","The use of robotics, computer vision, and their applications is becoming
increasingly widespread in various fields, including medicine. Many face
detection algorithms have found applications in neurosurgery, ophthalmology,
and plastic surgery. A common challenge in using these algorithms is variable
lighting conditions and the flexibility of detection positions to identify and
precisely localize patients. The proposed experiment tests the MediaPipe
algorithm for detecting facial landmarks in a controlled setting, using a
robotic arm that automatically adjusts positions while the surgical light and
the phantom remain in a fixed position. The results of this study demonstrate
that the improved accuracy of facial landmark detection under surgical lighting
significantly enhances the detection performance at larger yaw and pitch
angles. The increase in standard deviation/dispersion occurs due to imprecise
detection of selected facial landmarks. This analysis allows for a discussion
on the potential integration of the MediaPipe algorithm into medical
procedures.","Ines Frajtag, Marko Švaco, Filip Šuligoj",2025-07-24T09:40:47Z,2025-07-24T09:40:47Z,http://arxiv.org/abs/2507.18248v1,http://arxiv.org/pdf/2507.18248v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Autonomous UAV Navigation for Search and Rescue Missions Using Computer
  Vision and Convolutional Neural Networks","In this paper, we present a subsystem, using Unmanned Aerial Vehicles (UAV),
for search and rescue missions, focusing on people detection, face recognition
and tracking of identified individuals. The proposed solution integrates a UAV
with ROS2 framework, that utilizes multiple convolutional neural networks (CNN)
for search missions. System identification and PD controller deployment are
performed for autonomous UAV navigation. The ROS2 environment utilizes the
YOLOv11 and YOLOv11-pose CNNs for tracking purposes, and the dlib library CNN
for face recognition. The system detects a specific individual, performs face
recognition and starts tracking. If the individual is not yet known, the UAV
operator can manually locate the person, save their facial image and
immediately initiate the tracking process. The tracking process relies on
specific keypoints identified on the human body using the YOLOv11-pose CNN
model. These keypoints are used to track a specific individual and maintain a
safe distance. To enhance accurate tracking, system identification is
performed, based on measurement data from the UAVs IMU. The identified system
parameters are used to design PD controllers that utilize YOLOv11-pose to
estimate the distance between the UAVs camera and the identified individual.
The initial experiments, conducted on 14 known individuals, demonstrated that
the proposed subsystem can be successfully used in real time. The next step
involves implementing the system on a large experimental UAV for field use and
integrating autonomous navigation with GPS-guided control for rescue operations
planning.","Luka Šiktar, Branimir Ćaran, Bojan Šekoranja, Marko Švaco",2025-07-24T07:54:45Z,2025-07-24T07:54:45Z,http://arxiv.org/abs/2507.18160v1,http://arxiv.org/pdf/2507.18160v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Neuromorphic Computing for Embodied Intelligence in Autonomous Systems:
  Current Trends, Challenges, and Future Directions","The growing need for intelligent, adaptive, and energy-efficient autonomous
systems across fields such as robotics, mobile agents (e.g., UAVs), and
self-driving vehicles is driving interest in neuromorphic computing. By drawing
inspiration from biological neural systems, neuromorphic approaches offer
promising pathways to enhance the perception, decision-making, and
responsiveness of autonomous platforms. This paper surveys recent progress in
neuromorphic algorithms, specialized hardware, and cross-layer optimization
strategies, with a focus on their deployment in real-world autonomous
scenarios. Special attention is given to event-based dynamic vision sensors and
their role in enabling fast, efficient perception. The discussion highlights
new methods that improve energy efficiency, robustness, adaptability, and
reliability through the integration of spiking neural networks into autonomous
system architectures. We integrate perspectives from machine learning,
robotics, neuroscience, and neuromorphic engineering to offer a comprehensive
view of the state of the field. Finally, emerging trends and open challenges
are explored, particularly in the areas of real-time decision-making, continual
learning, and the development of secure, resilient autonomous systems.","Alberto Marchisio, Muhammad Shafique",2025-07-24T07:01:52Z,2025-07-24T07:01:52Z,http://arxiv.org/abs/2507.18139v1,http://arxiv.org/pdf/2507.18139v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Policy Disruption in Reinforcement Learning:Adversarial Attack with
  Large Language Models and Critical State Identification","Reinforcement learning (RL) has achieved remarkable success in fields like
robotics and autonomous driving, but adversarial attacks designed to mislead RL
systems remain challenging. Existing approaches often rely on modifying the
environment or policy, limiting their practicality. This paper proposes an
adversarial attack method in which existing agents in the environment guide the
target policy to output suboptimal actions without altering the environment. We
propose a reward iteration optimization framework that leverages large language
models (LLMs) to generate adversarial rewards explicitly tailored to the
vulnerabilities of the target agent, thereby enhancing the effectiveness of
inducing the target agent toward suboptimal decision-making. Additionally, a
critical state identification algorithm is designed to pinpoint the target
agent's most vulnerable states, where suboptimal behavior from the victim leads
to significant degradation in overall performance. Experimental results in
diverse environments demonstrate the superiority of our method over existing
approaches.","Junyong Jiang, Buwei Tian, Chenxing Xu, Songze Li, Lu Dong",2025-07-24T05:52:06Z,2025-07-24T05:52:06Z,http://arxiv.org/abs/2507.18113v1,http://arxiv.org/pdf/2507.18113v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Online Submission and Evaluation System Design for Competition
  Operations","Research communities have developed benchmark datasets across domains to
compare the performance of algorithms and techniques However, tracking the
progress in these research areas is not easy, as publications appear in
different venues at the same time, and many of them claim to represent the
state-of-the-art. To address this, research communities often organise periodic
competitions to evaluate the performance of various algorithms and techniques,
thereby tracking advancements in the field. However, these competitions pose a
significant operational burden. The organisers must manage and evaluate a large
volume of submissions. Furthermore, participants typically develop their
solutions in diverse environments, leading to compatibility issues during the
evaluation of their submissions. This paper presents an online competition
system that automates the submission and evaluation process for a competition.
The competition system allows organisers to manage large numbers of submissions
efficiently, utilising isolated environments to evaluate submissions. This
system has already been used successfully for several competitions, including
the Grid-Based Pathfinding Competition and the League of Robot Runners
competition.","Zhe Chen, Daniel Harabor, Ryan Hechnenberger, Nathan R. Sturtevant",2025-07-23T17:44:10Z,2025-07-23T17:44:10Z,http://arxiv.org/abs/2507.17730v1,http://arxiv.org/pdf/2507.17730v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"CA-Cut: Crop-Aligned Cutout for Data Augmentation to Learn More Robust
  Under-Canopy Navigation","State-of-the-art visual under-canopy navigation methods are designed with
deep learning-based perception models to distinguish traversable space from
crop rows. While these models have demonstrated successful performance, they
require large amounts of training data to ensure reliability in real-world
field deployment. However, data collection is costly, demanding significant
human resources for in-field sampling and annotation. To address this
challenge, various data augmentation techniques are commonly employed during
model training, such as color jittering, Gaussian blur, and horizontal flip, to
diversify training data and enhance model robustness. In this paper, we
hypothesize that utilizing only these augmentation techniques may lead to
suboptimal performance, particularly in complex under-canopy environments with
frequent occlusions, debris, and non-uniform spacing of crops. Instead, we
propose a novel augmentation method, so-called Crop-Aligned Cutout (CA-Cut)
which masks random regions out in input images that are spatially distributed
around crop rows on the sides to encourage trained models to capture high-level
contextual features even when fine-grained information is obstructed. Our
extensive experiments with a public cornfield dataset demonstrate that
masking-based augmentations are effective for simulating occlusions and
significantly improving robustness in semantic keypoint predictions for visual
navigation. In particular, we show that biasing the mask distribution toward
crop rows in CA-Cut is critical for enhancing both prediction accuracy and
generalizability across diverse environments achieving up to a 36.9% reduction
in prediction error. In addition, we conduct ablation studies to determine the
number of masks, the size of each mask, and the spatial distribution of masks
to maximize overall performance.","Robel Mamo, Taeyeong Choi",2025-07-23T17:41:55Z,2025-07-24T13:55:49Z,http://arxiv.org/abs/2507.17727v2,http://arxiv.org/pdf/2507.17727v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
IONext: Unlocking the Next Era of Inertial Odometry,"Researchers have increasingly adopted Transformer-based models for inertial
odometry. While Transformers excel at modeling long-range dependencies, their
limited sensitivity to local, fine-grained motion variations and lack of
inherent inductive biases often hinder localization accuracy and
generalization. Recent studies have shown that incorporating large-kernel
convolutions and Transformer-inspired architectural designs into CNN can
effectively expand the receptive field, thereby improving global motion
perception. Motivated by these insights, we propose a novel CNN-based module
called the Dual-wing Adaptive Dynamic Mixer (DADM), which adaptively captures
both global motion patterns and local, fine-grained motion features from
dynamic inputs. This module dynamically generates selective weights based on
the input, enabling efficient multi-scale feature aggregation. To further
improve temporal modeling, we introduce the Spatio-Temporal Gating Unit (STGU),
which selectively extracts representative and task-relevant motion features in
the temporal domain. This unit addresses the limitations of temporal modeling
observed in existing CNN approaches. Built upon DADM and STGU, we present a new
CNN-based inertial odometry backbone, named Next Era of Inertial Odometry
(IONext). Extensive experiments on six public datasets demonstrate that IONext
consistently outperforms state-of-the-art (SOTA) Transformer- and CNN-based
methods. For instance, on the RNIN dataset, IONext reduces the average ATE by
10% and the average RTE by 12% compared to the representative model iMOT.","Shanshan Zhang, Siyue Wang, Tianshui Wen, Qi Zhang, Ziheng Zhou, Lingxiang Zheng, Yu Yang",2025-07-23T00:09:36Z,2025-07-23T00:09:36Z,http://arxiv.org/abs/2507.17089v1,http://arxiv.org/pdf/2507.17089v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Evaluating Uncertainty and Quality of Visual Language Action-enabled
  Robots","Visual Language Action (VLA) models are a multi-modal class of Artificial
Intelligence (AI) systems that integrate visual perception, natural language
understanding, and action planning to enable agents to interpret their
environment, comprehend instructions, and perform embodied tasks autonomously.
Recently, significant progress has been made to advance this field. These kinds
of models are typically evaluated through task success rates, which fail to
capture the quality of task execution and the mode's confidence in its
decisions. In this paper, we propose eight uncertainty metrics and five quality
metrics specifically designed for VLA models for robotic manipulation tasks. We
assess their effectiveness through a large-scale empirical study involving 908
successful task executions from three state-of-the-art VLA models across four
representative robotic manipulation tasks. Human domain experts manually
labeled task quality, allowing us to analyze the correlation between our
proposed metrics and expert judgments. The results reveal that several metrics
show moderate to strong correlation with human assessments, highlighting their
utility for evaluating task quality and model confidence. Furthermore, we found
that some of the metrics can discriminate between high-, medium-, and
low-quality executions from unsuccessful tasks, which can be interesting when
test oracles are not available. Our findings challenge the adequacy of current
evaluation practices that rely solely on binary success rates and pave the way
for improved real-time monitoring and adaptive enhancement of VLA-enabled
robotic systems.","Pablo Valle, Chengjie Lu, Shaukat Ali, Aitor Arrieta",2025-07-22T22:15:59Z,2025-07-31T18:21:39Z,http://arxiv.org/abs/2507.17049v2,http://arxiv.org/pdf/2507.17049v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"RAPTAR: Radar Radiation Pattern Acquisition through Automated
  Collaborative Robotics","Accurate characterization of modern on-chip antennas remains challenging, as
current probe-station techniques offer limited angular coverage, rely on
bespoke hardware, and require frequent manual alignment. This research
introduces RAPTAR (Radiation Pattern Acquisition through Robotic Automation), a
portable, state-of-the-art, and autonomous system based on collaborative
robotics. RAPTAR enables 3D radiation-pattern measurement of integrated radar
modules without dedicated anechoic facilities. The system is designed to
address the challenges of testing radar modules mounted in diverse real-world
configurations, including vehicles, UAVs, AR/VR headsets, and biomedical
devices, where traditional measurement setups are impractical. A
7-degree-of-freedom Franka cobot holds the receiver probe and performs
collision-free manipulation across a hemispherical spatial domain, guided by
real-time motion planning and calibration accuracy with RMS error below 0.9 mm.
The system achieves an angular resolution upto 2.5 degree and integrates
seamlessly with RF instrumentation for near- and far-field power measurements.
Experimental scans of a 60 GHz radar module show a mean absolute error of less
than 2 dB compared to full-wave electromagnetic simulations ground truth.
Benchmarking against baseline method demonstrates 36.5% lower mean absolute
error, highlighting RAPTAR accuracy and repeatability.","Maaz Qureshi, Mohammad Omid Bagheri, Abdelrahman Elbadrawy, William Melek, George Shaker",2025-07-22T19:52:05Z,2025-07-22T19:52:05Z,http://arxiv.org/abs/2507.16988v1,http://arxiv.org/pdf/2507.16988v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Comparative validation of surgical phase recognition, instrument
  keypoint estimation, and instrument instance segmentation in endoscopy:
  Results of the PhaKIR 2024 challenge","Reliable recognition and localization of surgical instruments in endoscopic
video recordings are foundational for a wide range of applications in computer-
and robot-assisted minimally invasive surgery (RAMIS), including surgical
training, skill assessment, and autonomous assistance. However, robust
performance under real-world conditions remains a significant challenge.
Incorporating surgical context - such as the current procedural phase - has
emerged as a promising strategy to improve robustness and interpretability.
  To address these challenges, we organized the Surgical Procedure Phase,
Keypoint, and Instrument Recognition (PhaKIR) sub-challenge as part of the
Endoscopic Vision (EndoVis) challenge at MICCAI 2024. We introduced a novel,
multi-center dataset comprising thirteen full-length laparoscopic
cholecystectomy videos collected from three distinct medical institutions, with
unified annotations for three interrelated tasks: surgical phase recognition,
instrument keypoint estimation, and instrument instance segmentation. Unlike
existing datasets, ours enables joint investigation of instrument localization
and procedural context within the same data while supporting the integration of
temporal information across entire procedures.
  We report results and findings in accordance with the BIAS guidelines for
biomedical image analysis challenges. The PhaKIR sub-challenge advances the
field by providing a unique benchmark for developing temporally aware,
context-driven methods in RAMIS and offers a high-quality resource to support
future research in surgical scene understanding.","Tobias Rueckert, David Rauber, Raphaela Maerkl, Leonard Klausmann, Suemeyye R. Yildiran, Max Gutbrod, Danilo Weber Nunes, Alvaro Fernandez Moreno, Imanol Luengo, Danail Stoyanov, Nicolas Toussaint, Enki Cho, Hyeon Bae Kim, Oh Sung Choo, Ka Young Kim, Seong Tae Kim, Gonçalo Arantes, Kehan Song, Jianjun Zhu, Junchen Xiong, Tingyi Lin, Shunsuke Kikuchi, Hiroki Matsuzaki, Atsushi Kouno, João Renato Ribeiro Manesco, João Paulo Papa, Tae-Min Choi, Tae Kyeong Jeong, Juyoun Park, Oluwatosin Alabi, Meng Wei, Tom Vercauteren, Runzhi Wu, Mengya Xu, An Wang, Long Bai, Hongliang Ren, Amine Yamlahi, Jakob Hennighausen, Lena Maier-Hein, Satoshi Kondo, Satoshi Kasai, Kousuke Hirasawa, Shu Yang, Yihui Wang, Hao Chen, Santiago Rodríguez, Nicolás Aparicio, Leonardo Manrique, Juan Camilo Lyons, Olivia Hosie, Nicolás Ayobi, Pablo Arbeláez, Yiping Li, Yasmina Al Khalil, Sahar Nasirihaghighi, Stefanie Speidel, Daniel Rueckert, Hubertus Feussner, Dirk Wilhelm, Christoph Palm",2025-07-22T13:10:42Z,2025-07-22T13:10:42Z,http://arxiv.org/abs/2507.16559v1,http://arxiv.org/pdf/2507.16559v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Distributed Oscillatory Guidance for Formation Flight of Fixed-Wing
  Drones","The autonomous formation flight of fixed-wing drones is hard when the
coordination requires the actuation over their speeds since they are critically
bounded and aircraft are mostly designed to fly at a nominal airspeed. This
paper proposes an algorithm to achieve formation flights of fixed-wing drones
without requiring any actuation over their speed. In particular, we guide all
the drones to travel over specific paths, e.g., parallel straight lines, and we
superpose an oscillatory behavior onto the guiding vector field that drives the
drones to the paths. This oscillation enables control over the average velocity
along the path, thereby facilitating inter-drone coordination. Each drone
adjusts its oscillation amplitude distributively in a closed-loop manner by
communicating with neighboring agents in an undirected and connected graph. A
novel consensus algorithm is introduced, leveraging a non-negative, asymmetric
saturation function. This unconventional saturation is justified since negative
amplitudes do not make drones travel backward or have a negative velocity along
the path. Rigorous theoretical analysis of the algorithm is complemented by
validation through numerical simulations and a real-world formation flight.","Yang Xu, Jesús Bautista, José Hinojosa, Héctor García de Marina",2025-07-22T11:00:31Z,2025-07-22T11:00:31Z,http://arxiv.org/abs/2507.16458v1,http://arxiv.org/pdf/2507.16458v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
AI or Human? Understanding Perceptions of Embodied Robots with LLMs,"The pursuit of artificial intelligence has long been associated to the the
challenge of effectively measuring intelligence. Even if the Turing Test was
introduced as a means of assessing a system intelligence, its relevance and
application within the field of human-robot interaction remain largely
underexplored. This study investigates the perception of intelligence in
embodied robots by performing a Turing Test within a robotic platform. A total
of 34 participants were tasked with distinguishing between AI- and
human-operated robots while engaging in two interactive tasks: an information
retrieval and a package handover. These tasks assessed the robot perception and
navigation abilities under both static and dynamic conditions. Results indicate
that participants were unable to reliably differentiate between AI- and
human-controlled robots beyond chance levels. Furthermore, analysis of
participant responses reveals key factors influencing the perception of
artificial versus human intelligence in embodied robotic systems. These
findings provide insights into the design of future interactive robots and
contribute to the ongoing discourse on intelligence assessment in AI-driven
systems.","Lavinia Hriscu, Alberto Sanfeliu, Anais Garrell",2025-07-22T09:48:57Z,2025-07-22T09:48:57Z,http://arxiv.org/abs/2507.16398v1,http://arxiv.org/pdf/2507.16398v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"DWSFormer: A Lightweight Inertial Odometry Network for Complex Motion
  Modeling","Inertial odometry (IO) directly estimates the position of a carrier from
inertial sensor measurements and serves as a core technology for the widespread
deployment of consumer grade localization systems. While existing IO methods
can accurately reconstruct simple and near linear motion trajectories, they
often fail to account for drift errors caused by complex motion patterns such
as turning. This limitation significantly degrades localization accuracy and
restricts the applicability of IO systems in real world scenarios. To address
these challenges, we propose a lightweight IO framework. Specifically, inertial
data is projected into a high dimensional implicit nonlinear feature space
using the Star Operation method, enabling the extraction of complex motion
features that are typically overlooked. We further introduce a collaborative
attention mechanism that jointly models global motion dynamics across both
channel and temporal dimensions. In addition, we design Multi Scale Gated
Convolution Units to capture fine grained dynamic variations throughout the
motion process, thereby enhancing the model's ability to learn rich and
expressive motion representations. Extensive experiments demonstrate that our
proposed method consistently outperforms SOTA baselines across six widely used
inertial datasets. Compared to baseline models on the RoNIN dataset, it
achieves reductions in ATE ranging from 2.26% to 65.78%, thereby establishing a
new benchmark in the field.","Shanshan Zhang, Qi Zhang, Siyue Wang, Tianshui Wen, Ziheng Zhou, Lingxiang Zheng, Yu Yang",2025-07-22T00:28:58Z,2025-07-22T00:28:58Z,http://arxiv.org/abs/2507.16121v1,http://arxiv.org/pdf/2507.16121v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Leveraging multi-source and heterogeneous signals for fatigue detection,"Fatigue detection plays a critical role in safety-critical applications such
as aviation, mining, and long-haul transport. However, most existing methods
rely on high-end sensors and controlled environments, limiting their
applicability in real world settings. This paper formally defines a practical
yet underexplored problem setting for real world fatigue detection, where
systems operating with context-appropriate sensors aim to leverage knowledge
from differently instrumented sources including those using impractical sensors
deployed in controlled environments. To tackle this challenge, we propose a
heterogeneous and multi-source fatigue detection framework that adaptively
utilizes the available modalities in the target domain while benefiting from
the diverse configurations present in source domains. Our experiments,
conducted using a realistic field-deployed sensor setup and two publicly
available datasets, demonstrate the practicality, robustness, and improved
generalization of our approach, paving the practical way for effective fatigue
monitoring in sensor-constrained scenarios.","Luobin Cui, Yanlai Wu, Tang Ying, Weikai Li",2025-07-21T17:22:18Z,2025-07-24T14:41:42Z,http://arxiv.org/abs/2507.16859v2,http://arxiv.org/pdf/2507.16859v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Improving Functional Reliability of Near-Field Monitoring for Emergency
  Braking in Autonomous Vehicles","Autonomous vehicles require reliable hazard detection. However, primary
sensor systems may miss near-field obstacles, resulting in safety risks.
Although a dedicated fast-reacting near-field monitoring system can mitigate
this, it typically suffers from false positives. To mitigate these, in this
paper, we introduce three monitoring strategies based on dynamic spatial
properties, relevant object sizes, and motion-aware prediction. In experiments
in a validated simulation, we compare the initial monitoring strategy against
the proposed improvements. The results demonstrate that the proposed strategies
can significantly improve the reliability of near-field monitoring systems.","Junnan Pan, Prodromos Sotiriadis, Vladislav Nenchev, Ferdinand Englberger",2025-07-21T13:17:44Z,2025-07-21T13:17:44Z,http://arxiv.org/abs/2507.15594v1,http://arxiv.org/pdf/2507.15594v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"FollowUpBot: An LLM-Based Conversational Robot for Automatic
  Postoperative Follow-up","Postoperative follow-up plays a crucial role in monitoring recovery and
identifying complications. However, traditional approaches, typically involving
bedside interviews and manual documentation, are time-consuming and
labor-intensive. Although existing digital solutions, such as web
questionnaires and intelligent automated calls, can alleviate the workload of
nurses to a certain extent, they either deliver an inflexible scripted
interaction or face private information leakage issues. To address these
limitations, this paper introduces FollowUpBot, an LLM-powered edge-deployed
robot for postoperative care and monitoring. It allows dynamic planning of
optimal routes and uses edge-deployed LLMs to conduct adaptive and face-to-face
conversations with patients through multiple interaction modes, ensuring data
privacy. Moreover, FollowUpBot is capable of automatically generating
structured postoperative follow-up reports for healthcare institutions by
analyzing patient interactions during follow-up. Experimental results
demonstrate that our robot achieves high coverage and satisfaction in follow-up
interactions, as well as high report generation accuracy across diverse field
types. The demonstration video is available at
https://www.youtube.com/watch?v=_uFgDO7NoK0.","Chen Chen, Jianing Yin, Jiannong Cao, Zhiyuan Wen, Mingjin Zhang, Weixun Gao, Xiang Wang, Haihua Shu",2025-07-21T11:07:49Z,2025-07-21T11:07:49Z,http://arxiv.org/abs/2507.15502v1,http://arxiv.org/pdf/2507.15502v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Robots for Kiwifruit Harvesting and Pollination,"This research was a part of a project that developed mobile robots that
performed targeted pollen spraying and automated harvesting in pergola
structured kiwifruit orchards. Multiple kiwifruit detachment mechanisms were
designed and field testing of one of the concepts showed that the mechanism
could reliably pick kiwifruit. Furthermore, this kiwifruit detachment mechanism
was able to reach over 80 percent of fruit in the cluttered kiwifruit canopy,
whereas the previous state of the art mechanism was only able to reach less
than 70 percent of the fruit. Artificial pollination was performed by detecting
flowers and then spraying pollen in solution onto the detected flowers from a
line of sprayers on a boom, while driving at up to 1.4 ms-1. In addition, the
height of the canopy was measured and the spray boom was moved up and down to
keep the boom close enough to the flowers for the spray to reach the flowers,
while minimising collisions with the canopy. Mobile robot navigation was
performed using a 2D lidar in apple orchards and vineyards. Lidar navigation in
kiwifruit orchards was more challenging because the pergola structure only
provides a small amount of data for the direction of rows, compared to the
amount of data from the overhead canopy, the undulating ground and other
objects in the orchards. Multiple methods are presented here for extracting
structure defining features from 3D lidar data in kiwifruit orchards. In
addition, a 3D lidar navigation system -- which performed row following, row
end detection and row end turns -- was tested for over 30 km of autonomous
driving in kiwifruit orchards. Computer vision algorithms for row detection and
row following were also tested. The computer vision algorithm worked as well as
the 3D lidar row following method in testing.",Jamie Bell,2025-07-21T10:40:28Z,2025-07-21T10:40:28Z,http://arxiv.org/abs/2507.15484v1,http://arxiv.org/pdf/2507.15484v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Low-Latency Event-Based Velocimetry for Quadrotor Control in a Narrow
  Pipe","Autonomous quadrotor flight in confined spaces such as pipes and tunnels
presents significant challenges due to unsteady, self-induced aerodynamic
disturbances. Very recent advances have enabled flight in such conditions, but
they either rely on constant motion through the pipe to mitigate airflow
recirculation effects or suffer from limited stability during hovering. In this
work, we present the first closed-loop control system for quadrotors for
hovering in narrow pipes that leverages real-time flow field measurements. We
develop a low-latency, event-based smoke velocimetry method that estimates
local airflow at high temporal resolution. This flow information is used by a
disturbance estimator based on a recurrent convolutional neural network, which
infers force and torque disturbances in real time. The estimated disturbances
are integrated into a learning-based controller trained via reinforcement
learning. The flow-feedback control proves particularly effective during
lateral translation maneuvers in the pipe cross-section. There, the real-time
disturbance information enables the controller to effectively counteract
transient aerodynamic effects, thereby preventing collisions with the pipe
wall. To the best of our knowledge, this work represents the first
demonstration of an aerial robot with closed-loop control informed by real-time
flow field measurements. This opens new directions for research on flight in
aerodynamically complex environments. In addition, our work also sheds light on
the characteristic flow structures that emerge during flight in narrow,
circular pipes, providing new insights at the intersection of robotics and
fluid dynamics.","Leonard Bauersfeld, Davide Scaramuzza",2025-07-21T09:53:42Z,2025-07-21T09:53:42Z,http://arxiv.org/abs/2507.15444v1,http://arxiv.org/pdf/2507.15444v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Learning-Based Modeling of a Magnetically Steerable Soft Suction Device
  for Endoscopic Endonasal Interventions","This letter introduces a novel learning-based modeling framework for a
magnetically steerable soft suction device designed for endoscopic endonasal
brain tumor resection. The device is miniaturized (4 mm outer diameter, 2 mm
inner diameter, 40 mm length), 3D printed using biocompatible SIL 30 material,
and integrates embedded Fiber Bragg Grating (FBG) sensors for real-time shape
feedback. Shape reconstruction is represented using four Bezier control points,
enabling a compact and smooth model of the device's deformation. A data-driven
model was trained on 5,097 experimental samples covering a range of magnetic
field magnitudes (0-14 mT), actuation frequencies (0.2-1.0 Hz), and vertical
tip distances (90-100 mm), using both Neural Network (NN) and Random Forest
(RF) architectures. The RF model outperformed the NN across all metrics,
achieving a mean root mean square error of 0.087 mm in control point prediction
and a mean shape reconstruction error of 0.064 mm. Feature importance analysis
further revealed that magnetic field components predominantly influence distal
control points, while frequency and distance affect the base configuration.
This learning-based approach effectively models the complex nonlinear behavior
of hyperelastic soft robots under magnetic actuation without relying on
simplified physical assumptions. By enabling sub-millimeter shape prediction
accuracy and real-time inference, this work represents an advancement toward
the intelligent control of magnetically actuated soft robotic tools in
minimally invasive neurosurgery.","Majid Roshanfar, Alex Zhang, Changyan He, Amir Hooshiar, Dale J. Podolsky, Thomas Looi, Eric Diller",2025-07-20T23:27:44Z,2025-07-20T23:27:44Z,http://arxiv.org/abs/2507.15155v1,http://arxiv.org/pdf/2507.15155v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Analytical Formulation of Autonomous Vehicle Freeway Merging Control
  with State-Dependent Discharge Rates","The core of the freeway merging control problem lies in dynamic queue
propagation and dissipation linked to merging vehicle behavior. Traditionally,
queuing is modeled through demand-supply interactions with time varying demand
and fixed capacity. However, field observations show flow rates decrease during
congestion at freeway merges due to the impact of intersecting traffic, a
factor overlooked in fundamental diagrams. This manuscript introduces an
analytical approach to characterize and control the dynamic multi-stage merging
of autonomous vehicles, prioritizing traffic efficiency and safety. For the
first time, the effective discharge rate at the merging point, reduced by the
multi-stage dynamic merging process, is analytically derived using a closed
form formulation. Leveraging this expression, performance metrics such as queue
length and traffic delay are derived as the first objective. Additionally, a
crash risk function is established to quantitatively assess potential
collisions during the merging process, serving as the second objective.
Finally, the problem is formulated as a dynamic programming model to jointly
minimize delay and crash risk, with the merging location and speed as decision
variables. Given the terminal state, the ramp vehicle merging task is
formulated as a recursive optimization problem, employing backward induction to
find the minimum cost solution. Numerical experiments using the NGSIM dataset
validate the derived effective discharge rate. The results indicate that the
proposed model outperforms two benchmark algorithms, leading to a more
efficient and safer merging process.","Qing Tang, Xianbiao Hu",2025-07-20T19:19:38Z,2025-07-20T19:19:38Z,http://arxiv.org/abs/2507.16846v1,http://arxiv.org/pdf/2507.16846v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary
  queries in NeRF","3D semantic segmentation provides high-level scene understanding for
applications in robotics, autonomous systems, \textit{etc}. Traditional methods
adapt exclusively to either task-specific goals (open-vocabulary segmentation)
or scene content (unsupervised semantic segmentation). We propose DiSCO-3D, the
first method addressing the broader problem of 3D Open-Vocabulary Sub-concepts
Discovery, which aims to provide a 3D semantic segmentation that adapts to both
the scene and user queries. We build DiSCO-3D on Neural Fields representations,
combining unsupervised segmentation with weak open-vocabulary guidance. Our
evaluations demonstrate that DiSCO-3D achieves effective performance in
Open-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in
the edge cases of both open-vocabulary and unsupervised segmentation.","Doriand Petit, Steve Bourgeois, Vincent Gay-Bellile, Florian Chabot, Loïc Barthe",2025-07-19T12:46:20Z,2025-07-19T12:46:20Z,http://arxiv.org/abs/2507.14596v1,http://arxiv.org/pdf/2507.14596v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"BT-TL-DMPs: A Novel Robot TAMP Framework Combining Behavior Tree,
  Temporal Logic and Dynamical Movement Primitives","In the field of Learning from Demonstration (LfD), enabling robots to
generalize learned manipulation skills to novel scenarios for long-horizon
tasks remains challenging. Specifically, it is still difficult for robots to
adapt the learned skills to new environments with different task and motion
requirements, especially in long-horizon, multi-stage scenarios with intricate
constraints. This paper proposes a novel hierarchical framework, called
BT-TL-DMPs, that integrates Behavior Tree (BT), Temporal Logic (TL), and
Dynamical Movement Primitives (DMPs) to address this problem. Within this
framework, Signal Temporal Logic (STL) is employed to formally specify complex,
long-horizon task requirements and constraints. These STL specifications are
systematically transformed to generate reactive and modular BTs for high-level
decision-making task structure. An STL-constrained DMP optimization method is
proposed to optimize the DMP forcing term, allowing the learned motion
primitives to adapt flexibly while satisfying intricate spatiotemporal
requirements and, crucially, preserving the essential dynamics learned from
demonstrations. The framework is validated through simulations demonstrating
generalization capabilities under various STL constraints and real-world
experiments on several long-horizon robotic manipulation tasks. The results
demonstrate that the proposed framework effectively bridges the symbolic-motion
gap, enabling more reliable and generalizable autonomous manipulation for
complex robotic tasks.","Zezhi Liu, Shizhen Wu, Hanqian Luo, Deyun Qin, Yongchun Fang",2025-07-19T11:53:24Z,2025-07-19T11:53:24Z,http://arxiv.org/abs/2507.14582v1,http://arxiv.org/pdf/2507.14582v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey,"3D reconstruction and view synthesis are foundational problems in computer
vision, graphics, and immersive technologies such as augmented reality (AR),
virtual reality (VR), and digital twins. Traditional methods rely on
computationally intensive iterative optimization in a complex chain, limiting
their applicability in real-world scenarios. Recent advances in feed-forward
approaches, driven by deep learning, have revolutionized this field by enabling
fast and generalizable 3D reconstruction and view synthesis. This survey offers
a comprehensive review of feed-forward techniques for 3D reconstruction and
view synthesis, with a taxonomy according to the underlying representation
architectures including point cloud, 3D Gaussian Splatting (3DGS), Neural
Radiance Fields (NeRF), etc. We examine key tasks such as pose-free
reconstruction, dynamic 3D reconstruction, and 3D-aware image and video
synthesis, highlighting their applications in digital humans, SLAM, robotics,
and beyond. In addition, we review commonly used datasets with detailed
statistics, along with evaluation protocols for various downstream tasks. We
conclude by discussing open research challenges and promising directions for
future work, emphasizing the potential of feed-forward approaches to advance
the state of the art in 3D vision.","Jiahui Zhang, Yuelei Li, Anpei Chen, Muyu Xu, Kunhao Liu, Jianyuan Wang, Xiao-Xiao Long, Hanxue Liang, Zexiang Xu, Hao Su, Christian Theobalt, Christian Rupprecht, Andrea Vedaldi, Hanspeter Pfister, Shijian Lu, Fangneng Zhan",2025-07-19T06:13:25Z,2025-07-30T03:32:37Z,http://arxiv.org/abs/2507.14501v2,http://arxiv.org/pdf/2507.14501v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"A multi-strategy improved snake optimizer for three-dimensional UAV path
  planning and engineering problems","Metaheuristic algorithms have gained widespread application across various
fields owing to their ability to generate diverse solutions. One such algorithm
is the Snake Optimizer (SO), a progressive optimization approach. However, SO
suffers from the issues of slow convergence speed and susceptibility to local
optima. In light of these shortcomings, we propose a novel Multi-strategy
Improved Snake Optimizer (MISO). Firstly, we propose a new adaptive random
disturbance strategy based on sine function to alleviate the risk of getting
trapped in a local optimum. Secondly, we introduce adaptive Levy flight
strategy based on scale factor and leader and endow the male snake leader with
flight capability, which makes it easier for the algorithm to leap out of the
local optimum and find the global optimum. More importantly, we put forward a
position update strategy combining elite leadership and Brownian motion,
effectively accelerating the convergence speed while ensuring precision.
Finally, to demonstrate the performance of MISO, we utilize 30 CEC2017 test
functions and the CEC2022 test suite, comparing it with 11 popular algorithms
across different dimensions to validate its effectiveness. Moreover, Unmanned
Aerial Vehicle (UAV) has been widely used in various fields due to its
advantages of low cost, high mobility and easy operation. However, the UAV path
planning problem is crucial for flight safety and efficiency, and there are
still challenges in establishing and optimizing the path model. Therefore, we
apply MISO to the UAV 3D path planning problem as well as 6 engineering design
problems to assess its feasibility in practical applications. The experimental
results demonstrate that MISO exceeds other competitive algorithms in terms of
solution quality and stability, establishing its strong potential for
application.","Genliang Li, Yaxin Cui, Jinyu Su",2025-07-18T16:11:35Z,2025-08-13T14:12:28Z,http://arxiv.org/abs/2507.14043v2,http://arxiv.org/pdf/2507.14043v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Learning Deformable Body Interactions With Adaptive Spatial Tokenization,"Simulating interactions between deformable bodies is vital in fields like
material science, mechanical design, and robotics. While learning-based methods
with Graph Neural Networks (GNNs) are effective at solving complex physical
systems, they encounter scalability issues when modeling deformable body
interactions. To model interactions between objects, pairwise global edges have
to be created dynamically, which is computationally intensive and impractical
for large-scale meshes. To overcome these challenges, drawing on insights from
geometric representations, we propose an Adaptive Spatial Tokenization (AST)
method for efficient representation of physical states. By dividing the
simulation space into a grid of cells and mapping unstructured meshes onto this
structured grid, our approach naturally groups adjacent mesh nodes. We then
apply a cross-attention module to map the sparse cells into a compact,
fixed-length embedding, serving as tokens for the entire physical state.
Self-attention modules are employed to predict the next state over these tokens
in latent space. This framework leverages the efficiency of tokenization and
the expressive power of attention mechanisms to achieve accurate and scalable
simulation results. Extensive experiments demonstrate that our method
significantly outperforms state-of-the-art approaches in modeling deformable
body interactions. Notably, it remains effective on large-scale simulations
with meshes exceeding 100,000 nodes, where existing methods are hindered by
computational limitations. Additionally, we contribute a novel large-scale
dataset encompassing a wide range of deformable body interactions to support
future research in this area.","Hao Wang, Yu Liu, Daniel Biggs, Haoru Wang, Jiandong Yu, Ping Huang",2025-07-18T07:27:35Z,2025-07-18T07:27:35Z,http://arxiv.org/abs/2507.13707v1,http://arxiv.org/pdf/2507.13707v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Efficient Online Learning and Adaptive Planning for Robotic Information
  Gathering Based on Streaming Data","Robotic information gathering (RIG) techniques refer to methods where mobile
robots are used to acquire data about the physical environment with a suite of
sensors. Informative planning is an important part of RIG where the goal is to
find sequences of actions or paths that maximize efficiency or the quality of
information collected. Many existing solutions solve this problem by assuming
that the environment is known in advance. However, real environments could be
unknown or time-varying, and adaptive informative planning remains an active
area of research. Adaptive planning and incremental online mapping are required
for mapping initially unknown or varying spatial fields. Gaussian process (GP)
regression is a widely used technique in RIG for mapping continuous spatial
fields. However, it falls short in many applications as its real-time
performance does not scale well to large datasets. To address these challenges,
this paper proposes an efficient adaptive informative planning approach for
mapping continuous scalar fields with GPs with streaming sparse GPs. Simulation
experiments are performed with a synthetic dataset and compared against
existing benchmarks. Finally, it is also verified with a real-world dataset to
further validate the efficacy of the proposed method. Results show that our
method achieves similar mapping accuracy to the baselines while reducing
computational complexity for longer missions.","Sanjeev Ramkumar Sudha, Joel Jose, Erlend M. Coates",2025-07-17T12:26:03Z,2025-08-31T12:00:35Z,http://arxiv.org/abs/2507.13053v2,http://arxiv.org/pdf/2507.13053v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Intelligent Virtual Sonographer (IVS): Enhancing Physician-Robot-Patient
  Communication","The advancement and maturity of large language models (LLMs) and robotics
have unlocked vast potential for human-computer interaction, particularly in
the field of robotic ultrasound. While existing research primarily focuses on
either patient-robot or physician-robot interaction, the role of an intelligent
virtual sonographer (IVS) bridging physician-robot-patient communication
remains underexplored. This work introduces a conversational virtual agent in
Extended Reality (XR) that facilitates real-time interaction between
physicians, a robotic ultrasound system(RUS), and patients. The IVS agent
communicates with physicians in a professional manner while offering empathetic
explanations and reassurance to patients. Furthermore, it actively controls the
RUS by executing physician commands and transparently relays these actions to
the patient. By integrating LLM-powered dialogue with speech-to-text,
text-to-speech, and robotic control, our system enhances the efficiency,
clarity, and accessibility of robotic ultrasound acquisition. This work
constitutes a first step toward understanding how IVS can bridge communication
gaps in physician-robot-patient interaction, providing more control and
therefore trust into physician-robot interaction while improving patient
experience and acceptance of robotic ultrasound.","Tianyu Song, Feng Li, Yuan Bi, Angelos Karlas, Amir Yousefi, Daniela Branzan, Zhongliang Jiang, Ulrich Eck, Nassir Navab",2025-07-17T12:25:01Z,2025-07-17T12:25:01Z,http://arxiv.org/abs/2507.13052v1,http://arxiv.org/pdf/2507.13052v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"What Can Robots Teach Us About Trust and Reliance? An interdisciplinary
  dialogue between Social Sciences and Social Robotics","As robots find their way into more and more aspects of everyday life,
questions around trust are becoming increasingly important. What does it mean
to trust a robot? And how should we think about trust in relationships that
involve both humans and non-human agents? While the field of Human-Robot
Interaction (HRI) has made trust a central topic, the concept is often
approached in fragmented ways. At the same time, established work in sociology,
where trust has long been a key theme, is rarely brought into conversation with
developments in robotics. This article argues that we need a more
interdisciplinary approach. By drawing on insights from both social sciences
and social robotics, we explore how trust is shaped, tested and made visible.
Our goal is to open up a dialogue between disciplines and help build a more
grounded and adaptable framework for understanding trust in the evolving world
of human-robot interaction.","Julien Wacquez, Elisabetta Zibetti, Joffrey Becker, Lorenzo Aloe, Fabio Amadio, Salvatore Anzalone, Lola Cañamero, Serena Ivaldi",2025-07-17T12:10:34Z,2025-07-17T12:10:34Z,http://arxiv.org/abs/2507.13041v1,http://arxiv.org/pdf/2507.13041v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Learning to Predict Mobile Robot Stability in Off-Road Environments,"Navigating in off-road environments for wheeled mobile robots is challenging
due to dynamic and rugged terrain. Traditional physics-based stability metrics,
such as Static Stability Margin (SSM) or Zero Moment Point (ZMP) require
knowledge of contact forces, terrain geometry, and the robot's precise
center-of-mass that are difficult to measure accurately in real-world field
conditions. In this work, we propose a learning-based approach to estimate
robot platform stability directly from proprioceptive data using a lightweight
neural network, IMUnet. Our method enables data-driven inference of robot
stability without requiring an explicit terrain model or force sensing.
  We also develop a novel vision-based ArUco tracking method to compute a
scalar score to quantify robot platform stability called C3 score. The score
captures image-space perturbations over time as a proxy for physical
instability and is used as a training signal for the neural network based
model. As a pilot study, we evaluate our approach on data collected across
multiple terrain types and speeds and demonstrate generalization to previously
unseen conditions. These initial results highlight the potential of using IMU
and robot velocity as inputs to estimate platform stability. The proposed
method finds application in gating robot tasks such as precision actuation and
sensing, especially for mobile manipulation tasks in agricultural and space
applications. Our learning method also provides a supervision mechanism for
perception based traversability estimation and planning.","Nathaniel Rose, Arif Ahmed, Emanuel Gutierrez-Cornejo, Parikshit Maini",2025-07-17T02:24:35Z,2025-07-17T02:24:35Z,http://arxiv.org/abs/2507.12731v1,http://arxiv.org/pdf/2507.12731v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"MoistureMapper: An Autonomous Mobile Robot for High-Resolution Soil
  Moisture Mapping at Scale","Soil moisture is a quantity of interest in many application areas including
agriculture and climate modeling. Existing methods are not suitable for scale
applications due to large deployment costs in high-resolution sensing
applications such as for variable irrigation. In this work, we design, build
and field deploy an autonomous mobile robot, MoistureMapper, for soil moisture
sensing. The robot is equipped with Time Domain Reflectometry (TDR) sensors and
a direct push drill mechanism for deploying the sensor to measure volumetric
water content in the soil. Additionally, we implement and evaluate multiple
adaptive sampling strategies based on a Gaussian Process based modeling to
build a spatial mapping of moisture distribution in the soil. We present
results from large scale computational simulations and proof-of-concept
deployment on the field. The adaptive sampling approach outperforms a greedy
benchmark approach and results in up to 30\% reduction in travel distance and
5\% reduction in variance in the reconstructed moisture maps. Link to video
showing field experiments: https://youtu.be/S4bJ4tRzObg","Nathaniel Rose, Hannah Chuang, Manuel A Andrade-Rodriguez, Rishi Parashar, Dani Or, Parikshit Maini",2025-07-17T01:47:37Z,2025-07-17T01:47:37Z,http://arxiv.org/abs/2507.12716v1,http://arxiv.org/pdf/2507.12716v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"UniLGL: Learning Uniform Place Recognition for FOV-limited/Panoramic
  LiDAR Global Localization","Existing LGL methods typically consider only partial information (e.g.,
geometric features) from LiDAR observations or are designed for homogeneous
LiDAR sensors, overlooking the uniformity in LGL. In this work, a uniform LGL
method is proposed, termed UniLGL, which simultaneously achieves spatial and
material uniformity, as well as sensor-type uniformity. The key idea of the
proposed method is to encode the complete point cloud, which contains both
geometric and material information, into a pair of BEV images (i.e., a spatial
BEV image and an intensity BEV image). An end-to-end multi-BEV fusion network
is designed to extract uniform features, equipping UniLGL with spatial and
material uniformity. To ensure robust LGL across heterogeneous LiDAR sensors, a
viewpoint invariance hypothesis is introduced, which replaces the conventional
translation equivariance assumption commonly used in existing LPR networks and
supervises UniLGL to achieve sensor-type uniformity in both global descriptors
and local feature representations. Finally, based on the mapping between local
features on the 2D BEV image and the point cloud, a robust global pose
estimator is derived that determines the global minimum of the global pose on
SE(3) without requiring additional registration. To validate the effectiveness
of the proposed uniform LGL, extensive benchmarks are conducted in real-world
environments, and the results show that the proposed UniLGL is demonstratively
competitive compared to other State-of-the-Art LGL methods. Furthermore, UniLGL
has been deployed on diverse platforms, including full-size trucks and agile
Micro Aerial Vehicles (MAVs), to enable high-precision localization and mapping
as well as multi-MAV collaborative exploration in port and forest environments,
demonstrating the applicability of UniLGL in industrial and field scenarios.","Hongming Shen, Xun Chen, Yulin Hui, Zhenyu Wu, Wei Wang, Qiyang Lyu, Tianchen Deng, Danwei Wang",2025-07-16T12:45:56Z,2025-07-31T11:57:29Z,http://arxiv.org/abs/2507.12194v2,http://arxiv.org/pdf/2507.12194v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Fast and Scalable Game-Theoretic Trajectory Planning with Intentional
  Uncertainties","Trajectory planning involving multi-agent interactions has been a
long-standing challenge in the field of robotics, primarily burdened by the
inherent yet intricate interactions among agents. While game-theoretic methods
are widely acknowledged for their effectiveness in managing multi-agent
interactions, significant impediments persist when it comes to accommodating
the intentional uncertainties of agents. In the context of intentional
uncertainties, the heavy computational burdens associated with existing
game-theoretic methods are induced, leading to inefficiencies and poor
scalability. In this paper, we propose a novel game-theoretic interactive
trajectory planning method to effectively address the intentional uncertainties
of agents, and it demonstrates both high efficiency and enhanced scalability.
As the underpinning basis, we model the interactions between agents under
intentional uncertainties as a general Bayesian game, and we show that its
agent-form equivalence can be represented as a potential game under certain
minor assumptions. The existence and attainability of the optimal interactive
trajectories are illustrated, as the corresponding Bayesian Nash equilibrium
can be attained by optimizing a unified optimization problem. Additionally, we
present a distributed algorithm based on the dual consensus alternating
direction method of multipliers (ADMM) tailored to the parallel solving of the
problem, thereby significantly improving the scalability. The attendant
outcomes from simulations and experiments demonstrate that the proposed method
is effective across a range of scenarios characterized by general forms of
intentional uncertainties. Its scalability surpasses that of existing
centralized and decentralized baselines, allowing for real-time interactive
trajectory planning in uncertain game settings.","Zhenmin Huang, Yusen Xie, Benshan Ma, Shaojie Shen, Jun Ma",2025-07-16T12:12:25Z,2025-07-16T12:12:25Z,http://arxiv.org/abs/2507.12174v1,http://arxiv.org/pdf/2507.12174v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Physically Based Neural LiDAR Resimulation,"Methods for Novel View Synthesis (NVS) have recently found traction in the
field of LiDAR simulation and large-scale 3D scene reconstruction. While
solutions for faster rendering or handling dynamic scenes have been proposed,
LiDAR specific effects remain insufficiently addressed. By explicitly modeling
sensor characteristics such as rolling shutter, laser power variations, and
intensity falloff, our method achieves more accurate LiDAR simulation compared
to existing techniques. We demonstrate the effectiveness of our approach
through quantitative and qualitative comparisons with state-of-the-art methods,
as well as ablation studies that highlight the importance of each sensor model
component. Beyond that, we show that our approach exhibits advanced
resimulation capabilities, such as generating high resolution LiDAR scans in
the camera perspective.
  Our code and the resulting dataset are available at
https://github.com/richardmarcus/PBNLiDAR.","Richard Marcus, Marc Stamminger",2025-07-15T19:49:44Z,2025-07-15T19:49:44Z,http://arxiv.org/abs/2507.12489v1,http://arxiv.org/pdf/2507.12489v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"From Chaos to Automation: Enabling the Use of Unstructured Data for
  Robotic Process Automation","The growing volume of unstructured data within organizations poses
significant challenges for data analysis and process automation. Unstructured
data, which lacks a predefined format, encompasses various forms such as
emails, reports, and scans. It is estimated to constitute approximately 80% of
enterprise data. Despite the valuable insights it can offer, extracting
meaningful information from unstructured data is more complex compared to
structured data. Robotic Process Automation (RPA) has gained popularity for
automating repetitive tasks, improving efficiency, and reducing errors.
However, RPA is traditionally reliant on structured data, limiting its
application to processes involving unstructured documents. This study addresses
this limitation by developing the UNstructured Document REtrieval SyStem
(UNDRESS), a system that uses fuzzy regular expressions, techniques for natural
language processing, and large language models to enable RPA platforms to
effectively retrieve information from unstructured documents. The research
involved the design and development of a prototype system, and its subsequent
evaluation based on text extraction and information retrieval performance. The
results demonstrate the effectiveness of UNDRESS in enhancing RPA capabilities
for unstructured data, providing a significant advancement in the field. The
findings suggest that this system could facilitate broader RPA adoption across
processes traditionally hindered by unstructured data, thereby improving
overall business process efficiency.","Kelly Kurowski, Xixi Lu, Hajo A. Reijers",2025-07-15T14:32:49Z,2025-07-15T14:32:49Z,http://arxiv.org/abs/2507.11364v1,http://arxiv.org/pdf/2507.11364v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"All Eyes, no IMU: Learning Flight Attitude from Vision Alone","Vision is an essential part of attitude control for many flying animals, some
of which have no dedicated sense of gravity. Flying robots, on the other hand,
typically depend heavily on accelerometers and gyroscopes for attitude
stabilization. In this work, we present the first vision-only approach to
flight control for use in generic environments. We show that a quadrotor drone
equipped with a downward-facing event camera can estimate its attitude and
rotation rate from just the event stream, enabling flight control without
inertial sensors. Our approach uses a small recurrent convolutional neural
network trained through supervised learning. Real-world flight tests
demonstrate that our combination of event camera and low-latency neural network
is capable of replacing the inertial measurement unit in a traditional flight
control loop. Furthermore, we investigate the network's generalization across
different environments, and the impact of memory and different fields of view.
While networks with memory and access to horizon-like visual cues achieve best
performance, variants with a narrower field of view achieve better relative
generalization. Our work showcases vision-only flight control as a promising
candidate for enabling autonomous, insect-scale flying robots.","Jesse J. Hagenaars, Stein Stroobants, Sander M. Bohte, Guido C. H. E. De Croon",2025-07-15T13:31:27Z,2025-07-15T13:31:27Z,http://arxiv.org/abs/2507.11302v1,http://arxiv.org/pdf/2507.11302v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Uncertainty Aware Mapping for Vision-Based Underwater Robots,"Vision-based underwater robots can be useful in inspecting and exploring
confined spaces where traditional sensors and preplanned paths cannot be
followed. Sensor noise and situational change can cause significant uncertainty
in environmental representation. Thus, this paper explores how to represent
mapping inconsistency in vision-based sensing and incorporate depth estimation
confidence into the mapping framework. The scene depth and the confidence are
estimated using the RAFT-Stereo model and are integrated into a voxel-based
mapping framework, Voxblox. Improvements in the existing Voxblox weight
calculation and update mechanism are also proposed. Finally, a qualitative
analysis of the proposed method is performed in a confined pool and in a pier
in the Trondheim fjord. Experiments using an underwater robot demonstrated the
change in uncertainty in the visualization.","Abhimanyu Bhowmik, Mohit Singh, Madhushree Sannigrahi, Martin Ludvigsen, Kostas Alexis",2025-07-15T05:09:36Z,2025-07-15T05:09:36Z,http://arxiv.org/abs/2507.10991v1,http://arxiv.org/pdf/2507.10991v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"EquiContact: A Hierarchical SE(3) Vision-to-Force Equivariant Policy for
  Spatially Generalizable Contact-rich Tasks","This paper presents a framework for learning vision-based robotic policies
for contact-rich manipulation tasks that generalize spatially across task
configurations. We focus on achieving robust spatial generalization of the
policy for the peg-in-hole (PiH) task trained from a small number of
demonstrations. We propose EquiContact, a hierarchical policy composed of a
high-level vision planner (Diffusion Equivariant Descriptor Field, Diff-EDF)
and a novel low-level compliant visuomotor policy (Geometric Compliant ACT,
G-CompACT). G-CompACT operates using only localized observations (geometrically
consistent error vectors (GCEV), force-torque readings, and wrist-mounted RGB
images) and produces actions defined in the end-effector frame. Through these
design choices, we show that the entire EquiContact pipeline is
SE(3)-equivariant, from perception to force control. We also outline three key
components for spatially generalizable contact-rich policies: compliance,
localized policies, and induced equivariance. Real-world experiments on PiH
tasks demonstrate a near-perfect success rate and robust generalization to
unseen spatial configurations, validating the proposed framework and
principles. The experimental videos can be found on the project website:
https://sites.google.com/berkeley.edu/equicontact","Joohwan Seo, Arvind Kruthiventy, Soomi Lee, Megan Teng, Xiang Zhang, Seoyeon Choi, Jongeun Choi, Roberto Horowitz",2025-07-15T03:45:26Z,2025-07-15T03:45:26Z,http://arxiv.org/abs/2507.10961v1,http://arxiv.org/pdf/2507.10961v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"A Learning Framework For Cooperative Collision Avoidance of UAV Swarms
  Leveraging Domain Knowledge","This paper presents a multi-agent reinforcement learning (MARL) framework for
cooperative collision avoidance of UAV swarms leveraging domain
knowledge-driven reward. The reward is derived from knowledge in the domain of
image processing, approximating contours on a two-dimensional field. By
modeling obstacles as maxima on the field, collisions are inherently avoided as
contours never go through peaks or intersect. Additionally, counters are smooth
and energy-efficient. Our framework enables training with large swarm sizes as
the agent interaction is minimized and the need for complex credit assignment
schemes or observation sharing mechanisms in state-of-the-art MARL approaches
are eliminated. Moreover, UAVs obtain the ability to adapt to complex
environments where contours may be non-viable or non-existent through intensive
training. Extensive experiments are conducted to evaluate the performances of
our framework against state-of-the-art MARL algorithms.","Shuangyao Huang, Haibo Zhang, Zhiyi Huang",2025-07-15T02:09:53Z,2025-07-15T02:09:53Z,http://arxiv.org/abs/2507.10913v1,http://arxiv.org/pdf/2507.10913v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Object-Centric Mobile Manipulation through SAM2-Guided Perception and
  Imitation Learning","Imitation learning for mobile manipulation is a key challenge in the field of
robotic manipulation. However, current mobile manipulation frameworks typically
decouple navigation and manipulation, executing manipulation only after
reaching a certain location. This can lead to performance degradation when
navigation is imprecise, especially due to misalignment in approach angles. To
enable a mobile manipulator to perform the same task from diverse orientations,
an essential capability for building general-purpose robotic models, we propose
an object-centric method based on SAM2, a foundation model towards solving
promptable visual segmentation in images, which incorporates manipulation
orientation information into our model. Our approach enables consistent
understanding of the same task from different orientations. We deploy the model
on a custom-built mobile manipulator and evaluate it on a pick-and-place task
under varied orientation angles. Compared to Action Chunking Transformer, our
model maintains superior generalization when trained with demonstrations from
varied approach angles. This work significantly enhances the generalization and
robustness of imitation learning-based mobile manipulation systems.","Wang Zhicheng, Satoshi Yagi, Satoshi Yamamori, Jun Morimoto",2025-07-15T01:26:59Z,2025-07-15T01:26:59Z,http://arxiv.org/abs/2507.10899v1,http://arxiv.org/pdf/2507.10899v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
MP1: MeanFlow Tames Policy Learning in 1-step for Robotic Manipulation,"In robot manipulation, robot learning has become a prevailing approach.
However, generative models within this field face a fundamental trade-off
between the slow, iterative sampling of diffusion models and the architectural
constraints of faster Flow-based methods, which often rely on explicit
consistency losses. To address these limitations, we introduce MP1, which pairs
3D point-cloud inputs with the MeanFlow paradigm to generate action
trajectories in one network function evaluation (1-NFE). By directly learning
the interval-averaged velocity via the ""MeanFlow Identity"", our policy avoids
any additional consistency constraints. This formulation eliminates numerical
ODE-solver errors during inference, yielding more precise trajectories. MP1
further incorporates CFG for improved trajectory controllability while
retaining 1-NFE inference without reintroducing structural constraints. Because
subtle scene-context variations are critical for robot learning, especially in
few-shot learning, we introduce a lightweight Dispersive Loss that repels state
embeddings during training, boosting generalization without slowing inference.
We validate our method on the Adroit and Meta-World benchmarks, as well as in
real-world scenarios. Experimental results show MP1 achieves superior average
task success rates, outperforming DP3 by 10.2% and FlowPolicy by 7.3%. Its
average inference time is only 6.8 ms-19x faster than DP3 and nearly 2x faster
than FlowPolicy. Our code is available at https://github.com/LogSSim/MP1.git.","Juyi Sheng, Ziyi Wang, Peiming Li, Mengyuan Liu",2025-07-14T17:59:08Z,2025-07-29T02:07:42Z,http://arxiv.org/abs/2507.10543v3,http://arxiv.org/pdf/2507.10543v3.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"TOP: Trajectory Optimization via Parallel Optimization towards Constant
  Time Complexity","Optimization has been widely used to generate smooth trajectories for motion
planning. However, existing trajectory optimization methods show weakness when
dealing with large-scale long trajectories. Recent advances in parallel
computing have accelerated optimization in some fields, but how to efficiently
solve trajectory optimization via parallelism remains an open question. In this
paper, we propose a novel trajectory optimization framework based on the
Consensus Alternating Direction Method of Multipliers (CADMM) algorithm, which
decomposes the trajectory into multiple segments and solves the subproblems in
parallel. The proposed framework reduces the time complexity to O(1) per
iteration to the number of segments, compared to O(N) of the state-of-the-art
(SOTA) approaches. Furthermore, we introduce a closed-form solution that
integrates convex linear and quadratic constraints to speed up the
optimization, and we also present numerical solutions for general inequality
constraints. A series of simulations and experiments demonstrate that our
approach outperforms the SOTA approach in terms of efficiency and smoothness.
Especially for a large-scale trajectory, with one hundred segments, achieving
over a tenfold speedup. To fully explore the potential of our algorithm on
modern parallel computing architectures, we deploy our framework on a GPU and
show high performance with thousands of segments.","Jiajun Yu, Nanhe Chen, Guodong Liu, Chao Xu, Fei Gao, Yanjun Cao",2025-07-14T13:56:59Z,2025-07-16T20:42:16Z,http://arxiv.org/abs/2507.10290v2,http://arxiv.org/pdf/2507.10290v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"REACT: Real-time Entanglement-Aware Coverage Path Planning for Tethered
  Underwater Vehicles","Inspection of complex underwater structures with tethered underwater vehicles
is often hindered by the risk of tether entanglement. We propose REACT
(real-time entanglement-aware coverage path planning for tethered underwater
vehicles), a framework designed to overcome this limitation. REACT comprises a
fast geometry-based tether model using the signed distance field (SDF) map for
accurate, real-time simulation of taut tether configurations around arbitrary
structures in 3D. This model enables an efficient online replanning strategy by
enforcing a maximum tether length constraint, thereby actively preventing
entanglement. By integrating REACT into a coverage path planning framework, we
achieve safe and optimal inspection paths, previously challenging due to tether
constraints. The complete REACT framework's efficacy is validated in a pipe
inspection scenario, demonstrating safe, entanglement-free navigation and
full-coverage inspection. Simulation results show that REACT achieves complete
coverage while maintaining tether constraints and completing the total mission
20% faster than conventional planners, despite a longer inspection time due to
proactive avoidance of entanglement that eliminates extensive post-mission
disentanglement. Real-world experiments confirm these benefits, where REACT
completes the full mission, while the baseline planner fails due to physical
tether entanglement.","Abdelhakim Amer, Mohit Mehindratta, Yury Brodskiy, Bilal Wehbe, Erdal Kayacan",2025-07-14T12:18:01Z,2025-07-14T12:18:01Z,http://arxiv.org/abs/2507.10204v1,http://arxiv.org/pdf/2507.10204v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Ariel Explores: Vision-based underwater exploration and inspection via
  generalist drone-level autonomy","This work presents a vision-based underwater exploration and inspection
autonomy solution integrated into Ariel, a custom vision-driven underwater
robot. Ariel carries a $5$ camera and IMU based sensing suite, enabling a
refraction-aware multi-camera visual-inertial state estimation method aided by
a learning-based proprioceptive robot velocity prediction method that enhances
robustness against visual degradation. Furthermore, our previously developed
and extensively field-verified autonomous exploration and general visual
inspection solution is integrated on Ariel, providing aerial drone-level
autonomy underwater. The proposed system is field-tested in a submarine dry
dock in Trondheim under challenging visual conditions. The field demonstration
shows the robustness of the state estimation solution and the generalizability
of the path planning techniques across robot embodiments.","Mohit Singh, Mihir Dharmadhikari, Kostas Alexis",2025-07-14T07:36:25Z,2025-07-14T07:36:25Z,http://arxiv.org/abs/2507.10003v1,http://arxiv.org/pdf/2507.10003v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Demonstrating the Octopi-1.5 Visual-Tactile-Language Model,"Touch is recognized as a vital sense for humans and an equally important
modality for robots, especially for dexterous manipulation, material
identification, and scenarios involving visual occlusion. Building upon very
recent work in touch foundation models, this demonstration will feature
Octopi-1.5, our latest visual-tactile-language model. Compared to its
predecessor, Octopi-1.5 introduces the ability to process tactile signals from
multiple object parts and employs a simple retrieval-augmented generation (RAG)
module to improve performance on tasks and potentially learn new objects
on-the-fly. The system can be experienced live through a new handheld
tactile-enabled interface, the TMI, equipped with GelSight and TAC-02 tactile
sensors. This convenient and accessible setup allows users to interact with
Octopi-1.5 without requiring a robot. During the demonstration, we will
showcase Octopi-1.5 solving tactile inference tasks by leveraging tactile
inputs and commonsense knowledge. For example, in a Guessing Game, Octopi-1.5
will identify objects being grasped and respond to follow-up queries about how
to handle it (e.g., recommending careful handling for soft fruits). We also
plan to demonstrate Octopi-1.5's RAG capabilities by teaching it new items.
With live interactions, this demonstration aims to highlight both the progress
and limitations of VTLMs such as Octopi-1.5 and to foster further interest in
this exciting field. Code for Octopi-1.5 and design files for the TMI gripper
are available at https://github.com/clear-nus/octopi-1.5.","Samson Yu, Kelvin Lin, Harold Soh",2025-07-14T07:05:36Z,2025-07-14T07:05:36Z,http://arxiv.org/abs/2507.09985v1,http://arxiv.org/pdf/2507.09985v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Customize Harmonic Potential Fields via Hybrid Optimization over
  Homotopic Paths","Safe navigation within a workspace is a fundamental skill for autonomous
robots to accomplish more complex tasks. Harmonic potentials are artificial
potential fields that are analytical, globally convergent and provably free of
local minima. Thus, it has been widely used for generating safe and reliable
robot navigation control policies. However, most existing methods do not allow
customization of the harmonic potential fields nor the resulting paths,
particularly regarding their topological properties. In this paper, we propose
a novel method that automatically finds homotopy classes of paths that can be
generated by valid harmonic potential fields. The considered complex workspaces
can be as general as forest worlds consisting of numerous overlapping
star-obstacles. The method is based on a hybrid optimization algorithm that
searches over homotopy classes, selects the structure of each tree-of-stars
within the forest, and optimizes over the continuous weight parameters for each
purged tree via the projected gradient descent. The key insight is to transform
the forest world to the unbounded point world via proper diffeomorphic
transformations. It not only facilitates a simpler design of the
multi-directional D-signature between non-homotopic paths, but also retain the
safety and convergence properties. Extensive simulations and hardware
experiments are conducted for non-trivial scenarios, where the navigation
potentials are customized for desired homotopic properties. Project page:
https://shuaikang-wang.github.io/CustFields.","Shuaikang Wang, Tiecheng Guo, Meng Guo",2025-07-14T01:55:25Z,2025-07-14T01:55:25Z,http://arxiv.org/abs/2507.09858v1,http://arxiv.org/pdf/2507.09858v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Real-Time Adaptive Motion Planning via Point Cloud-Guided, Energy-Based
  Diffusion and Potential Fields","Motivated by the problem of pursuit-evasion, we present a motion planning
framework that combines energy-based diffusion models with artificial potential
fields for robust real time trajectory generation in complex environments. Our
approach processes obstacle information directly from point clouds, enabling
efficient planning without requiring complete geometric representations. The
framework employs classifier-free guidance training and integrates local
potential fields during sampling to enhance obstacle avoidance. In dynamic
scenarios, the system generates initial trajectories using the diffusion model
and continuously refines them through potential field-based adaptation,
demonstrating effective performance in pursuit-evasion scenarios with partial
pursuer observability.","Wondmgezahu Teshome, Kian Behzad, Octavia Camps, Michael Everett, Milad Siami, Mario Sznaier",2025-07-12T19:42:07Z,2025-07-12T19:42:07Z,http://arxiv.org/abs/2507.09383v1,http://arxiv.org/pdf/2507.09383v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Unified Linear Parametric Map Modeling and Perception-aware Trajectory
  Planning for Mobile Robotics","Autonomous navigation in mobile robots, reliant on perception and planning,
faces major hurdles in large-scale, complex environments. These include heavy
computational burdens for mapping, sensor occlusion failures for UAVs, and
traversal challenges on irregular terrain for UGVs, all compounded by a lack of
perception-aware strategies. To address these challenges, we introduce Random
Mapping and Random Projection (RMRP). This method constructs a lightweight
linear parametric map by first mapping data to a high-dimensional space,
followed by a sparse random projection for dimensionality reduction. Our novel
Residual Energy Preservation Theorem provides theoretical guarantees for this
process, ensuring critical geometric properties are preserved. Based on this
map, we propose the RPATR (Robust Perception-Aware Trajectory Planner)
framework. For UAVs, our method unifies grid and Euclidean Signed Distance
Field (ESDF) maps. The front-end uses an analytical occupancy gradient to
refine initial paths for safety and smoothness, while the back-end uses a
closed-form ESDF for trajectory optimization. Leveraging the trained RMRP
model's generalization, the planner predicts unobserved areas for proactive
navigation. For UGVs, the model characterizes terrain and provides closed-form
gradients, enabling online planning to circumvent large holes. Validated in
diverse scenarios, our framework demonstrates superior mapping performance in
time, memory, and accuracy, and enables computationally efficient, safe
navigation for high-speed UAVs and UGVs. The code will be released to foster
community collaboration.","Hongyu Nie, Xu Liu, Zhaotong Tan, Sen Mei, Wenbo Su",2025-07-12T16:39:19Z,2025-08-07T08:10:42Z,http://arxiv.org/abs/2507.09340v2,http://arxiv.org/pdf/2507.09340v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"DLBAcalib: Robust Extrinsic Calibration for Non-Overlapping LiDARs Based
  on Dual LBA","Accurate extrinsic calibration of multiple LiDARs is crucial for improving
the foundational performance of three-dimensional (3D) map reconstruction
systems. This paper presents a novel targetless extrinsic calibration framework
for multi-LiDAR systems that does not rely on overlapping fields of view or
precise initial parameter estimates. Unlike conventional calibration methods
that require manual annotations or specific reference patterns, our approach
introduces a unified optimization framework by integrating LiDAR bundle
adjustment (LBA) optimization with robust iterative refinement. The proposed
method constructs an accurate reference point cloud map via continuous scanning
from the target LiDAR and sliding-window LiDAR bundle adjustment, while
formulating extrinsic calibration as a joint LBA optimization problem. This
method effectively mitigates cumulative mapping errors and achieves
outlier-resistant parameter estimation through an adaptive weighting mechanism.
Extensive evaluations in both the CARLA simulation environment and real-world
scenarios demonstrate that our method outperforms state-of-the-art calibration
techniques in both accuracy and robustness. Experimental results show that for
non-overlapping sensor configurations, our framework achieves an average
translational error of 5 mm and a rotational error of 0.2{\deg}, with an
initial error tolerance of up to 0.4 m/30{\deg}. Moreover, the calibration
process operates without specialized infrastructure or manual parameter tuning.
The code is open source and available on GitHub
(\underline{https://github.com/Silentbarber/DLBAcalib})","Han Ye, Yuqiang Jin, Jinyuan Liu, Tao Li, Wen-An Zhang, Minglei Fu",2025-07-12T07:48:02Z,2025-07-12T07:48:02Z,http://arxiv.org/abs/2507.09176v1,http://arxiv.org/pdf/2507.09176v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
RoHOI: Robustness Benchmark for Human-Object Interaction Detection,"Human-Object Interaction (HOI) detection is crucial for robot-human
assistance, enabling context-aware support. However, models trained on clean
datasets degrade in real-world conditions due to unforeseen corruptions,
leading to inaccurate prediction. To address this, we introduce the first
robustness benchmark for HOI detection, evaluating model resilience under
diverse challenges. Despite advances, current models struggle with
environmental variability, occlusions, and noise. Our benchmark, RoHOI,
includes 20 corruption types based on the HICO-DET and V-COCO datasets and a
new robustness-focused metric. We systematically analyze existing models in the
HOI field, revealing significant performance drops under corruptions. To
improve robustness, we propose a Semantic-Aware Masking-based Progressive
Learning (SAMPL) strategy to guide the model to be optimized based on holistic
and partial cues, thus dynamically adjusting the model's optimization to
enhance robust feature learning. Extensive experiments show that our approach
outperforms state-of-the-art methods, setting a new standard for robust HOI
detection. Benchmarks, datasets, and code will be made publicly available at
https://github.com/Kratos-Wen/RoHOI.","Di Wen, Kunyu Peng, Kailun Yang, Yufan Chen, Ruiping Liu, Junwei Zheng, Alina Roitberg, Danda Pani Paudel, Luc Van Gool, Rainer Stiefelhagen",2025-07-12T01:58:04Z,2025-08-13T16:49:26Z,http://arxiv.org/abs/2507.09111v2,http://arxiv.org/pdf/2507.09111v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"SPLASH! Sample-efficient Preference-based inverse reinforcement learning
  for Long-horizon Adversarial tasks from Suboptimal Hierarchical
  demonstrations","Inverse Reinforcement Learning (IRL) presents a powerful paradigm for
learning complex robotic tasks from human demonstrations. However, most
approaches make the assumption that expert demonstrations are available, which
is often not the case. Those that allow for suboptimality in the demonstrations
are not designed for long-horizon goals or adversarial tasks. Many desirable
robot capabilities fall into one or both of these categories, thus highlighting
a critical shortcoming in the ability of IRL to produce field-ready robotic
agents. We introduce Sample-efficient Preference-based inverse reinforcement
learning for Long-horizon Adversarial tasks from Suboptimal Hierarchical
demonstrations (SPLASH), which advances the state-of-the-art in learning from
suboptimal demonstrations to long-horizon and adversarial settings. We
empirically validate SPLASH on a maritime capture-the-flag task in simulation,
and demonstrate real-world applicability with sim-to-real translation
experiments on autonomous unmanned surface vehicles. We show that our proposed
methods allow SPLASH to significantly outperform the state-of-the-art in reward
learning from suboptimal demonstrations.","Peter Crowley, Zachary Serlin, Tyler Paine, Makai Mann, Michael Benjamin, Calin Belta",2025-07-11T16:05:18Z,2025-07-11T16:05:18Z,http://arxiv.org/abs/2507.08707v1,http://arxiv.org/pdf/2507.08707v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Joint Optimization-based Targetless Extrinsic Calibration for Multiple
  LiDARs and GNSS-Aided INS of Ground Vehicles","Accurate extrinsic calibration between multiple LiDAR sensors and a
GNSS-aided inertial navigation system (GINS) is essential for achieving
reliable sensor fusion in intelligent mining environments. Such calibration
enables vehicle-road collaboration by aligning perception data from
vehicle-mounted sensors to a unified global reference frame. However, existing
methods often depend on artificial targets, overlapping fields of view, or
precise trajectory estimation, which are assumptions that may not hold in
practice. Moreover, the planar motion of mining vehicles leads to observability
issues that degrade calibration performance. This paper presents a targetless
extrinsic calibration method that aligns multiple onboard LiDAR sensors to the
GINS coordinate system without requiring overlapping sensor views or external
targets. The proposed approach introduces an observation model based on the
known installation height of the GINS unit to constrain unobservable
calibration parameters under planar motion. A joint optimization framework is
developed to refine both the extrinsic parameters and GINS trajectory by
integrating multiple constraints derived from geometric correspondences and
motion consistency. The proposed method is applicable to heterogeneous LiDAR
configurations, including both mechanical and solid-state sensors. Extensive
experiments on simulated and real-world datasets demonstrate the accuracy,
robustness, and practical applicability of the approach under diverse sensor
setups.","Junhui Wang, Yan Qiao, Chao Gao, Naiqi Wu",2025-07-11T06:48:53Z,2025-07-11T06:48:53Z,http://arxiv.org/abs/2507.08349v1,http://arxiv.org/pdf/2507.08349v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Improving AEBS Validation Through Objective Intervention Classification
  Leveraging the Prediction Divergence Principle","The safety validation of automatic emergency braking system (AEBS) requires
accurately distinguishing between false positive (FP) and true positive (TP)
system activations. While simulations allow straightforward differentiation by
comparing scenarios with and without interventions, analyzing activations from
open-loop resimulations - such as those from field operational testing (FOT) -
is more complex. This complexity arises from scenario parameter uncertainty and
the influence of driver interventions in the recorded data. Human labeling is
frequently used to address these challenges, relying on subjective assessments
of intervention necessity or situational criticality, potentially introducing
biases and limitations. This work proposes a rule-based classification approach
leveraging the Prediction Divergence Principle (PDP) to address those issues.
Applied to a simplified AEBS, the proposed method reveals key strengths,
limitations, and system requirements for effective implementation. The findings
suggest that combining this approach with human labeling may enhance the
transparency and consistency of classification, thereby improving the overall
validation process. While the rule set for classification derived in this work
adopts a conservative approach, the paper outlines future directions for
refinement and broader applicability. Finally, this work highlights the
potential of such methods to complement existing practices, paving the way for
more reliable and reproducible AEBS validation frameworks.","Daniel Betschinske, Steven Peters",2025-07-10T15:55:05Z,2025-07-21T12:23:53Z,http://arxiv.org/abs/2507.07872v2,http://arxiv.org/pdf/2507.07872v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Perceptual Distortions and Autonomous Representation Learning in a
  Minimal Robotic System","Autonomous agents, particularly in the field of robotics, rely on sensory
information to perceive and navigate their environment. However, these sensory
inputs are often imperfect, leading to distortions in the agent's internal
representation of the world. This paper investigates the nature of these
perceptual distortions and how they influence autonomous representation
learning using a minimal robotic system. We utilize a simulated two-wheeled
robot equipped with distance sensors and a compass, operating within a simple
square environment. Through analysis of the robot's sensor data during random
exploration, we demonstrate how a distorted perceptual space emerges. Despite
these distortions, we identify emergent structures within the perceptual space
that correlate with the physical environment, revealing how the robot
autonomously learns a structured representation for navigation without explicit
spatial information. This work contributes to the understanding of embodied
cognition, minimal agency, and the role of perception in self-generated
navigation strategies in artificial life.","David Warutumo, Ciira wa Maina",2025-07-10T15:22:32Z,2025-07-10T15:22:32Z,http://arxiv.org/abs/2507.07845v1,http://arxiv.org/pdf/2507.07845v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
FiDTouch: A 3D Wearable Haptic Display for the Finger Pad,"The applications of fingertip haptic devices have spread to various fields
from revolutionizing virtual reality and medical training simulations to
facilitating remote robotic operations, proposing great potential for enhancing
user experiences, improving training outcomes, and new forms of interaction. In
this work, we present FiDTouch, a 3D wearable haptic device that delivers
cutaneous stimuli to the finger pad, such as contact, pressure, encounter, skin
stretch, and vibrotactile feedback. The application of a tiny inverted Delta
robot in the mechanism design allows providing accurate contact and fast
changing dynamic stimuli to the finger pad surface. The performance of the
developed display was evaluated in a two-stage user study of the perception of
static spatial contact stimuli and skin stretch stimuli generated on the finger
pad. The proposed display, by providing users with precise touch and force
stimuli, can enhance user immersion and efficiency in the fields of
human-computer and human-robot interactions.","Daria Trinitatova, Dzmitry Tsetserukou",2025-07-10T11:36:27Z,2025-07-10T11:36:27Z,http://arxiv.org/abs/2507.07661v1,http://arxiv.org/pdf/2507.07661v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data,"Generating diverse and natural human motion sequences based on textual
descriptions constitutes a fundamental and challenging research area within the
domains of computer vision, graphics, and robotics. Despite significant
advancements in this field, current methodologies often face challenges
regarding zero-shot generalization capabilities, largely attributable to the
limited size of training datasets. Moreover, the lack of a comprehensive
evaluation framework impedes the advancement of this task by failing to
identify directions for improvement. In this work, we aim to push
text-to-motion into a new era, that is, to achieve the generalization ability
of zero-shot. To this end, firstly, we develop an efficient annotation pipeline
and introduce MotionMillion-the largest human motion dataset to date, featuring
over 2,000 hours and 2 million high-quality motion sequences. Additionally, we
propose MotionMillion-Eval, the most comprehensive benchmark for evaluating
zero-shot motion generation. Leveraging a scalable architecture, we scale our
model to 7B parameters and validate its performance on MotionMillion-Eval. Our
results demonstrate strong generalization to out-of-domain and complex
compositional motions, marking a significant step toward zero-shot human motion
generation. The code is available at
https://github.com/VankouF/MotionMillion-Codes.","Ke Fan, Shunlin Lu, Minyue Dai, Runyi Yu, Lixing Xiao, Zhiyang Dou, Junting Dong, Lizhuang Ma, Jingbo Wang",2025-07-09T17:52:04Z,2025-07-09T17:52:04Z,http://arxiv.org/abs/2507.07095v1,http://arxiv.org/pdf/2507.07095v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Sloan Digital Sky Survey-V: Pioneering Panoptic Spectroscopy,"The Sloan Digital Sky Survey-V (SDSS-V) is pioneering panoptic spectroscopy:
it is the first all-sky, multi-epoch, optical-to-infrared spectroscopic survey.
SDSS-V is mapping the sky with multi-object spectroscopy (MOS) at telescopes in
both hemispheres (the 2.5-m Sloan Foundation Telescope at Apache Point
Observatory and the 100-inch du Pont Telescope at Las Campanas Observatory),
where 500 zonal robotic fiber positioners feed light from a wide-field focal
plane to an optical (R$\sim 2000$, 500 fibers) and a near-infrared (R$\sim
22,000$, 300 fibers) spectrograph. In addition to these MOS capabilities, the
survey is pioneering ultra wide-field ($\sim$ 4000~deg$^2$) integral field
spectroscopy enabled by a new dedicated facility (LVM-I) at Las Campanas
Observatory, where an integral field spectrograph (IFS) with 1801
lenslet-coupled fibers arranged in a 0.5 degree diameter hexagon feeds multiple
R$\sim$4000 optical spectrographs that cover 3600-9800 angstroms. SDSS-V's
hardware and multi-year survey strategy are designed to decode the
chemo-dynamical history of the Milky Way Galaxy and tackle fundamental open
issues in stellar physics in its Milky Way Mapper program, trace the growth
physics of supermassive black holes in its Black Hole Mapper program, and
understand the self-regulation mechanisms and the chemical enrichment of
galactic ecosystems at the energy-injection scale in its Local Volume Mapper
program. The survey is well-timed to multiply the scientific output from major
all-sky space missions. The SDSS-V MOS programs began robotic operations in
2021; IFS observations began in 2023 with the completion of the LVM-I facility.
SDSS-V builds upon decades of heritage of SDSS's pioneering advances in data
analysis, collaboration spirit, infrastructure, and product deliverables in
astronomy.","Juna A. Kollmeier, Hans-Walter Rix, Conny Aerts, James Aird, Pablo Vera Alfaro, Andrés Almeida, Scott F. Anderson, Óscar Jiménez Arranz, Stefan M. Arseneau, Roberto Assef, Shir Aviram, Catarina Aydar, Carles Badenes, Avrajit Bandyopadhyay, Kat Barger, Robert H. Barkhouser, Franz E. Bauer, Chad Bender, Felipe Besser, Binod Bhattarai, Pavaman Bilgi, Jonathan Bird, Dmitry Bizyaev, Guillermo A. Blanc, Michael R. Blanton, John Bochanski, Jo Bovy, Christopher Brandon, William Nielsen Brandt, Joel R. Brownstein, Johannes Buchner, Joseph N. Burchett, Joleen Carlberg, Andrew R. Casey, Lesly Castaneda-Carlos, Priyanka Chakraborty, Julio Chanamé, Vedant Chandra, Brian Cherinka, Igor Chilingarian, Johan Comparat, Maren Cosens, Kevin Covey, Jeffrey D. Crane, Nicole R. Crumpler, Katia Cunha, Tim Cunningham, Xinyu Dai, Jeremy Darling, James W. Davidson Jr., Megan C. Davis, Nathan De Lee, Niall Deacon, José Eduardo Méndez Delgado, Sebastian Demasi, Mariia Demianenko, Mark Derwent, Elena D'Onghia, Francesco Di Mille, Bruno Dias, John Donor, Niv Drory, Tom Dwelly, Oleg Egorov, Evgeniya Egorova, Kareem El-Badry, Mike Engelman, Mike Eracleous, Xiaohui Fan, Emily Farr, Logan Fries, Peter Frinchaboy, Cynthia S. Froning, Boris T. Gänsicke, Pablo García, Joseph Gelfand, Nicola Pietro Gentile Fusillo, Simon Glover, Katie Grabowski, Eva K. Grebel, Paul J Green, Catherine Grier, Pramod Gupta, Aidan C. Gray, Maximilian Häberle, Patrick B. Hall, Randolph P. Hammond, Keith Hawkins, Albert C. Harding, Viola Hegedűs, Tom Herbst, J. J. Hermes, Paola Rodríguez Hidalgo, Thomas Hilder, David W Hogg, Jon A. Holtzman, Danny Horta, Yang Huang, Hsiang-Chih Hwang, Hector Javier Ibarra-Medel, Julie Imig, Keith Inight, Arghajit Jana, Alexander P. Ji, Paula Jofre, Matt Johns, Jennifer Johnson, James W. Johnson, Evelyn J. Johnston, Amy M Jones, Ivan Katkov, Anton M. Koekemoer, Marina Kounkel, Kathryn Kreckel, Dhanesh Krishnarao, Mirko Krumpe, Nimisha Kumari, Thomas Kupfer, Ivan Lacerna, Chervin Laporte, Sebastien Lepine, Jing Li, Xin Liu, Sarah Loebman, Knox Long, Alexandre Roman-Lopes, Yuxi Lu, Steven Raymond Majewski, Dan Maoz, Kevin A. McKinnon, Ilija Medan, Andrea Merloni, Dante Minniti, Sean Morrison, Natalie Myers, Szabolcs Mészáros, Kirpal Nandra, Prasanta K. Nayak, Melissa K Ness, David L. Nidever, Thomas O'Brien, Micah Oeur, Audrey Oravetz, Daniel Oravetz, Jonah Otto, Gautham Adamane Pallathadka, Povilas Palunas, Kaike Pan, Daniel Pappalardo, Rakesh Pandey, Castalia Alenka Negrete Peñaloza, Marc H. Pinsonneault, Richard W. Pogge, Manuchehr Taghizadeh Popp, Adrian M. Price-Whelan, Nadiia Pulatova, Dan Qiu, Solange Ramirez, Amy Rankine, Claudio Ricci, Jessie C. Runnoe, Sebastian Sanchez, Mara Salvato, Natascha Sattler, Andrew K. Saydjari, Conor Sayres, Kevin C. Schlaufman, Donald P. Schneider, Matthias R. Schreiber, Axel Schwope, Javier Serna, Yue Shen, Cristóbal Sifón, Amrita Singh, Amaya Sinha, Stephen Smee, Ying-Yi Song, Diogo Souto, Keivan G. Stassun, Matthias Steinmetz, Alexander Stone-Martinez, Guy Stringfellow, Amelia Stutz,  José,  Sá,  nchez-Gallego, Jonathan C. Tan, Jamie Tayar, Riley Thai, Ani Thakar, Yuan-Sen Ting, Andrew Tkachenko, Gagik Tovmasian, Benny Trakhtenbrot, José G. Fernández-Trincado, Nicholas Troup, Jonathan Trump, Sarah Tuttle, Roeland P. van der Marel, Sandro Villanova, Jaime Villaseñor, Stefanie Wachter, Zachary Way, Anne-Marie Weijmans, David Weinberg, Adam Wheeler, John Wilson, Alessa I. Wiggins, Tony Wong, Qiaoya Wu, Dominika Wylezalek, Xiang-Xiang Xue, Qian Yang, Nadia Zakamska, Eleonora Zari, Gail Zasowski, Grisha Zeltyn, Catherine Zucker, Carlos G. Román Zúñiga, Rodolfo de J. Zermeño",2025-07-09T16:13:26Z,2025-07-09T16:13:26Z,http://arxiv.org/abs/2507.06989v1,http://arxiv.org/pdf/2507.06989v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Aerial Maritime Vessel Detection and Identification,"Autonomous maritime surveillance and target vessel identification in
environments where Global Navigation Satellite Systems (GNSS) are not available
is critical for a number of applications such as search and rescue and threat
detection. When the target vessel is only described by visual cues and its last
known position is not available, unmanned aerial vehicles (UAVs) must rely
solely on on-board vision to scan a large search area under strict
computational constraints. To address this challenge, we leverage the YOLOv8
object detection model to detect all vessels in the field of view. We then
apply feature matching and hue histogram distance analysis to determine whether
any detected vessel corresponds to the target. When found, we localize the
target using simple geometric principles. We demonstrate the proposed method in
real-world experiments during the MBZIRC2023 competition, integrated into a
fully autonomous system with GNSS-denied navigation. We also evaluate the
impact of perspective on detection accuracy and localization precision and
compare it with the oracle approach.","Antonella Barisic Kulas, Frano Petric, Stjepan Bogdan",2025-07-09T11:43:02Z,2025-07-09T11:43:02Z,http://arxiv.org/abs/2507.07153v1,http://arxiv.org/pdf/2507.07153v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"A Neural Representation Framework with LLM-Driven Spatial Reasoning for
  Open-Vocabulary 3D Visual Grounding","Open-vocabulary 3D visual grounding aims to localize target objects based on
free-form language queries, which is crucial for embodied AI applications such
as autonomous navigation, robotics, and augmented reality. Learning 3D language
fields through neural representations enables accurate understanding of 3D
scenes from limited viewpoints and facilitates the localization of target
objects in complex environments. However, existing language field methods
struggle to accurately localize instances using spatial relations in language
queries, such as ``the book on the chair.'' This limitation mainly arises from
inadequate reasoning about spatial relations in both language queries and 3D
scenes. In this work, we propose SpatialReasoner, a novel neural
representation-based framework with large language model (LLM)-driven spatial
reasoning that constructs a visual properties-enhanced hierarchical feature
field for open-vocabulary 3D visual grounding. To enable spatial reasoning in
language queries, SpatialReasoner fine-tunes an LLM to capture spatial
relations and explicitly infer instructions for the target, anchor, and spatial
relation. To enable spatial reasoning in 3D scenes, SpatialReasoner
incorporates visual properties (opacity and color) to construct a hierarchical
feature field. This field represents language and instance features using
distilled CLIP features and masks extracted via the Segment Anything Model
(SAM). The field is then queried using the inferred instructions in a
hierarchical manner to localize the target 3D instance based on the spatial
relation in the language query. Extensive experiments show that our framework
can be seamlessly integrated into different neural representations,
outperforming baseline models in 3D visual grounding while empowering their
spatial reasoning capability.","Zhenyang Liu, Sixiao Zheng, Siyu Chen, Cairong Zhao, Longfei Liang, Xiangyang Xue, Yanwei Fu",2025-07-09T10:20:38Z,2025-07-09T10:20:38Z,http://arxiv.org/abs/2507.06719v1,http://arxiv.org/pdf/2507.06719v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"VisioPath: Vision-Language Enhanced Model Predictive Control for Safe
  Autonomous Navigation in Mixed Traffic","In this paper, we introduce VisioPath, a novel framework combining
vision-language models (VLMs) with model predictive control (MPC) to enable
safe autonomous driving in dynamic traffic environments. The proposed approach
leverages a bird's-eye view video processing pipeline and zero-shot VLM
capabilities to obtain structured information about surrounding vehicles,
including their positions, dimensions, and velocities. Using this rich
perception output, we construct elliptical collision-avoidance potential fields
around other traffic participants, which are seamlessly integrated into a
finite-horizon optimal control problem for trajectory planning. The resulting
trajectory optimization is solved via differential dynamic programming with an
adaptive regularization scheme and is embedded in an event-triggered MPC loop.
To ensure collision-free motion, a safety verification layer is incorporated in
the framework that provides an assessment of potential unsafe trajectories.
Extensive simulations in Simulation of Urban Mobility (SUMO) demonstrate that
VisioPath outperforms conventional MPC baselines across multiple metrics. By
combining modern AI-driven perception with the rigorous foundation of optimal
control, VisioPath represents a significant step forward in safe trajectory
planning for complex traffic systems.","Shanting Wang, Panagiotis Typaldos, Chenjun Li, Andreas A. Malikopoulos",2025-07-08T22:47:41Z,2025-07-08T22:47:41Z,http://arxiv.org/abs/2507.06441v1,http://arxiv.org/pdf/2507.06441v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Mapping the Catacombs: An Underwater Cave Segment of the Devil's Eye
  System","This paper presents a framework for mapping underwater caves. Underwater
caves are crucial for fresh water resource management, underwater archaeology,
and hydrogeology. Mapping the cave's outline and dimensions, as well as
creating photorealistic 3D maps, is critical for enabling a better
understanding of this underwater domain. In this paper, we present the mapping
of an underwater cave segment (the catacombs) of the Devil's Eye cave system at
Ginnie Springs, FL. We utilized a set of inexpensive action cameras in
conjunction with a dive computer to estimate the trajectories of the cameras
together with a sparse point cloud. The resulting reconstructions are utilized
to produce a one-dimensional retract of the cave passages in the form of the
average trajectory together with the boundaries (top, bottom, left, and right).
The use of the dive computer enables the observability of the z-dimension in
addition to the roll and pitch in a visual/inertial framework (SVIn2). In
addition, the keyframes generated by SVIn2 together with the estimated camera
poses for select areas are used as input to a global optimization (bundle
adjustment) framework -- COLMAP -- in order to produce a dense reconstruction
of those areas. The same cave segment is manually surveyed using the MNemo V2
instrument, providing an additional set of measurements validating the proposed
approach. It is worth noting that with the use of action cameras, the primary
components of a cave map can be constructed. Furthermore, with the utilization
of a global optimization framework guided by the results of VI-SLAM package
SVIn2, photorealistic dense 3D representations of selected areas can be
reconstructed.","Michalis Chatzispyrou, Luke Horgan, Hyunkil Hwang, Harish Sathishchandra, Monika Roznere, Alberto Quattrini Li, Philippos Mordohai, Ioannis Rekleitis",2025-07-08T21:03:35Z,2025-07-08T21:03:35Z,http://arxiv.org/abs/2507.06397v1,http://arxiv.org/pdf/2507.06397v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"BayesSDF: Surface-Based Laplacian Uncertainty Estimation for 3D Geometry
  with Neural Signed Distance Fields","Quantifying uncertainty in neural implicit 3D representations, particularly
those utilizing Signed Distance Functions (SDFs), remains a substantial
challenge due to computational inefficiencies, scalability issues, and
geometric inconsistencies. Existing methods typically neglect direct geometric
integration, leading to poorly calibrated uncertainty maps. We introduce
BayesSDF, a novel probabilistic framework for uncertainty quantification in
neural implicit SDF models, motivated by scientific simulation applications
with 3D environments (e.g., forests) such as modeling fluid flow through
forests, where precise surface geometry and reliable uncertainty estimates are
essential. Unlike radiance-based models such as Neural Radiance Fields (NeRF)
or 3D Gaussian splatting, which lack explicit surface formulations, Signed
Distance Functions (SDFs) define continuous and differentiable geometry, making
them better suited for physical modeling and analysis. BayesSDF leverages a
Laplace approximation to quantify local surface instability using Hessian-based
metrics, enabling efficient, surfaceaware uncertainty estimation. Our method
shows that uncertainty predictions correspond closely with poorly reconstructed
geometry, providing actionable confidence measures for downstream use.
Extensive evaluations on synthetic and real-world datasets demonstrate that
BayesSDF outperforms existing methods in both calibration and geometric
consistency, establishing a strong foundation for uncertainty-aware 3D scene
reconstruction, simulation, and robotic decision-making.",Rushil Desai,2025-07-08T03:21:12Z,2025-07-14T15:52:55Z,http://arxiv.org/abs/2507.06269v2,http://arxiv.org/pdf/2507.06269v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Gaussian Process-Based Active Exploration Strategies in Vision and Touch,"Robots struggle to understand object properties like shape, material, and
semantics due to limited prior knowledge, hindering manipulation in
unstructured environments. In contrast, humans learn these properties through
interactive multi-sensor exploration. This work proposes fusing visual and
tactile observations into a unified Gaussian Process Distance Field (GPDF)
representation for active perception of object properties. While primarily
focusing on geometry, this approach also demonstrates potential for modeling
surface properties beyond geometry. The GPDF encodes signed distance using
point cloud, analytic gradient and Hessian, and surface uncertainty estimates,
which are attributes that common neural network shape representation lack. By
utilizing a point cloud to construct a distance function, GPDF does not need
extensive pretraining on large datasets and can incorporate observations by
aggregation. Starting with an initial visual shape estimate, the framework
iteratively refines the geometry by integrating dense vision measurements using
differentiable rendering and tactile measurements at uncertain surface regions.
By quantifying multi-sensor uncertainties, it plans exploratory motions to
maximize information gain for recovering precise 3D structures. For the
real-world robot experiment, we utilize the Franka Research 3 robot
manipulator, which is fixed on a table and has a customized DIGIT tactile
sensor and an Intel Realsense D435 RGBD camera mounted on the end-effector. In
these experiments, the robot explores the shape and properties of objects
assumed to be static and placed on the table. To improve scalability, we
investigate approximation methods like inducing point method for Gaussian
Processes. This probabilistic multi-modal fusion enables active exploration and
mapping of complex object geometries, extending potentially beyond geometry.","Ho Jin Choi, Nadia Figueroa",2025-07-07T22:37:59Z,2025-07-07T22:37:59Z,http://arxiv.org/abs/2507.05522v1,http://arxiv.org/pdf/2507.05522v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Robotic System with AI for Real Time Weed Detection, Canopy Aware
  Spraying, and Droplet Pattern Evaluation","Uniform and excessive herbicide application in modern agriculture contributes
to increased input costs, environmental pollution, and the emergence of
herbicide resistant weeds. To address these challenges, we developed a vision
guided, AI-driven variable rate sprayer system capable of detecting weed
presence, estimating canopy size, and dynamically adjusting nozzle activation
in real time. The system integrates lightweight YOLO11n and YOLO11n-seg deep
learning models, deployed on an NVIDIA Jetson Orin Nano for onboard inference,
and uses an Arduino Uno-based relay interface to control solenoid actuated
nozzles based on canopy segmentation results. Indoor trials were conducted
using 15 potted Hibiscus rosa sinensis plants of varying canopy sizes to
simulate a range of weed patch scenarios. The YOLO11n model achieved a mean
average precision (mAP@50) of 0.98, with a precision of 0.99 and a recall close
to 1.0. The YOLO11n-seg segmentation model achieved a mAP@50 of 0.48, precision
of 0.55, and recall of 0.52. System performance was validated using water
sensitive paper, which showed an average spray coverage of 24.22% in zones
where canopy was present. An upward trend in mean spray coverage from 16.22%
for small canopies to 21.46% and 21.65% for medium and large canopies,
respectively, demonstrated the system's capability to adjust spray output based
on canopy size in real time. These results highlight the potential of combining
real time deep learning with low-cost embedded hardware for selective herbicide
application. Future work will focus on expanding the detection capabilities to
include three common weed species in South Dakota: water hemp (Amaranthus
tuberculatus), kochia (Bassia scoparia), and foxtail (Setaria spp.), followed
by further validation in both indoor and field trials within soybean and corn
production systems.","Inayat Rasool, Pappu Kumar Yadav, Amee Parmar, Hasan Mirzakhaninafchi, Rikesh Budhathoki, Zain Ul Abideen Usmani, Supriya Paudel, Ivan Perez Olivera, Eric Jone",2025-07-07T19:27:29Z,2025-07-07T19:27:29Z,http://arxiv.org/abs/2507.05432v1,http://arxiv.org/pdf/2507.05432v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Motion Generation: A Survey of Generative Approaches and Benchmarks,"Motion generation, the task of synthesizing realistic motion sequences from
various conditioning inputs, has become a central problem in computer vision,
computer graphics, and robotics, with applications ranging from animation and
virtual agents to human-robot interaction. As the field has rapidly progressed
with the introduction of diverse modeling paradigms including GANs,
autoencoders, autoregressive models, and diffusion-based techniques, each
approach brings its own advantages and limitations. This growing diversity has
created a need for a comprehensive and structured review that specifically
examines recent developments from the perspective of the generative approach
employed.
  In this survey, we provide an in-depth categorization of motion generation
methods based on their underlying generative strategies. Our main focus is on
papers published in top-tier venues since 2023, reflecting the most recent
advancements in the field. In addition, we analyze architectural principles,
conditioning mechanisms, and generation settings, and compile a detailed
overview of the evaluation metrics and datasets used across the literature. Our
objective is to enable clearer comparisons and identify open challenges,
thereby offering a timely and foundational reference for researchers and
practitioners navigating the rapidly evolving landscape of motion generation.","Aliasghar Khani, Arianna Rampini, Bruno Roy, Larasika Nadela, Noa Kaplan, Evan Atherton, Derek Cheung, Jacky Bibliowicz",2025-07-07T19:04:56Z,2025-07-07T19:04:56Z,http://arxiv.org/abs/2507.05419v1,http://arxiv.org/pdf/2507.05419v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Feature Geometry for Stereo Sidescan and Forward-looking Sonar,"In this paper, we address stereo acoustic data fusion for marine robotics and
propose a geometry-based method for projecting observed features from one sonar
to another for a cross-modal stereo sonar setup that consists of both a
forward-looking and a sidescan sonar. Our acoustic geometry for sidescan and
forward-looking sonar is inspired by the epipolar geometry for stereo cameras,
and we leverage relative pose information to project where an observed feature
in one sonar image will be found in the image of another sonar. Additionally,
we analyze how both the feature location relative to the sonar and the relative
pose between the two sonars impact the projection. From simulated results, we
identify desirable stereo configurations for applications in field robotics
like feature correspondence and recovery of the 3D information of the feature.","Kalin Norman, Joshua G. Mangelson",2025-07-07T18:49:06Z,2025-07-07T18:49:06Z,http://arxiv.org/abs/2507.05410v1,http://arxiv.org/pdf/2507.05410v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
VerifyLLM: LLM-Based Pre-Execution Task Plan Verification for Robots,"In the field of robotics, researchers face a critical challenge in ensuring
reliable and efficient task planning. Verifying high-level task plans before
execution significantly reduces errors and enhance the overall performance of
these systems. In this paper, we propose an architecture for automatically
verifying high-level task plans before their execution in simulator or
real-world environments. Leveraging Large Language Models (LLMs), our approach
consists of two key steps: first, the conversion of natural language
instructions into Linear Temporal Logic (LTL), followed by a comprehensive
analysis of action sequences. The module uses the reasoning capabilities of the
LLM to evaluate logical coherence and identify potential gaps in the plan.
Rigorous testing on datasets of varying complexity demonstrates the broad
applicability of the module to household tasks. We contribute to improving the
reliability and efficiency of task planning and addresses the critical need for
robust pre-execution verification in autonomous systems. The code is available
at https://verifyllm.github.io.","Danil S. Grigorev, Alexey K. Kovalev, Aleksandr I. Panov",2025-07-07T15:31:36Z,2025-07-07T15:31:36Z,http://arxiv.org/abs/2507.05118v1,http://arxiv.org/pdf/2507.05118v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Training-free Generation of Temporally Consistent Rewards from VLMs,"Recent advances in vision-language models (VLMs) have significantly improved
performance in embodied tasks such as goal decomposition and visual
comprehension. However, providing accurate rewards for robotic manipulation
without fine-tuning VLMs remains challenging due to the absence of
domain-specific robotic knowledge in pre-trained datasets and high
computational costs that hinder real-time applicability. To address this, we
propose $\mathrm{T}^2$-VLM, a novel training-free, temporally consistent
framework that generates accurate rewards through tracking the status changes
in VLM-derived subgoals. Specifically, our method first queries the VLM to
establish spatially aware subgoals and an initial completion estimate before
each round of interaction. We then employ a Bayesian tracking algorithm to
update the goal completion status dynamically, using subgoal hidden states to
generate structured rewards for reinforcement learning (RL) agents. This
approach enhances long-horizon decision-making and improves failure recovery
capabilities with RL. Extensive experiments indicate that $\mathrm{T}^2$-VLM
achieves state-of-the-art performance in two robot manipulation benchmarks,
demonstrating superior reward accuracy with reduced computation consumption. We
believe our approach not only advances reward generation techniques but also
contributes to the broader field of embodied AI. Project website:
https://t2-vlm.github.io/.","Yinuo Zhao, Jiale Yuan, Zhiyuan Xu, Xiaoshuai Hao, Xinyi Zhang, Kun Wu, Zhengping Che, Chi Harold Liu, Jian Tang",2025-07-07T09:11:08Z,2025-07-07T09:11:08Z,http://arxiv.org/abs/2507.04789v1,http://arxiv.org/pdf/2507.04789v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"VLM-TDP: VLM-guided Trajectory-conditioned Diffusion Policy for Robust
  Long-Horizon Manipulation","Diffusion policy has demonstrated promising performance in the field of
robotic manipulation. However, its effectiveness has been primarily limited in
short-horizon tasks, and its performance significantly degrades in the presence
of image noise. To address these limitations, we propose a VLM-guided
trajectory-conditioned diffusion policy (VLM-TDP) for robust and long-horizon
manipulation. Specifically, the proposed method leverages state-of-the-art
vision-language models (VLMs) to decompose long-horizon tasks into concise,
manageable sub-tasks, while also innovatively generating voxel-based
trajectories for each sub-task. The generated trajectories serve as a crucial
conditioning factor, effectively steering the diffusion policy and
substantially enhancing its performance. The proposed Trajectory-conditioned
Diffusion Policy (TDP) is trained on trajectories derived from demonstration
data and validated using the trajectories generated by the VLM. Simulation
experimental results indicate that our method significantly outperforms
classical diffusion policies, achieving an average 44% increase in success
rate, over 100% improvement in long-horizon tasks, and a 20% reduction in
performance degradation in challenging conditions, such as noisy images or
altered environments. These findings are further reinforced by our real-world
experiments, where the performance gap becomes even more pronounced in
long-horizon tasks. Videos are available on https://youtu.be/g0T6h32OSC8","Kefeng Huang, Tingguang Li, Yuzhen Liu, Zhe Zhang, Jiankun Wang, Lei Han",2025-07-06T20:27:11Z,2025-07-06T20:27:11Z,http://arxiv.org/abs/2507.04524v1,http://arxiv.org/pdf/2507.04524v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Free-Space Optical Communication-Driven NMPC Framework for Multi-Rotor
  Aerial Vehicles in Structured Inspection Scenarios","This paper introduces a Nonlinear Model Predictive Control (NMPC) framework
for communication-aware motion planning of Multi-Rotor Aerial Vehicles (MRAVs)
using Free-Space Optical (FSO) links. The scenario involves MRAVs equipped with
body-fixed optical transmitters and Unmanned Ground Vehicles (UGVs) acting as
mobile relays, each outfitted with fixed conical Field-of-View (FoV) receivers.
The controller integrates optical connectivity constraints into the NMPC
formulation to ensure beam alignment and minimum link quality, while also
enabling UGV tracking and obstacle avoidance. The method supports both coplanar
and tilted MRAV configurations. MATLAB simulations demonstrate its feasibility
and effectiveness.","Giuseppe Silano, Daniel Bonilla Licea, Hajar El Hammouti, Martin Saska",2025-07-06T16:08:00Z,2025-07-06T16:08:00Z,http://arxiv.org/abs/2507.04443v1,http://arxiv.org/pdf/2507.04443v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"TeleSim: A Network-Aware Testbed and Benchmark Dataset for Telerobotic
  Applications","Telerobotic technologies are becoming increasingly essential in fields such
as remote surgery, nuclear decommissioning, and space exploration. Reliable
datasets and testbeds are essential for evaluating telerobotic system
performance prior to real-world deployment. However, there is a notable lack of
datasets that capture the impact of network delays, as well as testbeds that
realistically model the communication link between the operator and the robot.
This paper introduces TeleSim, a network-aware teleoperation dataset and
testbed designed to assess the performance of telerobotic applications under
diverse network conditions. TeleSim systematically collects performance data
from fine manipulation tasks executed under three predefined network quality
tiers: High, Medium, and Low. Each tier is characterized through controlled
settings of bandwidth, latency, jitter, and packet loss. Using OMNeT++ for
precise network simulation, we record a wide range of metrics, including
completion time, success rates, video quality indicators (Peak Signal-to-Noise
Ratio (PSNR) and Structural Similarity Index Measure (SSIM)), and quality of
service (QoS) parameters. TeleSim comprises 300 experimental trials, providing
a robust benchmark for evaluating teleoperation systems across heterogeneous
network scenarios. In the worst network condition, completion time increases by
221.8% and success rate drops by 64%. Our findings reveal that network
degradation leads to compounding negative impacts, notably reduced video
quality and prolonged task execution, highlighting the need for adaptive,
resilient teleoperation protocols. The full dataset and testbed software are
publicly available on our GitHub repository:
https://github.com/ConnectedRoboticsLab and YouTube channel:
https://youtu.be/Fz_1iOYe104.","Zexin Deng, Zhenhui Yuan, Longhao Zou",2025-07-06T15:20:11Z,2025-07-06T15:20:11Z,http://arxiv.org/abs/2507.04425v1,http://arxiv.org/pdf/2507.04425v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Implicit Dual-Control for Visibility-Aware Navigation in Unstructured
  Environments","Navigating complex, cluttered, and unstructured environments that are a
priori unknown presents significant challenges for autonomous ground vehicles,
particularly when operating with a limited field of view(FOV) resulting in
frequent occlusion and unobserved space. This paper introduces a novel
visibility-aware model predictive path integral framework(VA-MPPI). Formulated
as a dual control problem where perceptual uncertainties and control decisions
are intertwined, it reasons over perception uncertainty evolution within a
unified planning and control pipeline. Unlike traditional methods that rely on
explicit uncertainty objectives, the VA-MPPI controller implicitly balances
exploration and exploitation, reducing uncertainty only when system performance
would be increased. The VA-MPPI framework is evaluated in simulation against
deterministic and prescient controllers across multiple scenarios, including a
cluttered urban alleyway and an occluded off-road environment. The results
demonstrate that VA-MPPI significantly improves safety by reducing collision
with unseen obstacles while maintaining competitive performance. For example,
in the off-road scenario with 400 control samples, the VA-MPPI controller
achieved a success rate of 84%, compared to only 8% for the deterministic
controller, with all VA-MPPI failures arising from unmet stopping criteria
rather than collisions. Furthermore, the controller implicitly avoids
unobserved space, improving safety without explicit directives. The proposed
framework highlights the potential for robust, visibility-aware navigation in
unstructured and occluded environments, paving the way for future advancements
in autonomous ground vehicle systems.","Benjamin Johnson, Qilun Zhu, Robert Prucka, Morgan Barron, Miriam Figueroa-Santos, Matthew Castanier",2025-07-06T12:32:44Z,2025-07-06T12:32:44Z,http://arxiv.org/abs/2507.04371v1,http://arxiv.org/pdf/2507.04371v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Robot-assisted Transcranial Magnetic Stimulation (Robo-TMS): A Review,"Transcranial magnetic stimulation (TMS) is a non-invasive and safe brain
stimulation procedure with growing applications in clinical treatments and
neuroscience research. However, achieving precise stimulation over prolonged
sessions poses significant challenges. By integrating advanced robotics with
conventional TMS, robot-assisted TMS (Robo-TMS) has emerged as a promising
solution to enhance efficacy and streamline procedures. Despite growing
interest, a comprehensive review from an engineering perspective has been
notably absent. This paper systematically examines four critical aspects of
Robo-TMS: hardware and integration, calibration and registration,
neuronavigation systems, and control systems. We review state-of-the-art
technologies in each area, identify current limitations, and propose future
research directions. Our findings suggest that broader clinical adoption of
Robo-TMS is currently limited by unverified clinical applicability, high
operational complexity, and substantial implementation costs. Emerging
technologies, including marker-less tracking, non-rigid registration,
learning-based electric field (E-field) modelling, individualised magnetic
resonance imaging (MRI) generation, robot-assisted multi-locus TMS (Robo-mTMS),
and automated calibration and registration, present promising pathways to
address these challenges.","Wenzhi Bai, Andrew Weightman, Rory J O Connor, Zhengtao Ding, Mingming Zhang, Sheng Quan Xie, Zhenhong Li",2025-07-06T11:14:16Z,2025-07-06T11:14:16Z,http://arxiv.org/abs/2507.04345v1,http://arxiv.org/pdf/2507.04345v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Surg-SegFormer: A Dual Transformer-Based Model for Holistic Surgical
  Scene Segmentation","Holistic surgical scene segmentation in robot-assisted surgery (RAS) enables
surgical residents to identify various anatomical tissues, articulated tools,
and critical structures, such as veins and vessels. Given the firm
intraoperative time constraints, it is challenging for surgeons to provide
detailed real-time explanations of the operative field for trainees. This
challenge is compounded by the scarcity of expert surgeons relative to
trainees, making the unambiguous delineation of go- and no-go zones
inconvenient. Therefore, high-performance semantic segmentation models offer a
solution by providing clear postoperative analyses of surgical procedures.
However, recent advanced segmentation models rely on user-generated prompts,
rendering them impractical for lengthy surgical videos that commonly exceed an
hour. To address this challenge, we introduce Surg-SegFormer, a novel
prompt-free model that outperforms current state-of-the-art techniques.
Surg-SegFormer attained a mean Intersection over Union (mIoU) of 0.80 on the
EndoVis2018 dataset and 0.54 on the EndoVis2017 dataset. By providing robust
and automated surgical scene comprehension, this model significantly reduces
the tutoring burden on expert surgeons, empowering residents to independently
and effectively understand complex surgical environments.","Fatimaelzahraa Ahmed, Muraam Abdel-Ghani, Muhammad Arsalan, Mahmoud Ali, Abdulaziz Al-Ali, Shidin Balakrishnan",2025-07-06T09:04:25Z,2025-07-06T09:04:25Z,http://arxiv.org/abs/2507.04304v1,http://arxiv.org/pdf/2507.04304v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Are Learning-Based Approaches Ready for Real-World Indoor Navigation? A
  Case for Imitation Learning","Traditional indoor robot navigation methods provide a reliable solution when
adapted to constrained scenarios, but lack flexibility or require manual
re-tuning when deployed in more complex settings. In contrast, learning-based
approaches learn directly from sensor data and environmental interactions,
enabling easier adaptability. While significant work has been presented in the
context of learning navigation policies, learning-based methods are rarely
compared to traditional navigation methods directly, which is a problem for
their ultimate acceptance in general navigation contexts. In this work, we
explore the viability of imitation learning (IL) for indoor navigation, using
expert (joystick) demonstrations to train various navigation policy networks
based on RGB images, LiDAR, and a combination of both, and we compare our IL
approach to a traditional potential field-based navigation method. We evaluate
the approach on a physical mobile robot platform equipped with a 2D LiDAR and a
camera in an indoor university environment. Our multimodal model demonstrates
superior navigation capabilities in most scenarios, but faces challenges in
dynamic environments, likely due to limited diversity in the demonstrations.
Nevertheless, the ability to learn directly from data and generalise across
layouts suggests that IL can be a practical navigation approach, and
potentially a useful initialisation strategy for subsequent lifelong learning.","Nigitha Selvaraj, Alex Mitrevski, Sebastian Houben",2025-07-05T16:18:48Z,2025-07-05T16:18:48Z,http://arxiv.org/abs/2507.04086v1,http://arxiv.org/pdf/2507.04086v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Low-Cost Infrastructure-Free 3D Relative Localization with Sub-Meter
  Accuracy in Near Field","Relative localization in the near-field scenario is critically important for
unmanned vehicle (UxV) applications. Although related works addressing 2D
relative localization problem have been widely studied for unmanned ground
vehicles (UGVs), the problem in 3D scenarios for unmanned aerial vehicles
(UAVs) involves more uncertainties and remains to be investigated. Inspired by
the phenomenon that animals can achieve swarm behaviors solely based on
individual perception of relative information, this study proposes an
infrastructure-free 3D relative localization framework that relies exclusively
on onboard ultra-wideband (UWB) sensors. Leveraging 2D relative positioning
research, we conducted feasibility analysis, system modeling, simulations,
performance evaluation, and field tests using UWB sensors. The key
contributions of this work include: derivation of the Cram\'er-Rao lower bound
(CRLB) and geometric dilution of precision (GDOP) for near-field scenarios;
development of two localization algorithms -- one based on Euclidean distance
matrix (EDM) and another employing maximum likelihood estimation (MLE);
comprehensive performance comparison and computational complexity analysis
against state-of-the-art methods; simulation studies and field experiments; a
novel sensor deployment strategy inspired by animal behavior, enabling
single-sensor implementation within the proposed framework for UxV
applications. The theoretical, simulation, and experimental results demonstrate
strong generalizability to other 3D near-field localization tasks, with
significant potential for a cost-effective cross-platform UxV collaborative
system.","Qiangsheng Gao, Ka Ho Cheng, Li Qiu, Zijun Gong",2025-06-23T23:58:06Z,2025-06-23T23:58:06Z,http://arxiv.org/abs/2506.19199v1,http://arxiv.org/pdf/2506.19199v1.pdf,all:field AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2025
"DRIVE Through the Unpredictability:From a Protocol Investigating Slip to
  a Metric Estimating Command Uncertainty","Off-road autonomous navigation is a challenging task as it is mainly
dependent on the accuracy of the motion model. Motion model performances are
limited by their ability to predict the interaction between the terrain and the
UGV, which an onboard sensor can not directly measure. In this work, we propose
using the DRIVE protocol to standardize the collection of data for system
identification and characterization of the slip state space. We validated this
protocol by acquiring a dataset with two platforms (from 75 kg to 470 kg) on
six terrains (i.e., asphalt, grass, gravel, ice, mud, sand) for a total of 4.9
hours and 14.7 km. Using this data, we evaluate the DRIVE protocol's ability to
explore the velocity command space and identify the reachable velocities for
terrain-robot interactions. We investigated the transfer function between the
command velocity space and the resulting steady-state slip for an SSMR. An
unpredictability metric is proposed to estimate command uncertainty and help
assess risk likelihood and severity in deployment. Finally, we share our
lessons learned on running system identification on large UGV to help the
community.","Nicolas Samson, William Larrivée-Hardy, William Dubois, Élie Roy-Brouard, Edith Brotherton, Dominic Baril, Julien Lépine, François Pomerleau",2025-06-19T20:33:33Z,2025-06-19T20:33:33Z,http://arxiv.org/abs/2506.16593v1,http://arxiv.org/pdf/2506.16593v1.pdf,all:field AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2025
"Long Duration Inspection of GNSS-Denied Environments with a Tethered
  UAV-UGV Marsupial System","Unmanned Aerial Vehicles (UAVs) have become essential tools in inspection and
emergency response operations due to their high maneuverability and ability to
access hard-to-reach areas. However, their limited battery life significantly
restricts their use in long-duration missions. This paper presents a novel
tethered marsupial robotic system composed of a UAV and an Unmanned Ground
Vehicle (UGV), specifically designed for autonomous, long-duration inspection
tasks in Global Navigation Satellite System (GNSS)-denied environments. The
system extends the UAV's operational time by supplying power through a tether
connected to high-capacity battery packs carried by the UGV. We detail the
hardware architecture based on off-the-shelf components to ensure replicability
and describe our full-stack software framework, which is composed of
open-source components and built upon the Robot Operating System (ROS). The
proposed software architecture enables precise localization using a Direct
LiDAR Localization (DLL) method and ensures safe path planning and coordinated
trajectory tracking for the integrated UGV-tether-UAV system. We validate the
system through three field experiments: (1) a manual flight endurance test to
estimate the operational duration, (2) an autonomous navigation test, and (3)
an inspection mission to demonstrate autonomous inspection capabilities.
Experimental results confirm the robustness and autonomy of the system, its
capacity to operate in GNSS-denied environments, and its potential for
long-endurance, autonomous inspection and monitoring tasks.","Simón Martínez-Rozas, David Alejo, José Javier Carpio, Fernando Caballero, Luis Merino",2025-05-29T14:05:25Z,2025-05-29T14:05:25Z,http://arxiv.org/abs/2505.23457v1,http://arxiv.org/pdf/2505.23457v1.pdf,all:field AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2025
"UAV See, UGV Do: Aerial Imagery and Virtual Teach Enabling Zero-Shot
  Ground Vehicle Repeat","This paper presents Virtual Teach and Repeat (VirT&R): an extension of the
Teach and Repeat (T&R) framework that enables GPS-denied, zero-shot autonomous
ground vehicle navigation in untraversed environments. VirT&R leverages aerial
imagery captured for a target environment to train a Neural Radiance Field
(NeRF) model so that dense point clouds and photo-textured meshes can be
extracted. The NeRF mesh is used to create a high-fidelity simulation of the
environment for piloting an unmanned ground vehicle (UGV) to virtually define a
desired path. The mission can then be executed in the actual target environment
by using NeRF-generated point cloud submaps associated along the path and an
existing LiDAR Teach and Repeat (LT&R) framework. We benchmark the
repeatability of VirT&R on over 12 km of autonomous driving data using physical
markings that allow a sim-to-real lateral path-tracking error to be obtained
and compared with LT&R. VirT&R achieved measured root mean squared errors
(RMSE) of 19.5 cm and 18.4 cm in two different environments, which are slightly
less than one tire width (24 cm) on the robot used for testing, and respective
maximum errors were 39.4 cm and 47.6 cm. This was done using only the
NeRF-derived teach map, demonstrating that VirT&R has similar closed-loop
path-tracking performance to LT&R but does not require a human to manually
teach the path to the UGV in the actual environment.","Desiree Fisker, Alexander Krawciw, Sven Lilge, Melissa Greeff, Timothy D. Barfoot",2025-05-22T17:10:28Z,2025-07-30T16:00:21Z,http://arxiv.org/abs/2505.16912v2,http://arxiv.org/pdf/2505.16912v2.pdf,all:field AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2025
Trailblazer: Learning offroad costmaps for long range planning,"Autonomous navigation in off-road environments remains a significant
challenge in field robotics, particularly for Unmanned Ground Vehicles (UGVs)
tasked with search and rescue, exploration, and surveillance. Effective
long-range planning relies on the integration of onboard perception systems
with prior environmental knowledge, such as satellite imagery and LiDAR data.
This work introduces Trailblazer, a novel framework that automates the
conversion of multi-modal sensor data into costmaps, enabling efficient path
planning without manual tuning. Unlike traditional approaches, Trailblazer
leverages imitation learning and a differentiable A* planner to learn costmaps
directly from expert demonstrations, enhancing adaptability across diverse
terrains. The proposed methodology was validated through extensive real-world
testing, achieving robust performance in dynamic and complex environments,
demonstrating Trailblazer's potential for scalable, efficient autonomous
navigation.","Kasi Viswanath, Felix Sanchez, Timothy Overbye, Jason M. Gregory, Srikanth Saripalli",2025-05-14T19:00:28Z,2025-06-11T01:50:51Z,http://arxiv.org/abs/2505.09739v2,http://arxiv.org/pdf/2505.09739v2.pdf,all:field AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2025
AGRO: An Autonomous AI Rover for Precision Agriculture,"Unmanned Ground Vehicles (UGVs) are emerging as a crucial tool in the world
of precision agriculture. The combination of UGVs with machine learning allows
us to find solutions for a range of complex agricultural problems. This
research focuses on developing a UGV capable of autonomously traversing
agricultural fields and capturing data. The project, known as AGRO (Autonomous
Ground Rover Observer) leverages machine learning, computer vision and other
sensor technologies. AGRO uses its capabilities to determine pistachio yields,
performing self-localization and real-time environmental mapping while avoiding
obstacles. The main objective of this research work is to automate
resource-consuming operations so that AGRO can support farmers in making
data-driven decisions. Furthermore, AGRO provides a foundation for advanced
machine learning techniques as it captures the world around it.","Simar Ghumman, Fabio Di Troia, William Andreopoulos, Mark Stamp, Sanjit Rai",2025-05-02T11:44:26Z,2025-05-02T11:44:26Z,http://arxiv.org/abs/2505.01200v1,http://arxiv.org/pdf/2505.01200v1.pdf,all:field AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2025
"Optimizing Indoor Farm Monitoring Efficiency Using UAV: Yield Estimation
  in a GNSS-Denied Cherry Tomato Greenhouse","As the agricultural workforce declines and labor costs rise, robotic yield
estimation has become increasingly important. While unmanned ground vehicles
(UGVs) are commonly used for indoor farm monitoring, their deployment in
greenhouses is often constrained by infrastructure limitations, sensor
placement challenges, and operational inefficiencies. To address these issues,
we develop a lightweight unmanned aerial vehicle (UAV) equipped with an RGB-D
camera, a 3D LiDAR, and an IMU sensor. The UAV employs a LiDAR-inertial
odometry algorithm for precise navigation in GNSS-denied environments and
utilizes a 3D multi-object tracking algorithm to estimate the count and weight
of cherry tomatoes. We evaluate the system using two dataset: one from a
harvesting row and another from a growing row. In the harvesting-row dataset,
the proposed system achieves 94.4\% counting accuracy and 87.5\% weight
estimation accuracy within a 13.2-meter flight completed in 10.5 seconds. For
the growing-row dataset, which consists of occluded unripened fruits, we
qualitatively analyze tracking performance and highlight future research
directions for improving perception in greenhouse with strong occlusions. Our
findings demonstrate the potential of UAVs for efficient robotic yield
estimation in commercial greenhouses.","Taewook Park, Jinwoo Lee, Hyondong Oh, Won-Jae Yun, Kyu-Wha Lee",2025-05-02T04:41:57Z,2025-05-02T04:41:57Z,http://arxiv.org/abs/2505.00995v1,http://arxiv.org/pdf/2505.00995v1.pdf,all:field AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2025
"Design and Evaluation of a UGV-Based Robotic Platform for Precision Soil
  Moisture Remote Sensing","This extended abstract presents the design and evaluation of AgriOne, an
automated unmanned ground vehicle (UGV) platform for high precision sensing of
soil moisture in large agricultural fields. The developed robotic system is
equipped with a volumetric water content (VWC) sensor mounted on a robotic
manipulator and utilizes a surface-aware data collection framework to ensure
accurate measurements in heterogeneous terrains. The framework identifies and
removes invalid data points where the sensor fails to penetrate the soil,
ensuring data reliability. Multiple field experiments were conducted to
validate the platform's performance, while the obtained results demonstrate the
efficacy of the AgriOne robot in real-time data acquisition, reducing the need
for permanent sensors and labor-intensive methods.","Ilektra Tsimpidi, Ilias Tevetzidis, Vidya Sumathy, George Nikolakopoulos",2025-04-25T11:52:06Z,2025-04-25T11:52:06Z,http://arxiv.org/abs/2504.18284v1,http://arxiv.org/pdf/2504.18284v1.pdf,all:field AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2025
"Strengthening Multi-Robot Systems for SAR: Co-Designing Robotics and
  Communication Towards 6G","This paper presents field-tested use cases from Search and Rescue (SAR)
missions, highlighting the co-design of mobile robots and communication systems
to support Edge-Cloud architectures based on 5G Standalone (SA). The main goal
is to contribute to the effective cooperation of multiple robots and first
responders. Our field experience includes the development of Hybrid Wireless
Sensor Networks (H-WSNs) for risk and victim detection, smartphones integrated
into the Robot Operating System (ROS) as Edge devices for mission requests and
path planning, real-time Simultaneous Localization and Mapping (SLAM) via
Multi-Access Edge Computing (MEC), and implementation of Uncrewed Ground
Vehicles (UGVs) for victim evacuation in different navigation modes. These
experiments, conducted in collaboration with actual first responders,
underscore the need for intelligent network resource management, balancing
low-latency and high-bandwidth demands. Network slicing is key to ensuring
critical emergency services are performed despite challenging communication
conditions. The paper identifies architectural needs, lessons learned, and
challenges to be addressed by 6G technologies to enhance emergency response
capabilities.","Juan Bravo-Arrabal, Ricardo Vázquez-Martín, J. J. Fernández-Lozano, Alfonso García-Cerezo",2025-04-02T17:47:11Z,2025-04-02T17:47:11Z,http://arxiv.org/abs/2504.01940v1,http://arxiv.org/pdf/2504.01940v1.pdf,all:field AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2025
"Curvature-Constrained Vector Field for Motion Planning of Nonholonomic
  Robots","Vector fields are advantageous in handling nonholonomic motion planning as
they provide reference orientation for robots. However, additionally
incorporating curvature constraints becomes challenging, due to the
interconnection between the design of the curvature-bounded vector field and
the tracking controller under underactuation. In this paper, we present a novel
framework to co-develop the vector field and the control laws, guiding the
nonholonomic robot to the target configuration with curvature-bounded
trajectory. First, we formulate the problem by introducing the target positive
limit set, which allows the robot to converge to or pass through the target
configuration, depending on different dynamics and tasks. Next, we construct a
curvature-constrained vector field (CVF) via blending and distributing basic
flow fields in workspace and propose the saturated control laws with a dynamic
gain, under which the tracking error's magnitude decreases even when saturation
occurs. Under the control laws, kinematically constrained nonholonomic robots
are guaranteed to track the reference CVF and converge to the target positive
limit set with bounded trajectory curvature. Numerical simulations show that
the proposed CVF method outperforms other vector-field-based algorithms.
Experiments on Ackermann UGVs and semi-physical fixed-wing UAVs demonstrate
that the method can be effectively implemented in real-world scenarios.","Yike Qiao, Xiaodong He, An Zhuo, Zhiyong Sun, Weimin Bao, Zhongkui Li",2025-03-25T10:28:24Z,2025-03-25T10:28:24Z,http://arxiv.org/abs/2504.02852v1,http://arxiv.org/pdf/2504.02852v1.pdf,all:field AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2025
Motion-Coupled Mapping Algorithm for Hybrid Rice Canopy,"This paper presents a motion-coupled mapping algorithm for contour mapping of
hybrid rice canopies, specifically designed for Agricultural Unmanned Ground
Vehicles (Agri-UGV) navigating complex and unknown rice fields. Precise canopy
mapping is essential for Agri-UGVs to plan efficient routes and avoid protected
zones. The motion control of Agri-UGVs, tasked with impurity removal and other
operations, depends heavily on accurate estimation of rice canopy height and
structure. To achieve this, the proposed algorithm integrates real-time RGB-D
sensor data with kinematic and inertial measurements, enabling efficient
mapping and proprioceptive localization. The algorithm produces grid-based
elevation maps that reflect the probabilistic distribution of canopy contours,
accounting for motion-induced uncertainties. It is implemented on a
high-clearance Agri-UGV platform and tested in various environments, including
both controlled and dynamic rice field settings. This approach significantly
enhances the mapping accuracy and operational reliability of Agri-UGVs,
contributing to more efficient autonomous agricultural operations.","Huaiqu Feng, Guoyang Zhao, Cheng Liu, Yongwei Wang, Jun Wang",2025-02-22T08:12:23Z,2025-02-22T08:12:23Z,http://arxiv.org/abs/2502.16134v1,http://arxiv.org/pdf/2502.16134v1.pdf,all:field AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2025
"Air-Ground Collaborative Robots for Fire and Rescue Missions: Towards
  Mapping and Navigation Perspective","Air-ground collaborative robots have shown great potential in the field of
fire and rescue, which can quickly respond to rescue needs and improve the
efficiency of task execution. Mapping and navigation, as the key foundation for
air-ground collaborative robots to achieve efficient task execution, have
attracted a great deal of attention. This growing interest in collaborative
robot mapping and navigation is conducive to improving the intelligence of fire
and rescue task execution, but there has been no comprehensive investigation of
this field to highlight their strengths. In this paper, we present a systematic
review of the ground-to-ground cooperative robots for fire and rescue from a
new perspective of mapping and navigation. First, an air-ground collaborative
robots framework for fire and rescue missions based on unmanned aerial vehicle
(UAV) mapping and unmanned ground vehicle (UGV) navigation is introduced. Then,
the research progress of mapping and navigation under this framework is
systematically summarized, including UAV mapping, UAV/UGV co-localization, and
UGV navigation, with their main achievements and limitations. Based on the
needs of fire and rescue missions, the collaborative robots with different
numbers of UAVs and UGVs are classified, and their practicality in fire and
rescue tasks is elaborated, with a focus on the discussion of their merits and
demerits. In addition, the application examples of air-ground collaborative
robots in various firefighting and rescue scenarios are given. Finally, this
paper emphasizes the current challenges and potential research opportunities,
rounding up references for practitioners and researchers willing to engage in
this vibrant area of air-ground collaborative robots.","Ying Zhang, Haibao Yan, Danni Zhu, Jiankun Wang, Cui-Hua Zhang, Weili Ding, Xi Luo, Changchun Hua, Max Q. -H. Meng",2024-12-30T04:24:31Z,2025-02-24T07:38:50Z,http://arxiv.org/abs/2412.20699v2,http://arxiv.org/pdf/2412.20699v2.pdf,all:field AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2024
"Forest Biomass Mapping with Terrestrial Hyperspectral Imaging for
  Wildfire Risk Monitoring","With the rapid increase in wildfires in the past decade, it has become
necessary to detect and predict these disasters to mitigate losses to
ecosystems and human lives. In this paper, we present a novel solution --
Hyper-Drive3D -- consisting of snapshot hyperspectral imaging and LiDAR,
mounted on an Unmanned Ground Vehicle (UGV) that identifies areas inside
forests at risk of becoming fuel for a forest fire. This system enables more
accurate classification by analyzing the spectral signatures of forest
vegetation. We conducted field trials in a controlled environment simulating
forest conditions, yielding valuable insights into the system's effectiveness.
Extensive data collection was also performed in a dense forest across varying
environmental conditions and topographies to enhance the system's predictive
capabilities for fire hazards and support a risk-informed, proactive forest
management strategy. Additionally, we propose a framework for extracting
moisture data from hyperspectral imagery and projecting it into 3D space.","Nathaniel Hanson, Sarvesh Prajapati, James Tukpah, Yash Mewada, Taşkın Padır",2024-11-25T05:42:15Z,2024-11-25T05:42:15Z,http://arxiv.org/abs/2411.16107v1,http://arxiv.org/pdf/2411.16107v1.pdf,all:field AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2024
"Towards Safer Planetary Exploration: A Hybrid Architecture for Terrain
  Traversability Analysis in Mars Rovers","The field of autonomous navigation for unmanned ground vehicles (UGVs) is in
continuous growth and increasing levels of autonomy have been reached in the
last few years. However, the task becomes more challenging when the focus is on
the exploration of planet surfaces such as Mars. In those situations, UGVs are
forced to navigate through unstable and rugged terrains which, inevitably, open
the vehicle to more hazards, accidents, and, in extreme cases, complete mission
failure. The paper addresses the challenges of autonomous navigation for
unmanned ground vehicles in planetary exploration, particularly on Mars,
introducing a hybrid architecture for terrain traversability analysis that
combines two approaches: appearance-based and geometry-based. The
appearance-based method uses semantic segmentation via deep neural networks to
classify different terrain types. This is further refined by pixel-level
terrain roughness classification obtained from the same RGB image, assigning
different costs based on the physical properties of the soil. The
geometry-based method complements the appearance-based approach by evaluating
the terrain's geometrical features, identifying hazards that may not be
detectable by the appearance-based side. The outputs of both methods are
combined into a comprehensive hybrid cost map. The proposed architecture was
trained on synthetic datasets and developed as a ROS2 application to integrate
into broader autonomous navigation systems for harsh environments. Simulations
have been performed in Unity, showing the ability of the method to assess
online traversability analysis.","Achille Chiuchiarelli, Giacomo Franchini, Francesco Messina, Marcello Chiaberge",2024-10-23T10:12:14Z,2024-10-23T10:12:14Z,http://arxiv.org/abs/2410.17738v1,http://arxiv.org/pdf/2410.17738v1.pdf,all:field AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2024
Jailbreaking LLM-Controlled Robots,"The recent introduction of large language models (LLMs) has revolutionized
the field of robotics by enabling contextual reasoning and intuitive
human-robot interaction in domains as varied as manipulation, locomotion, and
self-driving vehicles. When viewed as a stand-alone technology, LLMs are known
to be vulnerable to jailbreaking attacks, wherein malicious prompters elicit
harmful text by bypassing LLM safety guardrails. To assess the risks of
deploying LLMs in robotics, in this paper, we introduce RoboPAIR, the first
algorithm designed to jailbreak LLM-controlled robots. Unlike existing, textual
attacks on LLM chatbots, RoboPAIR elicits harmful physical actions from
LLM-controlled robots, a phenomenon we experimentally demonstrate in three
scenarios: (i) a white-box setting, wherein the attacker has full access to the
NVIDIA Dolphins self-driving LLM, (ii) a gray-box setting, wherein the attacker
has partial access to a Clearpath Robotics Jackal UGV robot equipped with a
GPT-4o planner, and (iii) a black-box setting, wherein the attacker has only
query access to the GPT-3.5-integrated Unitree Robotics Go2 robot dog. In each
scenario and across three new datasets of harmful robotic actions, we
demonstrate that RoboPAIR, as well as several static baselines, finds
jailbreaks quickly and effectively, often achieving 100% attack success rates.
Our results reveal, for the first time, that the risks of jailbroken LLMs
extend far beyond text generation, given the distinct possibility that
jailbroken robots could cause physical damage in the real world. Indeed, our
results on the Unitree Go2 represent the first successful jailbreak of a
deployed commercial robotic system. Addressing this emerging vulnerability is
critical for ensuring the safe deployment of LLMs in robotics. Additional media
is available at: https://robopair.org","Alexander Robey, Zachary Ravichandran, Vijay Kumar, Hamed Hassani, George J. Pappas",2024-10-17T15:55:36Z,2024-11-09T20:00:07Z,http://arxiv.org/abs/2410.13691v2,http://arxiv.org/pdf/2410.13691v2.pdf,all:field AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2024
LiPO: LiDAR Inertial Odometry for ICP Comparison,"We introduce a LiDAR inertial odometry (LIO) framework, called LiPO, that
enables direct comparisons of different iterative closest point (ICP) point
cloud registration methods. The two common ICP methods we compare are
point-to-point (P2P) and point-to-feature (P2F). In our experience, within the
context of LIO, P2F-ICP results in less drift and improved mapping accuracy
when robots move aggressively through challenging environments when compared to
P2P-ICP. However, P2F-ICP methods require more hand-tuned hyper-parameters that
make P2F-ICP less general across all environments and motions. In real-world
field robotics applications where robots are used across different
environments, more general P2P-ICP methods may be preferred despite increased
drift. In this paper, we seek to better quantify the trade-off between P2P-ICP
and P2F-ICP to help inform when each method should be used. To explore this
trade-off, we use LiPO to directly compare ICP methods and test on relevant
benchmark datasets as well as on our custom unpiloted ground vehicle (UGV). We
find that overall, P2F-ICP has reduced drift and improved mapping accuracy,
but, P2P-ICP is more consistent across all environments and motions with
minimal drift increase.","Darwin Mick, Taylor Pool, Madankumar Sathenahally Nagaraju, Michael Kaess, Howie Choset, Matt Travers",2024-10-10T16:40:06Z,2024-10-10T16:40:06Z,http://arxiv.org/abs/2410.08097v1,http://arxiv.org/pdf/2410.08097v1.pdf,all:field AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2024
"Dynamic Neural Potential Field: Online Trajectory Optimization in
  Presence of Moving Obstacles","We address a task of local trajectory planning for the mobile robot in the
presence of static and dynamic obstacles. Local trajectory is obtained as a
numerical solution of the Model Predictive Control (MPC) problem. Collision
avoidance may be provided by adding repulsive potential of the obstacles to the
cost function of MPC. We develop an approach, where repulsive potential is
estimated by the neural model. We propose and explore three possible strategies
of handling dynamic obstacles. First, environment with dynamic obstacles is
considered as a sequence of static environments. Second, the neural model
predict a sequence of repulsive potential at once. Third, the neural model
predict future repulsive potential step by step in autoregressive mode. We
implement these strategies and compare it with CIAO* and MPPI using BenchMR
framework. First two strategies showed higher performance than CIAO* and MPPI
while preserving safety constraints. The third strategy was a bit slower,
however it still satisfy time limits. We deploy our approach on Husky UGV
mobile platform, which move through the office corridors under proposed MPC
local trajectory planner. The code and trained models are available at
\url{https://github.com/CognitiveAISystems/Dynamic-Neural-Potential-Field}.","Aleksey Staroverov, Muhammad Alhaddad, Aditya Narendra, Konstantin Mironov, Aleksandr Panov",2024-10-09T12:27:09Z,2024-10-09T12:27:09Z,http://arxiv.org/abs/2410.06819v1,http://arxiv.org/pdf/2410.06819v1.pdf,all:field AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2024
Comparing Motion Distortion Between Vehicle Field Deployments,"Recent advances in autonomous driving for uncrewed ground vehicles (UGVs)
have spurred significant development, particularly in challenging terrains.
This paper introduces a classification system assessing various UGV deployments
reported in the literature. Our approach considers motion distortion features
that include internal UGV features, such as mass and speed, and external
features, such as terrain complexity, which all influence the efficiency of
models and navigation systems. We present results that map UGV deployments
relative to vehicle kinetic energy and terrain complexity, providing insights
into the level of complexity and risk associated with different operational
environments. Additionally, we propose a motion distortion metric to assess UGV
navigation performance that does not require an explicit quantification of
motion distortion features. Using this metric, we conduct a case study to
illustrate the impact of motion distortion features on modeling accuracy. This
research advocates for creating a comprehensive database containing many
different motion distortion features, which would contribute to advancing the
understanding of autonomous driving capabilities in rough conditions and
provide a validation framework for future developments in UGV navigation
systems.","Nicolas Samson, Dominic Baril, Julien Lépine, François Pomerleau",2024-04-30T20:36:38Z,2024-04-30T20:36:38Z,http://arxiv.org/abs/2405.00189v1,http://arxiv.org/pdf/2405.00189v1.pdf,all:field AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2024
"Towards Over-Canopy Autonomous Navigation: Crop-Agnostic LiDAR-Based
  Crop-Row Detection in Arable Fields","Autonomous navigation is crucial for various robotics applications in
agriculture. However, many existing methods depend on RTK-GPS devices, which
can be susceptible to loss of radio signal or intermittent reception of
corrections from the internet. Consequently, research has increasingly focused
on using RGB cameras for crop-row detection, though challenges persist when
dealing with grown plants. This paper introduces a LiDAR-based navigation
system that can achieve crop-agnostic over-canopy autonomous navigation in
row-crop fields, even when the canopy fully blocks the inter-row spacing. Our
algorithm can detect crop rows across diverse scenarios, encompassing various
crop types, growth stages, the presence of weeds, curved rows, and
discontinuities. Without utilizing a global localization method (i.e., based on
GPS), our navigation system can perform autonomous navigation in these
challenging scenarios, detect the end of the crop rows, and navigate to the
next crop row autonomously, providing a crop-agnostic approach to navigate an
entire field. The proposed navigation system has undergone tests in various
simulated and real agricultural fields, achieving an average cross-track error
of 3.55cm without human intervention. The system has been deployed on a
customized UGV robot, which can be reconfigured depending on the field
conditions.","Ruiji Liu, Francisco Yandun, George Kantor",2024-03-26T15:07:27Z,2024-09-18T19:29:55Z,http://arxiv.org/abs/2403.17774v3,http://arxiv.org/pdf/2403.17774v3.pdf,all:field AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2024
A Risk-aware Planning Framework of UGVs in Off-Road Environment,"Planning module is an essential component of intelligent vehicle study. In
this paper, we address the risk-aware planning problem of UGVs through a
global-local planning framework which seamlessly integrates risk assessment
methods. In particular, a global planning algorithm named Coarse2fine A* is
proposed, which incorporates a potential field approach to enhance the safety
of the planning results while ensuring the efficiency of the algorithm. A
deterministic sampling method for local planning is leveraged and modified to
suit off-road environment. It also integrates a risk assessment model to
emphasize the avoidance of local risks. The performance of the algorithm is
demonstrated through simulation experiments by comparing it with baseline
algorithms, where the results of Coarse2fine A* are shown to be approximately
30% safer than those of the baseline algorithms. The practicality and
effectiveness of the proposed planning framework are validated by deploying it
on a real-world system consisting of a control center and a practical UGV
platform.","Junkai Jiang, Zhenhua Hu, Zihan Xie, Changlong Hao, Hongyu Liu, Wenliang Xu, Yuning Wang, Lei He, Shaobing Xu, Jianqiang Wang",2024-02-04T12:07:23Z,2024-02-04T12:07:23Z,http://arxiv.org/abs/2402.02457v1,http://arxiv.org/pdf/2402.02457v1.pdf,all:field AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2024
Neural Potential Field for Obstacle-Aware Local Motion Planning,"Model predictive control (MPC) may provide local motion planning for mobile
robotic platforms. The challenging aspect is the analytic representation of
collision cost for the case when both the obstacle map and robot footprint are
arbitrary. We propose a Neural Potential Field: a neural network model that
returns a differentiable collision cost based on robot pose, obstacle map, and
robot footprint. The differentiability of our model allows its usage within the
MPC solver. It is computationally hard to solve problems with a very high
number of parameters. Therefore, our architecture includes neural image
encoders, which transform obstacle maps and robot footprints into embeddings,
which reduce problem dimensionality by two orders of magnitude. The reference
data for network training are generated based on algorithmic calculation of a
signed distance function. Comparative experiments showed that the proposed
approach is comparable with existing local planners: it provides trajectories
with outperforming smoothness, comparable path length, and safe distance from
obstacles. Experiment on Husky UGV mobile robot showed that our approach allows
real-time and safe local planning. The code for our approach is presented at
https://github.com/cog-isa/NPField together with demo video.","Muhammad Alhaddad, Konstantin Mironov, Aleksey Staroverov, Aleksandr Panov",2023-10-25T05:00:21Z,2023-10-25T05:00:21Z,http://arxiv.org/abs/2310.16362v1,http://arxiv.org/pdf/2310.16362v1.pdf,all:field AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2023
"AG-CVG: Coverage Planning with a Mobile Recharging UGV and an
  Energy-Constrained UAV","In this paper, we present an approach for coverage path planning for a team
of an energy-constrained Unmanned Aerial Vehicle (UAV) and an Unmanned Ground
Vehicle (UGV). Both the UAV and the UGV have predefined areas that they have to
cover. The goal is to perform complete coverage by both robots while minimizing
the coverage time. The UGV can also serve as a mobile recharging station. The
UAV and UGV need to occasionally rendezvous for recharging. We propose a
heuristic method to address this NP-Hard planning problem. Our approach
involves initially determining coverage paths without factoring in energy
constraints. Subsequently, we cluster segments of these paths and employ graph
matching to assign UAV clusters to UGV clusters for efficient recharging
management. We perform numerical analysis on real-world coverage applications
and show that compared with a greedy approach our method reduces rendezvous
overhead on average by 11.33%. We demonstrate proof-of-concept with a team of a
VOXL m500 drone and a Clearpath Jackal ground vehicle, providing a complete
system from the offline algorithm to the field execution.","Nare Karapetyan, Ahmad Bilal Asghar, Amisha Bhaskar, Guangyao Shi, Dinesh Manocha, Pratap Tokekar",2023-10-11T16:04:02Z,2024-03-15T15:03:54Z,http://arxiv.org/abs/2310.07621v2,http://arxiv.org/pdf/2310.07621v2.pdf,all:field AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2023
"Integrated Robotics Networks with Co-optimization of Drone Placement and
  Air-Ground Communications","Terrestrial robots, i.e., unmanned ground vehicles (UGVs), and aerial robots,
i.e., unmanned aerial vehicles (UAVs), operate in separate spaces. To exploit
their complementary features (e.g., fields of views, communication links,
computing capabilities), a promising paradigm termed integrated robotics
network emerges, which provides communications for cooperative UAVs-UGVs
applications. However, how to efficiently deploy UAVs and schedule the
UAVs-UGVs connections according to different UGV tasks become challenging. In
this paper, we propose a sum-rate maximization problem, where UGVs plan their
trajectories autonomously and are dynamically associated with UAVs according to
their planned trajectories. Although the problem is a NP-hard mixed integer
program, a fast polynomial time algorithm using alternating gradient descent
and penalty-based binary relaxation, is devised. Simulation results demonstrate
the effectiveness of the proposed algorithm.","Menghao Hu, Tong Zhang, Shuai Wang, Guoliang Li, Yingyang Chen, Qiang Li, Gaojie Chen",2023-09-09T09:29:21Z,2023-12-03T07:16:49Z,http://arxiv.org/abs/2309.04730v2,http://arxiv.org/pdf/2309.04730v2.pdf,all:field AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2023
"Room temperature giant magnetoresistance detection of spin hall
  nano-oscillator dynamics in synthetic antiferromagnetic Spin-Valve","Conventional spin Hall nano-oscillators (SHNOs) face fundamental power
limitations due to the low anisotropic magnetoresistance (AMR < 0.3%) of
ferromagnetic layers. To address this, we developed a synthetic
antiferromagnetic spin-valve (SAF-SV) heterostructure
[Ta/NiFe/Ru/NiFe/Cu/NiFe/Hf/Pt] that enables efficient giant magnetoresistance
(GMR)-based detection of SHNO dynamics at room temperature. The NiFe/Ru/NiFe
SAF reference layer, operating in the spin-flop state, couples with the NiFe
free layer through a Cu spacer to achieve a remarkable GMR ratio of 0.568% -
exhibiting complete independence of magnetic field/current orientation.
Spin-torque ferromagnetic resonance (ST-FMR) verifies that the ferromagnetic
resonance linewidth of the free layer can be effectively modulated by dc
current through the Pt heavy metal layer, while maintaining decoupled dynamics
from the SAF layer. Thermal management via high-thermal-conductivity SiC
substrates and AlN capping layers successfully mitigates
current-shunting-induced Joule heating. Notably, stable auto-oscillation peaks
are observed at 0.82 mA bias current, with oscillation frequency tunable by
external magnetic field and potential dual-mode behavior at low fields. This
work establishes a new paradigm for room-temperature, high-power spintronic
oscillators, offering significant potential for neuromorphic computing and
coherent RF communication applications.","Chunhao Li, Xiaotian Zhao, Wenlong Cai, Long Liu, Wei Liu, Zhidong Zhang",2025-08-26T07:54:42Z,2025-08-28T03:13:17Z,http://arxiv.org/abs/2508.18770v2,http://arxiv.org/pdf/2508.18770v2.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2025
"Application of a Pressured-Based OpenFOAM Solver for Rotating Detonation
  Engines","This study aims to develop a simulation framework for rotating detonation
engines (RDEs) using multicomponentFluid solver in OpenFOAM v12 and to
demonstrate reducing the computational costs by adaptive mesh refinement (AMR)
and dynamic load balancing (DLB). RDEs have been extensively studied for
improvements in efficiency for power generation and aircraft propulsion
systems. A well-established framework, showing both high accuracy and cost
efficiency, is required to facilitate further research and development in RDEs.
The multicomponentFluid solver is validated against two problems:
one-dimensional planar detonation simulation and two-dimensional RDE
simulation, in which the present study's results are compared to reference
results of experiments and simulations, respectively. In the problems, the
present simulation results agree well with the validation data both
qualitatively (e.g., pressure distribution and temperature field) and
quantitatively (e.g., detonation velocity, mass flux, and specific impulse and
thrust). In the two-dimensional RDE simulation, we propose a detonation
velocity correction method for fair comparison with Chapman-Jouguet (CJ)
detonation velocity. Moreover, the two-dimensional RDE simulation is optimized
using AMR and DLB. By adopting both, computational costs decrease by up to 11.2
times. The effect of each of them is examined as well, which highlights the
importance of DLB.","Keunjae Kwak, Hyoungwoo Kim, Je Ir Ryu, Donh-Hyuk Shin",2025-08-22T05:47:40Z,2025-08-22T05:47:40Z,http://arxiv.org/abs/2508.16105v1,http://arxiv.org/pdf/2508.16105v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2025
Investigation on high-order planar Hall effect in trigonal PtBi$_2$,"The trigonal PtBi$_2$ (t-PtBi$_2$) as a Weyl semimetal possessing triply
degenerate points in its electronic bands near the Fermi level endows it with
rich electronic properties. Previous studies have already measured the planar
Hall effect (PHE) and in-plane anisotropic magnetoresistance (AMR) of
t-PtBi$_2$. We noticed that their experimental results exhibited high-order
features in both the PHE and AMR, yet these features were not systematically
investigated. In our work, we conducted more systematic measurements and
analyses of the PHE and AMR in t-PtBi$_2$. Both PHE and AMR show high-order
features under low temperatures and strong magnetic fields, and these features
share a similar temperature and magnetic field dependence with the turn-on
behavior of resistance and temperature curves, indicating a common physical
origin for them. We further summarize the critical conditions for the emergence
of high-order PHE in t-PtBi$_2$, which will help to understand the origin of
high-order features. In addition, we performed computational simulations on the
AMR of t-PtBi$_2$, and the results were consistent with the experiments,
indicating the high-order features are the result of the combined contribution
of the Fermi surface anisotropy and the scaling behavior of magnetoresistance.
Our findings will contribute to a deeper understanding of the origins of
high-order features in non-magnetic topological materials.","Fangqi Cai, Mingxi Chi, Yingjie Hu, Heyao Liu, Yangyang Chen, Chao Jing, Wei Ren, He Wang",2025-07-19T11:47:42Z,2025-07-19T11:47:42Z,http://arxiv.org/abs/2507.14580v1,http://arxiv.org/pdf/2507.14580v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2025
"DUSE: A Data Expansion Framework for Low-resource Automatic Modulation
  Recognition based on Active Learning","Although deep neural networks have made remarkable achievements in the field
of automatic modulation recognition (AMR), these models often require a large
amount of labeled data for training. However, in many practical scenarios, the
available target domain data is scarce and difficult to meet the needs of model
training. The most direct way is to collect data manually and perform expert
annotation, but the high time and labor costs are unbearable. Another common
method is data augmentation. Although it can enrich training samples to a
certain extent, it does not introduce new data and therefore cannot
fundamentally solve the problem of data scarcity. To address these challenges,
we introduce a data expansion framework called Dynamic Uncertainty-driven
Sample Expansion (DUSE). Specifically, DUSE uses an uncertainty scoring
function to filter out useful samples from relevant AMR datasets and employs an
active learning strategy to continuously refine the scorer. Extensive
experiments demonstrate that DUSE consistently outperforms 8 coreset selection
baselines in both class-balance and class-imbalance settings. Besides, DUSE
exhibits strong cross-architecture generalization for unseen models.","Yao Lu, Hongyu Gao, Zhuangzhi Chen, Dongwei Xu, Yun Lin, Qi Xuan, Guan Gui",2025-07-16T08:09:41Z,2025-07-16T08:09:41Z,http://arxiv.org/abs/2507.12011v1,http://arxiv.org/pdf/2507.12011v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2025
Star Formation and Magnetic Field Amplification due to Galactic Spirals,"We use global MHD galaxy simulations to investigate the effects of spiral
arms on the evolution of magnetic fields and star formation within a
self-regulated interstellar medium (ISM). The same galaxy is simulated twice:
once with self-consistent stellar spiral arms and once more with the stellar
spirals suppressed via a novel numerical approach, using the Ramses AMR code.
Spiral arms continually promote star formation, with 2.6 times higher rates in
the spiral galaxy. The higher rate is due to high gas columns gathered along
the spiral arms, rather than increasing the star formation efficiency at a
given gas column. In both cases, the magnetic field is initially amplified via
a small-scale dynamo driven by turbulence due to supernova feedback. Only the
spiral galaxy exhibits late-time, consistent field growth due to a large-scale
dynamo (e-folding time $\sim600$ Myr). This results in volume-averaged field
strengths of $\sim 1$ $\mu$G after 1 Gyr of evolution. The mean-fields tend to
align themselves with the spiral arms and are coherent up to 10 kpc scales. We
demonstrate a novel large-scale dynamo mechanism, whereby spiral-driven radial
flows enable the mean-field amplification.","Hector Robinson, James Wadsley, J. A. Sellwood, Ralph E. Pudritz",2025-06-19T18:00:08Z,2025-06-19T18:00:08Z,http://arxiv.org/abs/2506.16515v1,http://arxiv.org/pdf/2506.16515v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2025
"Real-time adaptive tracking of fluctuating relaxation rates in
  superconducting qubits","The fidelity of operations on a solid-state quantum processor is ultimately
bounded by decoherence effects induced by a fluctuating environment.
Characterizing environmental fluctuations is challenging because the
acquisition time of experimental protocols limits the precision with which the
environment can be measured and may obscure the detailed structure of these
fluctuations. Here we present a real-time Bayesian method for estimating the
relaxation rate of a qubit, leveraging a classical controller with an
integrated field-programmable gate array (FPGA). Using our FPGA-powered
Bayesian method, we adaptively and continuously track the relaxation-time
fluctuations of two fixed-frequency superconducting transmon qubits, which
exhibit average relaxation times of approximately 0.17 ms and occasionally
exceed 0.5 ms. Our technique allows for the estimation of these relaxation
times in a few milliseconds, more than two orders of magnitude faster than
previous nonadaptive methods, and allows us to observe fluctuations up to 5
times the qubit's average relaxation rates on significantly shorter timescales
than previously reported. Our statistical analysis reveals that these
fluctuations occur on much faster timescales than previously understood, with
two-level-system switching rates reaching up to 10 Hz. Our work offers an
appealing solution for rapid relaxation-rate characterization in device
screening and for improved understanding of fast relaxation dynamics.","Fabrizio Berritta, Jacob Benestad, Jan A. Krzywda, Oswin Krause, Malthe A. Marciniak, Svend Krøjer, Christopher W. Warren, Emil Hogedal, Andreas Nylander, Irshad Ahmad, Amr Osman, Janka Biznárová, Marcus Rommel, Anita Fadavi Roudsari, Jonas Bylander, Giovanna Tancredi, Jeroen Danon, Jacob Hastrup, Ferdinand Kuemmeth, Morten Kjaergaard",2025-06-11T10:14:23Z,2025-06-11T10:14:23Z,http://arxiv.org/abs/2506.09576v1,http://arxiv.org/pdf/2506.09576v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2025
A unified fluid model for nonthermal plasmas and reacting flows,"This work presents a unified fluid modeling framework for reacting flows
coupled with nonthermal plasmas (NTPs). Building upon the gas-plasma kinetics
solver, ChemPlasKin, and the CFD library, OpenFOAM, the integrated solver,
reactPlasFOAM, allows simulation of fully coupled plasma-combustion systems
with versatility and high performance. By simplifying the governing equations
according to the dominant physical phenomena at each stage, the solver
seamlessly switches between four operating modes: streamer, spark, reacting
flow, and ionic wind, using coherent data structures. Unlike conventional
streamer solvers that rely on pre-tabulated or fitted electron transport
properties and reaction rates as functions of the reduced electric field or
electron temperature, our approach solves the electron Boltzmann equation (EBE)
on the fly to update the electron energy distribution function (EEDF) at the
cell level. This enables a high-fidelity representation of evolving plasma
chemistry and dynamics by capturing temporal and spatial variations in mixture
composition and temperature. To improve computational efficiency for this
multiscale, multiphysics system, we employ adaptive mesh refinement (AMR) in
the plasma channel, dynamic load balancing for parallelization, and time-step
subcycling for fast and slow transport processes. The solver is first verified
against six established plasma codes for positive-streamer simulations and
benchmarked against Cantera for a freely propagating hydrogen flame, then
applied to three cases: (1) spark discharge in airflow; (2) streamer
propagation in a premixed flame; and (3) flame dynamics under non-breakdown
electric fields. These applications validate the model's ability to predict NTP
properties such as fast heating and radical production and demonstrate its
potential to reveal two-way coupling between plasma and combustion.","Xiao Shao, Deanna A. Lacoste, Hong G. Im",2025-06-09T14:16:07Z,2025-06-09T14:16:07Z,http://arxiv.org/abs/2506.07792v1,http://arxiv.org/pdf/2506.07792v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2025
"Field-Level Comparison and Robustness Analysis of Cosmological N-body
  Simulations","We present the first field-level comparison of cosmological N-body
simulations, considering various widely used codes: Abacus, CUBEP$^3$M, Enzo,
Gadget, Gizmo, PKDGrav, and Ramses. Unlike previous comparisons focused on
summary statistics, we conduct a comprehensive field-level analysis: evaluating
statistical similarity, quantifying implications for cosmological parameter
inference, and identifying the regimes in which simulations are consistent. We
begin with a traditional comparison using the power spectrum, cross-correlation
coefficient, and visual inspection of the matter field. We follow this with a
statistical out-of-distribution (OOD) analysis to quantify distributional
differences between simulations, revealing insights not captured by the
traditional metrics. We then perform field-level simulation-based inference
(SBI) using convolutional neural networks (CNNs), training on one simulation
and testing on others, including a full hydrodynamic simulation for comparison.
We identify several causes of OOD behavior and biased inference, finding that
resolution effects, such as those arising from adaptive mesh refinement (AMR),
have a significant impact. Models trained on non-AMR simulations fail
catastrophically when evaluated on AMR simulations, introducing larger biases
than those from hydrodynamic effects. Differences in resolution, even when
using the same N-body code, likewise lead to biased inference. We attribute
these failures to a CNN's sensitivity to small-scale fluctuations, particularly
in voids and filaments, and demonstrate that appropriate smoothing brings the
simulations into statistical agreement. Our findings motivate the need for
careful data filtering and the use of field-level OOD metrics, such as PQMass,
to ensure robust inference.","Adrian E. Bayer, Francisco Villaescusa-Navarro, Sammy Sharief, Romain Teyssier, Lehman H. Garrison, Laurence Perreault-Levasseur, Greg L. Bryan, Marco Gatti, Eli Visbal",2025-05-19T18:02:44Z,2025-08-19T00:50:28Z,http://arxiv.org/abs/2505.13620v2,http://arxiv.org/pdf/2505.13620v2.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2025
"Survey of Abstract Meaning Representation: Then, Now, Future","This paper presents a survey of Abstract Meaning Representation (AMR), a
semantic representation framework that captures the meaning of sentences
through a graph-based structure. AMR represents sentences as rooted, directed
acyclic graphs, where nodes correspond to concepts and edges denote
relationships, effectively encoding the meaning of complex sentences. This
survey investigates AMR and its extensions, focusing on AMR capabilities. It
then explores the parsing (text-to-AMR) and generation (AMR-to-text) tasks by
showing traditional, current, and possible futures approaches. It also reviews
various applications of AMR including text generation, text classification, and
information extraction and information seeking. By analyzing recent
developments and challenges in the field, this survey provides insights into
future directions for research and the potential impact of AMR on enhancing
machine understanding of human language.",Behrooz Mansouri,2025-05-06T06:45:40Z,2025-05-06T06:45:40Z,http://arxiv.org/abs/2505.03229v1,http://arxiv.org/pdf/2505.03229v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2025
"Nonzero RMS Magnetoresistance Yielding Control Space Partition of CrTe2
  Monolayer","The study of magnetic phenomena in low-dimensional systems has largely
explored after the discovery of two-dimensional (2D) magnetic materials, such
as CrI3 and Cr2Ge2Te6 in 2017. These materials presents intrinsic magnetic
order, overcoming the limitations predicted by the Mermin-Wagner theorem, due
to magnetic crystalline anisotropy energy. Among these, CrTe2, a van der Waals
2D magnet, has gather significant interest due to its in-plane anisotropic
magnetoresistance (AMR) and high Curie temperature. This study investigates the
magnetic field-regulated resistance of CrTe2 monolayers in the context of
spintronics applications. Utilizing the zigzag-ordered parameters obtained from
prior simulations, we examine how external magnetic fields influence resistance
states and control the ON/OFF state of nano-devices. The analysis demonstrates
that specific magnetic field configurations, particularly those in the form of
(0, 0, Bz), which is out-of-plane directed field, gives a non-zero root mean
square resistance, indicating a functional ON state. This provides a novel
method for magnetically controlled current regulation in spintronic devices.
The experimental results also reveal an interesting spin-flop transition in
CrTe2 under a z-directed magnetic field, leading to y-directional
magnetization. This phenomenon, combined with the material's robust magnetic
properties, positions CrTe2 as a promising candidate for next-generation memory
and logic devices. By advancing the understanding of magnetic field
manipulation in 2D magnetic materials, this research opens new pathways in the
development of energy-efficient spintronics technology.","Chee Kian Yap, Arun Kumar Singh",2025-03-27T05:32:09Z,2025-03-27T05:32:09Z,http://arxiv.org/abs/2503.22750v1,http://arxiv.org/pdf/2503.22750v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2025
"$π$/4 phase shift in the angular magnetoresistance of infinite layer
  nickelates","The discovery of superconductivity in nickelates has generated significant
interest in condensed matter physics. Nickelate superconductors, which are
hole-doped within the layered structure of RNiO$_2$, share structural
similarities with high-$T_c$ cuprate superconductors. However, despite
similarities in formal valence and crystal symmetry, the fundamental nature of
the superconducting state and the parent compound phase in nickelates remains
elusive. Strong electronic correlations in infinite-layer nickelates suggest a
potentially complex phase diagram, akin to that observed in cuprates, yet a key
question about the magnetic ground state remains unanswered. Through
magnetoresistance measurements across varying field strengths and orientations,
we observe distinct angular-dependent magnetoresistance (AMR) oscillations with
four-fold symmetry. Notably, this four-fold symmetry displays a $\pi/4$ phase
shift with doping or applied magnetic field. Our findings parallel behaviors in
electron-doped cuprates, suggesting that a static or quasi-static magnetic
order exists in the infinite-layer nickelates, echoing characteristics of
electron-doped cuprates. Furthermore, our modeling of the system reveals that
the AMR is directly related to the underlying antiferromagnetic order,
reinforcing this interpretation.","Yoav Mairovich, Ariel Matzliach, Idan S. Wallerstein, Himadri R. Dakua, Eran Maniv, Eytan Grosfeld, Muntaser Naamneh",2025-03-25T21:11:08Z,2025-03-25T21:11:08Z,http://arxiv.org/abs/2503.20070v1,http://arxiv.org/pdf/2503.20070v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2025
"Localized Heating and Dynamics of the Solar Corona due to a Symbiosis of
  Waves and Reconnection","The Sun's outer atmosphere, the corona, is maintained at mega-Kelvin
temperatures and fills the heliosphere with a supersonic outflowing wind. The
dissipation of magnetic waves and direct electric currents are likely to be the
most significant processes for heating the corona, but a lively debate exists
on their relative roles. Here, we suggest that the two are often intrinsically
linked, since magnetic waves may trigger current dissipation, and impulsive
reconnection can launch magnetic waves. We present a study of the first of
these processes by using a 2D physics-based numerical simulation using the
Adaptive Mesh Refined (AMR) Versatile Advection Code (VAC). Magnetic waves such
as fast magnetoacoustic waves are often observed to propagate in the
large-scale corona and interact with local magnetic structures. The present
numerical simulations show how the propagation of magnetic disturbances towards
a null point or separator can lead to the accumulation of the electric
currents. Lorentz forces can laterally push and vertically stretch the magnetic
fields, forming a current sheet with a strong magnetic-field gradient. The
magnetic field lines then break and reconnect, and so contribute towards
coronal heating. Numerical results are presented that support these ideas and
support the concept of a symbiosis between waves and reconnection in heating
the solar corona.","A. K. Srivastava, Sripan Mondal, Eric R. Priest, Sudheer K. Mishra, David I. Pontin, R. Y. Kwon, Ding Yuan, K. Murawski, Ayumi Asai",2025-03-20T16:23:08Z,2025-03-20T16:23:08Z,http://arxiv.org/abs/2503.16300v1,http://arxiv.org/pdf/2503.16300v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2025
"AMR-Transformer: Enabling Efficient Long-range Interaction for Complex
  Neural Fluid Simulation","Accurately and efficiently simulating complex fluid dynamics is a challenging
task that has traditionally relied on computationally intensive methods. Neural
network-based approaches, such as convolutional and graph neural networks, have
partially alleviated this burden by enabling efficient local feature
extraction. However, they struggle to capture long-range dependencies due to
limited receptive fields, and Transformer-based models, while providing global
context, incur prohibitive computational costs. To tackle these challenges, we
propose AMR-Transformer, an efficient and accurate neural CFD-solving pipeline
that integrates a novel adaptive mesh refinement scheme with a Navier-Stokes
constraint-aware fast pruning module. This design encourages long-range
interactions between simulation cells and facilitates the modeling of global
fluid wave patterns, such as turbulence and shockwaves. Experiments show that
our approach achieves significant gains in efficiency while preserving critical
details, making it suitable for high-resolution physical simulations with
long-range dependencies. On CFDBench, PDEBench and a new shockwave dataset, our
pipeline demonstrates up to an order-of-magnitude improvement in accuracy over
baseline models. Additionally, compared to ViT, our approach achieves a
reduction in FLOPs of up to 60 times.","Zeyi Xu, Jinfan Liu, Kuangxu Chen, Ye Chen, Zhangli Hu, Bingbing Ni",2025-03-13T11:16:42Z,2025-03-13T11:16:42Z,http://arxiv.org/abs/2503.10257v1,http://arxiv.org/pdf/2503.10257v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2025
"Astrophysical properties of star clusters projected toward tidally
  perturbed SMC regions","We report on the astrophysical properties of a sample of star clusters in the
Small Magellanic Cloud (SMC). They have been selected with the aim of looking
for the connection between their ages, heliocentric distances and metallicities
with the existence of tidally perturbed/induced outermost SMC regions. We
derived the star cluster fundamental parameters from relatively deep Survey of
the Magellanic Stellar History (SMASH) DR2 color magnitude diagrams, cleaned
from field star contamination, and compared to thousand synthetic CMDs covering
a wide range of heliocentric distances, ages and metal content. Heliocentric
distances for 15 star clusters are derived for the first time, which represents
an increase of 50 per cent of SMC clusters with estimated heliocentric
distances. The analysis of the age-metallicity relationships (AMRs) of cluster
located in outermost regions distributed around the SMC and in the SMC Main
Body reveals that they have followed the overall galaxy chemical enrichment
history. However, since half of the studied clusters are placed in front of or
behind the SMC Main Body, we concluded that they formed in the SMC and have
traveled outward because of the tidal effects from the interaction with the
Large Magellanic Cloud (LMC). Furthermore, metal rich clusters formed recently
in some of these outermost regions from gas that was also dragged by tidal
effects from the inner SMC. This outcome leads to consider the SMC as a galaxy
scarred by the LMC tidal interaction with distance-perturbed and newly induced
outermost stellar substructures.","Denis M. F. Illesca, Andrés E. Piatti, Matías Chiarpotti, Roberto Butrón",2025-03-11T15:25:11Z,2025-03-11T15:25:11Z,http://arxiv.org/abs/2503.08535v1,http://arxiv.org/pdf/2503.08535v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2025
"MCLRL: A Multi-Domain Contrastive Learning with Reinforcement Learning
  Framework for Few-Shot Modulation Recognition","With the rapid advancements in wireless communication technology, automatic
modulation recognition (AMR) plays a critical role in ensuring communication
security and reliability. However, numerous challenges, including higher
performance demands, difficulty in data acquisition under specific scenarios,
limited sample size, and low-quality labeled data, hinder its development.
Few-shot learning (FSL) offers an effective solution by enabling models to
achieve satisfactory performance with only a limited number of labeled samples.
While most FSL techniques are applied in the field of computer vision, they are
not directly applicable to wireless signal processing. This study does not
propose a new FSL-specific signal model but introduces a framework called
MCLRL. This framework combines multi-domain contrastive learning with
reinforcement learning. Multi-domain representations of signals enhance feature
richness, while integrating contrastive learning and reinforcement learning
architectures enables the extraction of deep features for classification. In
downstream tasks, the model achieves excellent performance using only a few
samples and minimal training cycles. Experimental results show that the MCLRL
framework effectively extracts key features from signals, performs well in FSL
tasks, and maintains flexibility in signal model selection.","Dongwei Xu, Yutao Zhu, Yao Lu, Youpeng Feng, Yun Lin, Qi Xuan",2025-02-26T11:53:31Z,2025-02-26T11:53:31Z,http://arxiv.org/abs/2502.19071v1,http://arxiv.org/pdf/2502.19071v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2025
"Anisotropic galvanomagnetic effects in single-crystal Fe(001) films
  elucidated by a phenomenological theory","Utilizing the phenomenological theory based on crystal symmetry operation, we
have established the complete angular dependencies of the galvanomagnetic
effects, encompassing both anisotropic magnetoresistance (AMR) and the planar
Hall effect (PHE), for the ferromagnetic films with C4v symmetry. These
dependencies were experimentally confirmed via comprehensive angular-mapping of
AMR and PHE in single-crystal Fe(001) films at room temperature. We
demonstrated that the intrinsic magnetization-induced effects are independent
of the field strength by carefully separating the field-induced and
magnetization-induced galvanomagnetic effects. Our theoretical and experimental
findings highlight the absence of in-plane four-fold angular dependence in PHE,
a feature prohibited by the Onsager relation in systems with C4 symmetry. This
study affirms that the universal angular dependencies of AMR and PHE in single
crystals can be accurately predicted by the conventional phenomenological
theory.","Haoran Chen, Zhen Cheng, Yizi Feng, Hongyue Xu, Tong Wu, Chuanhang Chen, Yue Chen, Zhe Yuan, Yizheng Wu",2025-01-28T08:43:53Z,2025-01-28T08:43:53Z,http://arxiv.org/abs/2501.16791v1,http://arxiv.org/pdf/2501.16791v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2025
"Efficient Frame Extraction: A Novel Approach Through Frame Similarity
  and Surgical Tool Tracking for Video Segmentation","The interest in leveraging Artificial Intelligence (AI) for surgical
procedures to automate analysis has witnessed a significant surge in recent
years. One of the primary tools for recording surgical procedures and
conducting subsequent analyses, such as performance assessment, is through
videos. However, these operative videos tend to be notably lengthy compared to
other fields, spanning from thirty minutes to several hours, which poses a
challenge for AI models to effectively learn from them. Despite this challenge,
the foreseeable increase in the volume of such videos in the near future
necessitates the development and implementation of innovative techniques to
tackle this issue effectively. In this article, we propose a novel technique
called Kinematics Adaptive Frame Recognition (KAFR) that can efficiently
eliminate redundant frames to reduce dataset size and computation time while
retaining useful frames to improve accuracy. Specifically, we compute the
similarity between consecutive frames by tracking the movement of surgical
tools. Our approach follows these steps: $i)$ Tracking phase: a YOLOv8 model is
utilized to detect tools presented in the scene, $ii)$ Similarity phase:
Similarities between consecutive frames are computed by estimating variation in
the spatial positions and velocities of the tools, $iii$) Classification phase:
An X3D CNN is trained to classify segmentation. We evaluate the effectiveness
of our approach by analyzing datasets obtained through retrospective reviews of
cases at two referral centers. The newly annotated Gastrojejunostomy (GJ)
dataset covers procedures performed between 2017 and 2021, while the previously
annotated Pancreaticojejunostomy (PJ) dataset spans from 2011 to 2022 at the
same centers.","Huu Phong Nguyen, Shekhar Madhav Khairnar, Sofia Garces Palacios, Amr Al-Abbas, Melissa E. Hogg, Amer H. Zureikat, Patricio M. Polanco, Herbert Zeh III, Ganesh Sankaranarayanan",2025-01-19T19:36:09Z,2025-04-28T19:02:47Z,http://arxiv.org/abs/2501.11153v3,http://arxiv.org/pdf/2501.11153v3.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2025
"Anomalous and Planar Hall Effects in Cobalt-Holmium Thin Films Near
  Magnetic Sublattice Compensation","Metallic amorphous ferrimagnets derived from alloying 3d transition metals
with 4f electron rare earths host fascinating effects of compensation between
the 3d and 4f magnetic sublattices. Here, a detailed study of anisotropic
magnetoresistance (AMR), planar Hall effect (PHE) and anomalous Hall effect
(AHE) are reported on a series of CoHo thin films over a wide field temperature
phase space. Close to magnetic compensation temperature, the AHE loops show a
double sign reversal and signatures of spin flop transition at higher fields.
The AMR and PHE also display strong deviations from the classical angular
dependence seen in soft ferromagnets like permalloy as the angle between
in-plane current and magnetic field is scanned from 0 to 360 degrees. It is
argued that the non zero orbital angular momentum of Ho ions in the lattice and
stabilization of bubble domains below magnetic saturation may be responsible
for such features. Direct imaging of magnetic textures with X ray photoelectron
microscopy shows formation of stripe domain patterns in the regime of
sublattice compensation. Such stripes are likely to transform into magnetic
bubbles before full saturation is reached in a large magnetic field.","Ramesh C Budhani, Rajeev Nepal, Vinay Sharma, Jerzy Sadowski",2025-01-15T23:50:36Z,2025-01-15T23:50:36Z,http://arxiv.org/abs/2501.09206v1,http://arxiv.org/pdf/2501.09206v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2025
"Flow-field analysis and performance assessment of rotating detonation
  engines under different number of discrete inlet nozzles","This study explores in depth rotating detonation engines (RDEs) fueled by
premixed stoichiometric hydrogen/air mixtures through two-dimensional numerical
simulations including a detailed chemical kinetic mechanism. To model the
spatial reactant non-uniformities observed in practical RDE combustors, the
referred simulations incorporate different numbers of discrete inlet nozzles.
The primary focus here is to analyze the influence of reactant non-uniformities
on detonation combustion dynamics in RDEs. By systematically varying the number
of reactant injection nozzles (from 15 to 240), while maintaining a constant
total injection area, the study delves into how this variation influences the
behavior of rotating detonation waves (RDWs) and the associated overall flow
field structure. The numerical results obtained here reveal significant effects
of the number of inlets employed on both RDE stability (self-sustaining
detonation wave) and performance. RDE configurations with a lower number of
inlets exhibit a detonation front with chaotic behavior (pressure oscillations)
due to an increased amount of unburned gas ahead of the detonation wave. This
chaotic behavior can lead to the flame extinguishing or decreasing in
intensity, ultimately diminishing the engine's overall performance. Conversely,
RDE configurations with a higher number of inlets feature smoother detonation
propagations without chaotic transients, leading to more stable and reliable
performance metrics. This study uses high-fidelity numerical techniques such as
adaptive mesh refinement (AMR) and the PeleC compressible reacting flow solver.
This comprehensive approach enables a thorough evaluation of critical RDE
characteristics including detonation velocity, fuel mass flow rate, impulse,
thrust, and reverse pressure waves under varying reactant injection conditions.","Sebastian Valencia, Andres Mendiburu, Luis Bravo, Prashant Khare, Cesar Celis",2025-01-14T14:52:13Z,2025-01-14T14:52:13Z,http://arxiv.org/abs/2501.08171v1,http://arxiv.org/pdf/2501.08171v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2025
"Anomalous Magneto-transport and Anisotropic Multigap Superconductivity
  in Architecturally Misfit Layered System (PbS)$_{1.13}$TaS$_2$","Misfit-layered compounds, naturally occurring bulk heterostructures, present
a compelling alternative to artificially engineered ones, offering a unique
platform for exploring correlated phases and quantum phenomena. This study
investigates the magnetotransport and superconducting properties of the misfit
compound (PbS)$_{1.13}$TaS$_2$, comprising alternating PbS and 1$H$-TaS$_2$
layers. It exhibits distinctive transport properties, including a prominent
planar Hall effect and a four-fold oscillatory Butterfly-shaped anisotropic
magnetoresistance (AMR). Moreover, it shows multigap two-dimensional
superconductivity with an exceptionally high in-plane upper critical field,
exceeding the Pauli limit. The coexistence of unconventional superconductivity
and anomalous transport - two distinct quantum phenomena, within the same
material, suggests that misfit compounds provide an ideal platform for
realizing quantum effects in the two-dimensional limit of bulk crystals. This
opens the door to the development of simpler and more efficient quantum
devices.","Tarushi Agarwal, Chandan Patra, Poulami Manna, Shashank Srivastava, Priya Mishra, Suhani Sharma, Ravi Prakash Singh",2025-01-06T09:37:26Z,2025-01-06T09:37:26Z,http://arxiv.org/abs/2501.02876v1,http://arxiv.org/pdf/2501.02876v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2025
"Simultaneous achievement of record-breaking colossal magnetoresistance
  and angular magnetoresistance in an antiferromagnetic semiconductor EuSe2","Magnetoresistance effect lays the foundation for spintronics, magnetic
sensors and hard drives. The pursuit of magnetic materials with colossal
magnetoresistance (CMR) and/or angular magnetoresistance (AMR) has attracted
enduring research interest and extensive investigations over past decades. Here
we report on the discovery of field-induced record-breaking CMR of ~ -10^14 %
and AMR ~ 10^14% achieved simultaneously in an antiferromagnetic rare-earth
dichalcogenide EuSe2. Such intriguing observations are attributed to strong
magnetic anisotropy and magnetic-field induced antiferromagnetic to
ferromagnetic transition of the localized Eu2+ spins, which in turn closes the
bandgap by lifting the degeneracy of Se-5p bands near Fermi level. Our DFT
calculations perfectly replicate the experimental findings based on the
Brillouin function and carries transport model. The present work provides a
potential simple antiferromagnetic material for achieving angle-sensitive
spintronic devices.","Qingxin Dong, Pengtao Yang, Zhihao Liu, Yuzhi Wang, Ziyi Liu, Tong Shi, Zhaoming Tian, Jianping Sun, Yoshiya Uwatoko, Quansheng Wu, Genfu Chen, Bosen Wang, Jinguang Cheng",2024-12-23T14:10:26Z,2024-12-23T14:10:26Z,http://arxiv.org/abs/2412.17594v1,http://arxiv.org/pdf/2412.17594v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
A CNN Approach to Polygenic Risk Prediction of Kidney Stone Formation,"Kidney stones are a common and debilitating health issue, and genetic factors
play a crucial role in determining susceptibility. While Genome-Wide
Association Studies (GWAS) have identified numerous single nucleotide
polymorphisms (SNPs) linked to kidney stone risk, translating these findings
into effective clinical tools remains a challenge. In this study, we explore
the potential of deep learning techniques, particularly Convolutional Neural
Networks (CNNs), to enhance Polygenic Risk Score (PRS) models for predicting
kidney stone susceptibility. Using a curated dataset of kidney stone-associated
SNPs from a recent GWAS, we apply CNNs to model non-linear genetic interactions
and improve prediction accuracy. Our approach includes SNP selection, genotype
filtering, and model training using a dataset of 560 individuals, divided into
training and testing subsets. We compare our CNN-based model with traditional
machine learning models, including logistic regression, random forest, and
support vector machines, demonstrating that the CNN outperforms these models in
terms of classification accuracy and ROC-AUC. The proposed model achieved a
validation accuracy of 62%, with an ROC-AUC of 0.68, suggesting its potential
for improving genetic-based risk prediction for kidney stones. This study
contributes to the growing field of genomics-driven precision medicine and
highlights the promise of deep learning in enhancing PRS models for complex
diseases.","Amr Salem, Anirban Mondal",2024-12-23T13:28:05Z,2024-12-23T13:28:05Z,http://arxiv.org/abs/2412.17559v1,http://arxiv.org/pdf/2412.17559v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"Swept Volume-Aware Trajectory Planning and MPC Tracking for Multi-Axle
  Swerve-Drive AMRs","Multi-axle autonomous mobile robots (AMRs) are set to revolutionize the
future of robotics in logistics. As the backbone of next-generation solutions,
these robots face a critical challenge: managing and minimizing the swept
volume during turns while maintaining precise control. Traditional systems
designed for standard vehicles often struggle with the complex dynamics of
multi-axle configurations, leading to inefficiency and increased safety risk in
confined spaces. Our innovative framework overcomes these limitations by
combining swept volume minimization with Signed Distance Field (SDF) path
planning and model predictive control (MPC) for independent wheel steering.
This approach not only plans paths with an awareness of the swept volume but
actively minimizes it in real-time, allowing each axle to follow a precise
trajectory while significantly reducing the space the vehicle occupies. By
predicting future states and adjusting the turning radius of each wheel, our
method enhances both maneuverability and safety, even in the most constrained
environments. Unlike previous works, our solution goes beyond basic path
calculation and tracking, offering real-time path optimization with minimal
swept volume and efficient individual axle control. To our knowledge, this is
the first comprehensive approach to tackle these challenges, delivering
life-saving improvements in control, efficiency, and safety for multi-axle
AMRs. Furthermore, we will open-source our work to foster collaboration and
enable others to advance safer, more efficient autonomous systems.","Tianxin Hu, Shenghai Yuan, Ruofei Bai, Xinghang Xu, Yuwen Liao, Fen Liu, Lihua Xie",2024-12-22T06:11:02Z,2025-02-11T14:28:05Z,http://arxiv.org/abs/2412.16875v2,http://arxiv.org/pdf/2412.16875v2.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"Stochastic Analysis of Entanglement-assisted Quantum Communication
  Channels","In this paper, we present a queueing model for quantum communication
networks, a rapidly growing field of research inspired by its technological
promise and recent experimental successes. The model consists of a primary
queue and a service queue where Bell pairs are formed and stored. The Bell
pairs are by nature extremely short-lived rendering the service queue (the
quantum queue) much faster than the primary queue. We study the asymptotic
behaviour of this multi-scale queueing system utilizing the theory of
stochastic averaging principle. We prove a Functional Law of Large Numbers
(FLLN) and a Functional Central Limit Theorem (FCLT) for the standard queue
averaging the dynamics of the fast service queue. Our proofs are probablistic
and rely on the stochastic analysis of Stochastic Differential Equations (SDEs)
driven by Poisson Random Measures.","Karim S. Elsayed, Olga Izyumtseva, Wasiur R. KhudaBukhsh, Amr Rizk",2024-12-20T18:59:58Z,2024-12-20T18:59:58Z,http://arxiv.org/abs/2412.16157v1,http://arxiv.org/pdf/2412.16157v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
High-order AMR in two-dimensional magnetic monolayers from spin mixing,"Anisotropic magnetoresistance (AMR) is a well-known magnetoelectric coupling
phenomenon, commonly exhibiting two-fold symmetry relative to the magnetic
field. In this study, we reveal the existence of high-order AMRs in
two-dimensional (2D) magnetic monolayers. Based on density functional theory
(DFT) calculations of Fe3GeTe2 and CrTe2 monolayers, we find that different
energy bands contribute uniquely to AMR behavior. The high-order AMR is
attributed to strong spin mixing at band crossing points, which induces
significant Berry curvature. This curvature also contributes to the AMR for
electrons with dominant spin-up or spin-down polarization characteristics.
However, for electrons exhibiting strong spin mixing, the Berry curvature
effect becomes nontrivial, resulting in high-order AMR. Our findings provide an
effective approach to identifying and optimizing materials with high-order AMR,
which is critical for designing high-performance spintronic devices.","M. Q. Dong, Zhi-Xin Guo",2024-12-11T16:19:33Z,2024-12-11T16:19:33Z,http://arxiv.org/abs/2412.08509v1,http://arxiv.org/pdf/2412.08509v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"Four-fold Anisotropic Magnetoresistance in Antiferromagnetic Epitaxial
  Thin Films of MnPt$_{x}$Pd$_{1-x}$","Antiferromagnets are emerging as promising alternatives to ferromagnets in
spintronics applications. A key feature of antiferromagnets is their
anisotropic magnetoresistance (AMR), which has the potential to serve as a
sensitive marker for the antiferromagnetic order parameter. However, the
underlying origins of this behavior remains poorly understood, particularly, in
thin film geometries. In this study, we report the observation of AMR in
epitaxial thin films of the collinear L1$_{0}$ antiferromagnet
MnPt$_{x}$Pd$_{1-x}$. In the thicker films, AMR is dominated by a
non-crystalline two-fold component, which emerges from domain reconfiguration
and spin canting under applied magnetic field. As the film thickness is
reduced, however, a crystalline four-fold component emerges, accompanied by the
appearance of uncompensated magnetic moment, which strongly modifies the
magnetotransport properties in the thinner films. We demonstrate that
interfacial interactions lead to a large density of states (DOS) at the Fermi
level. This enhanced DOS, combined with disorder in the thinner films,
stabilizes the uncompensated moment and results in a four-fold modulation of
the DOS as the Neel vector rotates, explaining the observed AMR behavior.","Shivesh Yadav, Shikhar Kumar Gupta, Mohit Verma, Debjoty Paul, Abira Rashid, Bhagyashree Chalke, Rudheer Bapat, Nilesh Kulkarni, Abhay Gautam, Arti Kashyap, Shouvik Chatterjee",2024-12-05T14:42:05Z,2025-06-08T07:14:30Z,http://arxiv.org/abs/2412.04207v2,http://arxiv.org/pdf/2412.04207v2.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"AdaptoML-UX: An Adaptive User-centered GUI-based AutoML Toolkit for
  Non-AI Experts and HCI Researchers","The increasing integration of machine learning across various domains has
underscored the necessity for accessible systems that non-experts can utilize
effectively. To address this need, the field of automated machine learning
(AutoML) has developed tools to simplify the construction and optimization of
ML pipelines. However, existing AutoML solutions often lack efficiency in
creating online pipelines and ease of use for Human-Computer Interaction (HCI)
applications. Therefore, in this paper, we introduce AdaptoML-UX, an adaptive
framework that incorporates automated feature engineering, machine learning,
and incremental learning to assist non-AI experts in developing robust,
user-centered ML models. Our toolkit demonstrates the capability to adapt
efficiently to diverse problem domains and datasets, particularly in HCI,
thereby reducing the necessity for manual experimentation and conserving time
and resources. Furthermore, it supports model personalization through
incremental learning, customizing models to individual user behaviors. HCI
researchers can employ AdaptoML-UX
(\url{https://github.com/MichaelSargious/AdaptoML_UX}) without requiring
specialized expertise, as it automates the selection of algorithms, feature
engineering, and hyperparameter tuning based on the unique characteristics of
the data.","Amr Gomaa, Michael Sargious, Antonio Krüger",2024-10-22T22:52:14Z,2024-10-22T22:52:14Z,http://arxiv.org/abs/2410.17469v1,http://arxiv.org/pdf/2410.17469v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"Magnetic Field Simulation and Correlated Low-Frequency Noise Subtraction
  for an In-Orbit Demonstrator of Magnetic Measurements","In recent years, nanosatellites have revolutionized the space sector due to
their significant economic and time-saving advantages. As a result, they have
fostered the testing of advanced instruments intended for larger space science
missions. The case of MELISA is presented in this work. MELISA is a magnetic
measurement instrument which aims at demonstrating the in-orbit performance of
AMR sensors featuring dedicated noise reduction techniques at sub-millihertz
frequencies. Such low frequency ranges are relevant for future space-borne
gravitational wave detectors, where the local magnetic environment of the
satellite might yield a significant contribution to the overall noise budget of
the observatory. The demanding magnetic noise levels required for this
bandwidth, down to 0.1 mHz, make measurements arduous. To explore sensing
solutions within the H2020 European Commission Programme with the involvement
of ESA, the functional performance of MELISA-III will be validated in-orbit.
During operations, there is the possibility to measure the low-frequency
magnetic contribution stemming from orbiting the Earth's magnetic field,
impeding the characterization of the intrinsic performance of the sensor. With
the objective of minimizing excess noise during the in-flight operations, the
present research aims to simulate the environmental magnetic conditions in LEO
to identify and subtract undesired contributions to the measurements. The
in-orbit long-term magnetic fluctuations are replicated using a Helmholtz coil
system. A fluxgate magnetometer allows the correlation of the generated field
with the payload measurements, leading to the subsequent subtraction. Proving
the effect of this approach will facilitate the noise characterization of
magnetic sensors in LEO, paving the way for the in-orbit validation of
MELISA-III for use in magnetically demanding missions with long integration
times.","Cristian Maria-Moreno, Ignacio Mateos, Guillermo Pacheco-Ramos, Francisco Rivas, María-Ángeles Cifredo-Chacón, Ángel Quirós-Olozábal, José-María Guerrero-Rodríguez, Nikolaos Karnesis",2024-10-17T15:56:00Z,2024-10-17T15:56:00Z,http://arxiv.org/abs/2410.13692v1,http://arxiv.org/pdf/2410.13692v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"Modeling Turbulence in the Atmospheric Boundary Layer with Spectral
  Element and Finite Volume Methods","We present large-eddy-simulation (LES) modeling approaches for the simulation
of atmospheric boundary layer turbulence that are of direct relevance to wind
energy production. In this paper, we study a GABLS benchmark problem using
high-order spectral element code Nek5000/RS and a block-structured second-order
finite-volume code AMR-Wind which are supported under the DOE's Exascale
Computing Project (ECP) Center for Efficient Exascale Discretizations (CEED)
and ExaWind projects, respectively, targeting application simulations on
various acceleration-device based exascale computing platforms. As for
Nek5000/RS we demonstrate our newly developed subgrid-scale (SGS) models based
on mean-field eddy viscosity (MFEV), high-pass filter (HPF), and Smagorinsky
(SMG) with traction boundary conditions. For the traction boundary conditions,
a novel analytical approach is presented that solves for the surface friction
velocity and surface kinematic temperature flux. For AMR-Wind, standard SMG is
used and discussed in detail the traction boundary conditions for convergence.
We provide low-order statistics, convergence and turbulent structure analysis.
Verification and convergence studies were performed for both codes at various
resolutions and it was found that Nek5000/RS demonstrate convergence with
resolution for all ABL bulk parameters, including boundary layer and low level
jet (LLJ) height. Extensive comparisons are presented with simulation data from
the literature.","Ananias Tomboulides Matthew Churchfield, Paul Fischer, Michael Sprague, Misun Min",2024-09-30T18:40:14Z,2024-09-30T18:40:14Z,http://arxiv.org/abs/2410.00147v1,http://arxiv.org/pdf/2410.00147v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
Adaptive Mesh Refinement for Two-Phase Viscoelastic Fluid Mixture Models,"Multiphase flows are an important class of fluid flow and their study
facilitates the development of diverse applications in industrial, natural, and
biomedical systems. We consider a model that uses a continuum description of
both phases in which separate momentum equations are used for each phase along
with a co-incompressibility condition on the velocity fields. The resulting
system of equations poses numerical challenges due to the presence of multiple
non-linear terms and the co-incompressibility condition, and the resulting
fluid dynamics motivate the development of an adaptive mesh refinement (AMR)
technique to accurately capture regions of high stresses and large material
gradients while keeping computational costs low. We present an accurate,
robust, and efficient computational method for simulating multiphase mixtures
on adaptive grids, and utilize a multigrid solver to precondition the
saddle-point system. We demonstrate that the AMR discretization asymptotically
approaches second order accuracy in $L^1$, $L^2$ and $L^\infty$ norms. The
solver can accurately resolve sharp gradients in the solution and, with the
multigrid preconditioning strategy introduced herein, the linear solver
iterations are independent of grid spacing. Our AMR solver offers a major cost
savings benefit, providing up to ten fold speedup over a uniform grid in the
numerical experiments presented here, with greater speedup possible depending
on the problem set-up.","Bindi M. Nagda, Aaron Barrett, Boyce E. Griffith, Aaron L. Fogelson, Jian Du",2024-09-30T06:01:16Z,2025-08-01T05:11:04Z,http://arxiv.org/abs/2409.19974v3,http://arxiv.org/pdf/2409.19974v3.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
AthenaK: A Performance-Portable Version of the Athena++ AMR Framework,"We describe AthenaK: a new implementation of the Athena++ block-based
adaptive mesh refinement (AMR) framework using the Kokkos programming model.
Finite volume methods for Newtonian, special relativistic (SR), and general
relativistic (GR) hydrodynamics and magnetohydrodynamics (MHD), and
GR-radiation hydrodynamics and MHD, as well as a module for evolving Lagrangian
tracer or charged test particles (e.g., cosmic rays) are implemented using the
framework. In two companion papers we describe (1) a new solver for the
Einstein equations based on the Z4c formalism and (2) a GRMHD solver in
dynamical spacetimes also implemented using the framework, enabling new
applications in numerical relativity. By adopting Kokkos, the code can be run
on virtually any hardware, including CPUs, GPUs from multiple vendors, and
emerging ARM processors. AthenaK shows excellent performance and weak scaling,
achieving over one billion cell updates per second for hydrodynamics in
three-dimensions on a single NVIDIA Grace Hopper processor and with a typical
parallel efficiency of 80% on 65536 AMD GPUs on the OLCF Frontier system. Such
performance portability enables AthenaK to leverage modern exascale computing
systems for challenging applications in astrophysical fluid dynamics, numerical
relativity, and multimessenger astrophysics.","James M. Stone, Patrick D. Mullen, Drummond Fielding, Philipp Grete, Minghao Guo, Philipp Kempski, Elias R. Most, Christopher J. White, George N. Wong",2024-09-24T12:57:19Z,2024-09-24T12:57:19Z,http://arxiv.org/abs/2409.16053v1,http://arxiv.org/pdf/2409.16053v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"Adversarial Robustness in RGB-Skeleton Action Recognition: Leveraging
  Attention Modality Reweighter","Deep neural networks (DNNs) have been applied in many computer vision tasks
and achieved state-of-the-art (SOTA) performance. However, misclassification
will occur when DNNs predict adversarial examples which are created by adding
human-imperceptible adversarial noise to natural examples. This limits the
application of DNN in security-critical fields. In order to enhance the
robustness of models, previous research has primarily focused on the unimodal
domain, such as image recognition and video understanding. Although multi-modal
learning has achieved advanced performance in various tasks, such as action
recognition, research on the robustness of RGB-skeleton action recognition
models is scarce. In this paper, we systematically investigate how to improve
the robustness of RGB-skeleton action recognition models. We initially
conducted empirical analysis on the robustness of different modalities and
observed that the skeleton modality is more robust than the RGB modality.
Motivated by this observation, we propose the \formatword{A}ttention-based
\formatword{M}odality \formatword{R}eweighter (\formatword{AMR}), which
utilizes an attention layer to re-weight the two modalities, enabling the model
to learn more robust features. Our AMR is plug-and-play, allowing easy
integration with multimodal models. To demonstrate the effectiveness of AMR, we
conducted extensive experiments on various datasets. For example, compared to
the SOTA methods, AMR exhibits a 43.77\% improvement against PGD20 attacks on
the NTU-RGB+D 60 dataset. Furthermore, it effectively balances the differences
in robustness between different modalities.","Chao Liu, Xin Liu, Zitong Yu, Yonghong Hou, Huanjing Yue, Jingyu Yang",2024-07-29T13:15:51Z,2024-07-29T13:15:51Z,http://arxiv.org/abs/2407.19981v1,http://arxiv.org/pdf/2407.19981v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"Giant Anisotropic Magnetoresistance in Magnetic Monolayers CrPX3 (X = S,
  Se, Te) due to symmetry breaking between the in-plane and out-of-plane
  crystallographic axes","Anisotropic magnetoresistance (AMR) has a crucial feature for developing
highly sensitive sensors and innovative memory devices. While extensively
studied in bulk materials, AMR effects in these materials are typically weak.
Recent advancements indicate that two-dimensional (2D) van der Waals magnetic
materials possess unique magnetic properties, potentially including significant
AMR characteristics. In this study, we utilize density functional theory and
the Boltzmann transport equation to investigate AMR in magnetic monolayers
CrPX3 (X = S, Se, Te). Our findings reveal a substantially large AMR in these
2D magnetic compounds. This enhancement is attributed to magnetization
(M)-dependent spin-orbit coupling (SOC), arising from the broken symmetry
between in-plane and out-of-plane orientations. This results in significant
M-dependent band splitting and subsequent variations in electron velocity.
Additionally, we find that the M-dependent SOC is significantly enhanced by
increasing the atomic number of the chalcogen X in CrPX3, achieving an
exceptional 150% AMR in CrPTe3. Furthermore, our study demonstrates that AMR
can be effectively modulated by applying biaxial strain, resulting in a twofold
increase with a 4% strain. These findings propose a novel approach to enhancing
2D-based AMR spintronic devices, making a substantial contribution to the
field.","W. S. Hou, M. Q. Dong, X. ZHang, Zhi-Xin Guo",2024-07-15T04:43:09Z,2024-11-20T01:05:32Z,http://arxiv.org/abs/2407.10438v2,http://arxiv.org/pdf/2407.10438v2.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"Vortex-p: a Helmholtz-Hodge and Reynolds decomposition algorithm for
  particle-based simulations","Astrophysical turbulent flows display an intrinsically multi-scale nature,
making their numerical simulation and the subsequent analyses of simulated data
a complex problem. In particular, two fundamental steps in the study of
turbulent velocity fields are the Helmholtz-Hodge decomposition
(compressive+solenoidal; HHD) and the Reynolds decomposition (bulk+turbulent;
RD). These problems are relatively simple to perform numerically for
uniformly-sampled data, such as the one emerging from Eulerian, fix-grid
simulations; but their computation is remarkably more complex in the case of
non-uniformly sampled data, such as the one stemming from particle-based or
meshless simulations. In this paper, we describe, implement and test vortex-p,
a publicly available tool evolved from the vortex code, to perform both these
decompositions upon the velocity fields of particle-based simulations, either
from smoothed particle hydrodynamics (SPH), moving-mesh or meshless codes. The
algorithm relies on the creation of an ad-hoc adaptive mesh refinement (AMR)
set of grids, on which the input velocity field is represented. HHD is then
addressed by means of elliptic solvers, while for the RD we adapt an iterative,
multi-scale filter. We perform a series of idealised tests to assess the
accuracy, convergence and scaling of the code. Finally, we present some
applications of the code to various SPH and meshless finite-mass (MFM)
simulations of galaxy clusters performed with OpenGadget3, with different
resolutions and physics, to showcase the capabilities of the code.","David Vallés-Pérez, Susana Planelles, Vicent Quilis, Frederick Groth, Tirso Marin-Gilabert, Klaus Dolag",2024-07-02T18:00:02Z,2024-07-02T18:00:02Z,http://arxiv.org/abs/2407.02562v1,http://arxiv.org/pdf/2407.02562v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"LeapFrog: Getting the Jump on Multi-Scale Materials Simulations Using
  Machine Learning","The development of novel materials in recent years has been accelerated
greatly by the use of computational modelling techniques aimed at elucidating
the complex physics controlling microstructure formation in materials, the
properties of which control material function. One such technique is the phase
field method, a field theoretic approach that couples various thermophysical
fields to microscopic order parameter fields that track the phases of
microstructure. Phase field models are framed as multiple, non-linear, partial
differential equations, which are extremely challenging to compute efficiently.
Recent years have seen an explosion of computational algorithms aimed at
enhancing the efficiency of phase field simulations. One such technique,
adaptive mesh refinement (AMR), dynamically adapts numerical meshes to be
highly refined around steep spatial gradients of the PDE fields and coarser
where the fields are smooth. This reduces the number of computations per time
step significantly, thus reducing the total time of computation. What AMR
doesn't do is allow for adaptive time stepping. This work combines AMR with a
neural network algorithm that uses a U-Net with a Convolutional Long-Short Term
Memory (CLSTM) base to accelerate phase field simulations. Our neural network
algorithm is described in detail and tested in on simulations of directional
solidification of a dilute binary alloy, a paradigm that is highly practical
for its relevance to the solidification of alloys.","Damien Pinto, Michael Greenwood, Nikolas Provatas",2024-06-21T17:37:29Z,2024-08-02T01:33:05Z,http://arxiv.org/abs/2406.15326v2,http://arxiv.org/pdf/2406.15326v2.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
GR-Athena++: magnetohydrodynamical evolution with dynamical space-time,"We present a self-contained overview of GR-Athena++, a general-relativistic
magnetohydrodynamics (GRMHD) code, that incorporates treatment of dynamical
space-time, based on the recent work of (Daszuta+, 2021)[49] and (Cook+,
2023)[45].
  General aspects of the Athena++ framework we build upon, such as oct-tree
based, adaptive mesh refinement (AMR) and constrained transport, together with
our modifications, incorporating the Z4c formulation of numerical relativity,
judiciously coupled, enables GRMHD with dynamical space-times.
  Initial verification testing of GR-Athena++ is performed through benchmark
problems that involve isolated and binary neutron star space-times. This leads
to stable and convergent results. Gravitational collapse of a rapidly rotating
star through black hole formation is shown to be correctly handled. In the case
of non-rotating stars, magnetic field instabilities are demonstrated to be
correctly captured with total relative violation of the divergence-free
constraint remaining near machine precision.
  The use of AMR is show-cased through investigation of the Kelvin-Helmholtz
instability which is resolved at the collisional interface in a merger of
magnetised binary neutron stars.
  The underlying task-based computational model enables GR-Athena++ to achieve
strong scaling efficiencies above $80\%$ in excess of $10^5$ CPU cores and
excellent weak scaling up to $\sim 5 \times 10^5$ CPU cores in a realistic
production setup. GR-Athena++ thus provides a viable path towards robust
simulation of GRMHD flows in strong and dynamical gravity with exascale high
performance computational infrastructure.","Boris Daszuta, William Cook",2024-06-07T17:55:11Z,2024-06-07T17:55:11Z,http://arxiv.org/abs/2406.05126v1,http://arxiv.org/pdf/2406.05126v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"Quality-Aware Task Offloading for Cooperative Perception in Vehicular
  Edge Computing","Task offloading in Vehicular Edge Computing (VEC) can advance cooperative
perception (CP) to improve traffic awareness in Autonomous Vehicles. In this
paper, we propose the Quality-aware Cooperative Perception Task Offloading
(QCPTO) scheme. Q-CPTO is the first task offloading scheme that enhances
traffic awareness by prioritizing the quality rather than the quantity of
cooperative perception. Q-CPTO improves the quality of CP by curtailing
perception redundancy and increasing the Value of Information (VOI) procured by
each user. We use Kalman filters (KFs) for VOI assessment, predicting the next
movement of each vehicle to estimate its region of interest. The estimated VOI
is then integrated into the task offloading problem. We formulate the task
offloading problem as an Integer Linear Program (ILP) that maximizes the VOI of
users and reduces perception redundancy by leveraging the spatially diverse
fields of view (FOVs) of vehicles, while adhering to strict latency
requirements. We also propose the Q-CPTO-Heuristic (Q-CPTOH) scheme to solve
the task offloading problem in a time-efficient manner. Extensive evaluations
show that Q-CPTO significantly outperforms prominent task offloading schemes by
up to 14% and 20% in terms of response delay and traffic awareness,
respectively. Furthermore, Q-CPTO-H closely approaches the optimal solution,
with marginal gaps of up to 1.4% and 2.1% in terms of traffic awareness and the
number of collaborating users, respectively, while reducing the runtime by up
to 84%.","Amr M. Zaki, Sara A. Elsayed, Khalid Elgazzar, Hossam S. Hassanein",2024-05-31T02:56:24Z,2024-05-31T02:56:24Z,http://arxiv.org/abs/2405.20587v1,http://arxiv.org/pdf/2405.20587v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"Population III star formation in the presence of turbulence, magnetic
  fields and ionizing radiation feedback","Turbulence, magnetic fields and radiation feedback are key components that
shape the formation of stars, especially in the metal-free environments at high
redshifts where Population III stars form. Yet no 3D numerical simulations
exist that simultaneously take all of these into account. We present the first
suite of radiation-magnetohydrodynamics (RMHD) simulations of Population III
star formation using the adaptive mesh refinement (AMR) code FLASH as part of
the POPSICLE project. We include both turbulent magnetic fields and ionizing
radiation feedback coupled to primordial chemistry, and resolve the collapse of
primordial clouds down to few au. We find that dynamically strong magnetic
fields significantly slow down accretion onto protostars, while ionizing
feedback, as expected, is largely unable to weaken gas accretion at early
times. This is because the partially ionized H II region gets trapped near the
star due to insufficient radiative outputs from the star. The maximum stellar
mass in the HD and RHD simulations that only yield one star exceeds
$100\,\rm{M_{\odot}}$ within the first $5000\,\rm{yr}$. However, in the
corresponding MHD and RMHD runs, the maximum mass of Population III stars is
only $60\,\rm{M_{\odot}}$. In other realizations where we observe widespread
fragmentation leading to the formation of Population III star clusters, the
maximum stellar mass is further reduced by a factor of few due to
fragmentation-induced starvation. We thus show that magnetic fields are more
important than ionizing feedback in regulating the mass of the star during the
earliest stages of Population III star formation.","Piyush Sharda, Shyam H. Menon",2024-05-28T15:17:03Z,2025-05-16T09:24:45Z,http://arxiv.org/abs/2405.18265v2,http://arxiv.org/pdf/2405.18265v2.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"Exact symmetry conservation and automatic mesh refinement in discrete
  initial boundary value problems","We present a novel solution procedure for initial boundary value problems.
The procedure is based on an action principle, in which coordinate maps are
included as dynamical degrees of freedom. This reparametrization invariant
action is formulated in an abstract parameter space and an energy density scale
associated with the space-time coordinates separates the dynamics of the
coordinate maps and of the propagating fields. Treating coordinates as
dependent, i.e. dynamical quantities, offers the opportunity to discretize the
action while retaining all space-time symmetries and also provides the basis
for automatic adaptive mesh refinement (AMR). The presence of unbroken
space-time symmetries after discretization also ensures that the associated
continuum Noether charges remain exactly conserved. The presence of coordinate
maps in addition provides new freedom in the choice of boundary conditions. An
explicit numerical example for wave propagation in $1+1$ dimensions is
provided, using recently developed regularized summation-by-parts finite
difference operators.","Alexander Rothkopf, W. A. Horowitz, Jan Nordström",2024-04-29T13:15:22Z,2024-04-29T13:15:22Z,http://arxiv.org/abs/2404.18676v1,http://arxiv.org/pdf/2404.18676v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"Comparison of adaptive mesh refinement techniques for numerical weather
  prediction","This paper examines the application of adaptive mesh refinement (AMR) in the
field of numerical weather prediction (NWP). We implement and assess two
distinct AMR approaches and evaluate their performance through standard NWP
benchmarks. In both cases, we solve the fully compressible Euler equations,
fundamental to many non-hydrostatic weather models.
  The first approach utilizes oct-tree cell-based mesh refinement coupled with
a high-order discontinuous Galerkin method for spatial discretization. In the
second approach, we employ level-based AMR with the finite difference method.
Our study provides insights into the accuracy and benefits of employing these
AMR methodologies for the multi-scale problem of NWP. Additionally, we explore
essential properties including their impact on mass and energy conservation.
Moreover, we present and evaluate an AMR solution transfer strategy for the
tree-based AMR approach that is simple to implement, memory-efficient, and
ensures conservation for both flow in the box and sphere.
  Furthermore, we discuss scalability, performance portability, and the
practical utility of the AMR methodology within an NWP framework -- crucial
considerations in selecting an AMR approach. The current de facto standard for
mesh refinement in NWP employs a relatively simplistic approach of static
nested grids, either within a general circulation model or a separately
operated regional model with loose one-way synchronization. It is our hope that
this study will stimulate further interest in the adoption of AMR frameworks
like AMReX in NWP. These frameworks offer a triple advantage: a robust dynamic
AMR for tracking localized and consequential features such as tropical
cyclones, extreme scalability, and performance portability.","Daniel S. Abdi, Ann Almgren, Francis X. Giraldo, Isidora Jankov",2024-04-25T14:40:13Z,2024-04-25T14:40:13Z,http://arxiv.org/abs/2404.16648v1,http://arxiv.org/pdf/2404.16648v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"KI-GAN: Knowledge-Informed Generative Adversarial Networks for Enhanced
  Multi-Vehicle Trajectory Forecasting at Signalized Intersections","Reliable prediction of vehicle trajectories at signalized intersections is
crucial to urban traffic management and autonomous driving systems. However, it
presents unique challenges, due to the complex roadway layout at intersections,
involvement of traffic signal controls, and interactions among different types
of road users. To address these issues, we present in this paper a novel model
called Knowledge-Informed Generative Adversarial Network (KI-GAN), which
integrates both traffic signal information and multi-vehicle interactions to
predict vehicle trajectories accurately. Additionally, we propose a specialized
attention pooling method that accounts for vehicle orientation and proximity at
intersections. Based on the SinD dataset, our KI-GAN model is able to achieve
an Average Displacement Error (ADE) of 0.05 and a Final Displacement Error
(FDE) of 0.12 for a 6-second observation and 6-second prediction cycle. When
the prediction window is extended to 9 seconds, the ADE and FDE values are
further reduced to 0.11 and 0.26, respectively. These results demonstrate the
effectiveness of the proposed KI-GAN model in vehicle trajectory prediction
under complex scenarios at signalized intersections, which represents a
significant advancement in the target field.","Chuheng Wei, Guoyuan Wu, Matthew J. Barth, Amr Abdelraouf, Rohit Gupta, Kyungtae Han",2024-04-17T08:53:59Z,2024-04-19T14:28:00Z,http://arxiv.org/abs/2404.11181v2,http://arxiv.org/pdf/2404.11181v2.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
Improved Moving-Puncture Techniques for Compact Binary Simulations,"To fully unlock the scientific potential of upcoming gravitational wave (GW)
interferometers, numerical relativity (NR) simulation accuracy will need to be
greatly enhanced. We present three infrastructure-agnostic improvements to the
moving-puncture approach for binary black hole (BBH) simulations, aimed at
greatly reducing constraint violation and improving GW predictions. Although
these improvements were developed within the highly efficient NR code
BlackHoles@Home, we demonstrate their effectiveness in the widely-adopted
Einstein Toolkit/Carpet AMR framework. Our improvements include a modified
Kreiss-Oliger dissipation prescription, a Hamiltonian-constraint-damping
adjustment to the BSSN equations, and an extra term to the 1+log lapse
evolution equation that slows the development of the sharp lapse feature, which
dominates numerical errors in BBH simulations. With minimal increase in
computational cost, these improvements greatly reduce GW noise, enabling the
extraction of high-order GW modes previously obscured by numerical noise. They
also improve convergence properties near and inside the convergent regime,
reduce Hamiltonian (momentum) constraint violations in the strong-field region
by roughly two (three) orders of magnitude, and in the GW-extraction zone by
five (two) orders of magnitude. To promote community adoption, we have
open-sourced the improved Einstein Toolkit thorn BaikalVacuum used in this
work. Although our focus is on BBH evolutions and the BSSN formulation, these
improvements may also benefit compact binary simulations involving matter and
other formulations, a focus for future investigations.",Zachariah B. Etienne,2024-04-01T14:33:13Z,2024-09-13T18:04:05Z,http://arxiv.org/abs/2404.01137v3,http://arxiv.org/pdf/2404.01137v3.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"Carrier confinement and alloy disorder exacerbate Auger-Meitner
  recombination in AlGaN ultraviolet light-emitting diodes","The quantum efficiency of AlGaN ultraviolet light-emitting diodes (LEDs)
declines (droops) at increasing operating powers due to Auger-Meitner
recombination (AMR). Using first-principles density-functional theory, we show
that indirect AMR mediated by electron-phonon coupling and alloy disorder can
induce bulk $C$ coefficients as large as $\sim10^{-31}$ cm$^6$/s. Furthermore,
we find that the confinement of carriers by polarization fields within quantum
wells severely relaxes crystal-momentum conservation, which exacerbates the
rate of AMR over radiative recombination by an order of magnitude relative to
the bulk. This results in a striking decrease in quantum efficiency at high
power. Suppressing polarization fields and jointly increasing the well width
would greatly mitigate AMR and efficiency droop.","Nick Pant, Kyle Bushick, Andrew McAllister, Woncheol Lee, Chris G. Van de Walle, Emmanouil Kioupakis",2024-03-16T21:14:35Z,2024-03-16T21:14:35Z,http://arxiv.org/abs/2403.11019v1,http://arxiv.org/pdf/2403.11019v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"BHAC-QGP: three-dimensional MHD simulations of relativistic heavy-ion
  collisions, I. Methods and tests","We present BHAC-QGP, a new numerical code to simulate the evolution of matter
created in heavy-ion collisions in the presence of electromagnetic fields. It
is derived from the Black Hole Accretion Code (BHAC), which has been designed
to model astrophysical processes in a general-relativistic
magnetohydrodynamical description. As the original Black Hole Accretion Code,
BHAC-QGP benefits from the use of Adaptive Mesh Refinement (AMR), which allows
us to dynamically adjust the resolution where necessary, and makes use of
time-dependent Milne coordinates and the ultrarelativistic equation of state,
$P = e/3$. We demonstrate that BHAC-QGP accurately passes a number of
systematic and rigorous tests.","Markus Mayer, Ashutosh Dash, Gabriele Inghirami, Hannah Elfner, Luciano Rezzolla, Dirk H. Rischke",2024-03-13T16:20:27Z,2024-10-04T17:58:31Z,http://arxiv.org/abs/2403.08668v2,http://arxiv.org/pdf/2403.08668v2.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"Fragmentation of Dense Rotation-Dominated Structures Fed by Collapsing
  Gravomagneto-Sheetlets and Origin of Misaligned 100 au-Scale Binaries and
  Multiple Systems","The majority of stars are in binary/multiple systems. How such systems form
in turbulent, magnetized cores of molecular clouds in the presence of non-ideal
MHD effects remains relatively under-explored. Through ATHENA++-based non-ideal
MHD AMR simulations with ambipolar diffusion, we show that the collapsing
protostellar envelope is dominated by dense gravo-magneto-sheetlets, a
turbulence-warped version of the classic pseudodisk produced by anisotropic
magnetic resistance to the gravitational collapse, in agreement with previous
simulations of turbulent, magnetized single-star formation. The sheetlets feed
mass, magnetic fields, and angular momentum to a Dense ROtation-Dominated
(DROD) structure, which fragments into binary/multiple systems. This DROD
fragmentation scenario is a more dynamic variant of the traditional disk
fragmentation scenario for binary/multiple formation, with dense spiral
filaments created by inhomogeneous feeding from the highly structured
larger-scale sheetlets rather than the need for angular momentum transport,
which is dominated by magnetic braking. Provided that the local material is
sufficiently demagnetized, with a plasma-$\beta$ of 10 or more, collisions
between the dense spiraling filaments play a key role in facilitating
gravitational collapse and stellar companion formation by pushing the local
magnetic Toomre parameter $Q_\mathrm{m}$ below unity. This mechanism can
naturally produce {\it in situ} misaligned systems on the 100-au scale, often
detected with high-resolution Atacama Large Millimeter Array (ALMA)
observations. Our simulations also highlight the importance of non-ideal MHD
effects, which affect whether fragmentation occurs and, if so, the masses and
orbital parameters of the stellar companions formed.","Yisheng Tu, Zhi-Yun Li, Zhaohuan Zhu, Chun-Yen Hsu",2024-03-12T16:01:18Z,2025-07-10T16:40:22Z,http://arxiv.org/abs/2403.07777v3,http://arxiv.org/pdf/2403.07777v3.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
Take Your Best Shot: Sampling-Based Planning for Autonomous Photography,"Autonomous mobile robots (AMRs) equipped with high-quality cameras have
revolutionized the field of inspections by providing efficient and
cost-effective means of conducting surveys. The use of autonomous inspection is
becoming more widespread in a variety of contexts, yet it is still challenging
to acquire the best inspection information autonomously. In situations where
objects may block a robot's view, it is necessary to use reasoning to determine
the optimal points for collecting data. Although researchers have explored
cloud-based applications to store inspection data, these applications may not
operate optimally under network constraints, and parsing these datasets can be
manually intensive. Instead, there is an emerging requirement for AMRs to
autonomously capture the most informative views efficiently. To address this
challenge, we present an autonomous Next-Best-View (NBV) framework that
maximizes the inspection information while reducing the number of pictures
needed during operations. The framework consists of a formalized evaluation
metric using ray-tracing and Gaussian process interpolation to estimate
information reward based on the current understanding of the partially-known
environment. A derivative-free optimization (DFO) method is used to sample
candidate views in the environment and identify the NBV point. The proposed
approach's effectiveness is shown by comparing it with existing methods and
further validated through simulations and experiments with various vehicles.","Shijie Gao, Lauren Bramblett, Nicola Bezzo",2024-03-08T17:38:55Z,2025-05-17T22:33:52Z,http://arxiv.org/abs/2403.05477v2,http://arxiv.org/pdf/2403.05477v2.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"Multiscale graph neural networks with adaptive mesh refinement for
  accelerating mesh-based simulations","Mesh-based Graph Neural Networks (GNNs) have recently shown capabilities to
simulate complex multiphysics problems with accelerated performance times.
However, mesh-based GNNs require a large number of message-passing (MP) steps
and suffer from over-smoothing for problems involving very fine mesh. In this
work, we develop a multiscale mesh-based GNN framework mimicking a conventional
iterative multigrid solver, coupled with adaptive mesh refinement (AMR), to
mitigate challenges with conventional mesh-based GNNs. We use the framework to
accelerate phase field (PF) fracture problems involving coupled partial
differential equations with a near-singular operator due to near-zero modulus
inside the crack. We define the initial graph representation using all mesh
resolution levels. We perform a series of downsampling steps using Transformer
MP GNNs to reach the coarsest graph followed by upsampling steps to reach the
original graph. We use skip connectors from the generated embedding during
coarsening to prevent over-smoothing. We use Transfer Learning (TL) to
significantly reduce the size of training datasets needed to simulate different
crack configurations and loading conditions. The trained framework showed
accelerated simulation times, while maintaining high accuracy for all cases
compared to physics-based PF fracture model. Finally, this work provides a new
approach to accelerate a variety of mesh-based engineering multiphysics
problems","Roberto Perera, Vinamra Agrawal",2024-02-14T00:16:50Z,2024-02-14T00:16:50Z,http://arxiv.org/abs/2402.08863v1,http://arxiv.org/pdf/2402.08863v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"Orbital origin of fourfold anisotropic magnetoresistance in Dirac
  materials","Fourfold anisotropic magnetoresistance (AMR) have been widely observed in
quantum materials, but the underlying mechanisms remain poorly understood. Here
we find, in a variety of three-dimensional Dirac materials that can be
unifiedly described by the massive Dirac equation, the intrinsic orbital
magnetic moment of electrons vary synchronously with the magnetic field and
give rise to a {\pi} periodic correction to its velocity, further leading to
unusual fourfold AMR, dubbed orbital fourfold AMR. Our theory not only explains
the observation of fourfold AMR in bismuth but also uncovers the nature of the
dominant fourfold AMR in thin films of antiferromagnetic topological insulator
MnBi2Te4, which arises from the near cancellation of the twofold AMR from the
surface states and bulk states due to distinct spin-momentum lockings. Our work
provides a new mechanism for creation and manipulation of orbital fourfold AMR
in both conventional conductors and various topological insulators.","Daifeng Tu, Can Wang, Jianhui Zhou",2024-02-02T14:56:56Z,2025-07-21T23:21:14Z,http://arxiv.org/abs/2402.01470v2,http://arxiv.org/pdf/2402.01470v2.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"Angular Position Sensor Based on Anisotropic Magnetoresistive and
  Anomalous Nernst Effect","Magnetic position sensors find extensive applications in various industrial
sectors and consumer products. However, measuring angles in the full range of
0{\deg} to 360{\deg} in a wide field range using a single magnetic sensor
remains a challenge. Here, we propose a magnetic position sensor based on a
single Wheatstone bridge structure made from a single ferromagnetic layer. By
measuring the anisotropic magnetoresistance (AMR) signal from the bridge and
two sets of anomalous Nernst effect (ANE) signals from the transverse ports on
two perpendicular Wheatstone bridge arms concurrently, we show that it is
possible to achieve 0{\deg} to 360{\deg} angle detection using a single bridge
sensor. The combined use of AMR and ANE signals allows to achieve a mean angle
error in the range of 0.51{\deg} to 1.05{\deg} within a field range of 100 Oe
to 10,000 Oe.","Jiaqi Wang, Hang Xie, Yihong Wu",2024-01-30T04:26:53Z,2024-01-30T04:26:53Z,http://arxiv.org/abs/2401.16735v1,http://arxiv.org/pdf/2401.16735v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"The effect of AMR and grid stretching on the magnetized CME model in
  Icarus","Context. Coronal mass ejections (CMEs) are the main driver of solar wind
disturbances near Earth. When directed towards us, the internal magnetic field
of the CME can interact with the Earth`s magnetic field and cause geomagnetic
storms. In order to better predict and avoid damage coming from such events,
the optimized heliospheric model Icarus has been implemented. Aims. The impact
of a CME at Earth is greatly affected by its internal magnetic field structure.
The aim of this work is to enable modelling the evolution of the magnetic field
configuration of the CME throughout its propagation in Icarus. The focus of the
study is on the global magnetic structure of the CME and its evolution and
interaction with the solar wind. Methods. The magnetized CME model that is
implemented in Icarus is the Linear Force-Free Spheromak and is imported from
EUHFORIA. Advanced techniques, such as grid stretching and AMR are applied.
Different AMR levels are applied in order to obtain high resolution locally,
where needed. The results of all the simulations are compared in detail and the
wall-clock times of the simulations are provided. Results. The results from the
performed simulations are analyzed. The arrival time is better approximated by
the EUHFORIA simulation, with the CME shock arriving 1.6 and 1.09 hours later
than in the AMR level 4 and 5 simulations, respectively. The profile features
and variable strengths are best modelled by Icarus simulations with AMR level 4
and 5. Conclusions. The arrival time is closer to the observed time in the
EUHFORIA simulation, but the profiles of the different variables show more
features and details in the Icarus simulations. Considering the small
difference in the modelled results, and the large difference in computational
resources, the AMR level 4 simulation is considered to have performed the best.","Tinatin Baratashvili, Stefaan Poedts",2024-01-04T19:32:31Z,2024-02-06T13:07:24Z,http://arxiv.org/abs/2401.02504v2,http://arxiv.org/pdf/2401.02504v2.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"Enhancing Automatic Modulation Recognition through Robust Global Feature
  Extraction","Automatic Modulation Recognition (AMR) plays a crucial role in wireless
communication systems. Deep learning AMR strategies have achieved tremendous
success in recent years. Modulated signals exhibit long temporal dependencies,
and extracting global features is crucial in identifying modulation schemes.
Traditionally, human experts analyze patterns in constellation diagrams to
classify modulation schemes. Classical convolutional-based networks, due to
their limited receptive fields, excel at extracting local features but struggle
to capture global relationships. To address this limitation, we introduce a
novel hybrid deep framework named TLDNN, which incorporates the architectures
of the transformer and long short-term memory (LSTM). We utilize the
self-attention mechanism of the transformer to model the global correlations in
signal sequences while employing LSTM to enhance the capture of temporal
dependencies. To mitigate the impact like RF fingerprint features and channel
characteristics on model generalization, we propose data augmentation
strategies known as segment substitution (SS) to enhance the model's robustness
to modulation-related features. Experimental results on widely-used datasets
demonstrate that our method achieves state-of-the-art performance and exhibits
significant advantages in terms of complexity. Our proposed framework serves as
a foundational backbone that can be extended to different datasets. We have
verified the effectiveness of our augmentation approach in enhancing the
generalization of the models, particularly in few-shot scenarios. Code is
available at \url{https://github.com/AMR-Master/TLDNN}.","Yunpeng Qu, Zhilin Lu, Rui Zeng, Jintao Wang, Jian Wang",2024-01-02T06:31:24Z,2024-01-02T06:31:24Z,http://arxiv.org/abs/2401.01056v1,http://arxiv.org/pdf/2401.01056v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"Collaborative Optimization of the Age of Information under Partial
  Observability","The significance of the freshness of sensor and control data at the receiver
side, often referred to as Age of Information (AoI), is fundamentally
constrained by contention for limited network resources. Evidently, network
congestion is detrimental for AoI, where this congestion is partly self-induced
by the sensor transmission process in addition to the contention from other
transmitting sensors. In this work, we devise a decentralized AoI-minimizing
transmission policy for a number of sensor agents sharing capacity-limited,
non-FIFO duplex channels that introduce random delays in communication with a
common receiver. By implementing the same policy, however with no explicit
inter-agent communication, the agents minimize the expected AoI in this
partially observable system. We cater to the partial observability due to
random channel delays by designing a bootstrap particle filter that
independently maintains a belief over the AoI of each agent. We also leverage
mean-field control approximations and reinforcement learning to derive scalable
and optimal solutions for minimizing the expected AoI collaboratively.","Anam Tahir, Kai Cui, Bastian Alt, Amr Rizk, Heinz Koeppl",2023-12-20T12:34:54Z,2023-12-20T12:34:54Z,http://arxiv.org/abs/2312.12977v1,http://arxiv.org/pdf/2312.12977v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2023
A simple polynomial for a transposition over finite fields,"Let $q>2$, and let $a$ and $b$ be two elements of the finite field
$\mathbb{F}_q$ with $a\ne 0$. Carlitz represented the transposition $(0a)$ by a
polynomial of degree $(q-2)^3$. In this note, we represent the transposition
$(ab)$ by a polynomial of degree $q-2$. Also, we use this polynomial to
construct polynomials that represent permutations of finite local rings with
residue field $\mathbb{F}_q$.",Amr Ali Abdulkader Al-Maktry,2023-12-14T13:28:00Z,2023-12-14T13:28:00Z,http://arxiv.org/abs/2312.08921v1,http://arxiv.org/pdf/2312.08921v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2023
Regulating star formation in a magnetized disk galaxy,"We use high-resolution MHD simulations of isolated disk galaxies to
investigate the co-evolution of magnetic fields with a self-regulated,
star-forming interstellar medium (ISM). The simulations are conducted using the
Ramses AMR code on the standard Agora initial condition, with gas cooling, star
formation and feedback. We run galaxies with a variety of initial magnetic
field strengths. The fields grow rapidly and achieve approximate saturation
within 500 Myr, but at different levels. The galaxies reach a quasi-steady
state, with slowly declining star formation due to both gas consumption and
increases in the field strength at intermediate ISM densities. We connect this
behaviour to differences in the gas properties and overall structure of the
galaxies. In particular, strong fields limit feedback bubbles. Different cases
support the ISM using varying combinations of magnetic pressure, turbulence and
thermal energy. Magnetic support is closely linked to stellar feedback in the
case of initially weak fields but not for initially strong fields. The spatial
distribution of these supports is also different in each case, and this is
reflected in the stability of the gas disk. We relate this back to the overall
distribution of star formation in each case. We conclude that a weak initial
field can grow to produce a realistic model of a local disk galaxy, but
starting with typical field strengths will not.","Hector Robinson, James Wadsley",2023-10-23T18:01:04Z,2024-06-04T17:44:42Z,http://arxiv.org/abs/2310.15244v2,http://arxiv.org/pdf/2310.15244v2.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2023
"It's all about you: Personalized in-Vehicle Gesture Recognition with a
  Time-of-Flight Camera","Despite significant advances in gesture recognition technology, recognizing
gestures in a driving environment remains challenging due to limited and costly
data and its dynamic, ever-changing nature. In this work, we propose a
model-adaptation approach to personalize the training of a CNNLSTM model and
improve recognition accuracy while reducing data requirements. Our approach
contributes to the field of dynamic hand gesture recognition while driving by
providing a more efficient and accurate method that can be customized for
individual users, ultimately enhancing the safety and convenience of in-vehicle
interactions, as well as driver's experience and system trust. We incorporate
hardware enhancement using a time-of-flight camera and algorithmic enhancement
through data augmentation, personalized adaptation, and incremental learning
techniques. We evaluate the performance of our approach in terms of recognition
accuracy, achieving up to 90\%, and show the effectiveness of personalized
adaptation and incremental learning for a user-centered design.","Amr Gomaa, Guillermo Reyes, Michael Feld",2023-10-02T21:48:19Z,2023-10-02T21:48:19Z,http://arxiv.org/abs/2310.01659v1,http://arxiv.org/pdf/2310.01659v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2023
"Experimental and numerical investigation to elucidate the fluid flow
  through packed beds with structured particle packings","The present paper presents an experimental and numerical investigation of the
dispersion of the gaseous jet flow and co-flow for the simple unit cell (SUC)
and body centered cubic (BCC) configuration of particles in packed beds. The
experimental setup is built in such a way, that suitable and simplified
boundary conditions are imposed for the corresponding numerical framework. The
SUC and BCC particle beds consist of 3D-printed spheres. The flow velocities
are analysed directly at the exit of the particle bed, for both beds for
particle Reynolds numbers of 200, 300, and 400. Stereo particle image
velocimetry (SPIV) is experimentally arranged in such a way, that the
velocities over the entire region at the exit of the packed bed are obtained
instantaneously. The numerical method consists of a state-of-the-art IBM with
AMR. The paper presents the pore jet structure and velocity field exiting each
pore for the SUC and BCC packed particle beds. The numerical and experimental
studies show a good agreement for the SUC configuration for all flow
velocities. For the BCC configuration, some differences can be observed in the
pore jet flow structure between the simulations and the experiments, but the
general flow velocity distribution shows a good overall agreement. The axial
velocity is generally higher for the pores located near the centre of the
packed bed than for the pores near the wall. In addition, the axial velocities
are observed to increase near the peripheral pores of the packed bed. This
behaviour is predominant for the BCC configuration as compared to the SUC
configuration. It is shown that both the experiments and the simulations can be
used to study the complex fluid structures inside a packed bed reactor.","Shirin Patil, Christian Gorges, Joel López-Bonilla, Moritz Stelter, Frank Beyrau, Berend van Wachem",2023-09-27T14:21:33Z,2023-09-27T14:21:33Z,http://arxiv.org/abs/2309.15677v1,http://arxiv.org/pdf/2309.15677v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2023
"Integration of Quantum Accelerators with High Performance Computing -- A
  Review of Quantum Programming Tools","Quantum computing (QC) introduces a novel mode of computation with the
possibility of greater computational power that remains to be exploited -
presenting exciting opportunities for high performance computing (HPC)
applications. However, recent advancements in the field have made clear that QC
does not supplant conventional HPC, but can rather be incorporated into current
heterogeneous HPC infrastructures as an additional accelerator, thereby
enabling the optimal utilization of both paradigms. The desire for such
integration significantly affects the development of software for quantum
computers, which in turn influences the necessary software infrastructure. To
date, previous review papers have investigated various quantum programming
tools (QPTs) (such as languages, libraries, frameworks) in their ability to
program, compile, and execute quantum circuits. However, the integration effort
with classical HPC frameworks or systems has not been addressed. This study
aims to characterize existing QPTs from an HPC perspective, investigating if
existing QPTs have the potential to be efficiently integrated with classical
computing models and determining where work is still required. This work
structures a set of criteria into an analysis blueprint that enables HPC
scientists to assess whether a QPT is suitable for the quantum-accelerated
classical application at hand.","Amr Elsharkawy, Xiao-Ting Michelle To, Philipp Seitz, Yanbin Chen, Yannick Stade, Manuel Geiger, Qunsheng Huang, Xiaorang Guo, Muhammad Arslan Ansari, Christian B. Mendl, Dieter Kranzlmüller, Martin Schulz",2023-09-12T12:24:12Z,2023-09-18T08:02:54Z,http://arxiv.org/abs/2309.06167v2,http://arxiv.org/pdf/2309.06167v2.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2023
SoccerNet 2023 Challenges Results,"The SoccerNet 2023 challenges were the third annual video understanding
challenges organized by the SoccerNet team. For this third edition, the
challenges were composed of seven vision-based tasks split into three main
themes. The first theme, broadcast video understanding, is composed of three
high-level tasks related to describing events occurring in the video
broadcasts: (1) action spotting, focusing on retrieving all timestamps related
to global actions in soccer, (2) ball action spotting, focusing on retrieving
all timestamps related to the soccer ball change of state, and (3) dense video
captioning, focusing on describing the broadcast with natural language and
anchored timestamps. The second theme, field understanding, relates to the
single task of (4) camera calibration, focusing on retrieving the intrinsic and
extrinsic camera parameters from images. The third and last theme, player
understanding, is composed of three low-level tasks related to extracting
information about the players: (5) re-identification, focusing on retrieving
the same players across multiple views, (6) multiple object tracking, focusing
on tracking players and the ball through unedited video streams, and (7) jersey
number recognition, focusing on recognizing the jersey number of players from
tracklets. Compared to the previous editions of the SoccerNet challenges, tasks
(2-3-7) are novel, including new annotations and data, task (4) was enhanced
with more data and annotations, and task (6) now focuses on end-to-end
approaches. More information on the tasks, challenges, and leaderboards are
available on https://www.soccer-net.org. Baselines and development kits can be
found on https://github.com/SoccerNet.","Anthony Cioppa, Silvio Giancola, Vladimir Somers, Floriane Magera, Xin Zhou, Hassan Mkhallati, Adrien Deliège, Jan Held, Carlos Hinojosa, Amir M. Mansourian, Pierre Miralles, Olivier Barnich, Christophe De Vleeschouwer, Alexandre Alahi, Bernard Ghanem, Marc Van Droogenbroeck, Abdullah Kamal, Adrien Maglo, Albert Clapés, Amr Abdelaziz, Artur Xarles, Astrid Orcesi, Atom Scott, Bin Liu, Byoungkwon Lim, Chen Chen, Fabian Deuser, Feng Yan, Fufu Yu, Gal Shitrit, Guanshuo Wang, Gyusik Choi, Hankyul Kim, Hao Guo, Hasby Fahrudin, Hidenari Koguchi, Håkan Ardö, Ibrahim Salah, Ido Yerushalmy, Iftikar Muhammad, Ikuma Uchida, Ishay Be'ery, Jaonary Rabarisoa, Jeongae Lee, Jiajun Fu, Jianqin Yin, Jinghang Xu, Jongho Nang, Julien Denize, Junjie Li, Junpei Zhang, Juntae Kim, Kamil Synowiec, Kenji Kobayashi, Kexin Zhang, Konrad Habel, Kota Nakajima, Licheng Jiao, Lin Ma, Lizhi Wang, Luping Wang, Menglong Li, Mengying Zhou, Mohamed Nasr, Mohamed Abdelwahed, Mykola Liashuha, Nikolay Falaleev, Norbert Oswald, Qiong Jia, Quoc-Cuong Pham, Ran Song, Romain Hérault, Rui Peng, Ruilong Chen, Ruixuan Liu, Ruslan Baikulov, Ryuto Fukushima, Sergio Escalera, Seungcheon Lee, Shimin Chen, Shouhong Ding, Taiga Someya, Thomas B. Moeslund, Tianjiao Li, Wei Shen, Wei Zhang, Wei Li, Wei Dai, Weixin Luo, Wending Zhao, Wenjie Zhang, Xinquan Yang, Yanbiao Ma, Yeeun Joo, Yingsen Zeng, Yiyang Gan, Yongqiang Zhu, Yujie Zhong, Zheng Ruan, Zhiheng Li, Zhijian Huang, Ziyu Meng",2023-09-12T07:03:30Z,2023-09-12T07:03:30Z,http://arxiv.org/abs/2309.06006v1,http://arxiv.org/pdf/2309.06006v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2023
"Gap and magnetic engineering via doping and pressure in tuning the
  colossal magnetoresistance in (Mn$_{1-x}$Mg$_x$)$_3$Si$_2$Te$_6$","Ferrimagnetic nodal-line semiconductor Mn$_3$Si$_2$Te$_6$ keeps the records
of colossal magnetoresistance (CMR) and angular magnetoresistance (AMR). Here
we report tuning the electronic transport properties via doping and pressure in
(Mn$_{1-x}$Mg$_x$)$_3$Si$_2$Te$_6$. As the substitution of nonmagnetic
Mg$^{2+}$ for magnetic Mn$^{2+}$, ferrimagnetic transition temperature $T_C$
gradually decreases, while the resistivity increases significantly. At the same
time, the CMR and AMR are both enhanced for the low-doping compositions (e.g.,
$x = 0.1$ and 0.2), which can be attributed to doping-induced broadening of the
band gap and a larger variation range of the resistivity when undergoing a
metal-insulator transition by applying a magnetic field along the $c$ axis. On
the contrary, $T_C$ rises with increasing pressure due to the enhancement of
the magnetic exchange interactions until a structural transition occurs at
$\sim$13 GPa. Meanwhile, the activation gap is lowered under pressure and the
magnetoresistance is decreased dramatically above 6 GPa where the gap is
closed. At 20 and 26 GPa, evidences for a superconducting transition at $\sim$5
K are observed. The results reveal that doping and pressure are effective
methods to tune the activation gap, and correspondingly, the CMR and AMR in
nodal-line semiconductors, providing an approach to investigate the
magnetoresistance materials for novel spintronic devices.","Chaoxin Huang, Mengwu Huo, Xing Huang, Hui Liu, Lisi Li, Ziyou Zhang, Zhiqiang Chen, Yifeng Han, Lan Chen, Feixiang Liang, Hongliang Dong, Bing Shen, Hualei Sun, Meng Wang",2023-09-12T03:50:05Z,2024-03-15T02:36:04Z,http://arxiv.org/abs/2309.05945v2,http://arxiv.org/pdf/2309.05945v2.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2023
"Adaptive User-centered Neuro-symbolic Learning for Multimodal
  Interaction with Autonomous Systems","Recent advances in machine learning, particularly deep learning, have enabled
autonomous systems to perceive and comprehend objects and their environments in
a perceptual subsymbolic manner. These systems can now perform object
detection, sensor data fusion, and language understanding tasks. However, there
is a growing need to enhance these systems to understand objects and their
environments more conceptually and symbolically. It is essential to consider
both the explicit teaching provided by humans (e.g., describing a situation or
explaining how to act) and the implicit teaching obtained by observing human
behavior (e.g., through the system's sensors) to achieve this level of powerful
artificial intelligence. Thus, the system must be designed with multimodal
input and output capabilities to support implicit and explicit interaction
models. In this position paper, we argue for considering both types of inputs,
as well as human-in-the-loop and incremental learning techniques, for advancing
the field of artificial intelligence and enabling autonomous systems to learn
like humans. We propose several hypotheses and design guidelines and highlight
a use case from related work to achieve this goal.","Amr Gomaa, Michael Feld",2023-09-11T19:35:12Z,2023-09-11T19:35:12Z,http://arxiv.org/abs/2309.05787v1,http://arxiv.org/pdf/2309.05787v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2023
Worldline Formulations of Covariant Fracton Theories,"We develop worldline formulations of covariant fracton gauge theories. These
are a one-parameter family of gauge theories of a rank-two symmetric tensor
field, invariant under a scalar gauge transformation involving a double
derivative. These theories, which can be interpreted as linearized gravity
theories invariant under longitudinal diffeomorphisms, provide a covariant
framework for studying Lorentz-breaking fracton quasiparticles, which are
excitations with restricted mobility due to dipole-moment conservation. We
construct three worldline models. The first two are obtained by deducing their
constraint structure directly from the spacetime gauge transformations. By
applying BRST quantization, we show that these models reproduce the BV spectrum
and the associated BRST transformations of two specific fracton theories. The
third model is defined as a deformation of the second one: although free, it is
analyzed by drawing inspiration from the standard treatment of interacting
worldline systems, and is shown to capture almost the entire family of
covariant fracton theories. Finally, we discuss the gauge-fixing, comparing the
BV-BRST spacetime perspective with the worldline analogue of the ``Siegel
gauge"" employed in string field theory.","Filippo Fecit, Davide Rovere",2025-08-20T10:15:31Z,2025-08-20T10:15:31Z,http://arxiv.org/abs/2508.14591v1,http://arxiv.org/pdf/2508.14591v1.pdf,all:field AND all:rover AND submittedDate:[202309062228 TO 202509052228],2025
"M3Depth: Wavelet-Enhanced Depth Estimation on Mars via Mutual Boosting
  of Dual-Modal Data","Depth estimation plays a great potential role in obstacle avoidance and
navigation for further Mars exploration missions. Compared to traditional
stereo matching, learning-based stereo depth estimation provides a data-driven
approach to infer dense and precise depth maps from stereo image pairs.
However, these methods always suffer performance degradation in environments
with sparse textures and lacking geometric constraints, such as the
unstructured terrain of Mars. To address these challenges, we propose M3Depth,
a depth estimation model tailored for Mars rovers. Considering the sparse and
smooth texture of Martian terrain, which is primarily composed of low-frequency
features, our model incorporates a convolutional kernel based on wavelet
transform that effectively captures low-frequency response and expands the
receptive field. Additionally, we introduce a consistency loss that explicitly
models the complementary relationship between depth map and surface normal map,
utilizing the surface normal as a geometric constraint to enhance the accuracy
of depth estimation. Besides, a pixel-wise refinement module with mutual
boosting mechanism is designed to iteratively refine both depth and surface
normal predictions. Experimental results on synthetic Mars datasets with depth
annotations show that M3Depth achieves a 16% improvement in depth estimation
accuracy compared to other state-of-the-art methods in depth estimation.
Furthermore, the model demonstrates strong applicability in real-world Martian
scenarios, offering a promising solution for future Mars exploration missions.","Junjie Li, Jiawei Wang, Miyu Li, Yu Liu, Yumei Wang, Haitao Xu",2025-05-20T10:13:00Z,2025-06-14T06:34:14Z,http://arxiv.org/abs/2505.14159v2,http://arxiv.org/pdf/2505.14159v2.pdf,all:field AND all:rover AND submittedDate:[202309062228 TO 202509052228],2025
Certifiably-Correct Mapping for Safe Navigation Despite Odometry Drift,"Accurate perception, state estimation and mapping are essential for safe
robotic navigation as planners and controllers rely on these components for
safety-critical decisions. However, existing mapping approaches often assume
perfect pose estimates, an unrealistic assumption that can lead to incorrect
obstacle maps and therefore collisions. This paper introduces a framework for
certifiably-correct mapping that ensures that the obstacle map correctly
classifies obstacle-free regions despite the odometry drift in vision-based
localization systems (VIO}/SLAM). By deflating the safe region based on the
incremental odometry error at each timestep, we ensure that the map remains
accurate and reliable locally around the robot, even as the overall odometry
error with respect to the inertial frame grows unbounded.
  Our contributions include two approaches to modify popular obstacle mapping
paradigms, (I) Safe Flight Corridors, and (II) Signed Distance Fields. We
formally prove the correctness of both methods, and describe how they integrate
with existing planning and control modules. Simulations using the Replica
dataset highlight the efficacy of our methods compared to state-of-the-art
techniques. Real-world experiments with a robotic rover show that, while
baseline methods result in collisions with previously mapped obstacles, the
proposed framework enables the rover to safely stop before potential
collisions.","Devansh R. Agrawal, Taekyung Kim, Rajiv Govindjee, Trushant Adeshara, Jiangbo Yu, Anurekha Ravikumar, Dimitra Panagou",2025-04-25T21:53:33Z,2025-04-25T21:53:33Z,http://arxiv.org/abs/2504.18713v1,http://arxiv.org/pdf/2504.18713v1.pdf,all:field AND all:rover AND submittedDate:[202309062228 TO 202509052228],2025
"Field Report on Ground Penetrating Radar for Localization at the Mars
  Desert Research Station","In this field report, we detail the lessons learned from our field expedition
to collect Ground Penetrating Radar (GPR) data in a Mars analog environment for
the purpose of validating GPR localization techniques in rugged environments.
Planetary rovers are already equipped with GPR for geologic subsurface
characterization. GPR has been successfully used to localize vehicles on Earth,
but it has not yet been explored as another modality for localization on a
planetary rover. Leveraging GPR for localization can aid in efficient and
robust rover pose estimation. In order to demonstrate localizing GPR in a Mars
analog environment, we collected over 50 individual survey trajectories during
a two-week period at the Mars Desert Research Station (MDRS). In this report,
we discuss our methodology, lessons learned, and opportunities for future work.","Anja Sheppard, Katherine A. Skinner",2025-04-21T21:50:09Z,2025-06-06T02:56:10Z,http://arxiv.org/abs/2504.15455v2,http://arxiv.org/pdf/2504.15455v2.pdf,all:field AND all:rover AND submittedDate:[202309062228 TO 202509052228],2025
In silico clinical trials in drug development: a systematic review,"In the context of clinical research, computational models have received
increasing attention over the past decades. In this systematic review, we aimed
to provide an overview of the role of so-called in silico clinical trials
(ISCTs) in medical applications. Exemplary for the broad field of clinical
medicine, we focused on in silico (IS) methods applied in drug development,
sometimes also referred to as model informed drug development (MIDD). We
searched PubMed and ClinicalTrials.gov for published articles and registered
clinical trials related to ISCTs. We identified 202 articles and 48 trials, and
of these, 76 articles and 19 trials were directly linked to drug development.
We extracted information from all 202 articles and 48 clinical trials and
conducted a more detailed review of the methods used in the 76 articles that
are connected to drug development. Regarding application, most articles and
trials focused on cancer and imaging-related research while rare and pediatric
diseases were only addressed in 14 articles and 5 trials, respectively. While
some models were informed combining mechanistic knowledge with clinical or
preclinical (in-vivo or in-vitro) data, the majority of models were fully
data-driven, illustrating that clinical data is a crucial part in the process
of generating synthetic data in ISCTs. Regarding reproducibility, a more
detailed analysis revealed that only 24% (18 out of 76) of the articles
provided an open-source implementation of the applied models, and in only 20%
of the articles the generated synthetic data were publicly available. Despite
the widely raised interest, we also found that it is still uncommon for ISCTs
to be part of a registered clinical trial and their application is restricted
to specific diseases leaving potential benefits of ISCTs not fully exploited.","Bohua Chen, Lucia Chantal Schneider, Christian Röver, Emmanuelle Comets, Markus Christian Elze, Andrew Hooker, Joanna IntHout, Anne-Sophie Jannot, Daria Julkowska, Yanis Mimouni, Marina Savelieva, Nigel Stallard, Moreno Ursino, Marc Vandemeulebroecke, Sebastian Weber, Martin Posch, Sarah Zohar, Tim Friede",2025-03-11T10:10:20Z,2025-08-04T20:59:46Z,http://arxiv.org/abs/2503.08746v3,http://arxiv.org/pdf/2503.08746v3.pdf,all:field AND all:rover AND submittedDate:[202309062228 TO 202509052228],2025
"Blind-Wayfarer: A Minimalist, Probing-Driven Framework for Resilient
  Navigation in Perception-Degraded Environments","Navigating autonomous robots through dense forests and rugged terrains is
especially daunting when exteroceptive sensors -- such as cameras and LiDAR
sensors -- fail under occlusions, low-light conditions, or sensor noise. We
present Blind-Wayfarer, a probing-driven navigation framework inspired by
maze-solving algorithms that relies primarily on a compass to robustly traverse
complex, unstructured environments. In 1,000 simulated forest experiments,
Blind-Wayfarer achieved a 99.7% success rate. In real-world tests in two
distinct scenarios -- with rover platforms of different sizes -- our approach
successfully escaped forest entrapments in all 20 trials. Remarkably, our
framework also enabled a robot to escape a dense woodland, traveling from 45 m
inside the forest to a paved pathway at its edge. These findings highlight the
potential of probing-based methods for reliable navigation in challenging
perception-degraded field conditions. Videos and code are available on our
website https://sites.google.com/view/blind-wayfarer","Yanran Xu, Klaus-Peter Zauner, Danesh Tarapore",2025-03-10T16:12:13Z,2025-03-10T16:12:13Z,http://arxiv.org/abs/2503.07492v1,http://arxiv.org/pdf/2503.07492v1.pdf,all:field AND all:rover AND submittedDate:[202309062228 TO 202509052228],2025
"Breadboarding the European Moon Rover System: discussion and results of
  the analogue field test campaign","This document compiles results obtained from the test campaign of the
European Moon Rover System (EMRS) project. The test campaign, conducted at the
Planetary Exploration Lab of DLR in Wessling, aimed to understand the scope of
the EMRS breadboard design, its strengths, and the benefits of the modular
design. The discussion of test results is based on rover traversal analyses,
robustness assessments, wheel deflection analyses, and the overall
transportation cost of the rover. This not only enables the comparison of
locomotion modes on lunar regolith but also facilitates critical
decision-making in the design of future lunar missions.","Cristina Luna, Augusto Gómez Eguíluz, Jorge Barrientos-Díez, Almudena Moreno, Alba Guerra, Manuel Esquer, Marina L. Seoane, Steven Kay, Angus Cameron, Carmen Camañes, Philipp Haas, Vassilios Papantoniou, Armin Wedler, Bernhard Rebele, Jennifer Reynolds, Markus Landgraf",2024-11-21T09:45:28Z,2024-11-21T09:45:28Z,http://arxiv.org/abs/2411.13978v1,http://arxiv.org/pdf/2411.13978v1.pdf,all:field AND all:rover AND submittedDate:[202309062228 TO 202509052228],2024
Field Assessment of Force Torque Sensors for Planetary Rover Navigation,"Proprioceptive sensors on planetary rovers serve for state estimation and for
understanding terrain and locomotion performance. While inertial measurement
units (IMUs) are widely used to this effect, force-torque sensors are less
explored for planetary navigation despite their potential to directly measure
interaction forces and provide insights into traction performance. This paper
presents an evaluation of the performance and use cases of force-torque sensors
based on data collected from a six-wheeled rover during tests over varying
terrains, speeds, and slopes. We discuss challenges, such as sensor signal
reliability and terrain response accuracy, and identify opportunities regarding
the use of these sensors. The data is openly accessible and includes
force-torque measurements from each of the six-wheel assemblies as well as IMU
data from within the rover chassis. This paper aims to inform the design of
future studies and rover upgrades, particularly in sensor integration and
control algorithms, to improve navigation capabilities.","Levin Gerdes, Carlos Pérez del Pulgar, Raúl Castilla Arquillo, Martin Azkarate",2024-11-07T13:34:37Z,2024-11-07T13:34:37Z,http://arxiv.org/abs/2411.04700v1,http://arxiv.org/pdf/2411.04700v1.pdf,all:field AND all:rover AND submittedDate:[202309062228 TO 202509052228],2024
Effective dose equivalent estimation for humans on Mars,"Exposure to cosmic radiation is a major concern in space exploration. On the
Martian surface, a complex radiation field is present, formed by a constant
influx of galactic cosmic radiation and the secondary particles produced by
their interaction with the planet's atmosphere and regolith. In this work, a
Martian environment model was developed using MCNP6 following the guidelines of
the 1st Mars Space Radiation Modeling Workshop. The accuracy of the model was
tested by comparing particle spectra and dose rate results with other model
results and measurements from the Radiation Assessment Detector (RAD) onboard
the Curiosity rover, taken between November 15, 2015, and January 15, 2016. The
ICRP's voxel-type computational phantoms were then implemented into the code.
Organ dose and effective dose equivalent were assessed for the same time
period. The viability of a mission on the surface of Mars for extended periods
of time under the assumed conditions was here investigated.","Miguel Ralha, Pedro Teles, Nuno Santos, Daniel Matthiä, Thomas Berger, Marta Cortesão",2024-09-03T15:48:14Z,2024-09-03T15:48:14Z,http://arxiv.org/abs/2409.02001v1,http://arxiv.org/pdf/2409.02001v1.pdf,all:field AND all:rover AND submittedDate:[202309062228 TO 202509052228],2024
Efficient Reinforcement Learning On Passive RRAM Crossbar Array,"The unprecedented growth in the field of machine learning has led to the
development of deep neuromorphic networks trained on labelled dataset with
capability to mimic or even exceed human capabilities. However, for
applications involving continuous decision making in unknown environments, such
as rovers for space exploration, robots, unmanned aerial vehicles, etc.,
explicit supervision and generation of labelled data set is extremely difficult
and expensive. Reinforcement learning (RL) allows the agents to take decisions
without any (human/external) supervision or training on labelled dataset.
However, the conventional implementations of RL on advanced digital CPUs/GPUs
incur a significantly large power dissipation owing to their inherent
von-Neumann architecture. Although crossbar arrays of emerging non-volatile
memories such as resistive (R)RAMs with their innate capability to perform
energy-efficient in situ multiply-accumulate operation appear promising for
Q-learning-based RL implementations, their limited endurance restricts their
application in practical RL systems with overwhelming weight updates. To
address this issue and realize the true potential of RRAM-based RL
implementations, in this work, for the first time, we perform an
algorithm-hardware co-design and propose a novel implementation of Monte Carlo
(MC) RL algorithm on passive RRAM crossbar array. We analyse the performance of
the proposed MC RL implementation on the classical cart-pole problem and
demonstrate that it not only outperforms the prior digital and active
1-Transistor-1-RRAM (1T1R)-based implementations by more than five orders of
magnitude in terms of area but is also robust against the spatial and temporal
variations and endurance failure of RRAMs.","Arjun Tyagi, Shubham Sahay",2024-07-11T07:38:14Z,2024-07-11T07:38:14Z,http://arxiv.org/abs/2407.08242v1,http://arxiv.org/pdf/2407.08242v1.pdf,all:field AND all:rover AND submittedDate:[202309062228 TO 202509052228],2024
Anomalies in Covariant Fracton Theories,"Covariant (Lorentz invariant) fracton matter, minimally coupled and charged
under a symmetric rank two gauge tensor, is considered. The gauge
transformations correspond to linearized longitudinal diffeomorphisms.
Consistent possible anomalies are computed using the BRST cohomology method.
They depend only on the gauge field, treated as a background field, and on the
gauge parameter, promoted to an anticommuting scalar ghost field. The problem
is phrased in terms of polyforms, whose total degree is the sum of the form
degree and of the ghost number. The most general anomaly in two dimensions and
in four dimensions is computed and an anomaly in arbitrary dimensions is
individuated. In conclusion, it is shown that a simple higher-derivative scalar
field theory is an example of covariant fracton matter.",Davide Rovere,2024-06-10T18:00:04Z,2024-10-16T14:44:11Z,http://arxiv.org/abs/2406.06686v2,http://arxiv.org/pdf/2406.06686v2.pdf,all:field AND all:rover AND submittedDate:[202309062228 TO 202509052228],2024
RAPF: Efficient path planning for lunar microrovers,"Efficient path planning is key for safe autonomous navigation over complex
and unknown terrains. Lunar Zebro (LZ), a project of the Delft University of
Technology, aims to deploy a compact rover, no larger than an A4 sheet of paper
and weighing not more than 3 kilograms. In this work, we introduce a Robust
Artificial Potential Field (RAPF) algorithm, a new path-planning algorithm for
reliable local navigation solution for lunar microrovers. RAPF leverages and
improves state of the art Artificial Potential Field (APF)-based methods by
incorporating the position of the robot in the generation of bacteria points
and considering local minima as regions to avoid. We perform both simulations
and on field experiments to validate the performance of RAPF, which outperforms
state-of-the-art APF-based algorithms by over 15% in reachability within a
similar or shorter planning time. The improvements resulted in a 200% higher
success rate and 50% lower computing time compared to the conventional APF
algorithm. Near-optimal paths are computed in real-time with limited available
processing power. The bacterial approach of the RAPF algorithm proves faster to
execute and smaller to store than path planning algorithms used in existing
planetary rovers, showcasing its potential for reliable lunar exploration with
computationally constrained and energy constrained robotic systems.","Thomas Manteaux, David Rodríguez-Martínez, Raj Thilak Rajan",2024-05-26T18:42:00Z,2024-05-26T18:42:00Z,http://arxiv.org/abs/2405.16659v1,http://arxiv.org/pdf/2405.16659v1.pdf,all:field AND all:rover AND submittedDate:[202309062228 TO 202509052228],2024
"Enhancing Rover Mobility Monitoring: Autoencoder-driven Anomaly
  Detection for Curiosity","Over eleven years into its mission, the Mars Science Laboratory remains vital
to NASA's Mars exploration. Safeguarding the rover's long-term functionality is
a top mission priority. In this study, we introduce and test undercomplete
autoencoder models for detecting drive anomalies, using telemetry data from
wheel actuators, the Rover Inertial Measurement Unit (RIMU), and the suspension
system. Our approach enhances post-drive data analysis during tactical downlink
sessions. We explore various model architectures and input features to
understand their impact on performance. Evaluating the models involves testing
them on unseen data to mimic real-world scenarios. Our experiments demonstrate
the undercomplete autoencoder model's effectiveness in detecting drive
anomalies within the Curiosity rover dataset. Remarkably, the model even
identifies subtle anomalous telemetry patterns missed by human operators.
Additionally, we provide insights into optimal design choices by comparing
different model architectures and input features. The model's ability to
capture inconspicuous anomalies, potentially indicating early-stage failures,
holds promise for the field, by improving the reliability and safety of future
planetary exploration missions through early anomaly detection and proactive
maintenance.","Mielad Sabzehi, Peter Rollins",2024-05-13T17:53:51Z,2024-05-13T17:53:51Z,http://arxiv.org/abs/2405.07982v1,http://arxiv.org/pdf/2405.07982v1.pdf,all:field AND all:rover AND submittedDate:[202309062228 TO 202509052228],2024
"ShadowNav: Autonomous Global Localization for Lunar Navigation in
  Darkness","The ability to determine the pose of a rover in an inertial frame
autonomously is a crucial capability necessary for the next generation of
surface rover missions on other planetary bodies. Currently, most on-going
rover missions utilize ground-in-the-loop interventions to manually correct for
drift in the pose estimate and this human supervision bottlenecks the distance
over which rovers can operate autonomously and carry out scientific
measurements. In this paper, we present ShadowNav, an autonomous approach for
global localization on the Moon with an emphasis on driving in darkness and at
nighttime. Our approach uses the leading edge of Lunar craters as landmarks and
a particle filtering approach is used to associate detected craters with known
ones on an offboard map. We discuss the key design decisions in developing the
ShadowNav framework for use with a Lunar rover concept equipped with a stereo
camera and an external illumination source. Finally, we demonstrate the
efficacy of our proposed approach in both a Lunar simulation environment and on
data collected during a field test at Cinder Lakes, Arizona.","Deegan Atha, R. Michael Swan, Abhishek Cauligi, Anne Bettens, Edwin Goh, Dima Kogan, Larry Matthies, Masahiro Ono",2024-05-02T18:59:53Z,2024-09-14T00:27:16Z,http://arxiv.org/abs/2405.01673v3,http://arxiv.org/pdf/2405.01673v3.pdf,all:field AND all:rover AND submittedDate:[202309062228 TO 202509052228],2024
"Mars 2020 Perseverance rover studies of the Martian atmosphere over
  Jezero from pressure measurements","The pressure sensors on Mars rover Perseverance measure the pressure field in
the Jezero crater on regular hourly basis starting in sol 15 after landing. The
present study extends up to sol 460 encompassing the range of solar longitudes
from Ls 13{\deg} - 241{\deg} (Martian Year (MY) 36). The data show the changing
daily pressure cycle, the sol-to-sol seasonal evolution of the mean pressure
field driven by the CO2 sublimation and deposition cycle at the poles, the
characterization of up to six components of the atmospheric tides and their
relationship to dust content in the atmosphere. They also show the presence of
wave disturbances with periods 2-5 sols, exploring their baroclinic nature,
short period oscillations (mainly at night-time) in the range 8-24 minutes that
we interpret as internal gravity waves, transient pressure drops with duration
1-150 s produced by vortices, and rapid turbulent fluctuations. We also analyze
the effects on pressure measurements produced by a regional dust storm over
Jezero at Ls 155{\deg}.","A. Sánchez-Lavega, T. del Rio-Gaztelurrutia, R. Hueso, M. de la Torre Juárez, G. M. Martínez, A. -M. Harri, M. Genzer, M. Hieta, J. Polkko, J. A. Rodríguez-Manfredi, M. T. Lemmon, J. Pla-García, D. Toledo, A. Vicente-Retortillo, Daniel Viúdez-Moreiras, A. Munguira, L. K. Tamppari, C. Newman, J. Gómez-Elvira, S. Guzewich, T. Bertrand, V. Apéstigue, I. Arruego, M. Wolff, D. Banfield, I. Jaakonaho, T. Mäkinen",2024-01-23T17:33:59Z,2024-01-23T17:33:59Z,http://arxiv.org/abs/2401.12931v1,http://arxiv.org/pdf/2401.12931v1.pdf,all:field AND all:rover AND submittedDate:[202309062228 TO 202509052228],2024
"Battery-Swapping Multi-Agent System for Sustained Operation of Large
  Planetary Fleets","We propose a novel, heterogeneous multi-agent architecture that miniaturizes
rovers by outsourcing power generation to a central hub. By delegating power
generation and distribution functions to this hub, the size, weight, power, and
cost (SWAP-C) per rover are reduced, enabling efficient fleet scaling. As these
rovers conduct mission tasks around the terrain, the hub charges an array of
replacement battery modules. When a rover requires charging, it returns to the
hub to initiate an autonomous docking sequence and exits with a fully charged
battery. This confers an advantage over direct charging methods, such as
wireless or wired charging, by replenishing a rover in minutes as opposed to
hours, increasing net rover uptime.
  This work shares an open-source platform developed to demonstrate battery
swapping on unknown field terrain. We detail our design methodologies utilized
for increasing system reliability, with a focus on optimization, robust
mechanical design, and verification. Optimization of the system is discussed,
including the design of passive guide rails through simulation-based
optimization methods which increase the valid docking configuration space by
258%. The full system was evaluated during integrated testing, where an average
servicing time of 98 seconds was achieved on surfaces with a gradient up to
10{\deg}. We conclude by briefly proposing flight considerations for advancing
the system toward a space-ready design. In sum, this prototype represents a
proof of concept for autonomous docking and battery transfer on field terrain,
advancing its Technology Readiness Level (TRL) from 1 to 3.","Ethan Holand, Jarrod Homer, Alex Storrer, Musheeera Khandeker, Ethan F. Muhlon, Maulik Patel, Ben-oni Vainqueur, David Antaki, Naomi Cooke, Chloe Wilson, Bahram Shafai, Nathaniel Hanson, Taşkın Padır",2024-01-16T16:57:05Z,2024-01-16T16:57:05Z,http://arxiv.org/abs/2401.08497v1,http://arxiv.org/pdf/2401.08497v1.pdf,all:field AND all:rover AND submittedDate:[202309062228 TO 202509052228],2024
Superconformal anomalies from superconformal Chern-Simons polynomials,"We consider the 4-dimensional $\mathcal{N}=1$ Lie superconformal algebra and
search for completely ""symmetric"" (in the graded sense) 3-index invariant
tensors. The solution we find is unique and we show that the corresponding
invariant polynomial cubic in the generalized curvatures of superconformal
gravity vanishes. Consequently, the associated Chern-Simons polynomial is a
non-trivial anomaly cocycle. We explicitly compute this cocycle to all orders
in the independent fields of superconformal gravity and establish that it is
BRST equivalent to the so-called superconformal $a$-anomaly. We briefly discuss
the possibility that the superconformal $c$-anomaly also admits a similar
Chern-Simons formulation and the potential holographic, 5-dimensional,
interpretation of our results.","Camillo Imbimbo, Davide Rovere, Alison Warman",2023-11-09T19:00:03Z,2024-05-30T11:07:52Z,http://arxiv.org/abs/2311.05684v3,http://arxiv.org/pdf/2311.05684v3.pdf,all:field AND all:rover AND submittedDate:[202309062228 TO 202509052228],2023
"Enabling In-Situ Resources Utilisation by leveraging collaborative
  robotics and astronaut-robot interaction","Space exploration and establishing human presence on other planets demand
advanced technology and effective collaboration between robots and astronauts.
Efficient space resource utilization is also vital for extraterrestrial
settlements. The Collaborative In-Situ Resources Utilisation (CISRU) project
has developed a software suite comprising five key modules. The first module
manages multi-agent autonomy, facilitating communication between agents and
mission control. The second focuses on environment perception, employing AI
algorithms for tasks like environment segmentation and object pose estimation.
The third module ensures safe navigation, covering obstacle avoidance, social
navigation with astronauts, and cooperation among robots. The fourth module
addresses manipulation functions, including multi-tool capabilities and
tool-changer design for diverse tasks in In-Situ Resources Utilization (ISRU)
scenarios. Finally, the fifth module controls cooperative behaviour,
incorporating astronaut commands, Mixed Reality interfaces, map fusion, task
supervision, and error control. The suite was tested using an astronaut-rover
interaction dataset in a planetary environment and GMV SPoT analogue
environments. Results demonstrate the advantages of E4 autonomy and AI in space
systems, benefiting astronaut-robot collaboration. This paper details CISRU's
development, field test preparation, and analysis, highlighting its potential
to revolutionize planetary exploration through AI-powered technology.","Silvia Romero-Azpitarte, Cristina Luna, Alba Guerra, Mercedes Alonso, Pablo Romeo Manrique, Marina L. Seoane, Daniel Olayo, Almudena Moreno, Pablo Castellanos, Fernando Gandía, Gianfranco Visentin",2023-11-06T14:43:03Z,2023-11-06T14:43:03Z,http://arxiv.org/abs/2311.03146v1,http://arxiv.org/pdf/2311.03146v1.pdf,all:field AND all:rover AND submittedDate:[202309062228 TO 202509052228],2023
"Modularity for lunar exploration: European Moon Rover System Pre-Phase A
  Design and Field Test Campaign Results","The European Moon Rover System (EMRS) Pre-Phase A activity is part of the
European Exploration Envelope Programme (E3P) that seeks to develop a versatile
surface mobility solution for future lunar missions. These missions include:
the Polar Explorer (PE), In-Situ Resource Utilization (ISRU), and Astrophysics
Lunar Observatory (ALO) and Lunar Geological Exploration Mission (LGEM).
Therefore, designing a multipurpose rover that can serve these missions is
crucial. The rover needs to be compatible with three different mission
scenarios, each with an independent payload, making flexibility the key driver.
This study focuses on modularity in the rover's locomotion solution and
autonomous on-board system. Moreover, the proposed EMRS solution has been
tested at an analogue facility to prove the modular mobility concept. The tests
involved the rover's mobility in a lunar soil simulant testbed and different
locomotion modes in a rocky and uneven terrain, as well as robustness against
obstacles and excavation of lunar regolith. As a result, the EMRS project has
developed a multipurpose modular rover concept, with power, thermal control,
insulation, and dust protection systems designed for further phases. This paper
highlights the potential of the EMRS system for lunar exploration and the
importance of modularity in rover design.","Cristina Luna, Jorge Barrientos-Díez, Manuel Esquer, Alba Guerra, Marina López-Seoane, Iñaki Colmenarejo, Fernando Gandía, Steven Kay, Angus Cameron, Carmen Camañes, Íñigo Sard, Danel Juárez, Alessandro Orlandi, Federica Angeletti, Vassilios Papantoniou, Ares Papantoniou, Spiros Makris, Bernhard rebele, Armin Wedler, Jennifer Reynolds, Markus Landgraf",2023-11-06T13:42:08Z,2023-11-06T13:42:08Z,http://arxiv.org/abs/2311.03098v1,http://arxiv.org/pdf/2311.03098v1.pdf,all:field AND all:rover AND submittedDate:[202309062228 TO 202509052228],2023
"Efficient Multi-Camera Tokenization with Triplanes for End-to-End
  Driving","Autoregressive Transformers are increasingly being deployed as end-to-end
robot and autonomous vehicle (AV) policy architectures, owing to their
scalability and potential to leverage internet-scale pretraining for
generalization. Accordingly, tokenizing sensor data efficiently is paramount to
ensuring the real-time feasibility of such architectures on embedded hardware.
To this end, we present an efficient triplane-based multi-camera tokenization
strategy that leverages recent advances in 3D neural reconstruction and
rendering to produce sensor tokens that are agnostic to the number of input
cameras and their resolution, while explicitly accounting for their geometry
around an AV. Experiments on a large-scale AV dataset and state-of-the-art
neural simulator demonstrate that our approach yields significant savings over
current image patch-based tokenization strategies, producing up to 72% fewer
tokens, resulting in up to 50% faster policy inference while achieving the same
open-loop motion planning accuracy and improved offroad rates in closed-loop
driving simulations.","Boris Ivanovic, Cristiano Saltori, Yurong You, Yan Wang, Wenjie Luo, Marco Pavone",2025-06-13T21:56:52Z,2025-07-21T17:22:35Z,http://arxiv.org/abs/2506.12251v2,http://arxiv.org/pdf/2506.12251v2.pdf,all:offroad AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"CREStE: Scalable Mapless Navigation with Internet Scale Priors and
  Counterfactual Guidance","We introduce CREStE, a scalable learning-based mapless navigation framework
to address the open-world generalization and robustness challenges of outdoor
urban navigation. Key to achieving this is learning perceptual representations
that generalize to open-set factors (e.g. novel semantic classes, terrains,
dynamic entities) and inferring expert-aligned navigation costs from limited
demonstrations. CREStE addresses both these issues, introducing 1) a visual
foundation model (VFM) distillation objective for learning open-set structured
bird's-eye-view perceptual representations, and 2) counterfactual inverse
reinforcement learning (IRL), a novel active learning formulation that uses
counterfactual trajectory demonstrations to reason about the most important
cues when inferring navigation costs. We evaluate CREStE on the task of
kilometer-scale mapless navigation in a variety of city, offroad, and
residential environments and find that it outperforms all state-of-the-art
approaches with 70% fewer human interventions, including a 2-kilometer mission
in an unseen environment with just 1 intervention; showcasing its robustness
and effectiveness for long-horizon mapless navigation. Videos and additional
materials can be found on the project page: https://amrl.cs.utexas.edu/creste","Arthur Zhang, Harshit Sikchi, Amy Zhang, Joydeep Biswas",2025-03-05T21:42:46Z,2025-06-26T06:42:04Z,http://arxiv.org/abs/2503.03921v2,http://arxiv.org/pdf/2503.03921v2.pdf,all:offroad AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"FusionForce: End-to-end Differentiable Neural-Symbolic Layer for
  Trajectory Prediction","We propose end-to-end differentiable model that predicts robot trajectories
on rough offroad terrain from camera images and/or lidar point clouds. The
model integrates a learnable component that predicts robot-terrain interaction
forces with a neural-symbolic layer that enforces the laws of classical
mechanics and consequently improves generalization on out-of-distribution data.
The neural-symbolic layer includes a differentiable physics engine that
computes the robot's trajectory by querying these forces at the points of
contact with the terrain. As the proposed architecture comprises substantial
geometrical and physics priors, the resulting model can also be seen as a
learnable physics engine conditioned on real sensor data that delivers $10^4$
trajectories per second. We argue and empirically demonstrate that this
architecture reduces the sim-to-real gap and mitigates out-of-distribution
sensitivity. The differentiability, in conjunction with the rapid simulation
speed, makes the model well-suited for various applications including model
predictive control, trajectory shooting, supervised and reinforcement learning,
or SLAM.","Ruslan Agishev, Karel Zimmermann",2025-02-14T13:36:00Z,2025-06-24T13:44:58Z,http://arxiv.org/abs/2502.10156v4,http://arxiv.org/pdf/2502.10156v4.pdf,all:offroad AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"A Multi-Loss Strategy for Vehicle Trajectory Prediction: Combining
  Off-Road, Diversity, and Directional Consistency Losses","Trajectory prediction is essential for the safety and efficiency of planning
in autonomous vehicles. However, current models often fail to fully capture
complex traffic rules and the complete range of potential vehicle movements.
Addressing these limitations, this study introduces three novel loss functions:
Offroad Loss, Direction Consistency Error, and Diversity Loss. These functions
are designed to keep predicted paths within driving area boundaries, aligned
with traffic directions, and cover a wider variety of plausible driving
scenarios. As all prediction modes should adhere to road rules and conditions,
this work overcomes the shortcomings of traditional ""winner takes all"" training
methods by applying the loss functions to all prediction modes. These loss
functions not only improve model training but can also serve as metrics for
evaluating the realism and diversity of trajectory predictions. Extensive
validation on the nuScenes and Argoverse 2 datasets with leading baseline
models demonstrates that our approach not only maintains accuracy but
significantly improves safety and robustness, reducing offroad errors on
average by 47% on original and by 37% on attacked scenes. This work sets a new
benchmark for trajectory prediction in autonomous driving, offering substantial
improvements in navigating complex environments. Our code is available at
https://github.com/vita-epfl/stay-on-track .","Ahmad Rahimi, Alexandre Alahi",2024-11-29T14:47:08Z,2024-11-29T14:47:08Z,http://arxiv.org/abs/2411.19747v1,http://arxiv.org/pdf/2411.19747v1.pdf,all:offroad AND all:robot AND submittedDate:[202309062228 TO 202509052228],2024
Excavating in the Wild: The GOOSE-Ex Dataset for Semantic Segmentation,"The successful deployment of deep learning-based techniques for autonomous
systems is highly dependent on the data availability for the respective system
in its deployment environment. Especially for unstructured outdoor
environments, very few datasets exist for even fewer robotic platforms and
scenarios. In an earlier work, we presented the German Outdoor and Offroad
Dataset (GOOSE) framework along with 10000 multimodal frames from an offroad
vehicle to enhance the perception capabilities in unstructured environments. In
this work, we address the generalizability of the GOOSE framework. To
accomplish this, we open-source the GOOSE-Ex dataset, which contains additional
5000 labeled multimodal frames from various completely different environments,
recorded on a robotic excavator and a quadruped platform. We perform a
comprehensive analysis of the semantic segmentation performance on different
platforms and sensor modalities in unseen environments. In addition, we
demonstrate how the combined datasets can be utilized for different downstream
applications or competitions such as offroad navigation, object manipulation or
scene completion. The dataset, its platform documentation and pre-trained
state-of-the-art models for offroad perception will be made available on
https://goose-dataset.de/.
  \","Raphael Hagmanns, Peter Mortimer, Miguel Granero, Thorsten Luettel, Janko Petereit",2024-09-27T14:36:20Z,2024-09-27T14:36:20Z,http://arxiv.org/abs/2409.18788v1,http://arxiv.org/pdf/2409.18788v1.pdf,all:offroad AND all:robot AND submittedDate:[202309062228 TO 202509052228],2024
PANOS: Payload-Aware Navigation in Offroad Scenarios,"Nature has evolved humans to walk on different terrains by developing a
detailed understanding of their physical characteristics. Similarly, legged
robots need to develop their capability to walk on complex terrains with a
variety of task-dependent payloads to achieve their goals. However,
conventional terrain adaptation methods are susceptible to failure with varying
payloads. In this work, we introduce PANOS, a weakly supervised approach that
integrates proprioception and exteroception from onboard sensing to achieve a
stable gait while walking by a legged robot over various terrains. Our work
also provides evidence of its adaptability over varying payloads. We evaluate
our method on multiple terrains and payloads using a legged robot. PANOS
improves the stability up to 44% without any payload and 53% with 15 lbs
payload. We also notice a reduction in the vibration cost of 20% with the
payload for various terrain types when compared to state-of-the-art methods.","Kartikeya Singh, Yash Turkar, Christo Aluckal, Charuvarahan Adhivarahan, Karthik Dantu",2024-09-25T02:36:22Z,2024-09-25T02:36:22Z,http://arxiv.org/abs/2409.16566v1,http://arxiv.org/pdf/2409.16566v1.pdf,all:offroad AND all:robot AND submittedDate:[202309062228 TO 202509052228],2024
"ReFeree: Radar-based efficient global descriptor using a Feature and
  Free space for Place Recognition","Radar is highlighted for robust sensing capabilities in adverse weather
conditions (e.g. dense fog, heavy rain, or snowfall). In addition, Radar can
cover wide areas and penetrate small particles. Despite these advantages,
Radar-based place recognition remains in the early stages compared to other
sensors due to its unique characteristics such as low resolution, and
significant noise. In this paper, we propose a Radarbased place recognition
utilizing a descriptor called ReFeree using a feature and free space. Unlike
traditional methods, we overwhelmingly summarize the Radar image. Despite being
lightweight, it contains semi-metric information and is also outstanding from
the perspective of place recognition performance. For concrete validation, we
test a single session from the MulRan dataset and a multi-session from the
Oxford Offroad Radar, Oxford Radar RobotCar, and the Boreas dataset.","Byunghee Choi, Hogyun Kim, Younggun Cho",2024-03-21T06:57:28Z,2024-07-18T02:31:26Z,http://arxiv.org/abs/2403.14176v4,http://arxiv.org/pdf/2403.14176v4.pdf,all:offroad AND all:robot AND submittedDate:[202309062228 TO 202509052228],2024
OORD: The Oxford Offroad Radar Dataset,"There is a growing academic interest as well as commercial exploitation of
millimetre-wave scanning radar for autonomous vehicle localisation and scene
understanding. Although several datasets to support this research area have
been released, they are primarily focused on urban or semi-urban environments.
Nevertheless, rugged offroad deployments are important application areas which
also present unique challenges and opportunities for this sensor technology.
Therefore, the Oxford Offroad Radar Dataset (OORD) presents data collected in
the rugged Scottish highlands in extreme weather. The radar data we offer to
the community are accompanied by GPS/INS reference - to further stimulate
research in radar place recognition. In total we release over 90GiB of radar
scans as well as GPS and IMU readings by driving a diverse set of four routes
over 11 forays, totalling approximately 154km of rugged driving. This is an
area increasingly explored in literature, and we therefore present and release
examples of recent open-sourced radar place recognition systems and their
performance on our dataset. This includes a learned neural network, the weights
of which we also release. The data and tools are made freely available to the
community at https://oxford-robotics-institute.github.io/oord-dataset.","Matthew Gadd, Daniele De Martini, Oliver Bartlett, Paul Murcutt, Matt Towlson, Matthew Widojo, Valentina Muşat, Luke Robinson, Efimia Panagiotaki, Georgi Pramatarov, Marc Alexander Kühn, Letizia Marchegiani, Paul Newman, Lars Kunze",2024-03-05T10:35:52Z,2024-05-25T20:13:47Z,http://arxiv.org/abs/2403.02845v2,http://arxiv.org/pdf/2403.02845v2.pdf,all:offroad AND all:robot AND submittedDate:[202309062228 TO 202509052228],2024
CAHSOR: Competence-Aware High-Speed Off-Road Ground Navigation in SE(3),"While the workspace of traditional ground vehicles is usually assumed to be
in a 2D plane, i.e., SE(2), such an assumption may not hold when they drive at
high speeds on unstructured off-road terrain: High-speed sharp turns on
high-friction surfaces may lead to vehicle rollover; Turning aggressively on
loose gravel or grass may violate the non-holonomic constraint and cause
significant lateral sliding; Driving quickly on rugged terrain will produce
extensive vibration along the vertical axis. Therefore, most offroad vehicles
are currently limited to drive only at low speeds to assure vehicle stability
and safety. In this work, we aim at empowering high-speed off-road vehicles
with competence awareness in SE(3) so that they can reason about the
consequences of taking aggressive maneuvers on different terrain with a 6-DoF
forward kinodynamic model. The model is learned from visual and inertial
Terrain Representation for Off-road Navigation (TRON) using multimodal,
self-supervised vehicle-terrain interactions. We demonstrate the efficacy of
our Competence-Aware High-Speed Off-Road (CAHSOR) navigation approach on a
physical ground robot in both an autonomous navigation and a human
shared-control setup and show that CAHSOR can efficiently reduce vehicle
instability by 62% while only compromising 8.6% average speed with the help of
TRON.","Anuj Pokhrel, Aniket Datar, Mohammad Nazeri, Xuesu Xiao",2024-02-10T23:35:33Z,2025-03-24T20:38:45Z,http://arxiv.org/abs/2402.07065v2,http://arxiv.org/pdf/2402.07065v2.pdf,all:offroad AND all:robot AND submittedDate:[202309062228 TO 202509052228],2024
"Pixel to Elevation: Learning to Predict Elevation Maps at Long Range
  using Images for Autonomous Offroad Navigation","Understanding terrain topology at long-range is crucial for the success of
off-road robotic missions, especially when navigating at high-speeds. LiDAR
sensors, which are currently heavily relied upon for geometric mapping, provide
sparse measurements when mapping at greater distances. To address this
challenge, we present a novel learning-based approach capable of predicting
terrain elevation maps at long-range using only onboard egocentric images in
real-time. Our proposed method is comprised of three main elements. First, a
transformer-based encoder is introduced that learns cross-view associations
between the egocentric views and prior bird-eye-view elevation map predictions.
Second, an orientation-aware positional encoding is proposed to incorporate the
3D vehicle pose information over complex unstructured terrain with multi-view
visual image features. Lastly, a history-augmented learn-able map embedding is
proposed to achieve better temporal consistency between elevation map
predictions to facilitate the downstream navigational tasks. We experimentally
validate the applicability of our proposed approach for autonomous offroad
robotic navigation in complex and unstructured terrain using real-world offroad
driving data. Furthermore, the method is qualitatively and quantitatively
compared against the current state-of-the-art methods. Extensive field
experiments demonstrate that our method surpasses baseline models in accurately
predicting terrain elevation while effectively capturing the overall terrain
topology at long-ranges. Finally, ablation studies are conducted to highlight
and understand the effect of key components of the proposed approach and
validate their suitability to improve offroad robotic navigation capabilities.","Chanyoung Chung, Georgios Georgakis, Patrick Spieler, Curtis Padgett, Ali Agha, Shehryar Khattak",2024-01-30T22:37:24Z,2024-04-20T21:14:15Z,http://arxiv.org/abs/2401.17484v3,http://arxiv.org/pdf/2401.17484v3.pdf,all:offroad AND all:robot AND submittedDate:[202309062228 TO 202509052228],2024
"ROAMER: Robust Offroad Autonomy using Multimodal State Estimation with
  Radar Velocity Integration","Reliable offroad autonomy requires low-latency, high-accuracy state estimates
of pose as well as velocity, which remain viable throughout environments with
sub-optimal operating conditions for the utilized perception modalities. As
state estimation remains a single point of failure system in the majority of
aspiring autonomous systems, failing to address the environmental degradation
the perception sensors could potentially experience given the operating
conditions, can be a mission-critical shortcoming. In this work, a method for
integration of radar velocity information in a LiDAR-inertial odometry solution
is proposed, enabling consistent estimation performance even with degraded
LiDAR-inertial odometry. The proposed method utilizes the direct
velocity-measuring capabilities of an Frequency Modulated Continuous Wave
(FMCW) radar sensor to enhance the LiDAR-inertial smoother solution onboard the
vehicle through integration of the forward velocity measurement into the
graph-based smoother. This leads to increased robustness in the overall
estimation solution, even in the absence of LiDAR data. This method was
validated by hardware experiments conducted onboard an all-terrain vehicle
traveling at high speed, ~12 m/s, in demanding offroad environments.","Morten Nissov, Shehryar Khattak, Jeffrey A. Edlund, Curtis Padgett, Kostas Alexis, Patrick Spieler",2024-01-30T19:46:26Z,2024-01-30T19:46:26Z,http://arxiv.org/abs/2401.17404v1,http://arxiv.org/pdf/2401.17404v1.pdf,all:offroad AND all:robot AND submittedDate:[202309062228 TO 202509052228],2024
Manipulating Trajectory Prediction with Backdoors,"Autonomous vehicles ought to predict the surrounding agents' trajectories to
allow safe maneuvers in uncertain and complex traffic situations. As companies
increasingly apply trajectory prediction in the real world, security becomes a
relevant concern. In this paper, we focus on backdoors - a security threat
acknowledged in other fields but so far overlooked for trajectory prediction.
To this end, we describe and investigate four triggers that could affect
trajectory prediction. We then show that these triggers (for example, a braking
vehicle), when correlated with a desired output (for example, a curve) during
training, cause the desired output of a state-of-the-art trajectory prediction
model. In other words, the model has good benign performance but is vulnerable
to backdoors. This is the case even if the trigger maneuver is performed by a
non-casual agent behind the target vehicle. As a side-effect, our analysis
reveals interesting limitations within trajectory prediction models. Finally,
we evaluate a range of defenses against backdoors. While some, like simple
offroad checks, do not enable detection for all triggers, clustering is a
promising candidate to support manual inspection to find backdoors.","Kaouther Messaoud, Kathrin Grosse, Mickael Chen, Matthieu Cord, Patrick Pérez, Alexandre Alahi",2023-12-21T14:01:51Z,2024-01-03T15:52:24Z,http://arxiv.org/abs/2312.13863v2,http://arxiv.org/pdf/2312.13863v2.pdf,all:offroad AND all:robot AND submittedDate:[202309062228 TO 202509052228],2023
The GOOSE Dataset for Perception in Unstructured Environments,"The potential for deploying autonomous systems can be significantly increased
by improving the perception and interpretation of the environment. However, the
development of deep learning-based techniques for autonomous systems in
unstructured outdoor environments poses challenges due to limited data
availability for training and testing. To address this gap, we present the
German Outdoor and Offroad Dataset (GOOSE), a comprehensive dataset
specifically designed for unstructured outdoor environments. The GOOSE dataset
incorporates 10 000 labeled pairs of images and point clouds, which are
utilized to train a range of state-of-the-art segmentation models on both image
and point cloud data. We open source the dataset, along with an ontology for
unstructured terrain, as well as dataset standards and guidelines. This
initiative aims to establish a common framework, enabling the seamless
inclusion of existing datasets and a fast way to enhance the perception
capabilities of various robots operating in unstructured environments. The
dataset, pre-trained models for offroad perception, and additional
documentation can be found at https://goose-dataset.de/.","Peter Mortimer, Raphael Hagmanns, Miguel Granero, Thorsten Luettel, Janko Petereit, Hans-Joachim Wuensche",2023-10-25T17:20:38Z,2023-10-25T17:20:38Z,http://arxiv.org/abs/2310.16788v1,http://arxiv.org/pdf/2310.16788v1.pdf,all:offroad AND all:robot AND submittedDate:[202309062228 TO 202509052228],2023
"Uncertainty-aware hybrid paradigm of nonlinear MPC and model-based RL
  for offroad navigation: Exploration of transformers in the predictive model","In this paper, we investigate a hybrid scheme that combines nonlinear model
predictive control (MPC) and model-based reinforcement learning (RL) for
navigation planning of an autonomous model car across offroad, unstructured
terrains without relying on predefined maps. Our innovative approach takes
inspiration from BADGR, an LSTM-based network that primarily concentrates on
environment modeling, but distinguishes itself by substituting LSTM modules
with transformers to greatly elevate the performance our model. Addressing
uncertainty within the system, we train an ensemble of predictive models and
estimate the mutual information between model weights and outputs, facilitating
dynamic horizon planning through the introduction of variable speeds. Further
enhancing our methodology, we incorporate a nonlinear MPC controller that
accounts for the intricacies of the vehicle's model and states. The model-based
RL facet produces steering angles and quantifies inherent uncertainty. At the
same time, the nonlinear MPC suggests optimal throttle settings, striking a
balance between goal attainment speed and managing model uncertainty influenced
by velocity. In the conducted studies, our approach excels over the existing
baseline by consistently achieving higher metric values in predicting future
events and seamlessly integrating the vehicle's kinematic model for enhanced
decision-making. The code and the evaluation data are available at
https://github.com/FARAZLOTFI/offroad_autonomous_navigation/).","Faraz Lotfi, Khalil Virji, Farnoosh Faraji, Lucas Berry, Andrew Holliday, David Meger, Gregory Dudek",2023-10-01T18:47:02Z,2023-10-01T18:47:02Z,http://arxiv.org/abs/2310.00760v1,http://arxiv.org/pdf/2310.00760v1.pdf,all:offroad AND all:robot AND submittedDate:[202309062228 TO 202509052228],2023
"CTBC: Contact-Triggered Blind Climbing for Wheeled Bipedal Robots with
  Instruction Learning and Reinforcement Learning","In recent years, wheeled bipedal robots have gained increasing attention due
to their advantages in mobility, such as high-speed locomotion on flat terrain.
However, their performance on complex environments (e.g., staircases) remains
inferior to that of traditional legged robots. To overcome this limitation, we
propose a general contact-triggered blind climbing (CTBC) framework for wheeled
bipedal robots. Upon detecting wheel-obstacle contact, the robot triggers a
leg-lifting motion to overcome the obstacle. By leveraging a strongly-guided
feedforward trajectory, our method enables the robot to rapidly acquire agile
leg-lifting skills, significantly enhancing its capability to traverse
unstructured terrains. The approach has been experimentally validated and
successfully deployed on LimX Dynamics' wheeled bipedal robot, Tron1.
Real-world tests demonstrate that Tron1 can reliably climb obstacles well
beyond its wheel radius using only proprioceptive feedback.","Rankun Li, Hao Wang, Qi Li, Zhuo Han, Yifei Chu, Linqi Ye, Wende Xie, Wenlong Liao",2025-09-03T03:46:43Z,2025-09-03T03:46:43Z,http://arxiv.org/abs/2509.02986v1,http://arxiv.org/pdf/2509.02986v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"DUViN: Diffusion-Based Underwater Visual Navigation via
  Knowledge-Transferred Depth Features","Autonomous underwater navigation remains a challenging problem due to limited
sensing capabilities and the difficulty of constructing accurate maps in
underwater environments. In this paper, we propose a Diffusion-based Underwater
Visual Navigation policy via knowledge-transferred depth features, named DUViN,
which enables vision-based end-to-end 4-DoF motion control for underwater
vehicles in unknown environments. DUViN guides the vehicle to avoid obstacles
and maintain a safe and perception awareness altitude relative to the terrain
without relying on pre-built maps. To address the difficulty of collecting
large-scale underwater navigation datasets, we propose a method that ensures
robust generalization under domain shifts from in-air to underwater
environments by leveraging depth features and introducing a novel model
transfer strategy. Specifically, our training framework consists of two phases:
we first train the diffusion-based visual navigation policy on in-air datasets
using a pre-trained depth feature extractor. Secondly, we retrain the extractor
on an underwater depth estimation task and integrate the adapted extractor into
the trained navigation policy from the first step. Experiments in both
simulated and real-world underwater environments demonstrate the effectiveness
and generalization of our approach. The experimental videos are available at
https://www.youtube.com/playlist?list=PLqt2s-RyCf1gfXJgFzKjmwIqYhrP4I-7Y.","Jinghe Yang, Minh-Quan Le, Mingming Gong, Ye Pu",2025-09-03T03:43:12Z,2025-09-03T03:43:12Z,http://arxiv.org/abs/2509.02983v1,http://arxiv.org/pdf/2509.02983v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Acrobotics: A Generalist Approahc To Quadrupedal Robots' Parkour,"Climbing, crouching, bridging gaps, and walking up stairs are just a few of
the advantages that quadruped robots have over wheeled robots, making them more
suitable for navigating rough and unstructured terrain. However, executing such
manoeuvres requires precise temporal coordination and complex agent-environment
interactions. Moreover, legged locomotion is inherently more prone to slippage
and tripping, and the classical approach of modeling such cases to design a
robust controller thus quickly becomes impractical. In contrast, reinforcement
learning offers a compelling solution by enabling optimal control through trial
and error. We present a generalist reinforcement learning algorithm for
quadrupedal agents in dynamic motion scenarios. The learned policy rivals
state-of-the-art specialist policies trained using a mixture of experts
approach, while using only 25% as many agents during training. Our experiments
also highlight the key components of the generalist locomotion policy and the
primary factors contributing to its success.","Guillaume Gagné-Labelle, Vassil Atanassov, Ioannis Havoutis",2025-09-02T18:23:17Z,2025-09-02T18:23:17Z,http://arxiv.org/abs/2509.02727v1,http://arxiv.org/pdf/2509.02727v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Disentangled Multi-Context Meta-Learning: Unlocking robust and
  Generalized Task Learning","In meta-learning and its downstream tasks, many methods rely on implicit
adaptation to task variations, where multiple factors are mixed together in a
single entangled representation. This makes it difficult to interpret which
factors drive performance and can hinder generalization. In this work, we
introduce a disentangled multi-context meta-learning framework that explicitly
assigns each task factor to a distinct context vector. By decoupling these
variations, our approach improves robustness through deeper task understanding
and enhances generalization by enabling context vector sharing across tasks
with shared factors. We evaluate our approach in two domains. First, on a
sinusoidal regression task, our model outperforms baselines on
out-of-distribution tasks and generalizes to unseen sine functions by sharing
context vectors associated with shared amplitudes or phase shifts. Second, in a
quadruped robot locomotion task, we disentangle the robot-specific properties
and the characteristics of the terrain in the robot dynamics model. By
transferring disentangled context vectors acquired from the dynamics model into
reinforcement learning, the resulting policy achieves improved robustness under
out-of-distribution conditions, surpassing the baselines that rely on a single
unified context. Furthermore, by effectively sharing context, our model enables
successful sim-to-real policy transfer to challenging terrains with
out-of-distribution robot-specific properties, using just 20 seconds of real
data from flat terrain, a result not achievable with single-task adaptation.","Seonsoo Kim, Jun-Gill Kang, Taehong Kim, Seongil Hong",2025-09-01T09:33:52Z,2025-09-01T09:33:52Z,http://arxiv.org/abs/2509.01297v1,http://arxiv.org/pdf/2509.01297v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"QuadKAN: KAN-Enhanced Quadruped Motion Control via End-to-End
  Reinforcement Learning","We address vision-guided quadruped motion control with reinforcement learning
(RL) and highlight the necessity of combining proprioception with vision for
robust control. We propose QuadKAN, a spline-parameterized cross-modal policy
instantiated with Kolmogorov-Arnold Networks (KANs). The framework incorporates
a spline encoder for proprioception and a spline fusion head for
proprioception-vision inputs. This structured function class aligns the
state-to-action mapping with the piecewise-smooth nature of gait, improving
sample efficiency, reducing action jitter and energy consumption, and providing
interpretable posture-action sensitivities. We adopt Multi-Modal Delay
Randomization (MMDR) and perform end-to-end training with Proximal Policy
Optimization (PPO). Evaluations across diverse terrains, including both even
and uneven surfaces and scenarios with static or dynamic obstacles, demonstrate
that QuadKAN achieves consistently higher returns, greater distances, and fewer
collisions than state-of-the-art (SOTA) baselines. These results show that
spline-parameterized policies offer a simple, effective, and interpretable
alternative for robust vision-guided locomotion. A repository will be made
available upon acceptance.","Allen Wang, Gavin Tao",2025-08-26T16:05:32Z,2025-08-26T16:05:32Z,http://arxiv.org/abs/2508.19153v1,http://arxiv.org/pdf/2508.19153v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Scene-Agnostic Traversability Labeling and Estimation via a Multimodal
  Self-supervised Framework","Traversability estimation is critical for enabling robots to navigate across
diverse terrains and environments. While recent self-supervised learning
methods achieve promising results, they often fail to capture the
characteristics of non-traversable regions. Moreover, most prior works
concentrate on a single modality, overlooking the complementary strengths
offered by integrating heterogeneous sensory modalities for more robust
traversability estimation. To address these limitations, we propose a
multimodal self-supervised framework for traversability labeling and
estimation. First, our annotation pipeline integrates footprint, LiDAR, and
camera data as prompts for a vision foundation model, generating traversability
labels that account for both semantic and geometric cues. Then, leveraging
these labels, we train a dual-stream network that jointly learns from different
modalities in a decoupled manner, enhancing its capacity to recognize diverse
traversability patterns. In addition, we incorporate sparse LiDAR-based
supervision to mitigate the noise introduced by pseudo labels. Finally,
extensive experiments conducted across urban, off-road, and campus environments
demonstrate the effectiveness of our approach. The proposed automatic labeling
method consistently achieves around 88% IoU across diverse datasets. Compared
to existing self-supervised state-of-the-art methods, our multimodal
traversability estimation network yields consistently higher IoU, improving by
1.6-3.5% on all evaluated datasets.","Zipeng Fang, Yanbo Wang, Lei Zhao, Weidong Chen",2025-08-25T17:40:16Z,2025-08-25T17:40:16Z,http://arxiv.org/abs/2508.18249v1,http://arxiv.org/pdf/2508.18249v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Optimizing Grasping in Legged Robots: A Deep Learning Approach to
  Loco-Manipulation","Quadruped robots have emerged as highly efficient and versatile platforms,
excelling in navigating complex and unstructured terrains where traditional
wheeled robots might fail. Equipping these robots with manipulator arms unlocks
the advanced capability of loco-manipulation to perform complex physical
interaction tasks in areas ranging from industrial automation to
search-and-rescue missions. However, achieving precise and adaptable grasping
in such dynamic scenarios remains a significant challenge, often hindered by
the need for extensive real-world calibration and pre-programmed grasp
configurations. This paper introduces a deep learning framework designed to
enhance the grasping capabilities of quadrupeds equipped with arms, focusing on
improved precision and adaptability. Our approach centers on a sim-to-real
methodology that minimizes reliance on physical data collection. We developed a
pipeline within the Genesis simulation environment to generate a synthetic
dataset of grasp attempts on common objects. By simulating thousands of
interactions from various perspectives, we created pixel-wise annotated
grasp-quality maps to serve as the ground truth for our model. This dataset was
used to train a custom CNN with a U-Net-like architecture that processes
multi-modal input from an onboard RGB and depth cameras, including RGB images,
depth maps, segmentation masks, and surface normal maps. The trained model
outputs a grasp-quality heatmap to identify the optimal grasp point. We
validated the complete framework on a four-legged robot. The system
successfully executed a full loco-manipulation task: autonomously navigating to
a target object, perceiving it with its sensors, predicting the optimal grasp
pose using our model, and performing a precise grasp. This work proves that
leveraging simulated training with advanced sensing offers a scalable and
effective solution for object handling.","Dilermando Almeida, Guilherme Lazzarini, Juliano Negri, Thiago H. Segreto, Ricardo V. Godoy, Marcelo Becker",2025-08-24T17:47:56Z,2025-08-24T17:47:56Z,http://arxiv.org/abs/2508.17466v1,http://arxiv.org/pdf/2508.17466v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"On Kinodynamic Global Planning in a Simplicial Complex Environment: A
  Mixed Integer Approach","This work casts the kinodynamic planning problem for car-like vehicles as an
optimization task to compute a minimum-time trajectory and its associated
velocity profile, subject to boundary conditions on velocity, acceleration, and
steering. The approach simultaneously optimizes both the spatial path and the
sequence of acceleration and steering controls, ensuring continuous motion from
a specified initial position and velocity to a target end position and
velocity.The method analyzes the admissible control space and terrain to avoid
local minima. The proposed method operates efficiently in simplicial complex
environments, a preferred terrain representation for capturing intricate 3D
landscapes. The problem is initially posed as a mixed-integer fractional
program with quadratic constraints, which is then reformulated into a
mixed-integer bilinear objective through a variable transformation and
subsequently relaxed to a mixed-integer linear program using McCormick
envelopes. Comparative simulations against planners such as MPPI and log-MPPI
demonstrate that the proposed approach generates solutions 104 times faster
while strictly adhering to the specified constraints","Otobong Jerome, Alexandr Klimchik, Alexander Maloletov, Geesara Kulathunga",2025-08-22T16:35:01Z,2025-08-22T16:35:01Z,http://arxiv.org/abs/2508.16511v1,http://arxiv.org/pdf/2508.16511v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Validating Terrain Models in Digital Twins for Trustworthy sUAS
  Operations","With the increasing deployment of small Unmanned Aircraft Systems (sUAS) in
unfamiliar and complex environments, Environmental Digital Twins (EDT) that
comprise weather, airspace, and terrain data are critical for safe flight
planning and for maintaining appropriate altitudes during search and
surveillance operations. With the expansion of sUAS capabilities through edge
and cloud computing, accurate EDT are also vital for advanced sUAS
capabilities, like geolocation. However, real-world sUAS deployment introduces
significant sources of uncertainty, necessitating a robust validation process
for EDT components. This paper focuses on the validation of terrain models, one
of the key components of an EDT, for real-world sUAS tasks. These models are
constructed by fusing U.S. Geological Survey (USGS) datasets and satellite
imagery, incorporating high-resolution environmental data to support mission
tasks. Validating both the terrain models and their operational use by sUAS
under real-world conditions presents significant challenges, including limited
data granularity, terrain discontinuities, GPS and sensor inaccuracies, visual
detection uncertainties, as well as onboard resources and timing constraints.
We propose a 3-Dimensions validation process grounded in software engineering
principles, following a workflow across granularity of tests, simulation to
real world, and the analysis of simple to edge conditions. We demonstrate our
approach using a multi-sUAS platform equipped with a Terrain-Aware Digital
Shadow.","Arturo Miguel Russell Bernal, Maureen Petterson, Pedro Antonio Alarcon Granadeno, Michael Murphy, James Mason, Jane Cleland-Huang",2025-08-22T05:42:55Z,2025-08-22T05:42:55Z,http://arxiv.org/abs/2508.16104v1,http://arxiv.org/pdf/2508.16104v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Consistent Pose Estimation of Unmanned Ground Vehicles through
  Terrain-Aided Multi-Sensor Fusion on Geometric Manifolds","Aiming to enhance the consistency and thus long-term accuracy of Extended
Kalman Filters for terrestrial vehicle localization, this paper introduces the
Manifold Error State Extended Kalman Filter (M-ESEKF). By representing the
robot's pose in a space with reduced dimensionality, the approach ensures
feasible estimates on generic smooth surfaces, without introducing artificial
constraints or simplifications that may degrade a filter's performance. The
accompanying measurement models are compatible with common loosely- and
tightly-coupled sensor modalities and also implicitly account for the ground
geometry. We extend the formulation by introducing a novel correction scheme
that embeds additional domain knowledge into the sensor data, giving more
accurate uncertainty approximations and further enhancing filter consistency.
The proposed estimator is seamlessly integrated into a validated modular state
estimation framework, demonstrating compatibility with existing
implementations. Extensive Monte Carlo simulations across diverse scenarios and
dynamic sensor configurations show that the M-ESEKF outperforms classical
filter formulations in terms of consistency and stability. Moreover, it
eliminates the need for scenario-specific parameter tuning, enabling its
application in a variety of real-world settings.","Alexander Raab, Stephan Weiss, Alessandro Fornasier, Christian Brommer, Abdalrahman Ibrahim",2025-08-20T12:24:45Z,2025-08-20T12:24:45Z,http://arxiv.org/abs/2508.14661v1,http://arxiv.org/pdf/2508.14661v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"No More Blind Spots: Learning Vision-Based Omnidirectional Bipedal
  Locomotion for Challenging Terrain","Effective bipedal locomotion in dynamic environments, such as cluttered
indoor spaces or uneven terrain, requires agile and adaptive movement in all
directions. This necessitates omnidirectional terrain sensing and a controller
capable of processing such input. We present a learning framework for
vision-based omnidirectional bipedal locomotion, enabling seamless movement
using depth images. A key challenge is the high computational cost of rendering
omnidirectional depth images in simulation, making traditional sim-to-real
reinforcement learning (RL) impractical. Our method combines a robust blind
controller with a teacher policy that supervises a vision-based student policy,
trained on noise-augmented terrain data to avoid rendering costs during RL and
ensure robustness. We also introduce a data augmentation technique for
supervised student training, accelerating training by up to 10 times compared
to conventional methods. Our framework is validated through simulation and
real-world tests, demonstrating effective omnidirectional locomotion with
minimal reliance on expensive rendering. This is, to the best of our knowledge,
the first demonstration of vision-based omnidirectional bipedal locomotion,
showcasing its adaptability to diverse terrains.","Mohitvishnu S. Gadde, Pranay Dugar, Ashish Malik, Alan Fern",2025-08-16T06:20:46Z,2025-08-16T06:20:46Z,http://arxiv.org/abs/2508.11929v1,http://arxiv.org/pdf/2508.11929v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Control of Legged Robots using Model Predictive Optimized Path Integral,"Legged robots possess a unique ability to traverse rough terrains and
navigate cluttered environments, making them well-suited for complex,
real-world unstructured scenarios. However, such robots have not yet achieved
the same level as seen in natural systems. Recently, sampling-based predictive
controllers have demonstrated particularly promising results. This paper
investigates a sampling-based model predictive strategy combining model
predictive path integral (MPPI) with cross-entropy (CE) and covariance matrix
adaptation (CMA) methods to generate real-time whole-body motions for legged
robots across multiple scenarios. The results show that combining the benefits
of MPPI, CE and CMA, namely using model predictive optimized path integral
(MPOPI), demonstrates greater sample efficiency, enabling robots to attain
superior locomotion results using fewer samples when compared to typical MPPI
algorithms. Extensive simulation experiments in multiple scenarios on a
quadruped robot show that MPOPI can be used as an anytime control strategy,
increasing locomotion capabilities at each iteration.","Hossein Keshavarz, Alejandro Ramirez-Serrano, Majid Khadiv",2025-08-16T05:28:13Z,2025-08-16T05:28:13Z,http://arxiv.org/abs/2508.11917v1,http://arxiv.org/pdf/2508.11917v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"LocoMamba: Vision-Driven Locomotion via End-to-End Deep Reinforcement
  Learning with Mamba","We introduce LocoMamba, a vision-driven cross-modal DRL framework built on
selective state-space models, specifically leveraging Mamba, that achieves
near-linear-time sequence modeling, effectively captures long-range
dependencies, and enables efficient training with longer sequences. First, we
embed proprioceptive states with a multilayer perceptron and patchify depth
images with a lightweight convolutional neural network, producing compact
tokens that improve state representation. Second, stacked Mamba layers fuse
these tokens via near-linear-time selective scanning, reducing latency and
memory footprint, remaining robust to token length and image resolution, and
providing an inductive bias that mitigates overfitting. Third, we train the
policy end-to-end with Proximal Policy Optimization under terrain and
appearance randomization and an obstacle-density curriculum, using a compact
state-centric reward that balances progress, smoothness, and safety. We
evaluate our method in challenging simulated environments with static and
moving obstacles as well as uneven terrain. Compared with state-of-the-art
baselines, our method achieves higher returns and success rates with fewer
collisions, exhibits stronger generalization to unseen terrains and obstacle
densities, and improves training efficiency by converging in fewer updates
under the same compute budget.","Yinuo Wang, Gavin Tao",2025-08-16T00:13:24Z,2025-08-28T21:09:41Z,http://arxiv.org/abs/2508.11849v2,http://arxiv.org/pdf/2508.11849v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Anticipatory and Adaptive Footstep Streaming for Teleoperated Bipedal
  Robots","Achieving seamless synchronization between user and robot motion in
teleoperation, particularly during high-speed tasks, remains a significant
challenge. In this work, we propose a novel approach for transferring stepping
motions from the user to the robot in real-time. Instead of directly
replicating user foot poses, we retarget user steps to robot footstep
locations, allowing the robot to utilize its own dynamics for locomotion,
ensuring better balance and stability. Our method anticipates user footsteps to
minimize delays between when the user initiates and completes a step and when
the robot does it. The step estimates are continuously adapted to converge with
the measured user references. Additionally, the system autonomously adjusts the
robot's steps to account for its surrounding terrain, overcoming challenges
posed by environmental mismatches between the user's flat-ground setup and the
robot's uneven terrain. Experimental results on the humanoid robot Nadia
demonstrate the effectiveness of the proposed system.","Luigi Penco, Beomyeong Park, Stefan Fasano, Nehar Poddar, Stephen McCrory, Nicholas Kitchel, Tomasz Bialek, Dexton Anderson, Duncan Calvert, Robert Griffin",2025-08-15T21:13:48Z,2025-08-15T21:13:48Z,http://arxiv.org/abs/2508.11802v1,http://arxiv.org/pdf/2508.11802v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media,"Reliable autonomous navigation across the unstructured terrains of distant
planetary surfaces is a critical enabler for future space exploration. However,
the deployment of learning-based controllers is hindered by the inherent
sim-to-real gap, particularly for the complex dynamics of wheel interactions
with granular media. This work presents a complete sim-to-real framework for
developing and validating robust control policies for dynamic waypoint tracking
on such challenging surfaces. We leverage massively parallel simulation to
train reinforcement learning agents across a vast distribution of procedurally
generated environments with randomized physics. These policies are then
transferred zero-shot to a physical wheeled rover operating in a lunar-analogue
facility. Our experiments systematically compare multiple reinforcement
learning algorithms and action smoothing filters to identify the most effective
combinations for real-world deployment. Crucially, we provide strong empirical
evidence that agents trained with procedural diversity achieve superior
zero-shot performance compared to those trained on static scenarios. We also
analyze the trade-offs of fine-tuning with high-fidelity particle physics,
which offers minor gains in low-speed precision at a significant computational
cost. Together, these contributions establish a validated workflow for creating
reliable learning-based navigation systems, marking a critical step towards
deploying autonomous robots in the final frontier.","Andrej Orsula, Matthieu Geist, Miguel Olivares-Mendez, Carol Martinez",2025-08-15T14:30:07Z,2025-08-15T14:30:07Z,http://arxiv.org/abs/2508.11503v1,http://arxiv.org/pdf/2508.11503v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Verti-Arena: A Controllable and Standardized Indoor Testbed for
  Multi-Terrain Off-Road Autonomy","Off-road navigation is an important capability for mobile robots deployed in
environments that are inaccessible or dangerous to humans, such as disaster
response or planetary exploration. Progress is limited due to the lack of a
controllable and standardized real-world testbed for systematic data collection
and validation. To fill this gap, we introduce Verti-Arena, a reconfigurable
indoor facility designed specifically for off-road autonomy. By providing a
repeatable benchmark environment, Verti-Arena supports reproducible experiments
across a variety of vertically challenging terrains and provides precise ground
truth measurements through onboard sensors and a motion capture system.
Verti-Arena also supports consistent data collection and comparative evaluation
of algorithms in off-road autonomy research. We also develop a web-based
interface that enables research groups worldwide to remotely conduct
standardized off-road autonomy experiments on Verti-Arena.","Haiyue Chen, Aniket Datar, Tong Xu, Francesco Cancelliere, Harsh Rangwala, Madhan Balaji Rao, Daeun Song, David Eichinger, Xuesu Xiao",2025-08-11T17:44:27Z,2025-08-11T17:44:27Z,http://arxiv.org/abs/2508.08226v1,http://arxiv.org/pdf/2508.08226v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Capsizing-Guided Trajectory Optimization for Autonomous Navigation with
  Rough Terrain","It is a challenging task for ground robots to autonomously navigate in harsh
environments due to the presence of non-trivial obstacles and uneven terrain.
This requires trajectory planning that balances safety and efficiency. The
primary challenge is to generate a feasible trajectory that prevents robot from
tip-over while ensuring effective navigation. In this paper, we propose a
capsizing-aware trajectory planner (CAP) to achieve trajectory planning on the
uneven terrain. The tip-over stability of the robot on rough terrain is
analyzed. Based on the tip-over stability, we define the traversable
orientation, which indicates the safe range of robot orientations. This
orientation is then incorporated into a capsizing-safety constraint for
trajectory optimization. We employ a graph-based solver to compute a robust and
feasible trajectory while adhering to the capsizing-safety constraint.
Extensive simulation and real-world experiments validate the effectiveness and
robustness of the proposed method. The results demonstrate that CAP outperforms
existing state-of-the-art approaches, providing enhanced navigation performance
on uneven terrains.","Wei Zhang, Yinchuan Wang, Wangtao Lu, Pengyu Zhang, Xiang Zhang, Yue Wang, Chaoqun Wang",2025-08-11T15:47:24Z,2025-08-11T15:47:24Z,http://arxiv.org/abs/2508.08108v1,http://arxiv.org/pdf/2508.08108v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Whole-Body Coordination for Dynamic Object Grasping with Legged
  Manipulators","Quadrupedal robots with manipulators offer strong mobility and adaptability
for grasping in unstructured, dynamic environments through coordinated
whole-body control. However, existing research has predominantly focused on
static-object grasping, neglecting the challenges posed by dynamic targets and
thus limiting applicability in dynamic scenarios such as logistics sorting and
human-robot collaboration. To address this, we introduce DQ-Bench, a new
benchmark that systematically evaluates dynamic grasping across varying object
motions, velocities, heights, object types, and terrain complexities, along
with comprehensive evaluation metrics. Building upon this benchmark, we propose
DQ-Net, a compact teacher-student framework designed to infer grasp
configurations from limited perceptual cues. During training, the teacher
network leverages privileged information to holistically model both the static
geometric properties and dynamic motion characteristics of the target, and
integrates a grasp fusion module to deliver robust guidance for motion
planning. Concurrently, we design a lightweight student network that performs
dual-viewpoint temporal modeling using only the target mask, depth map, and
proprioceptive state, enabling closed-loop action outputs without reliance on
privileged data. Extensive experiments on DQ-Bench demonstrate that DQ-Net
achieves robust dynamic objects grasping across multiple task settings,
substantially outperforming baseline methods in both success rate and
responsiveness.","Qiwei Liang, Boyang Cai, Rongyi He, Hui Li, Tao Teng, Haihan Duan, Changxin Huang, Runhao Zeng",2025-08-10T09:21:00Z,2025-08-10T09:21:00Z,http://arxiv.org/abs/2508.08328v1,http://arxiv.org/pdf/2508.08328v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Learning a Vision-Based Footstep Planner for Hierarchical Walking
  Control","Bipedal robots demonstrate potential in navigating challenging terrains
through dynamic ground contact. However, current frameworks often depend solely
on proprioception or use manually designed visual pipelines, which are fragile
in real-world settings and complicate real-time footstep planning in
unstructured environments. To address this problem, we present a vision-based
hierarchical control framework that integrates a reinforcement learning
high-level footstep planner, which generates footstep commands based on a local
elevation map, with a low-level Operational Space Controller that tracks the
generated trajectories. We utilize the Angular Momentum Linear Inverted
Pendulum model to construct a low-dimensional state representation to capture
an informative encoding of the dynamics while reducing complexity. We evaluate
our method across different terrain conditions using the underactuated bipedal
robot Cassie and investigate the capabilities and challenges of our approach
through simulation and hardware experiments.","Minku Kim, Brian Acosta, Pratik Chaudhari, Michael Posa",2025-08-09T02:08:57Z,2025-08-09T02:08:57Z,http://arxiv.org/abs/2508.06779v1,http://arxiv.org/pdf/2508.06779v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Dynamical Trajectory Planning of Disturbance Consciousness for Air-Land
  Bimodal Unmanned Aerial Vehicles","Air-land bimodal vehicles provide a promising solution for navigating complex
environments by combining the flexibility of aerial locomotion with the energy
efficiency of ground mobility. To enhance the robustness of trajectory planning
under environmental disturbances, this paper presents a disturbance-aware
planning framework that incorporates real-time disturbance estimation into both
path searching and trajectory optimization. A key component of the framework is
a disturbance-adaptive safety boundary adjustment mechanism, which dynamically
modifies the vehicle's feasible dynamic boundaries based on estimated
disturbances to ensure trajectory feasibility. Leveraging the dynamics model of
the bimodal vehicle, the proposed approach achieves adaptive and reliable
motion planning across different terrains and operating conditions. A series of
real-world experiments and benchmark comparisons on a custom-built platform
validate the effectiveness and robustness of the method, demonstrating
improvements in tracking accuracy, task efficiency, and energy performance
under both ground and aerial disturbances.","Shaoting Liu, Zhou Liu",2025-08-08T03:09:53Z,2025-08-08T03:09:53Z,http://arxiv.org/abs/2508.05972v1,http://arxiv.org/pdf/2508.05972v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Computational Design and Fabrication of Modular Robots with Untethered
  Control","Natural organisms utilize distributed actuation through their musculoskeletal
systems to adapt their gait for traversing diverse terrains or to morph their
bodies for varied tasks. A longstanding challenge in robotics is to emulate
this capability of natural organisms, which has motivated the development of
numerous soft robotic systems. However, such systems are generally optimized
for a single functionality, lack the ability to change form or function on
demand, or remain tethered to bulky control systems. To address these
limitations, we present a framework for designing and controlling robots that
utilize distributed actuation. We propose a novel building block that
integrates 3D-printed bones with liquid crystal elastomer (LCE) muscles as
lightweight actuators, enabling the modular assembly of musculoskeletal robots.
We developed LCE rods that contract in response to infrared radiation, thereby
providing localized, untethered control over the distributed skeletal network
and producing global deformations of the robot. To fully capitalize on the
extensive design space, we introduce two computational tools: one for
optimizing the robot's skeletal graph to achieve multiple target deformations,
and another for co-optimizing skeletal designs and control gaits to realize
desired locomotion. We validate our framework by constructing several robots
that demonstrate complex shape morphing, diverse control schemes, and
environmental adaptability. Our system integrates advances in modular material
building, untethered and distributed control, and computational design to
introduce a new generation of robots that brings us closer to the capabilities
of living organisms.","Manas Bhargava, Takefumi Hiraki, Malina Strugaru, Yuhan Zhang, Michal Piovarci, Chiara Daraio, Daisuke Iwai, Bernd Bickel",2025-08-07T14:03:13Z,2025-08-31T20:06:07Z,http://arxiv.org/abs/2508.05410v2,http://arxiv.org/pdf/2508.05410v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Uncertainty-aware Accurate Elevation Modeling for Off-road Navigation
  via Neural Processes","Terrain elevation modeling for off-road navigation aims to accurately
estimate changes in terrain geometry in real-time and quantify the
corresponding uncertainties. Having precise estimations and uncertainties plays
a crucial role in planning and control algorithms to explore safe and reliable
maneuver strategies. However, existing approaches, such as Gaussian Processes
(GPs) and neural network-based methods, often fail to meet these needs. They
are either unable to perform in real-time due to high computational demands,
underestimating sharp geometry changes, or harming elevation accuracy when
learned with uncertainties. Recently, Neural Processes (NPs) have emerged as a
promising approach that integrates the Bayesian uncertainty estimation of GPs
with the efficiency and flexibility of neural networks. Inspired by NPs, we
propose an effective NP-based method that precisely estimates sharp elevation
changes and quantifies the corresponding predictive uncertainty without losing
elevation accuracy. Our method leverages semantic features from LiDAR and
camera sensors to improve interpolation and extrapolation accuracy in
unobserved regions. Also, we introduce a local ball-query attention mechanism
to effectively reduce the computational complexity of global attention by 17\%
while preserving crucial local and spatial information. We evaluate our method
on off-road datasets having interesting geometric features, collected from
trails, deserts, and hills. Our results demonstrate superior performance over
baselines and showcase the potential of neural processes for effective and
expressive terrain modeling in complex off-road environments.","Sanghun Jung, Daehoon Gwak, Byron Boots, James Hays",2025-08-05T20:19:02Z,2025-08-07T19:56:22Z,http://arxiv.org/abs/2508.03890v2,http://arxiv.org/pdf/2508.03890v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Model-agnostic Meta-learning for Adaptive Gait Phase and Terrain
  Geometry Estimation with Wearable Soft Sensors","This letter presents a model-agnostic meta-learning (MAML) based framework
for simultaneous and accurate estimation of human gait phase and terrain
geometry using a small set of fabric-based wearable soft sensors, with
efficient adaptation to unseen subjects and strong generalization across
different subjects and terrains. Compared to rigid alternatives such as
inertial measurement units, fabric-based soft sensors improve comfort but
introduce nonlinearities due to hysteresis, placement error, and fabric
deformation. Moreover, inter-subject and inter-terrain variability, coupled
with limited calibration data in real-world deployments, further complicate
accurate estimation. To address these challenges, the proposed framework
integrates MAML into a deep learning architecture to learn a generalizable
model initialization that captures subject- and terrain-invariant structure.
This initialization enables efficient adaptation (i.e., adaptation with only a
small amount of calibration data and a few fine-tuning steps) to new users,
while maintaining strong generalization (i.e., high estimation accuracy across
subjects and terrains). Experiments on nine participants walking at various
speeds over five terrain conditions demonstrate that the proposed framework
outperforms baseline approaches in estimating gait phase, locomotion mode, and
incline angle, with superior accuracy, adaptation efficiency, and
generalization.","Zenan Zhu, Wenxi Chen, Pei-Chun Kao, Janelle Clark, Lily Behnke, Rebecca Kramer-Bottiglio, Holly Yanco, Yan Gu",2025-08-04T22:06:05Z,2025-08-04T22:06:05Z,http://arxiv.org/abs/2508.02930v1,http://arxiv.org/pdf/2508.02930v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Tunable Leg Stiffness in a Monopedal Hopper for Energy-Efficient
  Vertical Hopping Across Varying Ground Profiles","We present the design and implementation of HASTA (Hopper with Adjustable
Stiffness for Terrain Adaptation), a vertical hopping robot with real-time
tunable leg stiffness, aimed at optimizing energy efficiency across various
ground profiles (a pair of ground stiffness and damping conditions). By
adjusting leg stiffness, we aim to maximize apex hopping height, a key metric
for energy-efficient vertical hopping. We hypothesize that softer legs perform
better on soft, damped ground by minimizing penetration and energy loss, while
stiffer legs excel on hard, less damped ground by reducing limb deformation and
energy dissipation. Through experimental tests and simulations, we find the
best leg stiffness within our selection for each combination of ground
stiffness and damping, enabling the robot to achieve maximum steady-state
hopping height with a constant energy input. These results support our
hypothesis that tunable stiffness improves energy-efficient locomotion in
controlled experimental conditions. In addition, the simulation provides
insights that could aid in the future development of controllers for selecting
leg stiffness.","Rongqian Chen, Jun Kwon, Kefan Wu, Wei-Hsi Chen",2025-08-04T20:02:03Z,2025-08-07T03:25:33Z,http://arxiv.org/abs/2508.02873v2,http://arxiv.org/pdf/2508.02873v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Towards Zero-Shot Terrain Traversability Estimation: Challenges and
  Opportunities","Terrain traversability estimation is crucial for autonomous robots,
especially in unstructured environments where visual cues and reasoning play a
key role. While vision-language models (VLMs) offer potential for zero-shot
estimation, the problem remains inherently ill-posed. To explore this, we
introduce a small dataset of human-annotated water traversability ratings,
revealing that while estimations are subjective, human raters still show some
consensus. Additionally, we propose a simple pipeline that integrates VLMs for
zero-shot traversability estimation. Our experiments reveal mixed results,
suggesting that current foundation models are not yet suitable for practical
deployment but provide valuable insights for further research.","Ida Germann, Mark O. Mints, Peer Neubert",2025-08-03T10:52:34Z,2025-08-03T10:52:34Z,http://arxiv.org/abs/2508.01715v1,http://arxiv.org/pdf/2508.01715v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Construction of Digital Terrain Maps from Multi-view Satellite Imagery
  using Neural Volume Rendering","Digital terrain maps (DTMs) are an important part of planetary exploration,
enabling operations such as terrain relative navigation during entry, descent,
and landing for spacecraft and aiding in navigation on the ground. As robotic
exploration missions become more ambitious, the need for high quality DTMs will
only increase. However, producing DTMs via multi-view stereo pipelines for
satellite imagery, the current state-of-the-art, can be cumbersome and require
significant manual image preprocessing to produce satisfactory results. In this
work, we seek to address these shortcomings by adapting neural volume rendering
techniques to learn textured digital terrain maps directly from satellite
imagery. Our method, neural terrain maps (NTM), only requires the locus for
each image pixel and does not rely on depth or any other structural priors. We
demonstrate our method on both synthetic and real satellite data from Earth and
Mars encompassing scenes on the order of $100 \textrm{km}^2$. We evaluate the
accuracy of our output terrain maps by comparing with existing high-quality
DTMs produced using traditional multi-view stereo pipelines. Our method shows
promising results, with the precision of terrain prediction almost equal to the
resolution of the satellite images even in the presence of imperfect camera
intrinsics and extrinsics.","Josef X. Biberstein, Guilherme Cavalheiro, Juyeop Han, Sertac Karaman",2025-08-02T14:29:20Z,2025-08-02T14:29:20Z,http://arxiv.org/abs/2508.01386v1,http://arxiv.org/pdf/2508.01386v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on
  Planetary Rovers Using RGB, Depth, and Thermal Imagery","Robot navigation in unstructured environments requires multimodal perception
systems that can support safe navigation. Multimodality enables the integration
of complementary information collected by different sensors. However, this
information must be processed by machine learning algorithms specifically
designed to leverage heterogeneous data. Furthermore, it is necessary to
identify which sensor modalities are most informative for navigation in the
target environment. In Martian exploration, thermal imagery has proven valuable
for assessing terrain safety due to differences in thermal behaviour between
soil types. This work presents OmniUnet, a transformer-based neural network
architecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T)
imagery. A custom multimodal sensor housing was developed using 3D printing and
mounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a
multimodal dataset in the Bardenas semi-desert in northern Spain. This location
serves as a representative environment of the Martian surface, featuring
terrain types such as sand, bedrock, and compact soil. A subset of this dataset
was manually labeled to support supervised training of the network. The model
was evaluated both quantitatively and qualitatively, achieving a pixel accuracy
of 80.37% and demonstrating strong performance in segmenting complex
unstructured terrain. Inference tests yielded an average prediction time of 673
ms on a resource-constrained computer (Jetson Orin Nano), confirming its
suitability for on-robot deployment. The software implementation of the network
and the labeled dataset have been made publicly available to support future
research in multimodal terrain perception for planetary robotics.","Raul Castilla-Arquillo, Carlos Perez-del-Pulgar, Levin Gerdes, Alfonso Garcia-Cerezo, Miguel A. Olivares-Mendez",2025-08-01T12:23:29Z,2025-08-01T12:23:29Z,http://arxiv.org/abs/2508.00580v1,http://arxiv.org/pdf/2508.00580v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"BarlowWalk: Self-supervised Representation Learning for Legged Robot
  Terrain-adaptive Locomotion","Reinforcement learning (RL), driven by data-driven methods, has become an
effective solution for robot leg motion control problems. However, the
mainstream RL methods for bipedal robot terrain traversal, such as
teacher-student policy knowledge distillation, suffer from long training times,
which limit development efficiency. To address this issue, this paper proposes
BarlowWalk, an improved Proximal Policy Optimization (PPO) method integrated
with self-supervised representation learning. This method employs the Barlow
Twins algorithm to construct a decoupled latent space, mapping historical
observation sequences into low-dimensional representations and implementing
self-supervision. Meanwhile, the actor requires only proprioceptive information
to achieve self-supervised learning over continuous time steps, significantly
reducing the dependence on external terrain perception. Simulation experiments
demonstrate that this method has significant advantages in complex terrain
scenarios. To enhance the credibility of the evaluation, this study compares
BarlowWalk with advanced algorithms through comparative tests, and the
experimental results verify the effectiveness of the proposed method.","Haodong Huang, Shilong Sun, Yuanpeng Wang, Chiyao Li, Hailin Huang, Wenfu Xu",2025-07-31T13:09:48Z,2025-07-31T13:09:48Z,http://arxiv.org/abs/2508.00939v1,http://arxiv.org/pdf/2508.00939v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"In-Situ Soil-Property Estimation and Bayesian Mapping with a Simulated
  Compact Track Loader","Existing earthmoving autonomy is largely confined to highly controlled and
well-characterized environments due to the complexity of vehicle-terrain
interaction dynamics and the partial observability of the terrain resulting
from unknown and spatially varying soil conditions. In this chapter, a a
soil-property mapping system is proposed to extend the environmental state, in
order to overcome these restrictions and facilitate development of more robust
autonomous earthmoving. A GPU accelerated elevation mapping system is extended
to incorporate a blind mapping component which traces the movement of the blade
through the terrain to displace and erode intersected soil, enabling separately
tracking undisturbed and disturbed soil. Each interaction is approximated as a
flat blade moving through a locally homogeneous soil, enabling modeling of
cutting forces using the fundamental equation of earthmoving (FEE). Building
upon our prior work on in situ soil-property estimation, a method is devised to
extract approximate geometric parameters of the model given the uneven terrain,
and an improved physics infused neural network (PINN) model is developed to
predict soil properties and uncertainties of these estimates. A simulation of a
compact track loader (CTL) with a blade attachment is used to collect data to
train the PINN model. Post-training, the model is leveraged online by the
mapping system to track soil property estimates spatially as separate layers in
the map, with updates being performed in a Bayesian manner. Initial experiments
show that the system accurately highlights regions requiring higher relative
interaction forces, indicating the promise of this approach in enabling
soil-aware planning for autonomous terrain shaping.","W. Jacob Wagner, Ahmet Soylemezoglu, Katherine Driggs-Campbell",2025-07-30T03:41:44Z,2025-07-30T03:41:44Z,http://arxiv.org/abs/2507.22356v1,http://arxiv.org/pdf/2507.22356v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"FLORES: A Reconfigured Wheel-Legged Robot for Enhanced Steering and
  Adaptability","Wheel-legged robots integrate the agility of legs for navigating rough
terrains while harnessing the efficiency of wheels for smooth surfaces.
However, most existing designs do not fully capitalize on the benefits of both
legged and wheeled structures, which limits overall system flexibility and
efficiency. We present FLORES (reconfigured wheel-legged robot for enhanced
steering and adaptability), a novel wheel-legged robot design featuring a
distinctive front-leg configuration that sets it beyond standard design
approaches. Specifically, FLORES replaces the conventional hip-roll degree of
freedom (DoF) of the front leg with hip-yaw DoFs, and this allows for efficient
movement on flat surfaces while ensuring adaptability when navigating complex
terrains. This innovative design facilitates seamless transitions between
different locomotion modes (i.e., legged locomotion and wheeled locomotion) and
optimizes the performance across varied environments. To fully exploit FLORES's
mechanical capabilities, we develop a tailored reinforcement learning (RL)
controller that adapts the Hybrid Internal Model (HIM) with a customized reward
structure optimized for our unique mechanical configuration. This framework
enables the generation of adaptive, multi-modal locomotion strategies that
facilitate smooth transitions between wheeled and legged movements.
Furthermore, our distinctive joint design enables the robot to exhibit novel
and highly efficient locomotion gaits that capitalize on the synergistic
advantages of both locomotion modes. Through comprehensive experiments, we
demonstrate FLORES's enhanced steering capabilities, improved navigation
efficiency, and versatile locomotion across various terrains. The open-source
project can be found at
https://github.com/ZhichengSong6/FLORES-A-Reconfigured-Wheel-Legged-Robot-for-Enhanced-Steering-and-Adaptability.git.","Zhicheng Song, Jinglan Xu, Chunxin Zheng, Yulin Li, Zhihai Bi, Jun Ma",2025-07-30T03:09:15Z,2025-07-30T03:09:15Z,http://arxiv.org/abs/2507.22345v1,http://arxiv.org/pdf/2507.22345v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Temporally Consistent Unsupervised Segmentation for Mobile Robot
  Perception","Rapid progress in terrain-aware autonomous ground navigation has been driven
by advances in supervised semantic segmentation. However, these methods rely on
costly data collection and labor-intensive ground truth labeling to train deep
models. Furthermore, autonomous systems are increasingly deployed in
unrehearsed, unstructured environments where no labeled data exists and
semantic categories may be ambiguous or domain-specific. Recent zero-shot
approaches to unsupervised segmentation have shown promise in such settings but
typically operate on individual frames, lacking temporal consistency-a critical
property for robust perception in unstructured environments. To address this
gap we introduce Frontier-Seg, a method for temporally consistent unsupervised
segmentation of terrain from mobile robot video streams. Frontier-Seg clusters
superpixel-level features extracted from foundation model
backbones-specifically DINOv2-and enforces temporal consistency across frames
to identify persistent terrain boundaries or frontiers without human
supervision. We evaluate Frontier-Seg on a diverse set of benchmark
datasets-including RUGD and RELLIS-3D-demonstrating its ability to perform
unsupervised segmentation across unstructured off-road environments.","Christian Ellis, Maggie Wigness, Craig Lennon, Lance Fiondella",2025-07-29T19:41:37Z,2025-07-29T19:41:37Z,http://arxiv.org/abs/2507.22194v1,http://arxiv.org/pdf/2507.22194v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Deployment of Objects with a Soft Everting Robot,"Soft everting robots present significant advantages over traditional rigid
robots, including enhanced dexterity, improved environmental interaction, and
safe navigation in unpredictable environments. While soft everting robots have
been widely demonstrated for exploration type tasks, their potential to move
and deploy payloads in such tasks has been less investigated, with previous
work focusing on sensors and tools for the robot. Leveraging the navigation
capabilities, and deployed body, of the soft everting robot to deliver payloads
in hazardous areas, e.g. carrying a water bottle to a person stuck under
debris, would represent a significant capability in many applications. In this
work, we present an analysis of how soft everting robots can be used to deploy
larger, heavier payloads through the inside of the robot. We analyze both what
objects can be deployed and what terrain features they can be carried through.
Building on existing models, we present methods to quantify the effects of
payloads on robot growth and self-support, and develop a model to predict
payload slip. We then experimentally quantify payload transport using soft
everting robot with a variety of payload shapes, sizes, and weights and though
a series of tasks: steering, vertical transport, movement through holes, and
movement across gaps. Overall, the results show that we can transport payloads
in a variety of shapes and up to 1.5kg in weight and that we can move through
circular apertures with as little as 0.01cm clearance around payloads, carry
out discrete turns up to 135 degrees, and move across unsupported gaps of 1.15m
in length.","Ethan DeVries, Jack Ferlazzo, Mustafa Ugur, Laura H. Blumenschein",2025-07-29T19:32:36Z,2025-07-29T19:32:36Z,http://arxiv.org/abs/2507.22188v1,http://arxiv.org/pdf/2507.22188v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"A Nonlinear MPC Framework for Loco-Manipulation of Quadrupedal Robots
  with Non-Negligible Manipulator Dynamics","Model predictive control (MPC) combined with reduced-order template models
has emerged as a powerful tool for trajectory optimization in dynamic legged
locomotion. However, loco-manipulation tasks performed by legged robots
introduce additional complexity, necessitating computationally efficient MPC
algorithms capable of handling high-degree-of-freedom (DoF) models. This letter
presents a computationally efficient nonlinear MPC (NMPC) framework tailored
for loco-manipulation tasks of quadrupedal robots equipped with robotic
manipulators whose dynamics are non-negligible relative to those of the
quadruped. The proposed framework adopts a decomposition strategy that couples
locomotion template models -- such as the single rigid body (SRB) model -- with
a full-order dynamic model of the robotic manipulator for torque-level control.
This decomposition enables efficient real-time solution of the NMPC problem in
a receding horizon fashion at 60 Hz. The optimal state and input trajectories
generated by the NMPC for locomotion are tracked by a low-level nonlinear
whole-body controller (WBC) running at 500 Hz, while the optimal torque
commands for the manipulator are directly applied. The layered control
architecture is validated through extensive numerical simulations and hardware
experiments on a 15-kg Unitree Go2 quadrupedal robot augmented with a 4.4-kg
4-DoF Kinova arm. Given that the Kinova arm dynamics are non-negligible
relative to the Go2 base, the proposed NMPC framework demonstrates robust
stability in performing diverse loco-manipulation tasks, effectively handling
external disturbances, payload variations, and uneven terrain.","Ruturaj Sambhus, Kapi Ketan Mehta, Ali MirMohammad Sadeghi, Basit Muhammad Imran, Jeeseop Kim, Taizoon Chunawala, Vittorio Pastore, Sujith Vijayan, Kaveh Akbari Hamed",2025-07-29T17:47:34Z,2025-07-29T17:47:34Z,http://arxiv.org/abs/2507.22042v1,http://arxiv.org/pdf/2507.22042v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Terrain-Aware Adaptation for Two-Dimensional UAV Path Planners,"Multi-UAV Coverage Path Planning (mCPP) algorithms in popular commercial
software typically treat a Region of Interest (RoI) only as a 2D plane,
ignoring important3D structure characteristics. This leads to incomplete
3Dreconstructions, especially around occluded or vertical surfaces. In this
paper, we propose a modular algorithm that can extend commercial
two-dimensional path planners to facilitate terrain-aware planning by adjusting
altitude and camera orientations. To demonstrate it, we extend the well-known
DARP (Divide Areas for Optimal Multi-Robot Coverage Path Planning) algorithm
and produce DARP-3D. We present simulation results in multiple 3D environments
and a real-world flight test using DJI hardware. Compared to baseline, our
approach consistently captures improved 3D reconstructions, particularly in
areas with significant vertical features. An open-source implementation of the
algorithm is available here:https://github.com/konskara/TerraPlan","Kostas Karakontis, Thanos Petsanis, Athanasios Ch. Kapoutsis, Pavlos Ch. Kapoutsis, Elias B. Kosmatopoulos",2025-07-23T13:55:37Z,2025-07-24T07:45:43Z,http://arxiv.org/abs/2507.17519v2,http://arxiv.org/pdf/2507.17519v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Hierarchical Reinforcement Learning Framework for Adaptive Walking
  Control Using General Value Functions of Lower-Limb Sensor Signals","Rehabilitation technology is a natural setting to study the shared learning
and decision-making of human and machine agents. In this work, we explore the
use of Hierarchical Reinforcement Learning (HRL) to develop adaptive control
strategies for lower-limb exoskeletons, aiming to enhance mobility and autonomy
for individuals with motor impairments. Inspired by prominent models of
biological sensorimotor processing, our investigated HRL approach breaks down
the complex task of exoskeleton control adaptation into a higher-level
framework for terrain strategy adaptation and a lower-level framework for
providing predictive information; this latter element is implemented via the
continual learning of general value functions (GVFs). GVFs generated temporal
abstractions of future signal values from multiple wearable lower-limb sensors,
including electromyography, pressure insoles, and goniometers. We investigated
two methods for incorporating actual and predicted sensor signals into a policy
network with the intent to improve the decision-making capacity of the control
system of a lower-limb exoskeleton during ambulation across varied terrains. As
a key result, we found that the addition of predictions made from GVFs
increased overall network accuracy. Terrain-specific performance increases were
seen while walking on even ground, uneven ground, up and down ramps, and turns,
terrains that are often misclassified without predictive information. This
suggests that predictive information can aid decision-making during
uncertainty, e.g., on terrains that have a high chance of being misclassified.
This work, therefore, contributes new insights into the nuances of HRL and the
future development of exoskeletons to facilitate safe transitioning and
traversing across different walking environments.","Sonny T. Jones, Grange M. Simpson, Patrick M. Pilarski, Ashley N. Dalrymple",2025-07-22T19:47:04Z,2025-07-22T19:47:04Z,http://arxiv.org/abs/2507.16983v1,http://arxiv.org/pdf/2507.16983v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Guided Reinforcement Learning for Omnidirectional 3D Jumping in
  Quadruped Robots","Jumping poses a significant challenge for quadruped robots, despite being
crucial for many operational scenarios. While optimisation methods exist for
controlling such motions, they are often time-consuming and demand extensive
knowledge of robot and terrain parameters, making them less robust in
real-world scenarios. Reinforcement learning (RL) is emerging as a viable
alternative, yet conventional end-to-end approaches lack efficiency in terms of
sample complexity, requiring extensive training in simulations, and
predictability of the final motion, which makes it difficult to certify the
safety of the final motion. To overcome these limitations, this paper
introduces a novel guided reinforcement learning approach that leverages
physical intuition for efficient and explainable jumping, by combining B\'ezier
curves with a Uniformly Accelerated Rectilinear Motion (UARM) model. Extensive
simulation and experimental results clearly demonstrate the advantages of our
approach over existing alternatives.","Riccardo Bussola, Michele Focchi, Giulio Turrisi, Claudio Semini, Luigi Palopoli",2025-07-22T11:36:45Z,2025-07-22T11:36:45Z,http://arxiv.org/abs/2507.16481v1,http://arxiv.org/pdf/2507.16481v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Topology Optimization of Leg Structures for Construction Robots Based on
  Variable Density Method","In complex terrain construction environments, there are high demands for
robots to achieve both high payload capacity and mobility flexibility. As the
key load-bearing component, the optimization of robotic leg structures is of
particular importance. Therefore, this study focuses on the optimization of leg
structures for construction robots, proposing a topology optimization strategy
based on the SIMP (Solid Isotropic Microstructures with Penalization) variable
density method along with a structural re-design approach. The design
performance is comprehensively validated through finite element analysis using
ANSYS. First, static and modal analyses are conducted to evaluate the
rationality of the initial design. Then, topology optimization using the
SIMP-based variable density method is applied to the femur section, which
accounts for the largest proportion of the leg's weight. Based on iterative
calculations, the femur undergoes secondary structural reconstruction. After
optimization, the mass of the femur is reduced by 19.45\%, and the overall leg
mass decreases by 7.92\%, achieving the goal of lightweight design. Finally,
static and modal analyses are conducted on the reconstructed leg. The results
demonstrate that the optimized leg still meets structural performance
requirements, validating the feasibility of lightweight design. This research
provides robust theoretical and technical support for lightweight construction
robot design and lays a foundation for their efficient operation in complex
construction environments.","Xiao Liu, Xianlong Yang, Weijun Wang, Wei Feng",2025-07-22T08:19:09Z,2025-07-22T08:19:09Z,http://arxiv.org/abs/2507.16335v1,http://arxiv.org/pdf/2507.16335v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Design and Dimensional Optimization of Legged Structures for
  Construction Robots","Faced with complex and unstructured construction environments, wheeled and
tracked robots exhibit significant limitations in terrain adaptability and
flexibility, making it difficult to meet the requirements of autonomous
operation. Inspired by ants in nature, this paper proposes a leg configuration
design and optimization method tailored for construction scenarios, aiming to
enhance the autonomous mobility of construction robots. This paper analyzes the
full operational motion performance of the leg during both swing and stance
phases. First, based on kinematic modeling and multi-dimensional workspace
analysis, the concept of an ""improved workspace"" is introduced, and graphical
methods are used to optimize the leg dimensions during the swing phase.
Furthermore, a new concept of ""average manipulability"" is introduced based on
the velocity Jacobian matrix, and numerical solutions are applied to obtain the
leg segment ratio that maximizes manipulability. To overcome the difficulties
associated with traditional analytical methods, virtual prototype simulations
are conducted in ADAMS to explore the relationship between the robot body's
optimal flexibility and leg segment proportions. In summary, the leg segment
proportions with the best comprehensive motion performance are obtained. This
study presents the first multi-dimensional quantitative evaluation framework
for leg motion performance tailored for construction environments, providing a
structural design foundation for legged construction robots to achieve
autonomous mobility in complex terrains.","Xiao Liu, Xianlong Yang, Weijun Wang, Wei Feng",2025-07-22T08:10:12Z,2025-07-22T08:10:12Z,http://arxiv.org/abs/2507.16328v1,http://arxiv.org/pdf/2507.16328v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Selective Densification for Rapid Motion Planning in High Dimensions
  with Narrow Passages","Sampling-based algorithms are widely used for motion planning in
high-dimensional configuration spaces. However, due to low sampling efficiency,
their performance often diminishes in complex configuration spaces with narrow
corridors. Existing approaches address this issue using handcrafted or learned
heuristics to guide sampling toward useful regions. Unfortunately, these
strategies often lack generalizability to various problems or require extensive
prior training. In this paper, we propose a simple yet efficient sampling-based
planning framework along with its bidirectional version that overcomes these
issues by integrating different levels of planning granularity. Our approach
probes configuration spaces with uniform random samples at varying resolutions
and explores these multi-resolution samples online with a bias towards sparse
samples when traveling large free configuration spaces. By seamlessly
transitioning between sparse and dense samples, our approach can navigate
complex configuration spaces while maintaining planning speed and completeness.
The simulation results demonstrate that our approach outperforms several
state-of-the-art sampling-based planners in $\mathbb{SE}(2)$, $\mathbb{SE}(3)$,
and $\mathbb{R}^{14}$ with challenging terrains. Furthermore, experiments
conducted with the Franka Emika Panda robot operating in a constrained
workspace provide additional evidence of the superiority of the proposed
method.","Lu Huang, Lingxiao Meng, Jiankun Wang, Xingjian Jing",2025-07-21T15:17:59Z,2025-07-21T15:17:59Z,http://arxiv.org/abs/2507.15710v1,http://arxiv.org/pdf/2507.15710v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Koopman Operator Based Linear Model Predictive Control for 2D Quadruped
  Trotting, Bounding, and Gait Transition","Online optimal control of quadrupedal robots would enable them to plan their
movement in novel scenarios. Linear Model Predictive Control (LMPC) has emerged
as a practical approach for real-time control. In LMPC, an optimization problem
with a quadratic cost and linear constraints is formulated over a finite
horizon and solved on the fly. However, LMPC relies on linearizing the
equations of motion (EOM), which may lead to poor solution quality. In this
paper, we use Koopman operator theory and the Extended Dynamic Mode
Decomposition (EDMD) to create a linear model of the system in high dimensional
space, thus retaining the nonlinearity of the EOM. We model the aerial phase
and ground contact phases using different linear models. Then, using LMPC, we
demonstrate bounding, trotting, and bound-to-trot and trot-to-bound gait
transitions in level and rough terrains. The main novelty is the use of Koopman
operator theory to create hybrid models of a quadrupedal system and demonstrate
the online generation of multiple gaits and gaits transitions.","Chun-Ming Yang, Pranav A. Bhounsule",2025-07-19T13:06:39Z,2025-07-19T13:06:39Z,http://arxiv.org/abs/2507.14605v1,http://arxiv.org/pdf/2507.14605v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Iteratively Learning Muscle Memory for Legged Robots to Master Adaptive
  and High Precision Locomotion","This paper presents a scalable and adaptive control framework for legged
robots that integrates Iterative Learning Control (ILC) with a biologically
inspired torque library (TL), analogous to muscle memory. The proposed method
addresses key challenges in robotic locomotion, including accurate trajectory
tracking under unmodeled dynamics and external disturbances. By leveraging the
repetitive nature of periodic gaits and extending ILC to nonperiodic tasks, the
framework enhances accuracy and generalization across diverse locomotion
scenarios. The control architecture is data-enabled, combining a physics-based
model derived from hybrid-system trajectory optimization with real-time
learning to compensate for model uncertainties and external disturbances. A
central contribution is the development of a generalized TL that stores learned
control profiles and enables rapid adaptation to changes in speed, terrain, and
gravitational conditions-eliminating the need for repeated learning and
significantly reducing online computation. The approach is validated on the
bipedal robot Cassie and the quadrupedal robot A1 through extensive simulations
and hardware experiments. Results demonstrate that the proposed framework
reduces joint tracking errors by up to 85% within a few seconds and enables
reliable execution of both periodic and nonperiodic gaits, including slope
traversal and terrain adaptation. Compared to state-of-the-art whole-body
controllers, the learned skills eliminate the need for online computation
during execution and achieve control update rates exceeding 30x those of
existing methods. These findings highlight the effectiveness of integrating ILC
with torque memory as a highly data-efficient and practical solution for legged
locomotion in unstructured and dynamic environments.","Jing Cheng, Yasser G. Alqaham, Zhenyu Gan, Amit K. Sanyal",2025-07-18T05:13:02Z,2025-07-18T05:13:02Z,http://arxiv.org/abs/2507.13662v1,http://arxiv.org/pdf/2507.13662v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Enhancing Autonomous Manipulator Control with Human-in-loop for
  Uncertain Assembly Environments","This study presents an advanced approach to enhance robotic manipulation in
uncertain and challenging environments, with a focus on autonomous operations
augmented by human-in-the-loop (HITL) control for lunar missions. By
integrating human decision-making with autonomous robotic functions, the
research improves task reliability and efficiency for space applications. The
key task addressed is the autonomous deployment of flexible solar panels using
an extendable ladder-like structure and a robotic manipulator with real-time
feedback for precision. The manipulator relays position and force-torque data,
enabling dynamic error detection and adaptive control during deployment. To
mitigate the effects of sinkage, variable payload, and low-lighting conditions,
efficient motion planning strategies are employed, supplemented by human
control that allows operators to intervene in ambiguous scenarios. Digital twin
simulation enhances system robustness by enabling continuous feedback,
iterative task refinement, and seamless integration with the deployment
pipeline. The system has been tested to validate its performance in simulated
lunar conditions and ensure reliability in extreme lighting, variable terrain,
changing payloads, and sensor limitations.","Ashutosh Mishra, Shreya Santra, Hazal Gozbasi, Kentaro Uno, Kazuya Yoshida",2025-07-15T05:53:12Z,2025-07-15T05:53:12Z,http://arxiv.org/abs/2507.11006v1,http://arxiv.org/pdf/2507.11006v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Robust RL Control for Bipedal Locomotion with Closed Kinematic Chains,"Developing robust locomotion controllers for bipedal robots with closed
kinematic chains presents unique challenges, particularly since most
reinforcement learning (RL) approaches simplify these parallel mechanisms into
serial models during training. We demonstrate that this simplification
significantly impairs sim-to-real transfer by failing to capture essential
aspects such as joint coupling, friction dynamics, and motor-space control
characteristics. In this work, we present an RL framework that explicitly
incorporates closed-chain dynamics and validate it on our custom-built robot
TopA. Our approach enhances policy robustness through symmetry-aware loss
functions, adversarial training, and targeted network regularization.
Experimental results demonstrate that our integrated approach achieves stable
locomotion across diverse terrains, significantly outperforming methods based
on simplified kinematic models.","Egor Maslennikov, Eduard Zaliaev, Nikita Dudorov, Oleg Shamanin, Karanov Dmitry, Gleb Afanasev, Alexey Burkov, Egor Lygin, Simeon Nedelchev, Evgeny Ponomarev",2025-07-14T11:25:13Z,2025-07-14T11:25:13Z,http://arxiv.org/abs/2507.10164v1,http://arxiv.org/pdf/2507.10164v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Domain Adaptation and Multi-view Attention for Learnable Landmark
  Tracking with Sparse Data","The detection and tracking of celestial surface terrain features are crucial
for autonomous spaceflight applications, including Terrain Relative Navigation
(TRN), Entry, Descent, and Landing (EDL), hazard analysis, and scientific data
collection. Traditional photoclinometry-based pipelines often rely on extensive
a priori imaging and offline processing, constrained by the computational
limitations of radiation-hardened systems. While historically effective, these
approaches typically increase mission costs and duration, operate at low
processing rates, and have limited generalization. Recently, learning-based
computer vision has gained popularity to enhance spacecraft autonomy and
overcome these limitations. While promising, emerging techniques frequently
impose computational demands exceeding the capabilities of typical spacecraft
hardware for real-time operation and are further challenged by the scarcity of
labeled training data for diverse extraterrestrial environments. In this work,
we present novel formulations for in-situ landmark tracking via detection and
description. We utilize lightweight, computationally efficient neural network
architectures designed for real-time execution on current-generation spacecraft
flight processors. For landmark detection, we propose improved domain
adaptation methods that enable the identification of celestial terrain features
with distinct, cheaply acquired training data. Concurrently, for landmark
description, we introduce a novel attention alignment formulation that learns
robust feature representations that maintain correspondence despite significant
landmark viewpoint variations. Together, these contributions form a unified
system for landmark tracking that demonstrates superior performance compared to
existing state-of-the-art techniques.","Timothy Chase Jr, Karthik Dantu",2025-07-12T23:00:52Z,2025-07-12T23:00:52Z,http://arxiv.org/abs/2507.09420v1,http://arxiv.org/pdf/2507.09420v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Learning Robust Motion Skills via Critical Adversarial Attacks for
  Humanoid Robots","Humanoid robots show significant potential in daily tasks. However,
reinforcement learning-based motion policies often suffer from robustness
degradation due to the sim-to-real dynamics gap, thereby affecting the agility
of real robots. In this work, we propose a novel robust adversarial training
paradigm designed to enhance the robustness of humanoid motion policies in real
worlds. The paradigm introduces a learnable adversarial attack network that
precisely identifies vulnerabilities in motion policies and applies targeted
perturbations, forcing the motion policy to enhance its robustness against
perturbations through dynamic adversarial training. We conduct experiments on
the Unitree G1 humanoid robot for both perceptive locomotion and whole-body
control tasks. The results demonstrate that our proposed method significantly
enhances the robot's motion robustness in real world environments, enabling
successful traversal of challenging terrains and highly agile whole-body
trajectory tracking.","Yang Zhang, Zhanxiang Cao, Buqing Nie, Haoyang Li, Yue Gao",2025-07-11T04:32:07Z,2025-07-11T04:32:07Z,http://arxiv.org/abs/2507.08303v1,http://arxiv.org/pdf/2507.08303v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Martian World Models: Controllable Video Synthesis with Physically
  Accurate 3D Reconstructions","Synthesizing realistic Martian landscape videos is crucial for mission
rehearsal and robotic simulation. However, this task poses unique challenges
due to the scarcity of high-quality Martian data and the significant domain gap
between Martian and terrestrial imagery. To address these challenges, we
propose a holistic solution composed of two key components: 1) A data curation
pipeline Multimodal Mars Synthesis (M3arsSynth), which reconstructs 3D Martian
environments from real stereo navigation images, sourced from NASA's Planetary
Data System (PDS), and renders high-fidelity multiview 3D video sequences. 2) A
Martian terrain video generator, MarsGen, which synthesizes novel videos
visually realistic and geometrically consistent with the 3D structure encoded
in the data. Our M3arsSynth engine spans a wide range of Martian terrains and
acquisition dates, enabling the generation of physically accurate 3D surface
models at metric-scale resolution. MarsGen, fine-tuned on M3arsSynth data,
synthesizes videos conditioned on an initial image frame and, optionally,
camera trajectories or textual prompts, allowing for video generation in novel
environments. Experimental results show that our approach outperforms video
synthesis models trained on terrestrial datasets, achieving superior visual
fidelity and 3D structural consistency.","Longfei Li, Zhiwen Fan, Wenyan Cong, Xinhang Liu, Yuyang Yin, Matt Foutter, Panwang Pan, Chenyu You, Yue Wang, Zhangyang Wang, Yao Zhao, Marco Pavone, Yunchao Wei",2025-07-10T17:54:27Z,2025-07-10T17:54:27Z,http://arxiv.org/abs/2507.07978v1,http://arxiv.org/pdf/2507.07978v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Beyond Robustness: Learning Unknown Dynamic Load Adaptation for
  Quadruped Locomotion on Rough Terrain","Unknown dynamic load carrying is one important practical application for
quadruped robots. Such a problem is non-trivial, posing three major challenges
in quadruped locomotion control. First, how to model or represent the dynamics
of the load in a generic manner. Second, how to make the robot capture the
dynamics without any external sensing. Third, how to enable the robot to
interact with load handling the mutual effect and stabilizing the load. In this
work, we propose a general load modeling approach called load characteristics
modeling to capture the dynamics of the load. We integrate this proposed
modeling technique and leverage recent advances in Reinforcement Learning (RL)
based locomotion control to enable the robot to infer the dynamics of load
movement and interact with the load indirectly to stabilize it and realize the
sim-to-real deployment to verify its effectiveness in real scenarios. We
conduct extensive comparative simulation experiments to validate the
effectiveness and superiority of our proposed method. Results show that our
method outperforms other methods in sudden load resistance, load stabilizing
and locomotion with heavy load on rough terrain.
\href{https://leixinjonaschang.github.io/leggedloadadapt.github.io/}{Project
Page}.","Leixin Chang, Yuxuan Nai, Hua Chen, Liangjing Yang",2025-07-10T14:54:52Z,2025-07-10T14:54:52Z,http://arxiv.org/abs/2507.07825v1,http://arxiv.org/pdf/2507.07825v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"KLEIYN : A Quadruped Robot with an Active Waist for Both Locomotion and
  Wall Climbing","In recent years, advancements in hardware have enabled quadruped robots to
operate with high power and speed, while robust locomotion control using
reinforcement learning (RL) has also been realized. As a result, expectations
are rising for the automation of tasks such as material transport and
exploration in unknown environments. However, autonomous locomotion in rough
terrains with significant height variations requires vertical movement, and
robots capable of performing such movements stably, along with their control
methods, have not yet been fully established. In this study, we developed the
quadruped robot KLEIYN, which features a waist joint, and aimed to expand
quadruped locomotion by enabling chimney climbing through RL. To facilitate the
learning of vertical motion, we introduced Contact-Guided Curriculum Learning
(CGCL). As a result, KLEIYN successfully climbed walls ranging from 800 mm to
1000 mm in width at an average speed of 150 mm/s, 50 times faster than
conventional robots. Furthermore, we demonstrated that the introduction of a
waist joint improves climbing performance, particularly enhancing tracking
ability on narrow walls.","Keita Yoneda, Kento Kawaharazuka, Temma Suzuki, Takahiro Hattori, Kei Okada",2025-07-09T05:30:53Z,2025-07-10T11:05:21Z,http://arxiv.org/abs/2507.06562v2,http://arxiv.org/pdf/2507.06562v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Comparison of Path Planning Algorithms for Autonomous Vehicle Navigation
  Using Satellite and Airborne LiDAR Data","Autonomous vehicle navigation in unstructured environments, such as forests
and mountainous regions, presents significant challenges due to irregular
terrain and complex road conditions. This work provides a comparative
evaluation of mainstream and well-established path planning algorithms applied
to weighted pixel-level road networks derived from high-resolution satellite
imagery and airborne LiDAR data. For 2D road-map navigation, where the weights
reflect road conditions and terrain difficulty, A*, Dijkstra, RRT*, and a Novel
Improved Ant Colony Optimization Algorithm (NIACO) are tested on the DeepGlobe
satellite dataset. For 3D road-map path planning, 3D A*, 3D Dijkstra,
RRT-Connect, and NIACO are evaluated using the Hamilton airborne LiDAR dataset,
which provides detailed elevation information. All algorithms are assessed
under identical start and end point conditions, focusing on path cost,
computation time, and memory consumption. Results demonstrate that Dijkstra
consistently offers the most stable and efficient performance in both 2D and 3D
scenarios, particularly when operating on dense, pixel-level geospatial
road-maps. These findings highlight the reliability of Dijkstra-based planning
for static terrain navigation and establish a foundation for future research on
dynamic path planning under complex environmental constraints.","Chang Liu, Zhexiong Xue, Tamas Sziranyi",2025-07-08T11:15:21Z,2025-07-08T11:15:21Z,http://arxiv.org/abs/2507.05884v1,http://arxiv.org/pdf/2507.05884v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"A Physics-Based Continuum Model for Versatile, Scalable, and Fast
  Terramechanics Simulation","This paper discusses Chrono's Continuous Representation Model (called herein
Chrono::CRM), a general-purpose, scalable, and efficient simulation solution
for terramechanics problems. Built on Chrono's Smoothed Particle Hydrodynamics
(SPH) framework, Chrono::CRM moves beyond semi-empirical terramechanics
approaches, e.g., Bekker-Wong/Janosi-Hanamoto, to provide a physics-based model
able to address complex tasks such as digging, grading, as well as interaction
with deformable wheels and complex grouser/lug patterns. The terramechanics
model is versatile in that it allows the terrain to interact with both rigid
and flexible implements simulated via the Chrono dynamics engine. We validate
Chrono::CRM against experimental data from three physical tests, including one
involving NASA's MGRU3 rover. In addition, the simulator is benchmarked against
a high-fidelity Discrete Element Method (DEM) simulation of a digging scenario
involving the Regolith Advanced Surface Systems Operations Robot (RASSOR).
Being GPU-accelerated, Chrono::CRM achieves computational efficiency comparable
to that of semi-empirical simulation approaches for terramechanics problems.
Through an ``active domains'' implementation, Chrono::CRM can handle terrain
stretches up to 10 km long with 100 million SPH particles at near interactive
rates, making high-fidelity off-road simulations at large scales feasible. As a
component of the Chrono package, the CRM model is open source and released
under a BSD-3 license. All models and simulations used in this contribution are
available in a public GitHub repository for reproducibility studies and further
research.","Huzaifa Unjhawala, Luning Bakke, Harry Zhang, Michael Taylor, Ganesh Arivoli, Radu Serban, Dan Negrut",2025-07-08T03:40:27Z,2025-08-29T20:35:19Z,http://arxiv.org/abs/2507.05643v2,http://arxiv.org/pdf/2507.05643v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Vibration-aware Lidar-Inertial Odometry based on Point-wise
  Post-Undistortion Uncertainty","High-speed ground robots moving on unstructured terrains generate intense
high-frequency vibrations, leading to LiDAR scan distortions in Lidar-inertial
odometry (LIO). Accurate and efficient undistortion is extremely challenging
due to (1) rapid and non-smooth state changes during intense vibrations and (2)
unpredictable IMU noise coupled with a limited IMU sampling frequency. To
address this issue, this paper introduces post-undistortion uncertainty. First,
we model the undistortion errors caused by linear and angular vibrations and
assign post-undistortion uncertainty to each point. We then leverage this
uncertainty to guide point-to-map matching, compute uncertainty-aware
residuals, and update the odometry states using an iterated Kalman filter. We
conduct vibration-platform and mobile-platform experiments on multiple public
datasets as well as our own recordings, demonstrating that our method achieves
better performance than other methods when LiDAR undergoes intense vibration.","Yan Dong, Enci Xu, Shaoqiang Qiu, Wenxuan Li, Yang Liu, Bin Han",2025-07-06T09:35:58Z,2025-07-06T09:35:58Z,http://arxiv.org/abs/2507.04311v1,http://arxiv.org/pdf/2507.04311v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Learning Humanoid Arm Motion via Centroidal Momentum Regularized
  Multi-Agent Reinforcement Learning","Humans naturally swing their arms during locomotion to regulate whole-body
dynamics, reduce angular momentum, and help maintain balance. Inspired by this
principle, we present a limb-level multi-agent reinforcement learning (RL)
framework that enables coordinated whole-body control of humanoid robots
through emergent arm motion. Our approach employs separate actor-critic
structures for the arms and legs, trained with centralized critics but
decentralized actors that share only base states and centroidal angular
momentum (CAM) observations, allowing each agent to specialize in task-relevant
behaviors through modular reward design. The arm agent guided by CAM tracking
and damping rewards promotes arm motions that reduce overall angular momentum
and vertical ground reaction moments, contributing to improved balance during
locomotion or under external perturbations. Comparative studies with
single-agent and alternative multi-agent baselines further validate the
effectiveness of our approach. Finally, we deploy the learned policy on a
humanoid platform, achieving robust performance across diverse locomotion
tasks, including flat-ground walking, rough terrain traversal, and stair
climbing.","Ho Jae Lee, Se Hwan Jeon, Sangbae Kim",2025-07-05T19:35:13Z,2025-07-05T19:35:13Z,http://arxiv.org/abs/2507.04140v1,http://arxiv.org/pdf/2507.04140v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Jump-Start Reinforcement Learning with Self-Evolving Priors for Extreme
  Monopedal Locomotion","Reinforcement learning (RL) has shown great potential in enabling quadruped
robots to perform agile locomotion. However, directly training policies to
simultaneously handle dual extreme challenges, i.e., extreme underactuation and
extreme terrains, as in monopedal hopping tasks, remains highly challenging due
to unstable early-stage interactions and unreliable reward feedback. To address
this, we propose JumpER (jump-start reinforcement learning via self-evolving
priors), an RL training framework that structures policy learning into multiple
stages of increasing complexity. By dynamically generating self-evolving priors
through iterative bootstrapping of previously learned policies, JumpER
progressively refines and enhances guidance, thereby stabilizing exploration
and policy optimization without relying on external expert priors or
handcrafted reward shaping. Specifically, when integrated with a structured
three-stage curriculum that incrementally evolves action modality, observation
space, and task objective, JumpER enables quadruped robots to achieve robust
monopedal hopping on unpredictable terrains for the first time. Remarkably, the
resulting policy effectively handles challenging scenarios that traditional
methods struggle to conquer, including wide gaps up to 60 cm, irregularly
spaced stairs, and stepping stones with distances varying from 15 cm to 35 cm.
JumpER thus provides a principled and scalable approach for addressing
locomotion tasks under the dual challenges of extreme underactuation and
extreme terrains.","Ziang Zheng, Guojian Zhan, Shiqi Liu, Yao Lyu, Tao Zhang, Shengbo Eben Li",2025-07-01T23:31:36Z,2025-07-01T23:31:36Z,http://arxiv.org/abs/2507.01243v1,http://arxiv.org/pdf/2507.01243v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Environment-Aware and Human-Cooperative Swing Control for Lower-Limb
  Prostheses in Diverse Obstacle Scenarios","Current control strategies for powered lower limb prostheses often lack
awareness of the environment and the user's intended interactions with it. This
limitation becomes particularly apparent in complex terrains. Obstacle
negotiation, a critical scenario exemplifying such challenges, requires both
real-time perception of obstacle geometry and responsiveness to user intention
about when and where to step over or onto, to dynamically adjust swing
trajectories. We propose a novel control strategy that fuses environmental
awareness and human cooperativeness: an on-board depth camera detects obstacles
ahead of swing phase, prompting an elevated early-swing trajectory to ensure
clearance, while late-swing control defers to natural biomechanical cues from
the user. This approach enables intuitive stepping strategies without requiring
unnatural movement patterns. Experiments with three non-amputee participants
demonstrated 100 percent success across more than 150 step-overs and 30
step-ons with randomly placed obstacles of varying heights (4-16 cm) and
distances (15-70 cm). By effectively addressing obstacle navigation -- a
gateway challenge for complex terrain mobility -- our system demonstrates
adaptability to both environmental constraints and user intentions, with
promising applications across diverse locomotion scenarios.","Haosen Xing, Haoran Ma, Sijin Zhang, Hartmut Geyer",2025-07-01T18:15:50Z,2025-07-01T18:15:50Z,http://arxiv.org/abs/2507.01111v1,http://arxiv.org/pdf/2507.01111v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Skill-Nav: Enhanced Navigation with Versatile Quadrupedal Locomotion via
  Waypoint Interface","Quadrupedal robots have demonstrated exceptional locomotion capabilities
through Reinforcement Learning (RL), including extreme parkour maneuvers.
However, integrating locomotion skills with navigation in quadrupedal robots
has not been fully investigated, which holds promise for enhancing
long-distance movement capabilities. In this paper, we propose Skill-Nav, a
method that incorporates quadrupedal locomotion skills into a hierarchical
navigation framework using waypoints as an interface. Specifically, we train a
waypoint-guided locomotion policy using deep RL, enabling the robot to
autonomously adjust its locomotion skills to reach targeted positions while
avoiding obstacles. Compared with direct velocity commands, waypoints offer a
simpler yet more flexible interface for high-level planning and low-level
control. Utilizing waypoints as the interface allows for the application of
various general planning tools, such as large language models (LLMs) and path
planning algorithms, to guide our locomotion policy in traversing terrains with
diverse obstacles. Extensive experiments conducted in both simulated and
real-world scenarios demonstrate that Skill-Nav can effectively traverse
complex terrains and complete challenging navigation tasks.","Dewei Wang, Chenjia Bai, Chenhui Li, Jiyuan Shi, Yan Ding, Chi Zhang, Bin Zhao",2025-06-27T02:08:40Z,2025-06-30T08:59:34Z,http://arxiv.org/abs/2506.21853v2,http://arxiv.org/pdf/2506.21853v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Experimental investigation of pose informed reinforcement learning for
  skid-steered visual navigation","Vision-based lane keeping is a topic of significant interest in the robotics
and autonomous ground vehicles communities in various on-road and off-road
applications. The skid-steered vehicle architecture has served as a useful
vehicle platform for human controlled operations. However, systematic modeling,
especially of the skid-slip wheel terrain interactions (primarily in off-road
settings) has created bottlenecks for automation deployment. End-to-end
learning based methods such as imitation learning and deep reinforcement
learning, have gained prominence as a viable deployment option to counter the
lack of accurate analytical models. However, the systematic formulation and
subsequent verification/validation in dynamic operation regimes (particularly
for skid-steered vehicles) remains a work in progress. To this end, a novel
approach for structured formulation for learning visual navigation is proposed
and investigated in this work. Extensive software simulations, hardware
evaluations and ablation studies now highlight the significantly improved
performance of the proposed approach against contemporary literature.","Ameya Salvi, Venkat Krovi",2025-06-26T19:36:49Z,2025-06-26T19:36:49Z,http://arxiv.org/abs/2506.21732v1,http://arxiv.org/pdf/2506.21732v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Real-time Terrain Analysis for Off-road Autonomous Vehicles,"This research addresses critical autonomous vehicle control challenges
arising from road roughness variation, which induces course deviations and
potential loss of road contact during steering operations. We present a novel
real-time road roughness estimation system employing Bayesian calibration
methodology that processes axle accelerations to predict terrain roughness with
quantifiable confidence measures. The technical framework integrates a Gaussian
process surrogate model with a simulated half-vehicle model, systematically
processing vehicle velocity and road surface roughness parameters to generate
corresponding axle acceleration responses. The Bayesian calibration routine
performs inverse estimation of road roughness from observed accelerations and
velocities, yielding posterior distributions that quantify prediction
uncertainty for adaptive risk management. Training data generation utilizes
Latin Hypercube sampling across comprehensive velocity and roughness parameter
spaces, while the calibrated model integrates seamlessly with a Simplex
controller architecture to dynamically adjust velocity limits based on
real-time roughness predictions. Experimental validation on stochastically
generated surfaces featuring varying roughness regions demonstrates robust
real-time characterization capabilities, with the integrated Simplex control
strategy effectively enhancing autonomous vehicle operational safety through
proactive surface condition response. This innovative Bayesian framework
establishes a comprehensive foundation for mitigating roughness-related
operational risks while simultaneously improving efficiency and safety margins
in autonomous vehicle systems.","Edwina Lewis, Aditya Parameshwaran, Laura Redmond, Yue Wang",2025-06-26T14:59:50Z,2025-06-26T14:59:50Z,http://arxiv.org/abs/2506.21347v1,http://arxiv.org/pdf/2506.21347v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Critical Anatomy-Preserving & Terrain-Augmenting Navigation (CAPTAiN):
  Application to Laminectomy Surgical Education","Surgical training remains a crucial milestone in modern medicine, with
procedures such as laminectomy exemplifying the high risks involved.
Laminectomy drilling requires precise manual control to mill bony tissue while
preserving spinal segment integrity and avoiding breaches in the dura: the
protective membrane surrounding the spinal cord. Despite unintended tears
occurring in up to 11.3% of cases, no assistive tools are currently utilized to
reduce this risk. Variability in patient anatomy further complicates learning
for novice surgeons. This study introduces CAPTAiN, a critical
anatomy-preserving and terrain-augmenting navigation system that provides
layered, color-coded voxel guidance to enhance anatomical awareness during
spinal drilling. CAPTAiN was evaluated against a standard non-navigated
approach through 110 virtual laminectomies performed by 11 orthopedic residents
and medical students. CAPTAiN significantly improved surgical completion rates
of target anatomy (87.99% vs. 74.42%) and reduced cognitive load across
multiple NASA-TLX domains. It also minimized performance gaps across experience
levels, enabling novices to perform on par with advanced trainees. These
findings highlight CAPTAiN's potential to optimize surgical execution and
support skill development across experience levels. Beyond laminectomy, it
demonstrates potential for broader applications across various surgical and
drilling procedures, including those in neurosurgery, otolaryngology, and other
medical fields.","Jonathan Wang, Hisashi Ishida, David Usevitch, Kesavan Venkatesh, Yi Wang, Mehran Armand, Rachel Bronheim, Amit Jain, Adnan Munawar",2025-06-25T14:43:58Z,2025-07-27T14:01:00Z,http://arxiv.org/abs/2506.20496v2,http://arxiv.org/pdf/2506.20496v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Hierarchical Reinforcement Learning and Value Optimization for
  Challenging Quadruped Locomotion","We propose a novel hierarchical reinforcement learning framework for
quadruped locomotion over challenging terrain. Our approach incorporates a
two-layer hierarchy in which a high-level policy (HLP) selects optimal goals
for a low-level policy (LLP). The LLP is trained using an on-policy
actor-critic RL algorithm and is given footstep placements as goals. We propose
an HLP that does not require any additional training or environment samples and
instead operates via an online optimization process over the learned value
function of the LLP. We demonstrate the benefits of this framework by comparing
it with an end-to-end reinforcement learning (RL) approach. We observe
improvements in its ability to achieve higher rewards with fewer collisions
across an array of different terrains, including terrains more difficult than
any encountered during training.","Jeremiah Coholich, Muhammad Ali Murtaza, Seth Hutchinson, Zsolt Kira",2025-06-24T22:19:15Z,2025-06-24T22:19:15Z,http://arxiv.org/abs/2506.20036v1,http://arxiv.org/pdf/2506.20036v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Robust Embodied Self-Identification of Morphology in Damaged
  Multi-Legged Robots","Multi-legged robots (MLRs) are vulnerable to leg damage during complex
missions, which can impair their performance. This paper presents a
self-modeling and damage identification algorithm that enables autonomous
adaptation to partial or complete leg loss using only data from a low-cost IMU.
A novel FFT-based filter is introduced to address time-inconsistent signals,
improving damage detection by comparing body orientation between the robot and
its model. The proposed method identifies damaged legs and updates the robot's
model for integration into its control system. Experiments on uneven terrain
validate its robustness and computational efficiency.","Sahand Farghdani, Mili Patel, Robin Chhabra",2025-06-24T19:57:46Z,2025-06-24T19:57:46Z,http://arxiv.org/abs/2506.19984v1,http://arxiv.org/pdf/2506.19984v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Robotics Under Construction: Challenges on Job Sites,"As labor shortages and productivity stagnation increasingly challenge the
construction industry, automation has become essential for sustainable
infrastructure development. This paper presents an autonomous payload
transportation system as an initial step toward fully unmanned construction
sites. Our system, based on the CD110R-3 crawler carrier, integrates autonomous
navigation, fleet management, and GNSS-based localization to facilitate
material transport in construction site environments. While the current system
does not yet incorporate dynamic environment adaptation algorithms, we have
begun fundamental investigations into external-sensor based perception and
mapping system. Preliminary results highlight the potential challenges,
including navigation in evolving terrain, environmental perception under
construction-specific conditions, and sensor placement optimization for
improving autonomy and efficiency. Looking forward, we envision a construction
ecosystem where collaborative autonomous agents dynamically adapt to site
conditions, optimizing workflow and reducing human intervention. This paper
provides foundational insights into the future of robotics-driven construction
automation and identifies critical areas for further technological development.","Haruki Uchiito, Akhilesh Bhat, Koji Kusaka, Xiaoya Zhang, Hiraku Kinjo, Honoka Uehara, Motoki Koyama, Shinji Natsume",2025-06-24T13:07:43Z,2025-06-24T13:07:43Z,http://arxiv.org/abs/2506.19597v1,http://arxiv.org/pdf/2506.19597v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"TritonZ: A Remotely Operated Underwater Rover with Manipulator Arm for
  Exploration and Rescue Operations","The increasing demand for underwater exploration and rescue operations
enforces the development of advanced wireless or semi-wireless underwater
vessels equipped with manipulator arms. This paper presents the implementation
of a semi-wireless underwater vehicle, ""TritonZ"" equipped with a manipulator
arm, tailored for effective underwater exploration and rescue operations. The
vehicle's compact design enables deployment in different submarine
surroundings, addressing the need for wireless systems capable of navigating
challenging underwater terrains. The manipulator arm can interact with the
environment, allowing the robot to perform sophisticated tasks during
exploration and rescue missions in emergency situations. TritonZ is equipped
with various sensors such as Pi-Camera, Humidity, and Temperature sensors to
send real-time environmental data. Our underwater vehicle controlled using a
customized remote controller can navigate efficiently in the water where
Pi-Camera enables live streaming of the surroundings. Motion control and video
capture are performed simultaneously using this camera. The manipulator arm is
designed to perform various tasks, similar to grasping, manipulating, and
collecting underwater objects. Experimental results shows the efficacy of the
proposed remotely operated vehicle in performing a variety of underwater
exploration and rescue tasks. Additionally, the results show that TritonZ can
maintain an average of 13.5cm/s with a minimal delay of 2-3 seconds.
Furthermore, the vehicle can sustain waves underwater by maintaining its
position as well as average velocity. The full project details and source code
can be accessed at this link: https://github.com/kawser-ahmed-byte/TritonZ","Kawser Ahmed, Mir Shahriar Fardin, Md Arif Faysal Nayem, Fahim Hafiz, Swakkhar Shatabda",2025-06-23T06:52:38Z,2025-06-27T06:11:26Z,http://arxiv.org/abs/2506.18343v2,http://arxiv.org/pdf/2506.18343v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"StereoTacTip: Vision-based Tactile Sensing with Biomimetic Skin-Marker
  Arrangements","Vision-Based Tactile Sensors (VBTSs) stand out for their superior performance
due to their high-information content output. Recently, marker-based VBTSs have
been shown to give accurate geometry reconstruction when using stereo cameras.
\uhl{However, many marker-based VBTSs use complex biomimetic skin-marker
arrangements, which presents issues for the geometric reconstruction of the
skin surface from the markers}. Here we investigate how the marker-based skin
morphology affects stereo vision-based tactile sensing, using a novel VBTS
called the StereoTacTip. To achieve accurate geometry reconstruction, we
introduce: (i) stereo marker matching and tracking using a novel
Delaunay-Triangulation-Ring-Coding algorithm; (ii) a refractive depth
correction model that corrects the depth distortion caused by refraction in the
internal media; (iii) a skin surface correction model from the marker
positions, relying on an inverse calculation of normals to the skin surface;
and (iv)~methods for geometry reconstruction over multiple contacts. To
demonstrate these findings, we reconstruct topographic terrains on a large 3D
map. Even though contributions (i) and (ii) were developed for biomimetic
markers, they should improve the performance of all marker-based VBTSs.
Overall, this work illustrates that a thorough understanding and evaluation of
the morphologically-complex skin and marker-based tactile sensor principles are
crucial for obtaining accurate geometric information.","Chenghua Lu, Kailuan Tang, Xueming Hui, Haoran Li, Saekwang Nam, Nathan F. Lepora",2025-06-22T13:49:16Z,2025-06-22T13:49:16Z,http://arxiv.org/abs/2506.18040v1,http://arxiv.org/pdf/2506.18040v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Risk-Guided Diffusion: Toward Deploying Robot Foundation Models in
  Space, Where Failure Is Not An Option","Safe, reliable navigation in extreme, unfamiliar terrain is required for
future robotic space exploration missions. Recent generative-AI methods learn
semantically aware navigation policies from large, cross-embodiment datasets,
but offer limited safety guarantees. Inspired by human cognitive science, we
propose a risk-guided diffusion framework that fuses a fast, learned ""System-1""
with a slow, physics-based ""System-2"", sharing computation at both training and
inference to couple adaptability with formal safety. Hardware experiments
conducted at the NASA JPL's Mars-analog facility, Mars Yard, show that our
approach reduces failure rates by up to $4\times$ while matching the
goal-reaching performance of learning-based robotic models by leveraging
inference-time compute without any additional training.","Rohan Thakker, Adarsh Patnaik, Vince Kurtz, Jonas Frey, Jonathan Becktor, Sangwoo Moon, Rob Royce, Marcel Kaufmann, Georgios Georgakis, Pascal Roth, Joel Burdick, Marco Hutter, Shehryar Khattak",2025-06-21T05:39:04Z,2025-06-21T05:39:04Z,http://arxiv.org/abs/2506.17601v1,http://arxiv.org/pdf/2506.17601v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"A workflow for generating synthetic LiDAR datasets in simulation
  environments","This paper presents a simulation workflow for generating synthetic LiDAR
datasets to support autonomous vehicle perception, robotics research, and
sensor security analysis. Leveraging the CoppeliaSim simulation environment and
its Python API, we integrate time-of-flight LiDAR, image sensors, and two
dimensional scanners onto a simulated vehicle platform operating within an
urban scenario. The workflow automates data capture, storage, and annotation
across multiple formats (PCD, PLY, CSV), producing synchronized multimodal
datasets with ground truth pose information. We validate the pipeline by
generating large-scale point clouds and corresponding RGB and depth imagery.
The study examines potential security vulnerabilities in LiDAR data, such as
adversarial point injection and spoofing attacks, and demonstrates how
synthetic datasets can facilitate the evaluation of defense strategies.
Finally, limitations related to environmental realism, sensor noise modeling,
and computational scalability are discussed, and future research directions,
such as incorporating weather effects, real-world terrain models, and advanced
scanner configurations, are proposed. The workflow provides a versatile,
reproducible framework for generating high-fidelity synthetic LiDAR datasets to
advance perception research and strengthen sensor security in autonomous
systems. Documentation and examples accompany this framework; samples of
animated cloud returns and image sensor data can be found at this Link.","Abhishek Phadke, Shakib Mahmud Dipto, Pratip Rana",2025-06-20T17:56:15Z,2025-06-20T17:56:15Z,http://arxiv.org/abs/2506.17378v1,http://arxiv.org/pdf/2506.17378v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
LunarLoc: Segment-Based Global Localization on the Moon,"Global localization is necessary for autonomous operations on the lunar
surface where traditional Earth-based navigation infrastructure, such as GPS,
is unavailable. As NASA advances toward sustained lunar presence under the
Artemis program, autonomous operations will be an essential component of tasks
such as robotic exploration and infrastructure deployment. Tasks such as
excavation and transport of regolith require precise pose estimation, but
proposed approaches such as visual-inertial odometry (VIO) accumulate odometry
drift over long traverses. Precise pose estimation is particularly important
for upcoming missions such as the ISRU Pilot Excavator (IPEx) that rely on
autonomous agents to operate over extended timescales and varied terrain. To
help overcome odometry drift over long traverses, we propose LunarLoc, an
approach to global localization that leverages instance segmentation for
zero-shot extraction of boulder landmarks from onboard stereo imagery. Segment
detections are used to construct a graph-based representation of the terrain,
which is then aligned with a reference map of the environment captured during a
previous session using graph-theoretic data association. This method enables
accurate and drift-free global localization in visually ambiguous settings.
LunarLoc achieves sub-cm level accuracy in multi-session global localization
experiments, significantly outperforming the state of the art in lunar global
localization. To encourage the development of further methods for global
localization on the Moon, we release our datasets publicly with a playback
module: https://github.com/mit-acl/lunarloc-data.","Annika Thomas, Robaire Galliath, Aleksander Garbuz, Luke Anger, Cormac O'Neill, Trevor Johst, Dami Thomas, George Lordos, Jonathan P. How",2025-06-20T12:06:47Z,2025-06-20T12:06:47Z,http://arxiv.org/abs/2506.16940v1,http://arxiv.org/pdf/2506.16940v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"VLM-Empowered Multi-Mode System for Efficient and Safe Planetary
  Navigation","The increasingly complex and diverse planetary exploration environment
requires more adaptable and flexible rover navigation strategy. In this study,
we propose a VLM-empowered multi-mode system to achieve efficient while safe
autonomous navigation for planetary rovers. Vision-Language Model (VLM) is used
to parse scene information by image inputs to achieve a human-level
understanding of terrain complexity. Based on the complexity classification,
the system switches to the most suitable navigation mode, composing of
perception, mapping and planning modules designed for different terrain types,
to traverse the terrain ahead before reaching the next waypoint. By integrating
the local navigation system with a map server and a global waypoint generation
module, the rover is equipped to handle long-distance navigation tasks in
complex scenarios. The navigation system is evaluated in various simulation
environments. Compared to the single-mode conservative navigation method, our
multi-mode system is able to bootstrap the time and energy efficiency in a
long-distance traversal with varied type of obstacles, enhancing efficiency by
79.5%, while maintaining its avoidance capabilities against terrain hazards to
guarantee rover safety. More system information is shown at
https://chengsn1234.github.io/multi-mode-planetary-navigation/.","Sinuo Cheng, Ruyi Zhou, Wenhao Feng, Huaiguang Yang, Haibo Gao, Zongquan Deng, Liang Ding",2025-06-20T02:47:55Z,2025-06-20T02:47:55Z,http://arxiv.org/abs/2506.16703v1,http://arxiv.org/pdf/2506.16703v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Robust control for multi-legged elongate robots in noisy environments,"Modern two and four legged robots exhibit impressive mobility on complex
terrain, largely attributed to advancement in learning algorithms. However,
these systems often rely on high-bandwidth sensing and onboard computation to
perceive/respond to terrain uncertainties. Further, current locomotion
strategies typically require extensive robot-specific training, limiting their
generalizability across platforms. Building on our prior research connecting
robot-environment interaction and communication theory, we develop a new
paradigm to construct robust and simply controlled multi-legged elongate robots
(MERs) capable of operating effectively in cluttered, unstructured
environments. In this framework, each leg-ground contact is thought of as a
basic active contact (bac), akin to bits in signal transmission. Reliable
locomotion can be achieved in open-loop on ""noisy"" landscapes via sufficient
redundancy in bacs. In such situations, robustness is achieved through passive
mechanical responses. We term such processes as those displaying mechanical
intelligence (MI) and analogize these processes to forward error correction
(FEC) in signal transmission. To augment MI, we develop feedback control
schemes, which we refer to as computational intelligence (CI) and such
processes analogize automatic repeat request (ARQ) in signal transmission.
Integration of these analogies between locomotion and communication theory
allow analysis, design, and prediction of embodied intelligence control schemes
(integrating MI and CI) in MERs, showing effective and reliable performance
(approximately half body lengths per cycle) on complex landscapes with terrain
""noise"" over twice the robot's height. Our work provides a foundation for
systematic development of MER control, paving the way for terrain-agnostic,
agile, and resilient robotic systems capable of operating in extreme
environments.","Baxi Chong, Juntao He, Daniel Irvine, Tianyu Wang, Esteban Flores, Daniel Soto, Jianfeng Lin, Zhaochen Xu, Vincent R Nienhusser, Grigoriy Blekherman, Daniel I. Goldman",2025-06-18T18:16:59Z,2025-06-18T18:16:59Z,http://arxiv.org/abs/2506.15788v1,http://arxiv.org/pdf/2506.15788v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Human Locomotion Implicit Modeling Based Real-Time Gait Phase Estimation,"Gait phase estimation based on inertial measurement unit (IMU) signals
facilitates precise adaptation of exoskeletons to individual gait variations.
However, challenges remain in achieving high accuracy and robustness,
particularly during periods of terrain changes. To address this, we develop a
gait phase estimation neural network based on implicit modeling of human
locomotion, which combines temporal convolution for feature extraction with
transformer layers for multi-channel information fusion. A channel-wise masked
reconstruction pre-training strategy is proposed, which first treats gait phase
state vectors and IMU signals as joint observations of human locomotion, thus
enhancing model generalization. Experimental results demonstrate that the
proposed method outperforms existing baseline approaches, achieving a gait
phase RMSE of $2.729 \pm 1.071%$ and phase rate MAE of $0.037 \pm 0.016%$ under
stable terrain conditions with a look-back window of 2 seconds, and a phase
RMSE of $3.215 \pm 1.303%$ and rate MAE of $0.050 \pm 0.023%$ under terrain
transitions. Hardware validation on a hip exoskeleton further confirms that the
algorithm can reliably identify gait cycles and key events, adapting to various
continuous motion scenarios. This research paves the way for more intelligent
and adaptive exoskeleton systems, enabling safer and more efficient human-robot
interaction across diverse real-world environments.","Yuanlong Ji, Xingbang Yang, Ruoqi Zhao, Qihan Ye, Quan Zheng, Yubo Fan",2025-06-18T05:15:20Z,2025-06-18T05:15:20Z,http://arxiv.org/abs/2506.15150v1,http://arxiv.org/pdf/2506.15150v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Booster Gym: An End-to-End Reinforcement Learning Framework for Humanoid
  Robot Locomotion","Recent advancements in reinforcement learning (RL) have led to significant
progress in humanoid robot locomotion, simplifying the design and training of
motion policies in simulation. However, the numerous implementation details
make transferring these policies to real-world robots a challenging task. To
address this, we have developed a comprehensive code framework that covers the
entire process from training to deployment, incorporating common RL training
methods, domain randomization, reward function design, and solutions for
handling parallel structures. This library is made available as a community
resource, with detailed descriptions of its design and experimental results. We
validate the framework on the Booster T1 robot, demonstrating that the trained
policies seamlessly transfer to the physical platform, enabling capabilities
such as omnidirectional walking, disturbance resistance, and terrain
adaptability. We hope this work provides a convenient tool for the robotics
community, accelerating the development of humanoid robots. The code can be
found in https://github.com/BoosterRobotics/booster_gym.","Yushi Wang, Penghui Chen, Xinyu Han, Feng Wu, Mingguo Zhao",2025-06-18T04:24:49Z,2025-06-18T04:24:49Z,http://arxiv.org/abs/2506.15132v1,http://arxiv.org/pdf/2506.15132v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Feedback-MPPI: Fast Sampling-Based MPC via Rollout Differentiation --
  Adios low-level controllers","Model Predictive Path Integral control is a powerful sampling-based approach
suitable for complex robotic tasks due to its flexibility in handling nonlinear
dynamics and non-convex costs. However, its applicability in real-time,
highfrequency robotic control scenarios is limited by computational demands.
This paper introduces Feedback-MPPI (F-MPPI), a novel framework that augments
standard MPPI by computing local linear feedback gains derived from sensitivity
analysis inspired by Riccati-based feedback used in gradient-based MPC. These
gains allow for rapid closed-loop corrections around the current state without
requiring full re-optimization at each timestep. We demonstrate the
effectiveness of F-MPPI through simulations and real-world experiments on two
robotic platforms: a quadrupedal robot performing dynamic locomotion on uneven
terrain and a quadrotor executing aggressive maneuvers with onboard
computation. Results illustrate that incorporating local feedback significantly
improves control performance and stability, enabling robust, high-frequency
operation suitable for complex robotic systems.","Tommaso Belvedere, Michael Ziegltrum, Giulio Turrisi, Valerio Modugno",2025-06-17T07:47:33Z,2025-06-17T07:47:33Z,http://arxiv.org/abs/2506.14855v1,http://arxiv.org/pdf/2506.14855v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"UAV Object Detection and Positioning in a Mining Industrial Metaverse
  with Custom Geo-Referenced Data","The mining sector increasingly adopts digital tools to improve operational
efficiency, safety, and data-driven decision-making. One of the key challenges
remains the reliable acquisition of high-resolution, geo-referenced spatial
information to support core activities such as extraction planning and on-site
monitoring. This work presents an integrated system architecture that combines
UAV-based sensing, LiDAR terrain modeling, and deep learning-based object
detection to generate spatially accurate information for open-pit mining
environments. The proposed pipeline includes geo-referencing, 3D
reconstruction, and object localization, enabling structured spatial outputs to
be integrated into an industrial digital twin platform. Unlike traditional
static surveying methods, the system offers higher coverage and automation
potential, with modular components suitable for deployment in real-world
industrial contexts. While the current implementation operates in post-flight
batch mode, it lays the foundation for real-time extensions. The system
contributes to the development of AI-enhanced remote sensing in mining by
demonstrating a scalable and field-validated geospatial data workflow that
supports situational awareness and infrastructure safety.","Vasiliki Balaska, Ioannis Tsampikos Papapetros, Katerina Maria Oikonomou, Loukas Bampis, Antonios Gasteratos",2025-06-16T13:59:56Z,2025-06-16T13:59:56Z,http://arxiv.org/abs/2506.13505v1,http://arxiv.org/pdf/2506.13505v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Data-Driven Prediction of Dynamic Interactions Between Robot Appendage
  and Granular Material","An alternative data-driven modeling approach has been proposed and employed
to gain fundamental insights into robot motion interaction with granular
terrain at certain length scales. The approach is based on an integration of
dimension reduction (Sequentially Truncated Higher-Order Singular Value
Decomposition), surrogate modeling (Gaussian Process), and data assimilation
techniques (Reduced Order Particle Filter). This approach can be used online
and is based on offline data, obtained from the offline collection of
high-fidelity simulation data and a set of sparse experimental data. The
results have shown that orders of magnitude reduction in computational time can
be obtained from the proposed data-driven modeling approach compared with
physics-based high-fidelity simulations. With only simulation data as input,
the data-driven prediction technique can generate predictions that have
comparable accuracy as simulations. With both simulation data and sparse
physical experimental measurement as input, the data-driven approach with its
embedded data assimilation techniques has the potential in outperforming only
high-fidelity simulations for the long-horizon predictions. In addition, it is
demonstrated that the data-driven modeling approach can also reproduce the
scaling relationship recovered by physics-based simulations for maximum
resistive forces, which may indicate its general predictability beyond a
case-by-case basis. The results are expected to help robot navigation and
exploration in unknown and complex terrains during both online and offline
phases.","Guanjin Wang, Xiangxue Zhao, Shapour Azarm, Balakumar Balachandran",2025-06-12T16:43:21Z,2025-06-12T16:43:21Z,http://arxiv.org/abs/2506.10875v1,http://arxiv.org/pdf/2506.10875v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Locomotion on Constrained Footholds via Layered Architectures and Model
  Predictive Control","Computing stabilizing and optimal control actions for legged locomotion in
real time is difficult due to the nonlinear, hybrid, and high dimensional
nature of these robots. The hybrid nature of the system introduces a
combination of discrete and continuous variables which causes issues for
numerical optimal control. To address these challenges, we propose a layered
architecture that separates the choice of discrete variables and a smooth Model
Predictive Controller (MPC). The layered formulation allows for online
flexibility and optimality without sacrificing real-time performance through a
combination of gradient-free and gradient-based methods. The architecture
leverages a sampling-based method for determining discrete variables, and a
classical smooth MPC formulation using these fixed discrete variables. We
demonstrate the results on a quadrupedal robot stepping over gaps and onto
terrain with varying heights. In simulation, we demonstrate the controller on a
humanoid robot for gap traversal. The layered approach is shown to be more
optimal and reliable than common heuristic-based approaches and faster to
compute than pure sampling methods.","Zachary Olkin, Aaron D. Ames",2025-06-11T17:54:44Z,2025-08-24T00:44:37Z,http://arxiv.org/abs/2506.09979v2,http://arxiv.org/pdf/2506.09979v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Tightly-Coupled LiDAR-IMU-Leg Odometry with Online Learned Leg
  Kinematics Incorporating Foot Tactile Information","In this letter, we present tightly coupled LiDAR-IMU-leg odometry, which is
robust to challenging conditions such as featureless environments and
deformable terrains. We developed an online learning-based leg kinematics model
named the neural leg kinematics model, which incorporates tactile information
(foot reaction force) to implicitly express the nonlinear dynamics between
robot feet and the ground. Online training of this model enhances its
adaptability to weight load changes of a robot (e.g., assuming delivery or
transportation tasks) and terrain conditions. According to the \textit{neural
adaptive leg odometry factor} and online uncertainty estimation of the leg
kinematics model-based motion predictions, we jointly solve online training of
this kinematics model and odometry estimation on a unified factor graph to
retain the consistency of both. The proposed method was verified through real
experiments using a quadruped robot in two challenging situations: 1) a sandy
beach, representing an extremely featureless area with a deformable terrain,
and 2) a campus, including multiple featureless areas and terrain types of
asphalt, gravel (deformable terrain), and grass. Experimental results showed
that our odometry estimation incorporating the \textit{neural leg kinematics
model} outperforms state-of-the-art works. Our project page is available for
further details: https://takuokawara.github.io/RAL2025_project_page/","Taku Okawara, Kenji Koide, Aoki Takanose, Shuji Oishi, Masashi Yokozuka, Kentaro Uno, Kazuya Yoshida",2025-06-11T09:28:07Z,2025-07-02T04:53:47Z,http://arxiv.org/abs/2506.09548v2,http://arxiv.org/pdf/2506.09548v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"MoRE: Mixture of Residual Experts for Humanoid Lifelike Gaits Learning
  on Complex Terrains","Humanoid robots have demonstrated robust locomotion capabilities using
Reinforcement Learning (RL)-based approaches. Further, to obtain human-like
behaviors, existing methods integrate human motion-tracking or motion prior in
the RL framework. However, these methods are limited in flat terrains with
proprioception only, restricting their abilities to traverse challenging
terrains with human-like gaits. In this work, we propose a novel framework
using a mixture of latent residual experts with multi-discriminators to train
an RL policy, which is capable of traversing complex terrains in controllable
lifelike gaits with exteroception. Our two-stage training pipeline first
teaches the policy to traverse complex terrains using a depth camera, and then
enables gait-commanded switching between human-like gait patterns. We also
design gait rewards to adjust human-like behaviors like robot base height.
Simulation and real-world experiments demonstrate that our framework exhibits
exceptional performance in traversing complex terrains, and achieves seamless
transitions between multiple human-like gait patterns.","Dewei Wang, Xinmiao Wang, Xinzhe Liu, Jiyuan Shi, Yingnan Zhao, Chenjia Bai, Xuelong Li",2025-06-10T14:25:58Z,2025-06-12T03:06:12Z,http://arxiv.org/abs/2506.08840v2,http://arxiv.org/pdf/2506.08840v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
CARoL: Context-aware Adaptation for Robot Learning,"Using Reinforcement Learning (RL) to learn new robotic tasks from scratch is
often inefficient. Leveraging prior knowledge has the potential to
significantly enhance learning efficiency, which, however, raises two critical
challenges: how to determine the relevancy of existing knowledge and how to
adaptively integrate them into learning a new task. In this paper, we propose
Context-aware Adaptation for Robot Learning (CARoL), a novel framework to
efficiently learn a similar but distinct new task from prior knowledge. CARoL
incorporates context awareness by analyzing state transitions in system
dynamics to identify similarities between the new task and prior knowledge. It
then utilizes these identified similarities to prioritize and adapt specific
knowledge pieces for the new task. Additionally, CARoL has a broad
applicability spanning policy-based, value-based, and actor-critic RL
algorithms. We validate the efficiency and generalizability of CARoL on both
simulated robotic platforms and physical ground vehicles. The simulations
include CarRacing and LunarLander environments, where CARoL demonstrates faster
convergence and higher rewards when learning policies for new tasks. In
real-world experiments, we show that CARoL enables a ground vehicle to quickly
and efficiently adapt policies learned in simulation to smoothly traverse
real-world off-road terrain.","Zechen Hu, Tong Xu, Xuesu Xiao, Xuan Wang",2025-06-08T06:05:32Z,2025-06-08T06:05:32Z,http://arxiv.org/abs/2506.07006v1,http://arxiv.org/pdf/2506.07006v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Spatiotemporal Contrastive Learning for Cross-View Video Localization in
  Unstructured Off-road Terrains","Robust cross-view 3-DoF localization in GPS-denied, off-road environments
remains challenging due to (1) perceptual ambiguities from repetitive
vegetation and unstructured terrain, and (2) seasonal shifts that significantly
alter scene appearance, hindering alignment with outdated satellite imagery. To
address this, we introduce MoViX, a self-supervised cross-view video
localization framework that learns viewpoint- and season-invariant
representations while preserving directional awareness essential for accurate
localization. MoViX employs a pose-dependent positive sampling strategy to
enhance directional discrimination and temporally aligned hard negative mining
to discourage shortcut learning from seasonal cues. A motion-informed frame
sampler selects spatially diverse frames, and a lightweight temporal aggregator
emphasizes geometrically aligned observations while downweighting ambiguous
ones. At inference, MoViX runs within a Monte Carlo Localization framework,
using a learned cross-view matching module in place of handcrafted models.
Entropy-guided temperature scaling enables robust multi-hypothesis tracking and
confident convergence under visual ambiguity. We evaluate MoViX on the
TartanDrive 2.0 dataset, training on under 30 minutes of data and testing over
12.29 km. Despite outdated satellite imagery, MoViX localizes within 25 meters
of ground truth 93% of the time, and within 50 meters 100% of the time in
unseen regions, outperforming state-of-the-art baselines without
environment-specific tuning. We further demonstrate generalization on a
real-world off-road dataset from a geographically distinct site with a
different robot platform.","Zhiyun Deng, Dongmyeong Lee, Amanda Adkins, Jesse Quattrociocchi, Christian Ellis, Joydeep Biswas",2025-06-05T17:10:29Z,2025-06-05T17:10:29Z,http://arxiv.org/abs/2506.05250v1,http://arxiv.org/pdf/2506.05250v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"PulseRide: A Robotic Wheelchair for Personalized Exertion Control with
  Human-in-the-Loop Reinforcement Learning","Maintaining an active lifestyle is vital for quality of life, yet challenging
for wheelchair users. For instance, powered wheelchairs face increasing risks
of obesity and deconditioning due to inactivity. Conversely, manual wheelchair
users, who propel the wheelchair by pushing the wheelchair's handrims, often
face upper extremity injuries from repetitive motions. These challenges
underscore the need for a mobility system that promotes activity while
minimizing injury risk. Maintaining optimal exertion during wheelchair use
enhances health benefits and engagement, yet the variations in individual
physiological responses complicate exertion optimization. To address this, we
introduce PulseRide, a novel wheelchair system that provides personalized
assistance based on each user's physiological responses, helping them maintain
their physical exertion goals. Unlike conventional assistive systems focused on
obstacle avoidance and navigation, PulseRide integrates real-time physiological
data-such as heart rate and ECG-with wheelchair speed to deliver adaptive
assistance. Using a human-in-the-loop reinforcement learning approach with Deep
Q-Network algorithm (DQN), the system adjusts push assistance to keep users
within a moderate activity range without under- or over-exertion. We conducted
preliminary tests with 10 users on various terrains, including carpet and
slate, to assess PulseRide's effectiveness. Our findings show that, for
individual users, PulseRide maintains heart rates within the moderate activity
zone as much as 71.7 percent longer than manual wheelchairs. Among all users,
we observed an average reduction in muscle contractions of 41.86 percent,
delaying fatigue onset and enhancing overall comfort and engagement. These
results indicate that PulseRide offers a healthier, adaptive mobility solution,
bridging the gap between passive and physically taxing mobility options.","Azizul Zahid, Bibek Poudel, Danny Scott, Jason Scott, Scott Crouter, Weizi Li, Sai Swaminathan",2025-06-05T14:00:59Z,2025-06-05T14:00:59Z,http://arxiv.org/abs/2506.05056v1,http://arxiv.org/pdf/2506.05056v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Multimodal Limbless Crawling Soft Robot with a Kirigami Skin,"Limbless creatures can crawl on flat surfaces by deforming their bodies and
interacting with asperities on the ground, offering a biological blueprint for
designing efficient limbless robots. Inspired by this natural locomotion, we
present a soft robot capable of navigating complex terrains using a combination
of rectilinear motion and asymmetric steering gaits. The robot is made of a
pair of antagonistic inflatable soft actuators covered with a flexible kirigami
skin with asymmetric frictional properties. The robot's rectilinear locomotion
is achieved through cyclic inflation of internal chambers with precise phase
shifts, enabling forward progression. Steering is accomplished using an
asymmetric gait, allowing for both in-place rotation and wide turns. To
validate its mobility in obstacle-rich environments, we tested the robot in an
arena with coarse substrates and multiple obstacles. Real-time feedback from
onboard proximity sensors, integrated with a human-machine interface (HMI),
allowed adaptive control to avoid collisions. This study highlights the
potential of bioinspired soft robots for applications in confined or
unstructured environments, such as search-and-rescue operations, environmental
monitoring, and industrial inspections.","Jonathan Tirado, Aida Parvaresh, Burcu Seyidoğlu, Darryl A. Bedford, Jonas Jørgensen, Ahmad Rafsanjani",2025-06-05T01:37:51Z,2025-06-05T01:37:51Z,http://arxiv.org/abs/2506.04547v1,http://arxiv.org/pdf/2506.04547v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Online Adaptation of Terrain-Aware Dynamics for Planning in Unstructured
  Environments","Autonomous mobile robots operating in remote, unstructured environments must
adapt to new, unpredictable terrains that can change rapidly during operation.
In such scenarios, a critical challenge becomes estimating the robot's dynamics
on changing terrain in order to enable reliable, accurate navigation and
planning. We present a novel online adaptation approach for terrain-aware
dynamics modeling and planning using function encoders. Our approach
efficiently adapts to new terrains at runtime using limited online data without
retraining or fine-tuning. By learning a set of neural network basis functions
that span the robot dynamics on diverse terrains, we enable rapid online
adaptation to new, unseen terrains and environments as a simple least-squares
calculation. We demonstrate our approach for terrain adaptation in a
Unity-based robotics simulator and show that the downstream controller has
better empirical performance due to higher accuracy of the learned model. This
leads to fewer collisions with obstacles while navigating in cluttered
environments as compared to a neural ODE baseline.","William Ward, Sarah Etter, Tyler Ingebrand, Christian Ellis, Adam J. Thorpe, Ufuk Topcu",2025-06-04T22:03:57Z,2025-07-16T20:26:15Z,http://arxiv.org/abs/2506.04484v2,http://arxiv.org/pdf/2506.04484v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Learning Smooth State-Dependent Traversability from Dense Point Clouds,"A key open challenge in off-road autonomy is that the traversability of
terrain often depends on the vehicle's state. In particular, some obstacles are
only traversable from some orientations. However, learning this interaction by
encoding the angle of approach as a model input demands a large and diverse
training dataset and is computationally inefficient during planning due to
repeated model inference. To address these challenges, we present SPARTA, a
method for estimating approach angle conditioned traversability from point
clouds. Specifically, we impose geometric structure into our network by
outputting a smooth analytical function over the 1-Sphere that predicts risk
distribution for any angle of approach with minimal overhead and can be reused
for subsequent queries. The function is composed of Fourier basis functions,
which has important advantages for generalization due to their periodic nature
and smoothness. We demonstrate SPARTA both in a high-fidelity simulation
platform, where our model achieves a 91\% success rate crossing a 40m boulder
field (compared to 73\% for the baseline), and on hardware, illustrating the
generalization ability of the model to real-world settings.","Zihao Dong, Alan Papalia, Leonard Jung, Alenna Spiro, Philip R. Osteen, Christa S. Robison, Michael Everett",2025-06-04T18:21:54Z,2025-06-04T18:21:54Z,http://arxiv.org/abs/2506.04362v1,http://arxiv.org/pdf/2506.04362v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Phase-based Nonlinear Model Predictive Control for Humanoid Walking
  Stabilization with Single and Double Support Time Adjustments","Balance control for humanoid robots has been extensively studied to enable
robots to navigate in real-world environments. However, balance controllers
that explicitly optimize the durations of both the single support phase, also
known as step timing, and the Double Support Phase (DSP) have not been widely
explored due to the inherent nonlinearity of the associated optimization
problem. Consequently, many recent approaches either ignore the DSP or adjust
its duration based on heuristics or on linearization techniques that rely on
sequential coordination of balance strategies. This study proposes a novel
phase-based nonlinear Model Predictive Control (MPC) framework that
simultaneously optimizes Zero Moment Point~(ZMP) modulation, step location,
step timing, and DSP duration to maintain balance under external disturbances.
In simulation, the proposed controller was compared with two state-of-the-art
frameworks that rely on heuristics or sequential coordination of balance
strategies under two scenarios: forward walking on terrain emulating compliant
ground and external push recovery while walking in place. Overall, the findings
suggest that the proposed method offers more flexible coordination of balance
strategies than the sequential approach, and consistently outperforms the
heuristic approach. The robustness and effectiveness of the proposed controller
were also validated through experiments with a real humanoid robot.","Kwanwoo Lee, Gyeongjae Park, Jaeheung Park",2025-06-04T11:41:28Z,2025-06-04T11:41:28Z,http://arxiv.org/abs/2506.03856v1,http://arxiv.org/pdf/2506.03856v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"High-speed control and navigation for quadrupedal robots on complex and
  discrete terrain","High-speed legged navigation in discrete and geometrically complex
environments is a challenging task because of the high-degree-of-freedom
dynamics and long-horizon, nonconvex nature of the optimization problem. In
this work, we propose a hierarchical navigation pipeline for legged robots that
can traverse such environments at high speed. The proposed pipeline consists of
a planner and tracker module. The planner module finds physically feasible
foothold plans by sampling-based optimization with fast sequential filtering
using heuristics and a neural network. Subsequently, rollouts are performed in
a physics simulation to identify the best foothold plan regarding the
engineered cost function and to confirm its physical consistency. This
hierarchical planning module is computationally efficient and physically
accurate at the same time. The tracker aims to accurately step on the target
footholds from the planning module. During the training stage, the foothold
target distribution is given by a generative model that is trained
competitively with the tracker. This process ensures that the tracker is
trained in an environment with the desired difficulty. The resulting tracker
can overcome terrains that are more difficult than what the previous methods
could manage. We demonstrated our approach using Raibo, our in-house dynamic
quadruped robot. The results were dynamic and agile motions: Raibo is capable
of running on vertical walls, jumping a 1.3-meter gap, running over stepping
stones at 4 meters per second, and autonomously navigating on terrains full of
30{\deg} ramps, stairs, and boxes of various sizes.","Hyeongjun Kim, Hyunsik Oh, Jeongsoo Park, Yunho Kim, Donghoon Youm, Moonkyu Jung, Minho Lee, Jemin Hwangbo",2025-06-03T13:04:26Z,2025-06-03T13:04:26Z,http://arxiv.org/abs/2506.02835v1,http://arxiv.org/pdf/2506.02835v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Generating Diverse Challenging Terrains for Legged Robots Using
  Quality-Diversity Algorithm","While legged robots have achieved significant advancements in recent years,
ensuring the robustness of their controllers on unstructured terrains remains
challenging. It requires generating diverse and challenging unstructured
terrains to test the robot and discover its vulnerabilities. This topic remains
underexplored in the literature. This paper presents a Quality-Diversity
framework to generate diverse and challenging terrains that uncover weaknesses
in legged robot controllers. Our method, applied to both simulated bipedal and
quadruped robots, produces an archive of terrains optimized to challenge the
controller in different ways. Quantitative and qualitative analyses show that
the generated archive effectively contains terrains that the robots struggled
to traverse, presenting different failure modes. Interesting results were
observed, including failure cases that were not necessarily expected.
Experiments show that the generated terrains can also be used to improve
RL-based controllers.","Arthur Esquerre-Pourtère, Minsoo Kim, Jaeheung Park",2025-06-02T06:44:58Z,2025-06-02T06:44:58Z,http://arxiv.org/abs/2506.01362v1,http://arxiv.org/pdf/2506.01362v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"STATE-NAV: Stability-Aware Traversability Estimation for Bipedal
  Navigation on Rough Terrain","Bipedal robots have advantages in maneuvering human-centered environments,
but face greater failure risk compared to other stable mobile plarforms such as
wheeled or quadrupedal robots. While learning-based traversability has been
widely studied for these platforms, bipedal traversability has instead relied
on manually designed rules with limited consideration of locomotion stability
on rough terrain. In this work, we present the first learning-based
traversability estimation and risk-sensitive navigation framework for bipedal
robots operating in diverse, uneven environments. TravFormer, a
transformer-based neural network, is trained to predict bipedal instability
with uncertainty, enabling risk-aware and adaptive planning. Based on the
network, we define traversability as stability-aware command velocity-the
fastest command velocity that keeps instability below a user-defined limit.
This velocity-based traversability is integrated into a hierarchical planner
that combines traversability-informed Rapid Random Tree Star (TravRRT*) for
time-efficient planning and Model Predictive Control (MPC) for safe execution.
We validate our method in MuJoCo simulation, demonstrating improved navigation
performance, with enhanced robustness and time efficiency across varying
terrains compared to existing methods.","Ziwon Yoon, Lawrence Y. Zhu, Lu Gan, Ye Zhao",2025-06-01T15:13:54Z,2025-06-03T03:50:34Z,http://arxiv.org/abs/2506.01046v2,http://arxiv.org/pdf/2506.01046v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Hold My Beer: Learning Gentle Humanoid Locomotion and End-Effector
  Stabilization Control","Can your humanoid walk up and hand you a full cup of beer, without spilling a
drop? While humanoids are increasingly featured in flashy demos like dancing,
delivering packages, traversing rough terrain, fine-grained control during
locomotion remains a significant challenge. In particular, stabilizing a filled
end-effector (EE) while walking is far from solved, due to a fundamental
mismatch in task dynamics: locomotion demands slow-timescale, robust control,
whereas EE stabilization requires rapid, high-precision corrections. To address
this, we propose SoFTA, a Slow-Fast Two-Agent framework that decouples
upper-body and lower-body control into separate agents operating at different
frequencies and with distinct rewards. This temporal and objective separation
mitigates policy interference and enables coordinated whole-body behavior.
SoFTA executes upper-body actions at 100 Hz for precise EE control and
lower-body actions at 50 Hz for robust gait. It reduces EE acceleration by 2-5x
relative to baselines and performs much closer to human-level stability,
enabling delicate tasks such as carrying nearly full cups, capturing steady
video during locomotion, and disturbance rejection with EE stability.","Yitang Li, Yuanhang Zhang, Wenli Xiao, Chaoyi Pan, Haoyang Weng, Guanqi He, Tairan He, Guanya Shi",2025-05-30T04:18:09Z,2025-06-03T22:45:46Z,http://arxiv.org/abs/2505.24198v2,http://arxiv.org/pdf/2505.24198v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
LocoTouch: Learning Dynamic Quadrupedal Transport with Tactile Sensing,"Quadrupedal robots have demonstrated remarkable agility and robustness in
traversing complex terrains. However, they struggle with dynamic object
interactions, where contact must be precisely sensed and controlled. To bridge
this gap, we present LocoTouch, a system that equips quadrupedal robots with
tactile sensing to address a particularly challenging task in this category:
long-distance transport of unsecured cylindrical objects, which typically
requires custom mounting or fastening mechanisms to maintain stability. For
efficient large-area tactile sensing, we design a high-density distributed
tactile sensor that covers the entire back of the robot. To effectively
leverage tactile feedback for robot control, we develop a simulation
environment with high-fidelity tactile signals, and train tactile-aware
transport policies using a two-stage learning pipeline. Furthermore, we design
a novel reward function to promote robust, symmetric, and frequency-adaptive
locomotion gaits. After training in simulation, LocoTouch transfers zero-shot
to the real world, reliably transporting a wide range of unsecured cylindrical
objects with diverse sizes, weights, and surface properties. Moreover, it
remains robust over long distances, on uneven terrain, and under severe
perturbations.","Changyi Lin, Yuxin Ray Song, Boda Huo, Mingyang Yu, Yikai Wang, Shiqi Liu, Yuxiang Yang, Wenhao Yu, Tingnan Zhang, Jie Tan, Yiyue Luo, Ding Zhao",2025-05-29T07:12:50Z,2025-08-30T21:47:24Z,http://arxiv.org/abs/2505.23175v2,http://arxiv.org/pdf/2505.23175v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
A simulation framework for autonomous lunar construction work,"We present a simulation framework for lunar construction work involving
multiple autonomous machines. The framework supports modelling of construction
scenarios and autonomy solutions, execution of the scenarios in simulation, and
analysis of work time and energy consumption throughout the construction
project. The simulations are based on physics-based models for contacting
multibody dynamics and deformable terrain, including vehicle-soil interaction
forces and soil flow in real time. A behaviour tree manages the operational
logic and error handling, which enables the representation of complex
behaviours through a discrete set of simpler tasks in a modular hierarchical
structure. High-level decision-making is separated from lower-level control
algorithms, with the two connected via ROS2. Excavation movements are
controlled through inverse kinematics and tracking controllers. The framework
is tested and demonstrated on two different lunar construction scenarios that
involve an excavator and dump truck with actively controlled articulated
crawlers.","Mattias Linde, Daniel Lindmark, Sandra Ålstig, Martin Servin",2025-05-28T08:16:05Z,2025-08-12T07:41:09Z,http://arxiv.org/abs/2505.22091v2,http://arxiv.org/pdf/2505.22091v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Convergent Functions, Divergent Forms","We introduce LOKI, a compute-efficient framework for co-designing
morphologies and control policies that generalize across unseen tasks. Inspired
by biological adaptation -- where animals quickly adjust to morphological
changes -- our method overcomes the inefficiencies of traditional evolutionary
and quality-diversity algorithms. We propose learning convergent functions:
shared control policies trained across clusters of morphologically similar
designs in a learned latent space, drastically reducing the training cost per
design. Simultaneously, we promote divergent forms by replacing mutation with
dynamic local search, enabling broader exploration and preventing premature
convergence. The policy reuse allows us to explore 780$\times$ more designs
using 78% fewer simulation steps and 40% less compute per design. Local
competition paired with a broader search results in a diverse set of
high-performing final morphologies. Using the UNIMAL design space and a
flat-terrain locomotion task, LOKI discovers a rich variety of designs --
ranging from quadrupeds to crabs, bipedals, and spinners -- far more diverse
than those produced by prior work. These morphologies also transfer better to
unseen downstream tasks in agility, stability, and manipulation domains (e.g.,
2$\times$ higher reward on bump and push box incline tasks). Overall, our
approach produces designs that are both diverse and adaptable, with
substantially greater sample efficiency than existing co-design methods.
(Project website: https://loki-codesign.github.io/)","Hyeonseong Jeon, Ainaz Eftekhar, Aaron Walsman, Kuo-Hao Zeng, Ali Farhadi, Ranjay Krishna",2025-05-27T18:45:37Z,2025-05-27T18:45:37Z,http://arxiv.org/abs/2505.21665v1,http://arxiv.org/pdf/2505.21665v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Situationally-Aware Dynamics Learning,"Autonomous robots operating in complex, unstructured environments face
significant challenges due to latent, unobserved factors that obscure their
understanding of both their internal state and the external world. Addressing
this challenge would enable robots to develop a more profound grasp of their
operational context. To tackle this, we propose a novel framework for online
learning of hidden state representations, with which the robots can adapt in
real-time to uncertain and dynamic conditions that would otherwise be ambiguous
and result in suboptimal or erroneous behaviors. Our approach is formalized as
a Generalized Hidden Parameter Markov Decision Process, which explicitly models
the influence of unobserved parameters on both transition dynamics and reward
structures. Our core innovation lies in learning online the joint distribution
of state transitions, which serves as an expressive representation of latent
ego- and environmental-factors. This probabilistic approach supports the
identification and adaptation to different operational situations, improving
robustness and safety. Through a multivariate extension of Bayesian Online
Changepoint Detection, our method segments changes in the underlying data
generating process governing the robot's dynamics. The robot's transition model
is then informed with a symbolic representation of the current situation
derived from the joint distribution of latest state transitions, enabling
adaptive and context-aware decision-making. To showcase the real-world
effectiveness, we validate our approach in the challenging task of unstructured
terrain navigation, where unmodeled and unmeasured terrain characteristics can
significantly impact the robot's motion. Extensive experiments in both
simulation and real world reveal significant improvements in data efficiency,
policy performance, and the emergence of safer, adaptive navigation strategies.","Alejandro Murillo-Gonzalez, Lantao Liu",2025-05-26T06:40:11Z,2025-05-26T06:40:11Z,http://arxiv.org/abs/2505.19574v1,http://arxiv.org/pdf/2505.19574v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Deriving The Fundamental Equation of Earthmoving and Configuring Vortex
  Studio Earthmoving Simulation for Soil Property Estimation Experimentation","This document serves as supplementary material for two International Society
for Terrain-Vehicle Systems conference publications regarding in situ soil
property estimation by Wagner et al. in 2023 and 2025. It covers the derivation
of the fundamental equation of earthmoving for a flat blade moving through
sloped soil and provides some information regarding the advanced configuration
of Vortex Studio's soil-tool interaction simulation.",W. Jacob Wagner,2025-05-25T21:35:14Z,2025-05-25T21:35:14Z,http://arxiv.org/abs/2505.19330v1,http://arxiv.org/pdf/2505.19330v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Omni-Perception: Omnidirectional Collision Avoidance for Legged
  Locomotion in Dynamic Environments","Agile locomotion in complex 3D environments requires robust spatial awareness
to safely avoid diverse obstacles such as aerial clutter, uneven terrain, and
dynamic agents. Depth-based perception approaches often struggle with sensor
noise, lighting variability, computational overhead from intermediate
representations (e.g., elevation maps), and difficulties with non-planar
obstacles, limiting performance in unstructured environments. In contrast,
direct integration of LiDAR sensing into end-to-end learning for legged
locomotion remains underexplored. We propose Omni-Perception, an end-to-end
locomotion policy that achieves 3D spatial awareness and omnidirectional
collision avoidance by directly processing raw LiDAR point clouds. At its core
is PD-RiskNet (Proximal-Distal Risk-Aware Hierarchical Network), a novel
perception module that interprets spatio-temporal LiDAR data for environmental
risk assessment. To facilitate efficient policy learning, we develop a
high-fidelity LiDAR simulation toolkit with realistic noise modeling and fast
raycasting, compatible with platforms such as Isaac Gym, Genesis, and MuJoCo,
enabling scalable training and effective sim-to-real transfer. Learning
reactive control policies directly from raw LiDAR data enables the robot to
navigate complex environments with static and dynamic obstacles more robustly
than approaches relying on intermediate maps or limited sensing. We validate
Omni-Perception through real-world experiments and extensive simulation,
demonstrating strong omnidirectional avoidance capabilities and superior
locomotion performance in highly dynamic environments.","Zifan Wang, Teli Ma, Yufei Jia, Xun Yang, Jiaming Zhou, Wenlong Ouyang, Qiang Zhang, Junwei Liang",2025-05-25T16:21:19Z,2025-08-28T13:09:08Z,http://arxiv.org/abs/2505.19214v2,http://arxiv.org/pdf/2505.19214v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Staircase Recognition and Location Based on Polarization Vision,"Staircase is one of the most common structures in artificial scenes. However,
it is difficult for humanoid robots and people with lower limb disabilities or
visual impairment to cross the scene without the help of sensors and
intelligent algorithms. Staircase scene perception technology is a prerequisite
for recognition and localization. This technology is of great significance for
the mode switching of the robot and the calculation of the footprint position
to adapt to the discontinuous terrain. However, there are still many problems
that constrain the application of this technology, such as low recognition
accuracy, high initial noise from sensors, unstable output signals and high
computational requirements. In terms of scene reconstruction, the binocular and
time of flight (TOF) reconstruction of the scene can be easily affected by
environmental light and the surface material of the target object. In contrast,
due to the special structure of the polarizer, the polarization can selectively
transmit polarized light in a specific direction and this reconstruction method
relies on the polarization information of the object surface. So the advantages
of polarization reconstruction are reflected, which are less affected by
environmental light and not dependent on the texture information of the object
surface. In this paper, in order to achieve the detection of staircase, this
paper proposes a contrast enhancement algorithm that integrates polarization
and light intensity information, and integrates point cloud segmentation based
on YOLOv11. To realize the high-quality reconstruction, we proposed a method of
fusing polarized binocular and TOF depth information to realize the
three-dimensional (3D) reconstruction of the staircase. Besides, it also
proposes a joint calibration algorithm of monocular camera and TOF camera based
on ICP registration and improved gray wolf optimization algorithm.","Weifeng Kong, Zhiying Tan",2025-05-25T08:27:09Z,2025-08-28T00:52:41Z,http://arxiv.org/abs/2505.19026v3,http://arxiv.org/pdf/2505.19026v3.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"One Policy but Many Worlds: A Scalable Unified Policy for Versatile
  Humanoid Locomotion","Humanoid locomotion faces a critical scalability challenge: traditional
reinforcement learning (RL) methods require task-specific rewards and struggle
to leverage growing datasets, even as more training terrains are introduced. We
propose DreamPolicy, a unified framework that enables a single policy to master
diverse terrains and generalize zero-shot to unseen scenarios by systematically
integrating offline data and diffusion-driven motion synthesis. At its core,
DreamPolicy introduces Humanoid Motion Imagery (HMI) - future state predictions
synthesized through an autoregressive terrain-aware diffusion planner curated
by aggregating rollouts from specialized policies across various distinct
terrains. Unlike human motion datasets requiring laborious retargeting, our
data directly captures humanoid kinematics, enabling the diffusion planner to
synthesize ""dreamed"" trajectories that encode terrain-specific physical
constraints. These trajectories act as dynamic objectives for our
HMI-conditioned policy, bypassing manual reward engineering and enabling
cross-terrain generalization. DreamPolicy addresses the scalability limitations
of prior methods: while traditional RL fails to exploit growing datasets, our
framework scales seamlessly with more offline data. As the dataset expands, the
diffusion prior learns richer locomotion skills, which the policy leverages to
master new terrains without retraining. Experiments demonstrate that
DreamPolicy achieves average 90% success rates in training environments and an
average of 20% higher success on unseen terrains than the prevalent method. It
also generalizes to perturbed and composite scenarios where prior approaches
collapse. By unifying offline data, diffusion-based trajectory synthesis, and
policy optimization, DreamPolicy overcomes the ""one task, one policy""
bottleneck, establishing a paradigm for scalable, data-driven humanoid control.","Yahao Fan, Tianxiang Gui, Kaiyang Ji, Shutong Ding, Chixuan Zhang, Jiayuan Gu, Jingyi Yu, Jingya Wang, Ye Shi",2025-05-24T16:33:44Z,2025-06-03T03:10:46Z,http://arxiv.org/abs/2505.18780v2,http://arxiv.org/pdf/2505.18780v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
YOPO-Rally: A Sim-to-Real Single-Stage Planner for Off-Road Terrain,"Off-road navigation remains challenging for autonomous robots due to the
harsh terrain and clustered obstacles. In this letter, we extend the YOPO (You
Only Plan Once) end-to-end navigation framework to off-road environments,
explicitly focusing on forest terrains, consisting of a high-performance,
multi-sensor supported off-road simulator YOPO-Sim, a zero-shot transfer
sim-to-real planner YOPO-Rally, and an MPC controller. Built on the Unity
engine, the simulator can generate randomized forest environments and export
depth images and point cloud maps for expert demonstrations, providing
competitive performance with mainstream simulators. Terrain Traversability
Analysis (TTA) processes cost maps, generating expert trajectories represented
as non-uniform cubic Hermite curves. The planner integrates TTA and the
pathfinding into a single neural network that inputs the depth image, current
velocity, and the goal vector, and outputs multiple trajectory candidates with
costs. The planner is trained by behavior cloning in the simulator and deployed
directly into the real-world without fine-tuning. Finally, a series of
simulated and real-world experiments is conducted to validate the performance
of the proposed framework.","Hongyu Cao, Junjie Lu, Xuewei Zhang, Yulin Hui, Zhiyu Li, Bailing Tian",2025-05-24T14:27:43Z,2025-05-24T14:27:43Z,http://arxiv.org/abs/2505.18714v1,http://arxiv.org/pdf/2505.18714v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Reinforcement Learning for Ballbot Navigation in Uneven Terrain,"Ballbot (i.e. Ball balancing robot) navigation usually relies on methods
rooted in control theory (CT), and works that apply Reinforcement learning (RL)
to the problem remain rare while generally being limited to specific subtasks
(e.g. balance recovery). Unlike CT based methods, RL does not require
(simplifying) assumptions about environment dynamics (e.g. the absence of
slippage between the ball and the floor). In addition to this increased
accuracy in modeling, RL agents can easily be conditioned on additional
observations such as depth-maps without the need for explicit formulations from
first principles, leading to increased adaptivity. Despite those advantages,
there has been little to no investigation into the capabilities,
data-efficiency and limitations of RL based methods for ballbot control and
navigation. Furthermore, there is a notable absence of an open-source,
RL-friendly simulator for this task. In this paper, we present an open-source
ballbot simulation based on MuJoCo, and show that with appropriate conditioning
on exteroceptive observations as well as reward shaping, policies learned by
classical model-free RL methods are capable of effectively navigating through
randomly generated uneven terrain, using a reasonable amount of data (four to
five hours on a system operating at 500hz).",Achkan Salehi,2025-05-23T22:48:36Z,2025-05-23T22:48:36Z,http://arxiv.org/abs/2505.18417v1,http://arxiv.org/pdf/2505.18417v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Motion Priors Reimagined: Adapting Flat-Terrain Skills for Complex
  Quadruped Mobility","Reinforcement learning (RL)-based motion imitation methods trained on
demonstration data can effectively learn natural and expressive motions with
minimal reward engineering but often struggle to generalize to novel
environments. We address this by proposing a hierarchical RL framework in which
a low-level policy is first pre-trained to imitate animal motions on flat
ground, thereby establishing motion priors. A subsequent high-level,
goal-conditioned policy then builds on these priors, learning residual
corrections that enable perceptive locomotion, local obstacle avoidance, and
goal-directed navigation across diverse and rugged terrains. Simulation
experiments illustrate the effectiveness of learned residuals in adapting to
progressively challenging uneven terrains while still preserving the locomotion
characteristics provided by the motion priors. Furthermore, our results
demonstrate improvements in motion regularization over baseline models trained
without motion priors under similar reward setups. Real-world experiments with
an ANYmal-D quadruped robot confirm our policy's capability to generalize
animal-like locomotion skills to complex terrains, demonstrating smooth and
efficient locomotion and local navigation performance amidst challenging
terrains with obstacles.","Zewei Zhang, Chenhao Li, Takahiro Miki, Marco Hutter",2025-05-21T23:56:11Z,2025-08-29T00:57:31Z,http://arxiv.org/abs/2505.16084v2,http://arxiv.org/pdf/2505.16084v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"A Hierarchical Graph-Based Terrain-Aware Autonomous Navigation Approach
  for Complementary Multimodal Ground-Aerial Exploration","Autonomous navigation in unknown environments is a fundamental challenge in
robotics, particularly in coordinating ground and aerial robots to maximize
exploration efficiency. This paper presents a novel approach that utilizes a
hierarchical graph to represent the environment, encoding both geometric and
semantic traversability. The framework enables the robots to compute a shared
confidence metric, which helps the ground robot assess terrain and determine
when deploying the aerial robot will extend exploration. The robot's confidence
in traversing a path is based on factors such as predicted volumetric gain,
path traversability, and collision risk. A hierarchy of graphs is used to
maintain an efficient representation of traversability and frontier information
through multi-resolution maps. Evaluated in a real subterranean exploration
scenario, the approach allows the ground robot to autonomously identify zones
that are no longer traversable but suitable for aerial deployment. By
leveraging this hierarchical structure, the ground robot can selectively share
graph information on confidence-assessed frontier targets from parts of the
scene, enabling the aerial robot to navigate beyond obstacles and continue
exploration.","Akash Patel, Mario A. V. Saucedo, Nikolaos Stathoulopoulos, Viswa Narayanan Sankaranarayanan, Ilias Tevetzidis, Christoforos Kanellakis, George Nikolakopoulos",2025-05-20T19:44:36Z,2025-05-20T19:44:36Z,http://arxiv.org/abs/2505.14859v1,http://arxiv.org/pdf/2505.14859v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Risk-Averse Traversal of Graphs with Stochastic and Correlated Edge
  Costs for Safe Global Planetary Mobility","In robotic planetary surface exploration, strategic mobility planning is an
important task that involves finding candidate long-distance routes on orbital
maps and identifying segments with uncertain traversability. Then, expert human
operators establish safe, adaptive traverse plans based on the actual
navigation difficulties encountered in these uncertain areas. In this paper, we
formalize this challenge as a new, risk-averse variant of the Canadian
Traveller Problem (CTP) tailored to global planetary mobility. The objective is
to find a traverse policy minimizing a conditional value-at-risk (CVaR)
criterion, which is a risk measure with an intuitive interpretation. We propose
a novel search algorithm that finds exact CVaR-optimal policies. Our approach
leverages well-established optimal AND-OR search techniques intended for
(risk-agnostic) expectation minimization and extends these methods to the
risk-averse domain. We validate our approach through simulated long-distance
planetary surface traverses; we employ real orbital maps of the Martian surface
to construct problem instances and use terrain maps to express traversal
probabilities in uncertain regions. Our results illustrate different adaptive
decision-making schemes depending on the level of risk aversion. Additionally,
our problem setup allows accounting for traversability correlations between
similar areas of the environment. In such a case, we empirically demonstrate
how information-seeking detours can mitigate risk.","Olivier Lamarre, Jonathan Kelly",2025-05-19T19:23:47Z,2025-05-19T19:23:47Z,http://arxiv.org/abs/2505.13674v1,http://arxiv.org/pdf/2505.13674v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"eStonefish-scenes: A synthetically generated dataset for underwater
  event-based optical flow prediction tasks","The combined use of event-based vision and Spiking Neural Networks (SNNs) is
expected to significantly impact robotics, particularly in tasks like visual
odometry and obstacle avoidance. While existing real-world event-based datasets
for optical flow prediction, typically captured with Unmanned Aerial Vehicles
(UAVs), offer valuable insights, they are limited in diversity, scalability,
and are challenging to collect. Moreover, there is a notable lack of labelled
datasets for underwater applications, which hinders the integration of
event-based vision with Autonomous Underwater Vehicles (AUVs). To address this,
synthetic datasets could provide a scalable solution while bridging the gap
between simulation and reality. In this work, we introduce eStonefish-scenes, a
synthetic event-based optical flow dataset based on the Stonefish simulator.
Along with the dataset, we present a data generation pipeline that enables the
creation of customizable underwater environments. This pipeline allows for
simulating dynamic scenarios, such as biologically inspired schools of fish
exhibiting realistic motion patterns, including obstacle avoidance and reactive
navigation around corals. Additionally, we introduce a scene generator that can
build realistic reef seabeds by randomly distributing coral across the terrain.
To streamline data accessibility, we present eWiz, a comprehensive library
designed for processing event-based data, offering tools for data loading,
augmentation, visualization, encoding, and training data generation, along with
loss functions and performance metrics.","Jad Mansour, Sebastian Realpe, Hayat Rajani, Michele Grimaldi, Rafael Garcia, Nuno Gracias",2025-05-19T16:26:18Z,2025-05-19T16:26:18Z,http://arxiv.org/abs/2505.13309v1,http://arxiv.org/pdf/2505.13309v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Granular Loco-Manipulation: Repositioning Rocks Through Strategic Sand
  Avalanche","Legged robots have the potential to leverage obstacles to climb steep sand
slopes. However, efficiently repositioning these obstacles to desired locations
is challenging. Here we present DiffusiveGRAIN, a learning-based method that
enables a multi-legged robot to strategically induce localized sand avalanches
during locomotion and indirectly manipulate obstacles. We conducted 375 trials,
systematically varying obstacle spacing, robot orientation, and leg actions in
75 of them. Results show that the movement of closely-spaced obstacles exhibits
significant interference, requiring joint modeling. In addition, different
multi-leg excavation actions could cause distinct robot state changes,
necessitating integrated planning of manipulation and locomotion. To address
these challenges, DiffusiveGRAIN includes a diffusion-based environment
predictor to capture multi-obstacle movements under granular flow interferences
and a robot state predictor to estimate changes in robot state from multi-leg
action patterns. Deployment experiments (90 trials) demonstrate that by
integrating the environment and robot state predictors, the robot can
autonomously plan its movements based on loco-manipulation goals, successfully
shifting closely located rocks to desired locations in over 65% of trials. Our
study showcases the potential for a locomoting robot to strategically
manipulate obstacles to achieve improved mobility on challenging terrains.","Haodi Hu, Yue Wu, Feifei Qian, Daniel Seita",2025-05-19T10:17:03Z,2025-05-19T10:17:03Z,http://arxiv.org/abs/2505.12934v1,http://arxiv.org/pdf/2505.12934v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Robust Reinforcement Learning-Based Locomotion for Resource-Constrained
  Quadrupeds with Exteroceptive Sensing","Compact quadrupedal robots are proving increasingly suitable for deployment
in real-world scenarios. Their smaller size fosters easy integration into human
environments. Nevertheless, real-time locomotion on uneven terrains remains
challenging, particularly due to the high computational demands of terrain
perception. This paper presents a robust reinforcement learning-based
exteroceptive locomotion controller for resource-constrained small-scale
quadrupeds in challenging terrains, which exploits real-time elevation mapping,
supported by a careful depth sensor selection. We concurrently train both a
policy and a state estimator, which together provide an odometry source for
elevation mapping, optionally fused with visual-inertial odometry (VIO). We
demonstrate the importance of positioning an additional time-of-flight sensor
for maintaining robustness even without VIO, thus having the potential to free
up computational resources. We experimentally demonstrate that the proposed
controller can flawlessly traverse steps up to 17.5 cm in height and achieve an
80% success rate on 22.5 cm steps, both with and without VIO. The proposed
controller also achieves accurate forward and yaw velocity tracking of up to
1.0 m/s and 1.5 rad/s respectively. We open-source our training code at
github.com/ETH-PBL/elmap-rl-controller.","Davide Plozza, Patricia Apostol, Paul Joseph, Simon Schläpfer, Michele Magno",2025-05-18T20:29:23Z,2025-05-18T20:29:23Z,http://arxiv.org/abs/2505.12537v1,http://arxiv.org/pdf/2505.12537v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Human-Centered Development of Guide Dog Robots: Quiet and Stable
  Locomotion Control","A quadruped robot is a promising system that can offer assistance comparable
to that of dog guides due to its similar form factor. However, various
challenges remain in making these robots a reliable option for blind and
low-vision (BLV) individuals. Among these challenges, noise and jerky motion
during walking are critical drawbacks of existing quadruped robots. While these
issues have largely been overlooked in guide dog robot research, our interviews
with guide dog handlers and trainers revealed that acoustic and physical
disturbances can be particularly disruptive for BLV individuals, who rely
heavily on environmental sounds for navigation. To address these issues, we
developed a novel walking controller for slow stepping and smooth foot
swing/contact while maintaining human walking speed, as well as robust and
stable balance control. The controller integrates with a perception system to
facilitate locomotion over non-flat terrains, such as stairs. Our controller
was extensively tested on the Unitree Go1 robot and, when compared with other
control methods, demonstrated significant noise reduction -- half of the
default locomotion controller. In this study, we adopt a mixed-methods approach
to evaluate its usability with BLV individuals. In our indoor walking
experiments, participants compared our controller to the robot's default
controller. Results demonstrated superior acceptance of our controller,
highlighting its potential to improve the user experience of guide dog robots.
Video demonstration (best viewed with audio) available at:
https://youtu.be/8-pz_8Hqe6s.","Shangqun Yu, Hochul Hwang, Trung M. Dang, Joydeep Biswas, Nicholas A. Giudice, Sunghoon Ivan Lee, Donghyun Kim",2025-05-17T03:35:58Z,2025-05-27T12:30:37Z,http://arxiv.org/abs/2505.11808v2,http://arxiv.org/pdf/2505.11808v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Parkour in the Wild: Learning a General and Extensible Agile Locomotion
  Policy Using Multi-expert Distillation and RL Fine-tuning","Legged robots are well-suited for navigating terrains inaccessible to wheeled
robots, making them ideal for applications in search and rescue or space
exploration. However, current control methods often struggle to generalize
across diverse, unstructured environments. This paper introduces a novel
framework for agile locomotion of legged robots by combining multi-expert
distillation with reinforcement learning (RL) fine-tuning to achieve robust
generalization. Initially, terrain-specific expert policies are trained to
develop specialized locomotion skills. These policies are then distilled into a
unified foundation policy via the DAgger algorithm. The distilled policy is
subsequently fine-tuned using RL on a broader terrain set, including real-world
3D scans. The framework allows further adaptation to new terrains through
repeated fine-tuning. The proposed policy leverages depth images as
exteroceptive inputs, enabling robust navigation across diverse, unstructured
terrains. Experimental results demonstrate significant performance improvements
over existing methods in synthesizing multi-terrain skills into a single
controller. Deployment on the ANYmal D robot validates the policy's ability to
navigate complex environments with agility and robustness, setting a new
benchmark for legged robot locomotion.","Nikita Rudin, Junzhe He, Joshua Aurand, Marco Hutter",2025-05-16T12:07:37Z,2025-05-16T12:07:37Z,http://arxiv.org/abs/2505.11164v1,http://arxiv.org/pdf/2505.11164v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Geofenced Unmanned Aerial Robotic Defender for Deer Detection and
  Deterrence (GUARD)","Wildlife-induced crop damage, particularly from deer, threatens agricultural
productivity. Traditional deterrence methods often fall short in scalability,
responsiveness, and adaptability to diverse farmland environments. This paper
presents an integrated unmanned aerial vehicle (UAV) system designed for
autonomous wildlife deterrence, developed as part of the Farm Robotics
Challenge. Our system combines a YOLO-based real-time computer vision module
for deer detection, an energy-efficient coverage path planning algorithm for
efficient field monitoring, and an autonomous charging station for continuous
operation of the UAV. In collaboration with a local Minnesota farmer, the
system is tailored to address practical constraints such as terrain,
infrastructure limitations, and animal behavior. The solution is evaluated
through a combination of simulation and field testing, demonstrating robust
detection accuracy, efficient coverage, and extended operational time. The
results highlight the feasibility and effectiveness of drone-based wildlife
deterrence in precision agriculture, offering a scalable framework for future
deployment and extension.","Ebasa Temesgen, Mario Jerez, Greta Brown, Graham Wilson, Sree Ganesh Lalitaditya Divakarla, Sarah Boelter, Oscar Nelson, Robert McPherson, Maria Gini",2025-05-16T00:59:31Z,2025-05-16T00:59:31Z,http://arxiv.org/abs/2505.10770v1,http://arxiv.org/pdf/2505.10770v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"APEX: Action Priors Enable Efficient Exploration for Skill Imitation on
  Articulated Robots","Learning by imitation provides an effective way for robots to develop
well-regulated complex behaviors and directly benefit from natural
demonstrations. State-of-the-art imitation learning (IL) approaches typically
leverage Adversarial Motion Priors (AMP), which, despite their impressive
results, suffer from two key limitations. They are prone to mode collapse,
which often leads to overfitting to the simulation environment and thus
increased sim-to-real gap, and they struggle to learn diverse behaviors
effectively. To overcome these limitations, we introduce APEX (Action Priors
enable Efficient eXploration): a simple yet versatile IL framework that
integrates demonstrations directly into reinforcement learning (RL),
maintaining high exploration while grounding behavior with expert-informed
priors. We achieve this through a combination of decaying action priors, which
initially bias exploration toward expert demonstrations but gradually allow the
policy to explore independently. This is complemented by a multi-critic RL
framework that effectively balances stylistic consistency with task
performance. Our approach achieves sample-efficient IL and enables the
acquisition of diverse skills within a single policy. APEX generalizes to
varying velocities and preserves reference-like styles across complex tasks
such as navigating rough terrain and climbing stairs, utilizing only
flat-terrain kinematic motion data as a prior. We validate our framework
through extensive hardware experiments on the Unitree Go2 quadruped. There,
APEX yields diverse and agile locomotion gaits, inherent gait transitions, and
the highest reported speed for the platform to the best of our knowledge (peak
velocity of ~3.3 m/s on hardware). Our results establish APEX as a compelling
alternative to existing IL methods, offering better efficiency, adaptability,
and real-world performance. https://marmotlab.github.io/APEX/","Shivam Sood, Laukik B Nakhwa, Yuhong Cao, Sun Ge, Guillaume Sartoretti",2025-05-15T07:09:23Z,2025-06-12T14:15:33Z,http://arxiv.org/abs/2505.10022v2,http://arxiv.org/pdf/2505.10022v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Learning Rock Pushability on Rough Planetary Terrain,"In the context of mobile navigation in unstructured environments, the
predominant approach entails the avoidance of obstacles. The prevailing path
planning algorithms are contingent upon deviating from the intended path for an
indefinite duration and returning to the closest point on the route after the
obstacle is left behind spatially. However, avoiding an obstacle on a path that
will be used repeatedly by multiple agents can hinder long-term efficiency and
lead to a lasting reliance on an active path planning system. In this study, we
propose an alternative approach to mobile navigation in unstructured
environments by leveraging the manipulation capabilities of a robotic
manipulator mounted on top of a mobile robot. Our proposed framework integrates
exteroceptive and proprioceptive feedback to assess the push affordance of
obstacles, facilitating their repositioning rather than avoidance. While our
preliminary visual estimation takes into account the characteristics of both
the obstacle and the surface it relies on, the push affordance estimation
module exploits the force feedback obtained by interacting with the obstacle
via a robotic manipulator as the guidance signal. The objective of our
navigation approach is to enhance the efficiency of routes utilized by multiple
agents over extended periods by reducing the overall time spent by a fleet in
environments where autonomous infrastructure development is imperative, such as
lunar or Martian surfaces.","Tuba Girgin, Emre Girgin, Cagri Kilic",2025-05-14T22:23:30Z,2025-06-05T15:00:47Z,http://arxiv.org/abs/2505.09833v2,http://arxiv.org/pdf/2505.09833v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Deploying Foundation Model-Enabled Air and Ground Robots in the Field:
  Challenges and Opportunities","The integration of foundation models (FMs) into robotics has enabled robots
to understand natural language and reason about the semantics in their
environments. However, existing FM-enabled robots primary operate in
closed-world settings, where the robot is given a full prior map or has a full
view of its workspace. This paper addresses the deployment of FM-enabled robots
in the field, where missions often require a robot to operate in large-scale
and unstructured environments. To effectively accomplish these missions, robots
must actively explore their environments, navigate obstacle-cluttered terrain,
handle unexpected sensor inputs, and operate with compute constraints. We
discuss recent deployments of SPINE, our LLM-enabled autonomy framework, in
field robotic settings. To the best of our knowledge, we present the first
demonstration of large-scale LLM-enabled robot planning in unstructured
environments with several kilometers of missions. SPINE is agnostic to a
particular LLM, which allows us to distill small language models capable of
running onboard size, weight and power (SWaP) limited platforms. Via
preliminary model distillation work, we then present the first language-driven
UAV planner using on-device language models. We conclude our paper by proposing
several promising directions for future research.","Zachary Ravichandran, Fernando Cladera, Jason Hughes, Varun Murali, M. Ani Hsieh, George J. Pappas, Camillo J. Taylor, Vijay Kumar",2025-05-14T15:28:43Z,2025-05-14T15:28:43Z,http://arxiv.org/abs/2505.09477v1,http://arxiv.org/pdf/2505.09477v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Parameter-Efficient Fine-Tuning of Vision Foundation Model for Forest
  Floor Segmentation from UAV Imagery","Unmanned Aerial Vehicles (UAVs) are increasingly used for reforestation and
forest monitoring, including seed dispersal in hard-to-reach terrains. However,
a detailed understanding of the forest floor remains a challenge due to high
natural variability, quickly changing environmental parameters, and ambiguous
annotations due to unclear definitions. To address this issue, we adapt the
Segment Anything Model (SAM), a vision foundation model with strong
generalization capabilities, to segment forest floor objects such as tree
stumps, vegetation, and woody debris. To this end, we employ
parameter-efficient fine-tuning (PEFT) to fine-tune a small subset of
additional model parameters while keeping the original weights fixed. We adjust
SAM's mask decoder to generate masks corresponding to our dataset categories,
allowing for automatic segmentation without manual prompting. Our results show
that the adapter-based PEFT method achieves the highest mean intersection over
union (mIoU), while Low-rank Adaptation (LoRA), with fewer parameters, offers a
lightweight alternative for resource-constrained UAV platforms.","Mohammad Wasil, Ahmad Drak, Brennan Penfold, Ludovico Scarton, Maximilian Johenneken, Alexander Asteroth, Sebastian Houben",2025-05-13T19:59:29Z,2025-05-13T19:59:29Z,http://arxiv.org/abs/2505.08932v1,http://arxiv.org/pdf/2505.08932v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Motion Control of High-Dimensional Musculoskeletal Systems with
  Hierarchical Model-Based Planning","Controlling high-dimensional nonlinear systems, such as those found in
biological and robotic applications, is challenging due to large state and
action spaces. While deep reinforcement learning has achieved a number of
successes in these domains, it is computationally intensive and time consuming,
and therefore not suitable for solving large collections of tasks that require
significant manual tuning. In this work, we introduce Model Predictive Control
with Morphology-aware Proportional Control (MPC^2), a hierarchical model-based
learning algorithm for zero-shot and near-real-time control of high-dimensional
complex dynamical systems. MPC^2 uses a sampling-based model predictive
controller for target posture planning, and enables robust control for
high-dimensional tasks by incorporating a morphology-aware proportional
controller for actuator coordination. The algorithm enables motion control of a
high-dimensional human musculoskeletal model in a variety of motion tasks, such
as standing, walking on different terrains, and imitating sports activities.
The reward function of MPC^2 can be tuned via black-box optimization,
drastically reducing the need for human-intensive reward engineering.","Yunyue Wei, Shanning Zhuang, Vincent Zhuang, Yanan Sui",2025-05-13T05:31:32Z,2025-05-13T05:31:32Z,http://arxiv.org/abs/2505.08238v1,http://arxiv.org/pdf/2505.08238v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"SKiD-SLAM: Robust, Lightweight, and Distributed Multi-Robot LiDAR SLAM
  in Resource-Constrained Field Environments","Distributed LiDAR SLAM is crucial for achieving efficient robot autonomy and
improving the scalability of mapping. However, two issues need to be considered
when applying it in field environments: one is resource limitation, and the
other is inter/intra-robot association. The resource limitation issue arises
when the data size exceeds the processing capacity of the network or memory,
especially when utilizing communication systems or onboard computers in the
field. The inter/intra-robot association issue occurs due to the narrow
convergence region of ICP under large viewpoint differences, triggering many
false positive loops and ultimately resulting in an inconsistent global map for
multi-robot systems. To tackle these problems, we propose a distributed LiDAR
SLAM framework designed for versatile field applications, called SKiD-SLAM.
Extending our previous work that solely focused on lightweight place
recognition and fast and robust global registration, we present a multi-robot
mapping framework that focuses on robust and lightweight inter-robot loop
closure in distributed LiDAR SLAM. Through various environmental experiments,
we demonstrate that our method is more robust and lightweight compared to other
state-of-the-art distributed SLAM approaches, overcoming resource limitation
and inter/intra-robot association issues. Also, we validated the field
applicability of our approach through mapping experiments in real-world
planetary emulation terrain and cave environments, which are in-house datasets.
Our code will be available at https://sparolab.github.io/research/skid_slam/.","Hogyun Kim, Jiwon Choi, Juwon Kim, Geonmo Yang, Dongjin Cho, Hyungtae Lim, Younggun Cho",2025-05-13T05:17:26Z,2025-07-30T00:16:56Z,http://arxiv.org/abs/2505.08230v3,http://arxiv.org/pdf/2505.08230v3.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Land-Coverage Aware Path-Planning for Multi-UAV Swarms in Search and
  Rescue Scenarios","Unmanned Aerial Vehicles (UAVs) have become vital in search-and-rescue (SAR)
missions, with autonomous mission planning improving response times and
coverage efficiency. Early approaches primarily used path planning techniques
such as A*, potential-fields, or Dijkstra's algorithm, while recent approaches
have incorporated meta-heuristic frameworks like genetic algorithms and
particle swarm optimization to balance competing objectives such as network
connectivity, energy efficiency, and strategic placement of charging stations.
However, terrain-aware path planning remains under-explored, despite its
critical role in optimizing UAV SAR deployments. To address this gap, we
present a computer-vision based terrain-aware mission planner that autonomously
extracts and analyzes terrain topology to enhance SAR pre-flight planning. Our
framework uses a deep segmentation network fine-tuned on our own collection of
landcover datasets to transform satellite imagery into a structured, grid-based
representation of the operational area. This classification enables
terrain-specific UAV-task allocation, improving deployment strategies in
complex environments. We address the challenge of irregular terrain partitions,
by introducing a two-stage partitioning scheme that first evaluates terrain
monotonicity along coordinate axes before applying a cost-based recursive
partitioning process, minimizing unnecessary splits and optimizing path
efficiency. Empirical validation in a high-fidelity simulation environment
demonstrates that our approach improves search and dispatch time over multiple
meta-heuristic techniques and against a competing state-of-the-art method.
These results highlight its potential for large-scale SAR operations, where
rapid response and efficient UAV coordination are critical.","Pedro Antonio Alarcon Granadeno, Jane Cleland-Huang",2025-05-12T20:56:52Z,2025-05-12T20:56:52Z,http://arxiv.org/abs/2505.08060v1,http://arxiv.org/pdf/2505.08060v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Autonomous Robotic Pruning in Orchards and Vineyards: a Review,"Manual pruning is labor intensive and represents up to 25% of annual labor
costs in fruit production, notably in apple orchards and vineyards where
operational challenges and cost constraints limit the adoption of large-scale
machinery. In response, a growing body of research is investigating compact,
flexible robotic platforms capable of precise pruning in varied terrains,
particularly where traditional mechanization falls short.
  This paper reviews recent advances in autonomous robotic pruning for orchards
and vineyards, addressing a critical need in precision agriculture. Our review
examines literature published between 2014 and 2024, focusing on innovative
contributions across key system components. Special attention is given to
recent developments in machine vision, perception, plant skeletonization, and
control strategies, areas that have experienced significant influence from
advancements in artificial intelligence and machine learning. The analysis
situates these technological trends within broader agricultural challenges,
including rising labor costs, a decline in the number of young farmers, and the
diverse pruning requirements of different fruit species such as apple,
grapevine, and cherry trees.
  By comparing various robotic architectures and methodologies, this survey not
only highlights the progress made toward autonomous pruning but also identifies
critical open challenges and future research directions. The findings
underscore the potential of robotic systems to bridge the gap between manual
and mechanized operations, paving the way for more efficient, sustainable, and
precise agricultural practices.","Alessandro Navone, Mauro Martini, Marcello Chiaberge",2025-05-12T08:05:15Z,2025-05-12T08:05:15Z,http://arxiv.org/abs/2505.07318v1,http://arxiv.org/pdf/2505.07318v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Terrain-aware Low Altitude Path Planning,"In this paper, we study the problem of generating low-altitude path plans for
nap-of-the-earth (NOE) flight in real time with only RGB images from onboard
cameras and the vehicle pose. We propose a novel training method that combines
behavior cloning and self-supervised learning, where the self-supervision
component allows the learned policy to refine the paths generated by the expert
planner. Simulation studies show 24.7% reduction in average path elevation
compared to the standard behavior cloning approach.","Yixuan Jia, Andrea Tagliabue, Annika Thomas, Navid Dadkhah Tehrani, Jonathan P. How",2025-05-11T22:53:45Z,2025-06-23T20:18:37Z,http://arxiv.org/abs/2505.07141v2,http://arxiv.org/pdf/2505.07141v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"FACET: Force-Adaptive Control via Impedance Reference Tracking for
  Legged Robots","Reinforcement learning (RL) has made significant strides in legged robot
control, enabling locomotion across diverse terrains and complex
loco-manipulation capabilities. However, the commonly used position or velocity
tracking-based objectives are agnostic to forces experienced by the robot,
leading to stiff and potentially dangerous behaviors and poor control during
forceful interactions. To address this limitation, we present
\emph{Force-Adaptive Control via Impedance Reference Tracking} (FACET).
Inspired by impedance control, we use RL to train a control policy to imitate a
virtual mass-spring-damper system, allowing fine-grained control under external
forces by manipulating the virtual spring. In simulation, we demonstrate that
our quadruped robot achieves improved robustness to large impulses (up to 200
Ns) and exhibits controllable compliance, achieving an 80% reduction in
collision impulse. The policy is deployed to a physical robot to showcase both
compliance and the ability to engage with large forces by kinesthetic control
and pulling payloads up to 2/3 of its weight. Further extension to a legged
loco-manipulator and a humanoid shows the applicability of our method to more
complex settings to enable whole-body compliance control. Project Website:
https://facet.pages.dev/","Botian Xu, Haoyang Weng, Qingzhou Lu, Yang Gao, Huazhe Xu",2025-05-11T07:23:26Z,2025-05-19T11:28:40Z,http://arxiv.org/abs/2505.06883v2,http://arxiv.org/pdf/2505.06883v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Let Humanoids Hike! Integrative Skill Development on Complex Trails,"Hiking on complex trails demands balance, agility, and adaptive
decision-making over unpredictable terrain. Current humanoid research remains
fragmented and inadequate for hiking: locomotion focuses on motor skills
without long-term goals or situational awareness, while semantic navigation
overlooks real-world embodiment and local terrain variability. We propose
training humanoids to hike on complex trails, driving integrative skill
development across visual perception, decision making, and motor execution. We
develop a learning framework, LEGO-H, that enables a vision-equipped humanoid
robot to hike complex trails autonomously. We introduce two technical
innovations: 1) A temporal vision transformer variant - tailored into
Hierarchical Reinforcement Learning framework - anticipates future local goals
to guide movement, seamlessly integrating locomotion with goal-directed
navigation. 2) Latent representations of joint movement patterns, combined with
hierarchical metric learning - enhance Privileged Learning scheme - enable
smooth policy transfer from privileged training to onboard execution. These
components allow LEGO-H to handle diverse physical and environmental challenges
without relying on predefined motion patterns. Experiments across varied
simulated trails and robot morphologies highlight LEGO-H's versatility and
robustness, positioning hiking as a compelling testbed for embodied autonomy
and LEGO-H as a baseline for future humanoid development.","Kwan-Yee Lin, Stella X. Yu",2025-05-09T17:53:02Z,2025-05-09T17:53:02Z,http://arxiv.org/abs/2505.06218v1,http://arxiv.org/pdf/2505.06218v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"PARC: Physics-based Augmentation with Reinforcement Learning for
  Character Controllers","Humans excel in navigating diverse, complex environments with agile motor
skills, exemplified by parkour practitioners performing dynamic maneuvers, such
as climbing up walls and jumping across gaps. Reproducing these agile movements
with simulated characters remains challenging, in part due to the scarcity of
motion capture data for agile terrain traversal behaviors and the high cost of
acquiring such data. In this work, we introduce PARC (Physics-based
Augmentation with Reinforcement Learning for Character Controllers), a
framework that leverages machine learning and physics-based simulation to
iteratively augment motion datasets and expand the capabilities of terrain
traversal controllers. PARC begins by training a motion generator on a small
dataset consisting of core terrain traversal skills. The motion generator is
then used to produce synthetic data for traversing new terrains. However, these
generated motions often exhibit artifacts, such as incorrect contacts or
discontinuities. To correct these artifacts, we train a physics-based tracking
controller to imitate the motions in simulation. The corrected motions are then
added to the dataset, which is used to continue training the motion generator
in the next iteration. PARC's iterative process jointly expands the
capabilities of the motion generator and tracker, creating agile and versatile
models for interacting with complex environments. PARC provides an effective
approach to develop controllers for agile terrain traversal, which bridges the
gap between the scarcity of motion data and the need for versatile character
controllers.","Michael Xu, Yi Shi, KangKang Yin, Xue Bin Peng",2025-05-06T22:29:07Z,2025-05-06T22:29:07Z,http://arxiv.org/abs/2505.04002v1,http://arxiv.org/pdf/2505.04002v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"LiDAR-Inertial SLAM-Based Navigation and Safety-Oriented AI-Driven
  Control System for Skid-Steer Robots","Integrating artificial intelligence (AI) and stochastic technologies into the
mobile robot navigation and control (MRNC) framework while adhering to rigorous
safety standards presents significant challenges. To address these challenges,
this paper proposes a comprehensively integrated MRNC framework for skid-steer
wheeled mobile robots (SSWMRs), in which all components are actively engaged in
real-time execution. The framework comprises: 1) a LiDAR-inertial simultaneous
localization and mapping (SLAM) algorithm for estimating the current pose of
the robot within the built map; 2) an effective path-following control system
for generating desired linear and angular velocity commands based on the
current pose and the desired pose; 3) inverse kinematics for transferring
linear and angular velocity commands into left and right side velocity
commands; and 4) a robust AI-driven (RAID) control system incorporating a
radial basis function network (RBFN) with a new adaptive algorithm to enforce
in-wheel actuation systems to track each side motion commands. To further meet
safety requirements, the proposed RAID control within the MRNC framework of the
SSWMR constrains AI-generated tracking performance within predefined overshoot
and steady-state error limits, while ensuring robustness and system stability
by compensating for modeling errors, unknown RBF weights, and external forces.
Experimental results verify the proposed MRNC framework performance for a 4,836
kg SSWMR operating on soft terrain.","Mehdi Heydari Shahna, Eemil Haaparanta, Pauli Mustalahti, Jouni Mattila",2025-05-05T12:07:35Z,2025-05-05T12:07:35Z,http://arxiv.org/abs/2505.02598v1,http://arxiv.org/pdf/2505.02598v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Toward Teach and Repeat Across Seasonal Deep Snow Accumulation,"Teach and repeat is a rapid way to achieve autonomy in challenging terrain
and off-road environments. A human operator pilots the vehicles to create a
network of paths that are mapped and associated with odometry. Immediately
after teaching, the system can drive autonomously within its tracks. This
precision lets operators remain confident that the robot will follow a
traversable route. However, this operational paradigm has rarely been explored
in off-road environments that change significantly through seasonal variation.
This paper presents preliminary field trials using lidar and radar
implementations of teach and repeat. Using a subset of the data from the
upcoming FoMo dataset, we attempted to repeat routes that were 4 days, 44 days,
and 113 days old. Lidar teach and repeat demonstrated a stronger ability to
localize when the ground points were removed. FMCW radar was often able to
localize on older maps, but only with small deviations from the taught path.
Additionally, we highlight specific cases where radar localization failed with
recent maps due to the high pitch or roll of the vehicle. We highlight lessons
learned during the field deployment and highlight areas to improve to achieve
reliable teach and repeat with seasonal changes in the environment. Please
follow the dataset at https://norlab-ulaval.github.io/FoMo-website for updates
and information on the data release.","Matěj Boxan, Alexander Krawciw, Timothy D. Barfoot, François Pomerleau",2025-05-02T15:11:22Z,2025-06-24T17:32:37Z,http://arxiv.org/abs/2505.01339v2,http://arxiv.org/pdf/2505.01339v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Towards Autonomous Micromobility through Scalable Urban Simulation,"Micromobility, which utilizes lightweight mobile machines moving in urban
public spaces, such as delivery robots and mobility scooters, emerges as a
promising alternative to vehicular mobility. Current micromobility depends
mostly on human manual operation (in-person or remote control), which raises
safety and efficiency concerns when navigating busy urban environments full of
unpredictable obstacles and pedestrians. Assisting humans with AI agents in
maneuvering micromobility devices presents a viable solution for enhancing
safety and efficiency. In this work, we present a scalable urban simulation
solution to advance autonomous micromobility. First, we build URBAN-SIM - a
high-performance robot learning platform for large-scale training of embodied
agents in interactive urban scenes. URBAN-SIM contains three critical modules:
Hierarchical Urban Generation pipeline, Interactive Dynamics Generation
strategy, and Asynchronous Scene Sampling scheme, to improve the diversity,
realism, and efficiency of robot learning in simulation. Then, we propose
URBAN-BENCH - a suite of essential tasks and benchmarks to gauge various
capabilities of the AI agents in achieving autonomous micromobility.
URBAN-BENCH includes eight tasks based on three core skills of the agents:
Urban Locomotion, Urban Navigation, and Urban Traverse. We evaluate four robots
with heterogeneous embodiments, such as the wheeled and legged robots, across
these tasks. Experiments on diverse terrains and urban structures reveal each
robot's strengths and limitations.","Wayne Wu, Honglin He, Chaoyuan Zhang, Jack He, Seth Z. Zhao, Ran Gong, Quanyi Li, Bolei Zhou",2025-05-01T17:52:29Z,2025-05-01T17:52:29Z,http://arxiv.org/abs/2505.00690v1,http://arxiv.org/pdf/2505.00690v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"MULE: Multi-terrain and Unknown Load Adaptation for Effective
  Quadrupedal Locomotion","Quadrupedal robots are increasingly deployed for load-carrying tasks across
diverse terrains. While Model Predictive Control (MPC)-based methods can
account for payload variations, they often depend on predefined gait schedules
or trajectory generators, limiting their adaptability in unstructured
environments. To address these limitations, we propose an Adaptive
Reinforcement Learning (RL) framework that enables quadrupedal robots to
dynamically adapt to both varying payloads and diverse terrains. The framework
consists of a nominal policy responsible for baseline locomotion and an
adaptive policy that learns corrective actions to preserve stability and
improve command tracking under payload variations. We validate the proposed
approach through large-scale simulation experiments in Isaac Gym and real-world
hardware deployment on a Unitree Go1 quadruped. The controller was tested on
flat ground, slopes, and stairs under both static and dynamic payload changes.
Across all settings, our adaptive controller consistently outperformed the
controller in tracking body height and velocity commands, demonstrating
enhanced robustness and adaptability without requiring explicit gait design or
manual tuning.","Vamshi Kumar Kurva, Shishir Kolathaya",2025-05-01T12:41:35Z,2025-05-01T12:41:35Z,http://arxiv.org/abs/2505.00488v1,http://arxiv.org/pdf/2505.00488v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Characterizing gaussian mixture of motion modes for skid-steer vehicle
  state estimation","Skid-steered wheel mobile robots (SSWMRs) are characterized by the unique
domination of the tire-terrain skidding for the robot to move. The lack of
reliable friction models cascade into unreliable motion models, especially the
reduced ordered variants used for state estimation and robot control. Ensemble
modeling is an emerging research direction where the overall motion model is
broken down into a family of local models to distribute the performance and
resource requirement and provide a fast real-time prediction. To this end, a
gaussian mixture model based modeling identification of model clusters is
adopted and implemented within an interactive multiple model (IMM) based state
estimation. The framework is adopted and implemented for angular velocity as
the estimated state for a mid scaled skid-steered wheel mobile robot platform.","Ameya Salvi, Mark Brudnak, Jonathon M. Smereka, Matthias Schmid, Venkat Krovi",2025-04-30T21:59:51Z,2025-07-15T02:43:45Z,http://arxiv.org/abs/2505.00200v2,http://arxiv.org/pdf/2505.00200v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Whleaper: A 10-DOF Flexible Bipedal Wheeled Robot,"Wheel-legged robots combine the advantages of both wheeled robots and legged
robots, offering versatile locomotion capabilities with excellent stability on
challenging terrains and high efficiency on flat surfaces. However, existing
wheel-legged robots typically have limited hip joint mobility compared to
humans, while hip joint plays a crucial role in locomotion. In this paper, we
introduce Whleaper, a novel 10-degree-of-freedom (DOF) bipedal wheeled robot,
with 3 DOFs at the hip of each leg. Its humanoid joint design enables adaptable
motion in complex scenarios, ensuring stability and flexibility. This paper
introduces the details of Whleaper, with a focus on innovative mechanical
design, control algorithms and system implementation. Firstly, stability stems
from the increased DOFs at the hip, which expand the range of possible postures
and improve the robot's foot-ground contact. Secondly, the extra DOFs also
augment its mobility. During walking or sliding, more complex movements can be
adopted to execute obstacle avoidance tasks. Thirdly, we utilize two control
algorithms to implement multimodal motion for walking and sliding. By
controlling specific DOFs of the robot, we conducted a series of simulations
and practical experiments, demonstrating that a high-DOF hip joint design can
effectively enhance the stability and flexibility of wheel-legged robots.
Whleaper shows its capability to perform actions such as squatting, obstacle
avoidance sliding, and rapid turning in real-world scenarios.","Yinglei Zhu, Sixiao He, Zhenghao Qi, Zhuoyuan Yong, Yihua Qin, Jianyu Chen",2025-04-30T16:07:44Z,2025-04-30T16:07:44Z,http://arxiv.org/abs/2504.21767v1,http://arxiv.org/pdf/2504.21767v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Path Planning on Multi-level Point Cloud with a Weighted Traversability
  Graph","This article proposes a new path planning method for addressing multi-level
terrain situations. The proposed method includes innovations in three aspects:
1) the pre-processing of point cloud maps with a multi-level skip-list
structure and data-slimming algorithm for well-organized and simplified map
formalization and management, 2) the direct acquisition of local traversability
indexes through vehicle and point cloud interaction analysis, which saves work
in surface fitting, and 3) the assignment of traversability indexes on a
multi-level connectivity graph to generate a weighted traversability graph for
generally search-based path planning. The A* algorithm is modified to utilize
the traversability graph to generate a short and safe path. The effectiveness
and reliability of the proposed method are verified through indoor and outdoor
experiments conducted in various environments, including multi-floor buildings,
woodland, and rugged mountainous regions. The results demonstrate that the
proposed method can properly address 3D path planning problems for ground
vehicles in a wide range of situations.","Yujie Tang, Quan Li, Hao Geng, Yangmin Xie, Hang Shi, Yusheng Yang",2025-04-30T13:24:53Z,2025-04-30T13:24:53Z,http://arxiv.org/abs/2504.21622v1,http://arxiv.org/pdf/2504.21622v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"NavEX: A Multi-Agent Coverage in Non-Convex and Uneven Environments via
  Exemplar-Clustering","This paper addresses multi-agent deployment in non-convex and uneven
environments. To overcome the limitations of traditional approaches, we
introduce Navigable Exemplar-Based Dispatch Coverage (NavEX), a novel dispatch
coverage framework that combines exemplar-clustering with obstacle-aware and
traversability-aware shortest distances, offering a deployment framework based
on submodular optimization. NavEX provides a unified approach to solve two
critical coverage tasks: (a) fair-access deployment, aiming to provide
equitable service by minimizing agent-target distances, and (b) hotspot
deployment, prioritizing high-density target regions. A key feature of NavEX is
the use of exemplar-clustering for the coverage utility measure, which provides
the flexibility to employ non-Euclidean distance metrics that do not
necessarily conform to the triangle inequality. This allows NavEX to
incorporate visibility graphs for shortest-path computation in environments
with planar obstacles, and traversability-aware RRT* for complex, rugged
terrains. By leveraging submodular optimization, the NavEX framework enables
efficient, near-optimal solutions with provable performance guarantees for
multi-agent deployment in realistic and complex settings, as demonstrated by
our simulations.","Donipolo Ghimire, Carlos Nieto-Granda, Solmaz S. Kia",2025-04-29T18:50:49Z,2025-04-29T18:50:49Z,http://arxiv.org/abs/2504.21113v1,http://arxiv.org/pdf/2504.21113v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"System Identification of Thrust and Torque Characteristics for a Bipedal
  Robot with Integrated Propulsion","Bipedal robots represent a remarkable and sophisticated class of robotics,
designed to emulate human form and movement. Their development marks a
significant milestone in the field. However, even the most advanced bipedal
robots face challenges related to terrain variation, obstacle negotiation,
payload management, weight distribution, and recovering from stumbles. These
challenges can be mitigated by incorporating thrusters, which enhance stability
on uneven terrain, facilitate obstacle avoidance, and improve recovery after
stumbling. Harpy is a bipedal robot equipped with six joints and two thrusters,
serving as a hardware platform for implementing and testing advanced control
algorithms. This thesis focuses on characterizing Harpy's hardware to improve
the system's overall robustness, controllability, and predictability. It also
examines simulation results for predicting thrust in propeller-based
mechanisms, the integration of thrusters into the Harpy platform and associated
testing, as well as an exploration of motor torque characterization methods and
their application to hardware in relation to closed-loop force-based impedance
control.",Thomas Cahill,2025-04-28T23:54:51Z,2025-04-28T23:54:51Z,http://arxiv.org/abs/2504.20313v1,http://arxiv.org/pdf/2504.20313v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Feelbert: A Feedback Linearization-based Embedded Real-Time Quadrupedal
  Locomotion Framework","Quadruped robots have become quite popular for their ability to adapt their
locomotion to generic uneven terrains. For this reason, over time, several
frameworks for quadrupedal locomotion have been proposed, but with little
attention to ensuring a predictable timing behavior of the controller.
  To address this issue, this work presents Feelbert, a modular control
framework for quadrupedal locomotion suitable for execution on an embedded
system under hard real-time execution constraints. It leverages the feedback
linearization control technique to obtain a closed-form control law for the
body, valid for all configurations of the robot. The control law was derived
after defining an appropriate rigid body model that uses the accelerations of
the feet as control variables, instead of the estimated contact forces. This
work also provides a novel algorithm to compute footholds and gait temporal
parameters using the concept of imaginary wheels, and a heuristic algorithm to
select the best gait schedule for the current velocity commands.
  The proposed framework is developed entirely in C++, with no dependencies on
third-party libraries and no dynamic memory allocation, to ensure
predictability and real-time performance. Its implementation allows Feelbert to
be both compiled and executed on an embedded system for critical applications,
as well as integrated into larger systems such as Robot Operating System 2 (ROS
2). For this reason, Feelbert has been tested in both scenarios, demonstrating
satisfactory results both in terms of reference tracking and temporal
predictability, whether integrated into ROS 2 or compiled as a standalone
application on a Raspberry Pi 5.","Aristide Emanuele Casucci, Federico Nesti, Mauro Marinoni, Giorgio Buttazzo",2025-04-28T16:36:28Z,2025-04-29T09:44:33Z,http://arxiv.org/abs/2504.19965v2,http://arxiv.org/pdf/2504.19965v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Tensegrity-based Robot Leg Design with Variable Stiffness,"Animals can finely modulate their leg stiffness to interact with complex
terrains and absorb sudden shocks. In feats like leaping and sprinting, animals
demonstrate a sophisticated interplay of opposing muscle pairs that actively
modulate joint stiffness, while tendons and ligaments act as biological springs
storing and releasing energy. Although legged robots have achieved notable
progress in robust locomotion, they still lack the refined adaptability
inherent in animal motor control. Integrating mechanisms that allow active
control of leg stiffness presents a pathway towards more resilient robotic
systems. This paper proposes a novel mechanical design to integrate compliancy
into robot legs based on tensegrity - a structural principle that combines
flexible cables and rigid elements to balance tension and compression.
Tensegrity structures naturally allow for passive compliance, making them
well-suited for absorbing impacts and adapting to diverse terrains. Our design
features a robot leg with tensegrity joints and a mechanism to control the
joint's rotational stiffness by modulating the tension of the cable actuation
system. We demonstrate that the robot leg can reduce the impact forces of
sudden shocks by at least 34.7 % and achieve a similar leg flexion under a load
difference of 10.26 N by adjusting its stiffness configuration. The results
indicate that tensegrity-based leg designs harbors potential towards more
resilient and adaptable legged robots.","Erik Mortensen, Jan Petrs, Alexander Dittrich, Dario Floreano",2025-04-28T11:22:35Z,2025-04-28T11:22:35Z,http://arxiv.org/abs/2504.19685v1,http://arxiv.org/pdf/2504.19685v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Adaptive Locomotion on Mud through Proprioceptive Sensing of Substrate
  Properties","Muddy terrains present significant challenges for terrestrial robots, as
subtle changes in composition and water content can lead to large variations in
substrate strength and force responses, causing the robot to slip or get stuck.
This paper presents a method to estimate mud properties using proprioceptive
sensing, enabling a flipper-driven robot to adapt its locomotion through muddy
substrates of varying strength. First, we characterize mud reaction forces
through actuator current and position signals from a statically mounted robotic
flipper. We use the measured force to determine key coefficients that
characterize intrinsic mud properties. The proprioceptively estimated
coefficients match closely with measurements from a lab-grade load cell,
validating the effectiveness of the proposed method. Next, we extend the method
to a locomoting robot to estimate mud properties online as it crawls across
different mud mixtures. Experimental data reveal that mud reaction forces
depend sensitively on robot motion, requiring joint analysis of robot movement
with proprioceptive force to determine mud properties correctly. Lastly, we
deploy this method in a flipper-driven robot moving across muddy substrates of
varying strengths, and demonstrate that the proposed method allows the robot to
use the estimated mud properties to adapt its locomotion strategy, and
successfully avoid locomotion failures. Our findings highlight the potential of
proprioception-based terrain sensing to enhance robot mobility in complex,
deformable natural environments, paving the way for more robust field
exploration capabilities.","Shipeng Liu, Jiaze Tang, Siyuan Meng, Feifei Qian",2025-04-28T09:12:21Z,2025-06-05T19:16:01Z,http://arxiv.org/abs/2504.19607v2,http://arxiv.org/pdf/2504.19607v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"VTire: A Bimodal Visuotactile Tire with High-Resolution Sensing
  Capability","Developing smart tires with high sensing capability is significant for
improving the moving stability and environmental adaptability of wheeled robots
and vehicles. However, due to the classical manufacturing design, it is always
challenging for tires to infer external information precisely. To this end,
this paper introduces a bimodal sensing tire, which can simultaneously capture
tactile and visual data. By leveraging the emerging visuotactile techniques,
the proposed smart tire can realize various functions, including terrain
recognition, ground crack detection, load sensing, and tire damage detection.
Besides, we optimize the material and structure of the tire to ensure its
outstanding elasticity, toughness, hardness, and transparency. In terms of
algorithms, a transformer-based multimodal classification algorithm, a load
detection method based on finite element analysis, and a contact segmentation
algorithm have been developed. Furthermore, we construct an intelligent mobile
platform to validate the system's effectiveness and develop visual and tactile
datasets in complex terrains. The experimental results show that our multimodal
terrain sensing algorithm can achieve a classification accuracy of 99.2\%, a
tire damage detection accuracy of 97\%, a 98\% success rate in object search,
and the ability to withstand tire loading weights exceeding 35 kg. In addition,
we open-source our algorithms, hardware, and datasets at
https://sites.google.com/view/vtire.","Shoujie Li, Jianle Xu, Tong Wu, Yang Yang, Yanbo Chen, Xueqian Wang, Wenbo Ding, Xiao-Ping Zhang",2025-04-27T11:00:51Z,2025-04-27T11:00:51Z,http://arxiv.org/abs/2504.19194v1,http://arxiv.org/pdf/2504.19194v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Terrain-Aware Kinodynamic Planning with Efficiently Adaptive State
  Lattices for Mobile Robot Navigation in Off-Road Environments","To safely traverse non-flat terrain, robots must account for the influence of
terrain shape in their planned motions. Terrain-aware motion planners use an
estimate of the vehicle roll and pitch as a function of pose, vehicle
suspension, and ground elevation map to weigh the cost of edges in the search
space. Encoding such information in a traditional two-dimensional cost map is
limiting because it is unable to capture the influence of orientation on the
roll and pitch estimates from sloped terrain. The research presented herein
addresses this problem by encoding kinodynamic information in the edges of a
recombinant motion planning search space based on the Efficiently Adaptive
State Lattice (EASL). This approach, which we describe as a Kinodynamic
Efficiently Adaptive State Lattice (KEASL), differs from the prior
representation in two ways. First, this method uses a novel encoding of
velocity and acceleration constraints and vehicle direction at expanded nodes
in the motion planning graph. Second, this approach describes additional steps
for evaluating the roll, pitch, constraints, and velocities associated with
poses along each edge during search in a manner that still enables the graph to
remain recombinant. Velocities are computed using an iterative bidirectional
method using Eulerian integration that more accurately estimates the duration
of edges that are subject to terrain-dependent velocity limits. Real-world
experiments on a Clearpath Robotics Warthog Unmanned Ground Vehicle were
performed in a non-flat, unstructured environment. Results from 2093 planning
queries from these experiments showed that KEASL provided a more efficient
route than EASL in 83.72% of cases when EASL plans were adjusted to satisfy
terrain-dependent velocity constraints. An analysis of relative runtimes and
differences between planned routes is additionally presented.","Eric R. Damm, Jason M. Gregory, Eli S. Lancaster, Felix A. Sanchez, Daniel M. Sahu, Thomas M. Howard",2025-04-24T19:00:22Z,2025-04-24T19:00:22Z,http://arxiv.org/abs/2504.17889v1,http://arxiv.org/pdf/2504.17889v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Meta-Learning Online Dynamics Model Adaptation in Off-Road Autonomous
  Driving","High-speed off-road autonomous driving presents unique challenges due to
complex, evolving terrain characteristics and the difficulty of accurately
modeling terrain-vehicle interactions. While dynamics models used in
model-based control can be learned from real-world data, they often struggle to
generalize to unseen terrain, making real-time adaptation essential. We propose
a novel framework that combines a Kalman filter-based online adaptation scheme
with meta-learned parameters to address these challenges. Offline meta-learning
optimizes the basis functions along which adaptation occurs, as well as the
adaptation parameters, while online adaptation dynamically adjusts the onboard
dynamics model in real time for model-based control. We validate our approach
through extensive experiments, including real-world testing on a full-scale
autonomous off-road vehicle, demonstrating that our method outperforms baseline
approaches in prediction accuracy, performance, and safety metrics,
particularly in safety-critical scenarios. Our results underscore the
effectiveness of meta-learned dynamics model adaptation, advancing the
development of reliable autonomous systems capable of navigating diverse and
unseen environments. Video is available at: https://youtu.be/cCKHHrDRQEA","Jacob Levy, Jason Gibson, Bogdan Vlahov, Erica Tevere, Evangelos Theodorou, David Fridovich-Keil, Patrick Spieler",2025-04-23T17:51:36Z,2025-04-23T17:51:36Z,http://arxiv.org/abs/2504.16923v1,http://arxiv.org/pdf/2504.16923v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Dynamic Legged Ball Manipulation on Rugged Terrains with Hierarchical
  Reinforcement Learning","Advancing the dynamic loco-manipulation capabilities of quadruped robots in
complex terrains is crucial for performing diverse tasks. Specifically, dynamic
ball manipulation in rugged environments presents two key challenges. The first
is coordinating distinct motion modalities to integrate terrain traversal and
ball control seamlessly. The second is overcoming sparse rewards in end-to-end
deep reinforcement learning, which impedes efficient policy convergence. To
address these challenges, we propose a hierarchical reinforcement learning
framework. A high-level policy, informed by proprioceptive data and ball
position, adaptively switches between pre-trained low-level skills such as ball
dribbling and rough terrain navigation. We further propose Dynamic
Skill-Focused Policy Optimization to suppress gradients from inactive skills
and enhance critical skill learning. Both simulation and real-world experiments
validate that our methods outperform baseline approaches in dynamic ball
manipulation across rugged terrains, highlighting its effectiveness in
challenging environments. Videos are on our website: dribble-hrl.github.io.","Dongjie Zhu, Zhuo Yang, Tianhang Wu, Luzhou Ge, Xuesong Li, Qi Liu, Xiang Li",2025-04-21T09:38:38Z,2025-04-21T09:38:38Z,http://arxiv.org/abs/2504.14989v1,http://arxiv.org/pdf/2504.14989v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Coordinating Spinal and Limb Dynamics for Enhanced Sprawling Robot
  Mobility","Among vertebrates, salamanders, with their unique ability to transition
between walking and swimming gaits, highlight the role of spinal mobility in
locomotion. A flexible spine enables undulation of the body through a wavelike
motion along the spine, aiding navigation over uneven terrains and obstacles.
Yet environmental uncertainties, such as surface irregularities and variations
in friction, can significantly disrupt body-limb coordination and cause
discrepancies between predictions from mathematical models and real-world
outcomes. Addressing this challenge requires the development of sophisticated
control strategies capable of dynamically adapting to uncertain conditions
while maintaining efficient locomotion. Deep reinforcement learning (DRL)
offers a promising framework for handling non-deterministic environments and
enabling robotic systems to adapt effectively and perform robustly under
challenging conditions. In this study, we comparatively examine learning-based
control strategies and biologically inspired gait design methods on a
salamander-like robot.","Merve Atasever, Ali Okhovat, Azhang Nazaripouya, John Nisbet, Omer Kurkutlu, Jyotirmoy V. Deshmukh, Yasemin Ozkan Aydin",2025-04-18T23:08:48Z,2025-04-18T23:08:48Z,http://arxiv.org/abs/2504.14103v1,http://arxiv.org/pdf/2504.14103v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Robust Humanoid Walking on Compliant and Uneven Terrain with Deep
  Reinforcement Learning","For the deployment of legged robots in real-world environments, it is
essential to develop robust locomotion control methods for challenging terrains
that may exhibit unexpected deformability and irregularity. In this paper, we
explore the application of sim-to-real deep reinforcement learning (RL) for the
design of bipedal locomotion controllers for humanoid robots on compliant and
uneven terrains. Our key contribution is to show that a simple training
curriculum for exposing the RL agent to randomized terrains in simulation can
achieve robust walking on a real humanoid robot using only proprioceptive
feedback. We train an end-to-end bipedal locomotion policy using the proposed
approach, and show extensive real-robot demonstration on the HRP-5P humanoid
over several difficult terrains inside and outside the lab environment.
Further, we argue that the robustness of a bipedal walking policy can be
improved if the robot is allowed to exhibit aperiodic motion with variable
stepping frequency. We propose a new control policy to enable modification of
the observed clock signal, leading to adaptive gait frequencies depending on
the terrain and command velocity. Through simulation experiments, we show the
effectiveness of this policy specifically for walking over challenging terrains
by controlling swing and stance durations. The code for training and evaluation
is available online at https://github.com/rohanpsingh/LearningHumanoidWalking.
Demo video is available at https://www.youtube.com/watch?v=ZgfNzGAkk2Q.","Rohan P. Singh, Mitsuharu Morisawa, Mehdi Benallegue, Zhaoming Xie, Fumio Kanehiro",2025-04-18T10:49:07Z,2025-04-18T10:49:07Z,http://arxiv.org/abs/2504.13619v1,http://arxiv.org/pdf/2504.13619v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Physical Reservoir Computing in Hook-Shaped Rover Wheel Spokes for
  Real-Time Terrain Identification","Effective terrain detection in unknown environments is crucial for safe and
efficient robotic navigation. Traditional methods often rely on computationally
intensive data processing, requiring extensive onboard computational capacity
and limiting real-time performance for rovers. This study presents a novel
approach that combines physical reservoir computing with piezoelectric sensors
embedded in rover wheel spokes for real-time terrain identification. By
leveraging wheel dynamics, terrain-induced vibrations are transformed into
high-dimensional features for machine learning-based classification.
Experimental results show that strategically placing three sensors on the wheel
spokes achieves 90$\%$ classification accuracy, which demonstrates the accuracy
and feasibility of the proposed method. The experiment results also showed that
the system can effectively distinguish known terrains and identify unknown
terrains by analyzing their similarity to learned categories. This method
provides a robust, low-power framework for real-time terrain classification and
roughness estimation in unstructured environments, enhancing rover autonomy and
adaptability.","Xiao Jin, Zihan Wang, Zhenhua Yu, Changrak Choi, Kalind Carpenter, Thrishantha Nanayakkara",2025-04-17T21:29:17Z,2025-04-17T21:29:17Z,http://arxiv.org/abs/2504.13348v1,http://arxiv.org/pdf/2504.13348v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Long Range Navigator (LRN): Extending robot planning horizons beyond
  metric maps","A robot navigating an outdoor environment with no prior knowledge of the
space must rely on its local sensing to perceive its surroundings and plan.
This can come in the form of a local metric map or local policy with some fixed
horizon. Beyond that, there is a fog of unknown space marked with some fixed
cost. A limited planning horizon can often result in myopic decisions leading
the robot off course or worse, into very difficult terrain. Ideally, we would
like the robot to have full knowledge that can be orders of magnitude larger
than a local cost map. In practice, this is intractable due to sparse sensing
information and often computationally expensive. In this work, we make a key
observation that long-range navigation only necessitates identifying good
frontier directions for planning instead of full map knowledge. To this end, we
propose Long Range Navigator (LRN), that learns an intermediate affordance
representation mapping high-dimensional camera images to `affordable' frontiers
for planning, and then optimizing for maximum alignment with the desired goal.
LRN notably is trained entirely on unlabeled ego-centric videos making it easy
to scale and adapt to new platforms. Through extensive off-road experiments on
Spot and a Big Vehicle, we find that augmenting existing navigation stacks with
LRN reduces human interventions at test-time and leads to faster decision
making indicating the relevance of LRN. https://personalrobotics.github.io/lrn","Matt Schmittle, Rohan Baijal, Nathan Hatch, Rosario Scalise, Mateo Guaman Castro, Sidharth Talia, Khimya Khetarpal, Byron Boots, Siddhartha Srinivasa",2025-04-17T17:55:08Z,2025-04-17T17:55:08Z,http://arxiv.org/abs/2504.13149v1,http://arxiv.org/pdf/2504.13149v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"A Genetic Approach to Gradient-Free Kinodynamic Planning in Uneven
  Terrains","This paper proposes a genetic algorithm-based kinodynamic planning algorithm
(GAKD) for car-like vehicles navigating uneven terrains modeled as triangular
meshes. The algorithm's distinct feature is trajectory optimization over a
fixed-length receding horizon using a genetic algorithm with heuristic-based
mutation, ensuring the vehicle's controls remain within its valid operational
range. By addressing challenges posed by uneven terrain meshes, such as
changing face normals, GAKD offers a practical solution for path planning in
complex environments. Comparative evaluations against Model Predictive Path
Integral (MPPI) and log-MPPI methods show that GAKD achieves up to 20 percent
improvement in traversability cost while maintaining comparable path length.
These results demonstrate GAKD's potential in improving vehicle navigation on
challenging terrains.","Otobong Jerome, Alexandr Klimchik, Alexander Maloletov, Geesara Kulathunga",2025-04-17T06:11:31Z,2025-04-17T06:11:31Z,http://arxiv.org/abs/2504.12678v1,http://arxiv.org/pdf/2504.12678v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Self-Supervised Traversability Learning with Online Prototype Adaptation
  for Off-Road Autonomous Driving","Achieving reliable and safe autonomous driving in off-road environments
requires accurate and efficient terrain traversability analysis. However, this
task faces several challenges, including the scarcity of large-scale datasets
tailored for off-road scenarios, the high cost and potential errors of manual
annotation, the stringent real-time requirements of motion planning, and the
limited computational power of onboard units. To address these challenges, this
paper proposes a novel traversability learning method that leverages
self-supervised learning, eliminating the need for manual annotation. For the
first time, a Birds-Eye View (BEV) representation is used as input, reducing
computational burden and improving adaptability to downstream motion planning.
During vehicle operation, the proposed method conducts online analysis of
traversed regions and dynamically updates prototypes to adaptively assess the
traversability of the current environment, effectively handling dynamic scene
changes. We evaluate our approach against state-of-the-art benchmarks on both
public datasets and our own dataset, covering diverse seasons and geographical
locations. Experimental results demonstrate that our method significantly
outperforms recent approaches. Additionally, real-world vehicle experiments
show that our method operates at 10 Hz, meeting real-time requirements, while a
5.5 km autonomous driving experiment further validates the generated
traversability cost maps compatibility with downstream motion planning.","Yafeng Bu, Zhenping Sun, Xiaohui Li, Jun Zeng, Xin Zhang, Hui Shen",2025-04-16T14:17:31Z,2025-04-16T14:17:31Z,http://arxiv.org/abs/2504.12109v1,http://arxiv.org/pdf/2504.12109v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Shape Your Ground: Refining Road Surfaces Beyond Planar Representations,"Road surface reconstruction from aerial images is fundamental for autonomous
driving, urban planning, and virtual simulation, where smoothness, compactness,
and accuracy are critical quality factors. Existing reconstruction methods
often produce artifacts and inconsistencies that limit usability, while
downstream tasks have a tendency to represent roads as planes for simplicity
but at the cost of accuracy. We introduce FlexRoad, the first framework to
directly address road surface smoothing by fitting Non-Uniform Rational
B-Splines (NURBS) surfaces to 3D road points obtained from photogrammetric
reconstructions or geodata providers. Our method at its core utilizes the
Elevation-Constrained Spatial Road Clustering (ECSRC) algorithm for robust
anomaly correction, significantly reducing surface roughness and fitting
errors. To facilitate quantitative comparison between road surface
reconstruction methods, we present GeoRoad Dataset (GeRoD), a diverse
collection of road surface and terrain profiles derived from openly accessible
geodata. Experiments on GeRoD and the photogrammetry-based DeepScenario Open 3D
Dataset (DSC3D) demonstrate that FlexRoad considerably surpasses commonly used
road surface representations across various metrics while being insensitive to
various input sources, terrains, and noise types. By performing ablation
studies, we identify the key role of each component towards high-quality
reconstruction performance, making FlexRoad a generic method for realistic road
surface modeling.","Oussema Dhaouadi, Johannes Meier, Jacques Kaiser, Daniel Cremers",2025-04-15T21:20:44Z,2025-04-15T21:20:44Z,http://arxiv.org/abs/2504.16103v1,http://arxiv.org/pdf/2504.16103v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Teacher Motion Priors: Enhancing Robot Locomotion over Challenging
  Terrain","Achieving robust locomotion on complex terrains remains a challenge due to
high dimensional control and environmental uncertainties. This paper introduces
a teacher prior framework based on the teacher student paradigm, integrating
imitation and auxiliary task learning to improve learning efficiency and
generalization. Unlike traditional paradigms that strongly rely on
encoder-based state embeddings, our framework decouples the network design,
simplifying the policy network and deployment. A high performance teacher
policy is first trained using privileged information to acquire generalizable
motion skills. The teacher's motion distribution is transferred to the student
policy, which relies only on noisy proprioceptive data, via a generative
adversarial mechanism to mitigate performance degradation caused by
distributional shifts. Additionally, auxiliary task learning enhances the
student policy's feature representation, speeding up convergence and improving
adaptability to varying terrains. The framework is validated on a humanoid
robot, showing a great improvement in locomotion stability on dynamic terrains
and significant reductions in development costs. This work provides a practical
solution for deploying robust locomotion strategies in humanoid robots.","Fangcheng Jin, Yuqi Wang, Peixin Ma, Guodong Yang, Pan Zhao, En Li, Zhengtao Zhang",2025-04-14T16:36:56Z,2025-06-25T09:27:22Z,http://arxiv.org/abs/2504.10390v2,http://arxiv.org/pdf/2504.10390v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"GenTe: Generative Real-world Terrains for General Legged Robot
  Locomotion Control","Developing bipedal robots capable of traversing diverse real-world terrains
presents a fundamental robotics challenge, as existing methods using predefined
height maps and static environments fail to address the complexity of
unstructured landscapes. To bridge this gap, we propose GenTe, a framework for
generating physically realistic and adaptable terrains to train generalizable
locomotion policies. GenTe constructs an atomic terrain library that includes
both geometric and physical terrains, enabling curriculum training for
reinforcement learning-based locomotion policies. By leveraging
function-calling techniques and reasoning capabilities of Vision-Language
Models (VLMs), GenTe generates complex, contextually relevant terrains from
textual and graphical inputs. The framework introduces realistic force modeling
for terrain interactions, capturing effects such as soil sinkage and
hydrodynamic resistance. To the best of our knowledge, GenTe is the first
framework that systemically generates simulation environments for legged robot
locomotion control. Additionally, we introduce a benchmark of 100 generated
terrains. Experiments demonstrate improved generalization and robustness in
bipedal robot locomotion.","Hanwen Wan, Mengkang Li, Donghao Wu, Yebin Zhong, Yixuan Deng, Zhenglong Sun, Xiaoqiang Ji",2025-04-14T09:01:44Z,2025-04-14T09:01:44Z,http://arxiv.org/abs/2504.09997v1,http://arxiv.org/pdf/2504.09997v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"PPF: Pre-training and Preservative Fine-tuning of Humanoid Locomotion
  via Model-Assumption-based Regularization","Humanoid locomotion is a challenging task due to its inherent complexity and
high-dimensional dynamics, as well as the need to adapt to diverse and
unpredictable environments. In this work, we introduce a novel learning
framework for effectively training a humanoid locomotion policy that imitates
the behavior of a model-based controller while extending its capabilities to
handle more complex locomotion tasks, such as more challenging terrain and
higher velocity commands. Our framework consists of three key components:
pre-training through imitation of the model-based controller, fine-tuning via
reinforcement learning, and model-assumption-based regularization (MAR) during
fine-tuning. In particular, MAR aligns the policy with actions from the
model-based controller only in states where the model assumption holds to
prevent catastrophic forgetting. We evaluate the proposed framework through
comprehensive simulation tests and hardware experiments on a full-size humanoid
robot, Digit, demonstrating a forward speed of 1.5 m/s and robust locomotion
across diverse terrains, including slippery, sloped, uneven, and sandy
terrains.","Hyunyoung Jung, Zhaoyuan Gu, Ye Zhao, Hae-Won Park, Sehoon Ha",2025-04-14T03:02:02Z,2025-09-02T19:36:04Z,http://arxiv.org/abs/2504.09833v2,http://arxiv.org/pdf/2504.09833v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Development of a PPO-Reinforcement Learned Walking Tripedal Soft-Legged
  Robot using SOFA","Rigid robots were extensively researched, whereas soft robotics remains an
underexplored field. Utilizing soft-legged robots in performing tasks as a
replacement for human beings is an important stride to take, especially under
harsh and hazardous conditions over rough terrain environments. For the demand
to teach any robot how to behave in different scenarios, a real-time physical
and visual simulation is essential. When it comes to soft robots specifically,
a simulation framework is still an arduous problem that needs to be disclosed.
Using the simulation open framework architecture (SOFA) is an advantageous
step. However, neither SOFA's manual nor prior public SOFA projects show its
maximum capabilities the users can reach. So, we resolved this by establishing
customized settings and handling the framework components appropriately.
Settling on perfect, fine-tuned SOFA parameters has stimulated our motivation
towards implementing the state-of-the-art (SOTA) reinforcement learning (RL)
method of proximal policy optimization (PPO). The final representation is a
well-defined, ready-to-deploy walking, tripedal, soft-legged robot based on
PPO-RL in a SOFA environment. Robot navigation performance is a key metric to
be considered for measuring the success resolution. Although in the simulated
soft robots case, an 82\% success rate in reaching a single goal is a
groundbreaking output, we pushed the boundaries to further steps by evaluating
the progress under assigning a sequence of goals. While trailing the platform
steps, outperforming discovery has been observed with an accumulative squared
error deviation of 19 mm. The full code is publicly available at
\href{https://github.com/tarekshohdy/PPO_SOFA_Soft_Legged_Robot.git}{github.com/tarekshohdy/PPO$\textunderscore$SOFA$\textunderscore$Soft$\textunderscore$Legged$\textunderscore$
Robot.git}","Yomna Mokhtar, Tarek Shohdy, Abdallah A. Hassan, Mostafa Eshra, Omar Elmenawy, Osama Khalil, Haitham El-Hussieny",2025-04-12T14:46:51Z,2025-04-12T14:46:51Z,http://arxiv.org/abs/2504.09242v1,http://arxiv.org/pdf/2504.09242v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Tactile sensing enables vertical obstacle negotiation for elongate
  many-legged robots","Many-legged elongated robots show promise for reliable mobility on rugged
landscapes. However, most studies on these systems focus on planar motion
planning without addressing rapid vertical motion. Despite their success on
mild rugged terrains, recent field tests reveal a critical need for 3D
behaviors (e.g., climbing or traversing tall obstacles). The challenges of 3D
motion planning partially lie in designing sensing and control for a complex
high-degree-of-freedom system, typically with over 25 degrees of freedom. To
address the first challenge regarding sensing, we propose a tactile antenna
system that enables the robot to probe obstacles to gather information about
their structure. Building on this sensory input, we develop a control framework
that integrates data from the antenna and foot contact sensors to dynamically
adjust the robot's vertical body undulation for effective climbing. With the
addition of simple, low-bandwidth tactile sensors, a robot with high static
stability and redundancy exhibits predictable climbing performance in complex
environments using a simple feedback controller. Laboratory and outdoor
experiments demonstrate the robot's ability to climb obstacles up to five times
its height. Moreover, the robot exhibits robust climbing capabilities on
obstacles covered with shifting, robot-sized random items and those
characterized by rapidly changing curvatures. These findings demonstrate an
alternative solution to perceive the environment and facilitate effective
response for legged robots, paving ways towards future highly capable,
low-profile many-legged robots.","Juntao He, Baxi Chong, Massimiliano Iaschi, Vincent R. Nienhusser, Sehoon Ha, Daniel I. Goldman",2025-04-11T15:20:31Z,2025-04-21T05:20:43Z,http://arxiv.org/abs/2504.08615v2,http://arxiv.org/pdf/2504.08615v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
UWB Anchor Based Localization of a Planetary Rover,"Localization of an autonomous mobile robot during planetary exploration is
challenging due to the unknown terrain, the difficult lighting conditions and
the lack of any global reference such as satellite navigation systems. We
present a novel approach for robot localization based on ultra-wideband (UWB)
technology. The robot sets up its own reference coordinate system by
distributing UWB anchor nodes in the environment via a rocket-propelled
launcher system. This allows the creation of a localization space in which UWB
measurements are employed to supplement traditional SLAM-based techniques. The
system was developed for our involvement in the ESA-ESRIC challenge 2021 and
the AMADEE-24, an analog Mars simulation in Armenia by the Austrian Space Forum
(\""OWF).","Andreas Nüchter, Lennart Werner, Martin Hesse, Dorit Borrmann, Thomas Walter, Sergio Montenegro, Gernot Grömer",2025-04-10T11:15:47Z,2025-04-10T11:15:47Z,http://arxiv.org/abs/2504.07658v1,http://arxiv.org/pdf/2504.07658v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Anti-Slip AI-Driven Model-Free Control with Global Exponential Stability
  in Skid-Steering Robots","Undesired lateral and longitudinal wheel slippage can disrupt a mobile
robot's heading angle, traction, and, eventually, desired motion. This issue
makes the robotization and accurate modeling of heavy-duty machinery very
challenging because the application primarily involves off-road terrains, which
are susceptible to uneven motion and severe slippage. As a step toward
robotization in skid-steering heavy-duty robot (SSHDR), this paper aims to
design an innovative robust model-free control system developed by neural
networks to strongly stabilize the robot dynamics in the presence of a broad
range of potential wheel slippages. Before the control design, the dynamics of
the SSHDR are first investigated by mathematically incorporating slippage
effects, assuming that all functional modeling terms of the system are unknown
to the control system. Then, a novel tracking control framework to guarantee
global exponential stability of the SSHDR is designed as follows: 1) the
unknown modeling of wheel dynamics is approximated using radial basis function
neural networks (RBFNNs); and 2) a new adaptive law is proposed to compensate
for slippage effects and tune the weights of the RBFNNs online during
execution. Simulation and experimental results verify the proposed tracking
control performance of a 4,836 kg SSHDR operating on slippery terrain.","Mehdi Heydari Shahna, Pauli Mustalahti, Jouni Mattila",2025-04-10T09:21:10Z,2025-04-10T09:21:10Z,http://arxiv.org/abs/2504.08831v1,http://arxiv.org/pdf/2504.08831v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Bipedal Robust Walking on Uneven Footholds: Piecewise Slope LIPM with
  Discrete Model Predictive Control","This study presents an enhanced theoretical formulation for bipedal
hierarchical control frameworks under uneven terrain conditions. Specifically,
owing to the inherent limitations of the Linear Inverted Pendulum Model (LIPM)
in handling terrain elevation variations, we develop a Piecewise Slope LIPM
(PS-LIPM). This innovative model enables dynamic adjustment of the Center of
Mass (CoM) height to align with topographical undulations during single-step
cycles. Another contribution is proposed a generalized Angular Momentum-based
LIPM (G-ALIP) for CoM velocity compensation using Centroidal Angular Momentum
(CAM) regulation. Building upon these advancements, we derive the DCM
step-to-step dynamics for Model Predictive Control MPC formulation, enabling
simultaneous optimization of step position and step duration. A hierarchical
control framework integrating MPC with a Whole-Body Controller (WBC) is
implemented for bipedal locomotion across uneven stepping stones. The results
validate the efficacy of the proposed hierarchical control framework and the
theoretical formulation.","Yapeng Shi, Sishu Li, Yongqiang Wu, Junjie Liu, Xiaokun Leng, Xizhe Zang, Songhao Piao",2025-04-03T03:54:10Z,2025-04-03T03:54:10Z,http://arxiv.org/abs/2504.02255v1,http://arxiv.org/pdf/2504.02255v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Time-optimal Convexified Reeds-Shepp Paths on a Sphere,"This article addresses time-optimal path planning for a vehicle capable of
moving both forward and backward on a unit sphere with a unit maximum speed,
and constrained by a maximum absolute turning rate $U_{max}$. The proposed
formulation can be utilized for optimal attitude control of underactuated
satellites, optimal motion planning for spherical rolling robots, and optimal
path planning for mobile robots on spherical surfaces or uneven terrains. By
utilizing Pontryagin's Maximum Principle and analyzing phase portraits, it is
shown that for $U_{max}\geq1$, the optimal path connecting a given initial
configuration to a desired terminal configuration falls within a sufficient
list of 23 path types, each comprising at most 6 segments. These segments
belong to the set $\{C,G,T\}$, where $C$ represents a tight turn with radius
$r=\frac{1}{\sqrt{1+U_{max}^2}}$, $G$ represents a great circular arc, and $T$
represents a turn-in-place motion. Closed-form expressions for the angles of
each path in the sufficient list are derived. The source code for solving the
time-optimal path problem and visualization is publicly available at
https://github.com/sixuli97/Optimal-Spherical-Convexified-Reeds-Shepp-Paths.","Sixu Li, Deepak Prakash Kumar, Swaroop Darbha, Yang Zhou",2025-04-01T17:03:56Z,2025-04-01T17:03:56Z,http://arxiv.org/abs/2504.00966v1,http://arxiv.org/pdf/2504.00966v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Learning Bipedal Locomotion on Gear-Driven Humanoid Robot Using
  Foot-Mounted IMUs","Sim-to-real reinforcement learning (RL) for humanoid robots with high-gear
ratio actuators remains challenging due to complex actuator dynamics and the
absence of torque sensors. To address this, we propose a novel RL framework
leveraging foot-mounted inertial measurement units (IMUs). Instead of pursuing
detailed actuator modeling and system identification, we utilize foot-mounted
IMU measurements to enhance rapid stabilization capabilities over challenging
terrains. Additionally, we propose symmetric data augmentation dedicated to the
proposed observation space and random network distillation to enhance bipedal
locomotion learning over rough terrain. We validate our approach through
hardware experiments on a miniature-sized humanoid EVAL-03 over a variety of
environments. The experimental results demonstrate that our method improves
rapid stabilization capabilities over non-rigid surfaces and sudden
environmental transitions.","Sotaro Katayama, Yuta Koda, Norio Nagatsuka, Masaya Kinoshita",2025-04-01T10:11:55Z,2025-04-11T07:30:23Z,http://arxiv.org/abs/2504.00614v2,http://arxiv.org/pdf/2504.00614v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Localized Graph-Based Neural Dynamics Models for Terrain Manipulation,"Predictive models can be particularly helpful for robots to effectively
manipulate terrains in construction sites and extraterrestrial surfaces.
However, terrain state representations become extremely high-dimensional
especially to capture fine-resolution details and when depth is unknown or
unbounded. This paper introduces a learning-based approach for terrain dynamics
modeling and manipulation, leveraging the Graph-based Neural Dynamics (GBND)
framework to represent terrain deformation as motion of a graph of particles.
Based on the principle that the moving portion of a terrain is usually
localized, our approach builds a large terrain graph (potentially millions of
particles) but only identifies a very small active subgraph (hundreds of
particles) for predicting the outcomes of robot-terrain interaction. To
minimize the size of the active subgraph we introduce a learning-based approach
that identifies a small region of interest (RoI) based on the robot's control
inputs and the current scene. We also introduce a novel domain boundary feature
encoding that allows GBNDs to perform accurate dynamics prediction in the RoI
interior while avoiding particle penetration through RoI boundaries. Our
proposed method is both orders of magnitude faster than naive GBND and it
achieves better overall prediction accuracy. We further evaluated our framework
on excavation and shaping tasks on terrain with different granularity.","Chaoqi Liu, Yunzhu Li, Kris Hauser",2025-03-30T01:24:10Z,2025-03-30T01:24:10Z,http://arxiv.org/abs/2503.23270v1,http://arxiv.org/pdf/2503.23270v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"ZodiAq: An Isotropic Flagella-Inspired Soft Underwater Drone for Safe
  Marine Exploration","The inherent challenges of robotic underwater exploration, such as
hydrodynamic effects, the complexity of dynamic coupling, and the necessity for
sensitive interaction with marine life, call for the adoption of soft robotic
approaches in marine exploration. To address this, we present a novel
prototype, ZodiAq, a soft underwater drone inspired by prokaryotic bacterial
flagella. ZodiAq's unique dodecahedral structure, equipped with 12
flagella-like arms, ensures design redundancy and compliance, ideal for
navigating complex underwater terrains. The prototype features a central unit
based on a Raspberry Pi, connected to a sensory system for inertial, depth, and
vision detection, and an acoustic modem for communication. Combined with the
implemented control law, it renders ZodiAq an intelligent system. This paper
details the design and fabrication process of ZodiAq, highlighting design
choices and prototype capabilities. Based on the strain-based modeling of
Cosserat rods, we have developed a digital twin of the prototype within a
simulation toolbox to ease analysis and control. To optimize its operation in
dynamic aquatic conditions, a simplified model-based controller has been
developed and implemented, facilitating intelligent and adaptive movement in
the hydrodynamic environment. Extensive experimental demonstrations highlight
the drone's potential, showcasing its design redundancy, embodied intelligence,
crawling gait, and practical applications in diverse underwater settings. This
research contributes significantly to the field of underwater soft robotics,
offering a promising new avenue for safe, efficient, and environmentally
conscious underwater exploration.","Anup Teejo Mathew, Daniel Feliu-Talegon, Yusuf Abdullahi Adamu, Ikhlas Ben Hmida, Costanza Armanini, Cesare Stefanini, Lakmal Seneviratne, Federico Renda",2025-03-25T11:23:31Z,2025-03-25T11:23:31Z,http://arxiv.org/abs/2503.19556v1,http://arxiv.org/pdf/2503.19556v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Dom, cars don't fly! -- Or do they? In-Air Vehicle Maneuver for
  High-Speed Off-Road Navigation","When pushing the speed limit for aggressive off-road navigation on uneven
terrain, it is inevitable that vehicles may become airborne from time to time.
During time-sensitive tasks, being able to fly over challenging terrain can
also save time, instead of cautiously circumventing or slowly negotiating
through. However, most off-road autonomy systems operate under the assumption
that the vehicles are always on the ground and therefore limit operational
speed. In this paper, we present a novel approach for in-air vehicle maneuver
during high-speed off-road navigation. Based on a hybrid forward kinodynamic
model using both physics principles and machine learning, our fixed-horizon,
sampling-based motion planner ensures accurate vehicle landing poses and their
derivatives within a short airborne time window using vehicle throttle and
steering commands. We test our approach in extensive in-air experiments both
indoors and outdoors, compare it against an error-driven control method, and
demonstrate that precise and timely in-air vehicle maneuver is possible through
existing ground vehicle controls.","Anuj Pokhrel, Aniket Datar, Xuesu Xiao",2025-03-24T20:51:22Z,2025-03-24T20:51:22Z,http://arxiv.org/abs/2503.19140v1,http://arxiv.org/pdf/2503.19140v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Parental Guidance: Efficient Lifelong Learning through Evolutionary
  Distillation","Developing robotic agents that can perform well in diverse environments while
showing a variety of behaviors is a key challenge in AI and robotics.
Traditional reinforcement learning (RL) methods often create agents that
specialize in narrow tasks, limiting their adaptability and diversity. To
overcome this, we propose a preliminary, evolution-inspired framework that
includes a reproduction module, similar to natural species reproduction,
balancing diversity and specialization. By integrating RL, imitation learning
(IL), and a coevolutionary agent-terrain curriculum, our system evolves agents
continuously through complex tasks. This approach promotes adaptability,
inheritance of useful traits, and continual learning. Agents not only refine
inherited skills but also surpass their predecessors. Our initial experiments
show that this method improves exploration efficiency and supports open-ended
learning, offering a scalable solution where sparse reward coupled with diverse
terrain environments induces a multi-task setting.","Octi Zhang, Quanquan Peng, Rosario Scalise, Bryon Boots",2025-03-24T10:40:03Z,2025-03-24T10:40:03Z,http://arxiv.org/abs/2503.18531v1,http://arxiv.org/pdf/2503.18531v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Deep Learning for Human Locomotion Analysis in Lower-Limb Exoskeletons:
  A Comparative Study","Wearable robotics for lower-limb assistance have become a pivotal area of
research, aiming to enhance mobility for individuals with physical impairments
or augment the performance of able-bodied users. Accurate and adaptive control
systems are essential to ensure seamless interaction between the wearer and the
robotic device, particularly when navigating diverse and dynamic terrains.
Despite the recent advances in neural networks for time series analysis, no
attempts have been directed towards the classification of ground conditions,
categorized into five classes and subsequently determining the ramp's slope and
stair's height. In this respect, this paper presents an experimental comparison
between eight deep neural network backbones to predict high-level locomotion
parameters across diverse terrains.
  All the models are trained on the publicly available CAMARGO 2021 dataset.
IMU-only data equally or outperformed IMU+EMG inputs, promoting a
cost-effective and efficient design. Indeeds, using three IMU sensors, the LSTM
achieved high terrain classification accuracy (0.94 +- 0.04) and precise ramp
slope (1.95 +- 0.58{\deg}) and the CNN-LSTM a stair height (15.65 +- 7.40 mm)
estimations. As a further contribution, SHAP analysis justified sensor
reduction without performance loss, ensuring a lightweight setup. The system
operates with ~2 ms inference time, supporting real-time applications. The code
is code available at
https://github.com/cosbidev/Human-Locomotion-Identification.","Omar Coser, Christian Tamantini, Matteo Tortora, Leonardo Furia, Rosa Sicilia, Loredana Zollo, Paolo Soda",2025-03-21T07:12:44Z,2025-03-21T07:12:44Z,http://arxiv.org/abs/2503.16904v1,http://arxiv.org/pdf/2503.16904v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Human locomotor control timescales depend on the environmental context
  and sensory input modality","Everyday locomotion is a complex sensorimotor process that can unfold over
multiple timescales, from long-term path planning to rapid, reactive
adjustments. However, we lack an understanding of how factors such as
environmental demands, or the available sensory information simultaneously
influence these control timescales. To address this, we present a unified
data-driven framework to quantify the control timescales by identifying how
early we can predict future actions from past inputs. We apply this framework
across tasks including walking and running, environmental contexts including
treadmill, overground, and varied terrains, and sensory input modalities
including gaze fixations and body states. We find that deep neural network
architectures that effectively handle long-range dependencies, specifically
Gated Recurrent Units and Transformers, outperform other architectures and
widely used linear models when predicting future actions. Our framework reveals
the factors that influence locomotor foot placement control timescales. Across
environmental contexts, we discover that humans rely more on fast timescale
control in more complex terrain. Across input modalities, we find a hierarchy
of control timescales where gaze predicts foot placement before full-body
states, which predict before center-of-mass states. Our model also identifies
mid-swing as a critical phase when the swing foot's state predicts its future
placement, with this timescale adapting across environments. Overall, this work
offers data-driven insights into locomotor control in everyday settings,
offering models that can be integrated with rehabilitation technologies and
movement simulations to improve their applicability in everyday settings.","Wei-Chen Wang, Antoine De Comite, Alexandra Voloshina, Monica Daley, Nidhi Seethapathi",2025-03-20T16:57:15Z,2025-08-27T03:07:09Z,http://arxiv.org/abs/2503.16340v5,http://arxiv.org/pdf/2503.16340v5.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Reward Training Wheels: Adaptive Auxiliary Rewards for Robotics
  Reinforcement Learning","Robotics Reinforcement Learning (RL) often relies on carefully engineered
auxiliary rewards to supplement sparse primary learning objectives to
compensate for the lack of large-scale, real-world, trial-and-error data. While
these auxiliary rewards accelerate learning, they require significant
engineering effort, may introduce human biases, and cannot adapt to the robot's
evolving capabilities during training. In this paper, we introduce Reward
Training Wheels (RTW), a teacher-student framework that automates auxiliary
reward adaptation for robotics RL. To be specific, the RTW teacher dynamically
adjusts auxiliary reward weights based on the student's evolving capabilities
to determine which auxiliary reward aspects require more or less emphasis to
improve the primary objective. We demonstrate RTW on two challenging robot
tasks: navigation in highly constrained spaces and off-road vehicle mobility on
vertically challenging terrain. In simulation, RTW outperforms expert-designed
rewards by 2.35% in navigation success rate and improves off-road mobility
performance by 122.62%, while achieving 35% and 3X faster training efficiency,
respectively. Physical robot experiments further validate RTW's effectiveness,
achieving a perfect success rate (5/5 trials vs. 2/5 for expert-designed
rewards) and improving vehicle stability with up to 47.4% reduction in
orientation angles.","Linji Wang, Tong Xu, Yuanjie Lu, Xuesu Xiao",2025-03-19T22:45:59Z,2025-03-19T22:45:59Z,http://arxiv.org/abs/2503.15724v1,http://arxiv.org/pdf/2503.15724v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"StyleLoco: Generative Adversarial Distillation for Natural Humanoid
  Robot Locomotion","Humanoid robots are anticipated to acquire a wide range of locomotion
capabilities while ensuring natural movement across varying speeds and
terrains. Existing methods encounter a fundamental dilemma in learning humanoid
locomotion: reinforcement learning with handcrafted rewards can achieve agile
locomotion but produces unnatural gaits, while Generative Adversarial Imitation
Learning (GAIL) with motion capture data yields natural movements but suffers
from unstable training processes and restricted agility. Integrating these
approaches proves challenging due to the inherent heterogeneity between expert
policies and human motion datasets. To address this, we introduce StyleLoco, a
novel two-stage framework that bridges this gap through a Generative
Adversarial Distillation (GAD) process. Our framework begins by training a
teacher policy using reinforcement learning to achieve agile and dynamic
locomotion. It then employs a multi-discriminator architecture, where distinct
discriminators concurrently extract skills from both the teacher policy and
motion capture data. This approach effectively combines the agility of
reinforcement learning with the natural fluidity of human-like movements while
mitigating the instability issues commonly associated with adversarial
training. Through extensive simulation and real-world experiments, we
demonstrate that StyleLoco enables humanoid robots to perform diverse
locomotion tasks with the precision of expertly trained policies and the
natural aesthetics of human motion, successfully transferring styles across
different movement types while maintaining stable locomotion across a broad
spectrum of command inputs.","Le Ma, Ziyu Meng, Tengyu Liu, Yuhan Li, Ran Song, Wei Zhang, Siyuan Huang",2025-03-19T10:27:44Z,2025-03-19T10:27:44Z,http://arxiv.org/abs/2503.15082v1,http://arxiv.org/pdf/2503.15082v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Safety-Critical and Distributed Nonlinear Predictive Controllers for
  Teams of Quadrupedal Robots","This paper presents a novel hierarchical, safety-critical control framework
that integrates distributed nonlinear model predictive controllers (DNMPCs)
with control barrier functions (CBFs) to enable cooperative locomotion of
multi-agent quadrupedal robots in complex environments. While NMPC-based
methods are widely adopted for enforcing safety constraints and navigating
multi-robot systems (MRSs) through intricate environments, ensuring the safety
of MRSs requires a formal definition grounded in the concept of invariant sets.
CBFs, typically implemented via quadratic programs (QPs) at the planning layer,
provide formal safety guarantees. However, their zero-control horizon limits
their effectiveness for extended trajectory planning in inherently unstable,
underactuated, and nonlinear legged robot models. Furthermore, the integration
of CBFs into real-time NMPC for sophisticated MRSs, such as quadrupedal robot
teams, remains underexplored. This paper develops computationally efficient,
distributed NMPC algorithms that incorporate CBF-based collision safety
guarantees within a consensus protocol, enabling longer planning horizons for
safe cooperative locomotion under disturbances and rough terrain conditions.
The optimal trajectories generated by the DNMPCs are tracked using full-order,
nonlinear whole-body controllers at the low level. The proposed approach is
validated through extensive numerical simulations with up to four Unitree A1
robots and hardware experiments involving two A1 robots subjected to external
pushes, rough terrain, and uncertain obstacle information. Comparative analysis
demonstrates that the proposed CBF-based DNMPCs achieve a 27.89% higher success
rate than conventional NMPCs without CBF constraints.","Basit Muhammad Imran, Jeeseop Kim, Taizoon Chunawala, Alexander Leonessa, Kaveh Akbari Hamed",2025-03-18T19:05:57Z,2025-03-18T19:05:57Z,http://arxiv.org/abs/2503.14656v1,http://arxiv.org/pdf/2503.14656v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
ADAPT: An Autonomous Forklift for Construction Site Operation,"Efficient material logistics play a critical role in controlling costs and
schedules in the construction industry. However, manual material handling
remains prone to inefficiencies, delays, and safety risks. Autonomous forklifts
offer a promising solution to streamline on-site logistics, reducing reliance
on human operators and mitigating labor shortages. This paper presents the
development and evaluation of ADAPT (Autonomous Dynamic All-terrain Pallet
Transporter), a fully autonomous off-road forklift designed for construction
environments. Unlike structured warehouse settings, construction sites pose
significant challenges, including dynamic obstacles, unstructured terrain, and
varying weather conditions. To address these challenges, our system integrates
AI-driven perception techniques with traditional approaches for decision
making, planning, and control, enabling reliable operation in complex
environments. We validate the system through extensive real-world testing,
comparing its continuous performance against an experienced human operator
across various weather conditions. Our findings demonstrate that autonomous
outdoor forklifts can operate near human-level performance, offering a viable
path toward safer and more efficient construction logistics.","Johannes Huemer, Markus Murschitz, Matthias Schörghuber, Lukas Reisinger, Thomas Kadiofsky, Christoph Weidinger, Mario Niedermeyer, Benedikt Widy, Marcel Zeilinger, Csaba Beleznai, Tobias Glück, Andreas Kugi, Patrik Zips",2025-03-18T15:03:28Z,2025-05-02T09:17:40Z,http://arxiv.org/abs/2503.14331v3,http://arxiv.org/pdf/2503.14331v3.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"A Chain-Driven, Sandwich-Legged Quadruped Robot: Design and Experimental
  Analysis","This paper introduces a chain-driven, sandwich-legged, mid-size quadruped
robot designed as an accessible research platform. The design prioritizes
enhanced locomotion capabilities, improved reliability and safety of the
actuation system, and simplified, cost-effective manufacturing processes.
Locomotion performance is optimized through a sandwiched leg design and a
dual-motor configuration, reducing leg inertia for agile movements. Reliability
and safety are achieved by integrating robust cable strain reliefs, efficient
heat sinks for motor thermal management, and mechanical limits to restrict leg
motion. Simplified design considerations include a quasi-direct drive (QDD)
actuator and the adoption of low-cost fabrication techniques, such as laser
cutting and 3D printing, to minimize cost and ensure rapid prototyping. The
robot weighs approximately 25 kg and is developed at a cost under \$8000,
making it a scalable and affordable solution for robotics research.
Experimental validations demonstrate the platform's capability to execute trot
and crawl gaits on flat terrain and slopes, highlighting its potential as a
versatile and reliable quadruped research platform.","Aman Singh, Bhavya Giri Goswami, Ketan Nehete, Shishir N. Y. Kolathaya",2025-03-18T13:44:34Z,2025-03-18T13:44:34Z,http://arxiv.org/abs/2503.14255v1,http://arxiv.org/pdf/2503.14255v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"A bio-inspired sand-rolling robot: effect of body shape on sand rolling
  performance","The capability of effectively moving on complex terrains such as sand and
gravel can empower our robots to robustly operate in outdoor environments, and
assist with critical tasks such as environment monitoring, search-and-rescue,
and supply delivery. Inspired by the Mount Lyell salamander's ability to curl
its body into a loop and effectively roll down {\Revision hill slopes}, in this
study we develop a sand-rolling robot and investigate how its locomotion
performance is governed by the shape of its body. We experimentally tested
three different body shapes: Hexagon, Quadrilateral, and Triangle. We found
that Hexagon and Triangle can achieve a faster rolling speed on sand, but
exhibited more frequent failures of getting stuck. Analysis of the interaction
between robot and sand revealed the failure mechanism: the deformation of the
sand produced a local ``sand incline'' underneath robot contact segments,
increasing the effective region of supporting polygon (ERSP) and preventing the
robot from shifting its center of mass (CoM) outside the ERSP to produce
sustainable rolling. Based on this mechanism, a highly-simplified model
successfully captured the critical body pitch for each rolling shape to produce
sustained rolling on sand, and informed design adaptations that mitigated the
locomotion failures and improved robot speed by more than 200$\%$. Our results
provide insights into how locomotors can utilize different morphological
features to achieve robust rolling motion across deformable substrates.","Xingjue Liao, Wenhao Liu, Hao Wu, Feifei Qian",2025-03-18T05:31:56Z,2025-03-18T05:31:56Z,http://arxiv.org/abs/2503.13919v1,http://arxiv.org/pdf/2503.13919v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Bio-Inspired Plastic Neural Networks for Zero-Shot Out-of-Distribution
  Generalization in Complex Animal-Inspired Robots","Artificial neural networks can be used to solve a variety of robotic tasks.
However, they risk failing catastrophically when faced with out-of-distribution
(OOD) situations. Several approaches have employed a type of synaptic
plasticity known as Hebbian learning that can dynamically adjust weights based
on local neural activities. Research has shown that synaptic plasticity can
make policies more robust and help them adapt to unforeseen changes in the
environment. However, networks augmented with Hebbian learning can lead to
weight divergence, resulting in network instability. Furthermore, such Hebbian
networks have not yet been applied to solve legged locomotion in complex real
robots with many degrees of freedom. In this work, we improve the Hebbian
network with a weight normalization mechanism for preventing weight divergence,
analyze the principal components of the Hebbian's weights, and perform a
thorough evaluation of network performance in locomotion control for real
18-DOF dung beetle-like and 16-DOF gecko-like robots. We find that the
Hebbian-based plastic network can execute zero-shot sim-to-real adaptation
locomotion and generalize to unseen conditions, such as uneven terrain and
morphological damage.","Binggwong Leung, Worasuchad Haomachai, Joachim Winther Pedersen, Sebastian Risi, Poramate Manoonpong",2025-03-16T08:13:53Z,2025-03-16T08:13:53Z,http://arxiv.org/abs/2503.12406v1,http://arxiv.org/pdf/2503.12406v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"M2UD: A Multi-model, Multi-scenario, Uneven-terrain Dataset for Ground
  Robot with Localization and Mapping Evaluation","Ground robots play a crucial role in inspection, exploration, rescue, and
other applications. In recent years, advancements in LiDAR technology have made
sensors more accurate, lightweight, and cost-effective. Therefore, researchers
increasingly integrate sensors, for SLAM studies, providing robust technical
support for ground robots and expanding their application domains. Public
datasets are essential for advancing SLAM technology. However, existing
datasets for ground robots are typically restricted to flat-terrain motion with
3 DOF and cover only a limited range of scenarios. Although handheld devices
and UAV exhibit richer and more aggressive movements, their datasets are
predominantly confined to small-scale environments due to endurance
limitations. To fill these gap, we introduce M2UD, a multi-modal,
multi-scenario, uneven-terrain SLAM dataset for ground robots. This dataset
contains a diverse range of highly challenging environments, including cities,
open fields, long corridors, and mixed scenarios. Additionally, it presents
extreme weather conditions. The aggressive motion and degradation
characteristics of this dataset not only pose challenges for testing and
evaluating existing SLAM methods but also advance the development of more
advanced SLAM algorithms. To benchmark SLAM algorithms, M2UD provides smoothed
ground truth localization data obtained via RTK and introduces a novel
localization evaluation metric that considers both accuracy and efficiency.
Additionally, we utilize a high-precision laser scanner to acquire ground truth
maps of two representative scenes, facilitating the development and evaluation
of mapping algorithms. We select 12 localization sequences and 2 mapping
sequences to evaluate several classical SLAM algorithms, verifying usability of
the dataset. To enhance usability, the dataset is accompanied by a suite of
development kits.","Yanpeng Jia, Shiyi Wang, Shiliang Shao, Yue Wang, Fu Zhang, Ting Wang",2025-03-16T07:16:49Z,2025-03-16T07:16:49Z,http://arxiv.org/abs/2503.12387v1,http://arxiv.org/pdf/2503.12387v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
MUSE: A Real-Time Multi-Sensor State Estimator for Quadruped Robots,"This paper introduces an innovative state estimator, MUSE (MUlti-sensor State
Estimator), designed to enhance state estimation's accuracy and real-time
performance in quadruped robot navigation. The proposed state estimator builds
upon our previous work presented in [1]. It integrates data from a range of
onboard sensors, including IMUs, encoders, cameras, and LiDARs, to deliver a
comprehensive and reliable estimation of the robot's pose and motion, even in
slippery scenarios. We tested MUSE on a Unitree Aliengo robot, successfully
closing the locomotion control loop in difficult scenarios, including slippery
and uneven terrain. Benchmarking against Pronto [2] and VILENS [3] showed 67.6%
and 26.7% reductions in translational errors, respectively. Additionally, MUSE
outperformed DLIO [4], a LiDAR-inertial odometry system in rotational errors
and frequency, while the proprioceptive version of MUSE (P-MUSE) outperformed
TSIF [5], with a 45.9% reduction in absolute trajectory error (ATE).","Ylenia Nisticò, João Carlos Virgolino Soares, Lorenzo Amatucci, Geoff Fink, Claudio Semini",2025-03-15T12:12:11Z,2025-03-27T09:28:37Z,http://arxiv.org/abs/2503.12101v2,http://arxiv.org/pdf/2503.12101v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"MRS-CWC: A Weakly Constrained Multi-Robot System with Controllable
  Constraint Stiffness for Mobility and Navigation in Unknown 3D Rough
  Environments","Navigating unknown three-dimensional (3D) rugged environments is challenging
for multi-robot systems. Traditional discrete systems struggle with rough
terrain due to limited individual mobility, while modular systems--where rigid,
controllable constraints link robot units--improve traversal but suffer from
high control complexity and reduced flexibility. To address these limitations,
we propose the Multi-Robot System with Controllable Weak Constraints (MRS-CWC),
where robot units are connected by constraints with dynamically adjustable
stiffness. This adaptive mechanism softens or stiffens in real-time during
environmental interactions, ensuring a balance between flexibility and
mobility. We formulate the system's dynamics and control model and evaluate
MRS-CWC against six baseline methods and an ablation variant in a benchmark
dataset with 100 different simulation terrains. Results show that MRS-CWC
achieves the highest navigation completion rate and ranks second in success
rate, efficiency, and energy cost in the highly rugged terrain group,
outperforming all baseline methods without relying on environmental modeling,
path planning, or complex control. Even where MRS-CWC ranks second, its
performance is only slightly behind a more complex ablation variant with
environmental modeling and path planning. Finally, we develop a physical
prototype and validate its feasibility in a constructed rugged environment. For
videos, simulation benchmarks, and code, please visit
https://wyd0817.github.io/project-mrs-cwc/.","Runze Xiao, Yongdong Wang, Yusuke Tsunoda, Koichi Osuka, Hajime Asama",2025-03-14T14:47:58Z,2025-03-14T14:47:58Z,http://arxiv.org/abs/2503.11461v1,http://arxiv.org/pdf/2503.11461v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
LEVA: A high-mobility logistic vehicle with legged suspension,"The autonomous transportation of materials over challenging terrain is a
challenge with major economic implications and remains unsolved. This paper
introduces LEVA, a high-payload, high-mobility robot designed for autonomous
logistics across varied terrains, including those typical in agriculture,
construction, and search and rescue operations. LEVA uniquely integrates an
advanced legged suspension system using parallel kinematics. It is capable of
traversing stairs using a rl controller, has steerable wheels, and includes a
specialized box pickup mechanism that enables autonomous payload loading as
well as precise and reliable cargo transportation of up to 85 kg across uneven
surfaces, steps and inclines while maintaining a cot of as low as 0.15. Through
extensive experimental validation, LEVA demonstrates its off-road capabilities
and reliability regarding payload loading and transport.","Marco Arnold, Lukas Hildebrandt, Kaspar Janssen, Efe Ongan, Pascal Bürge, Ádám Gyula Gábriel, James Kennedy, Rishi Lolla, Quanisha Oppliger, Micha Schaaf, Joseph Church, Michael Fritsche, Victor Klemm, Turcan Tuna, Giorgio Valsecchi, Cedric Weibel, Michael Wüthrich, Marco Hutter",2025-03-13T04:14:09Z,2025-03-22T04:34:53Z,http://arxiv.org/abs/2503.10028v4,http://arxiv.org/pdf/2503.10028v4.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Unified Locomotion Transformer with Simultaneous Sim-to-Real Transfer
  for Quadrupeds","Quadrupeds have gained rapid advancement in their capability of traversing
across complex terrains. The adoption of deep Reinforcement Learning (RL),
transformers and various knowledge transfer techniques can greatly reduce the
sim-to-real gap. However, the classical teacher-student framework commonly used
in existing locomotion policies requires a pre-trained teacher and leverages
the privilege information to guide the student policy. With the implementation
of large-scale models in robotics controllers, especially transformers-based
ones, this knowledge distillation technique starts to show its weakness in
efficiency, due to the requirement of multiple supervised stages. In this
paper, we propose Unified Locomotion Transformer (ULT), a new transformer-based
framework to unify the processes of knowledge transfer and policy optimization
in a single network while still taking advantage of privilege information. The
policies are optimized with reinforcement learning, next state-action
prediction, and action imitation, all in just one training stage, to achieve
zero-shot deployment. Evaluation results demonstrate that with ULT, optimal
teacher and student policies can be obtained at the same time, greatly easing
the difficulty in knowledge transfer, even with complex transformer-based
models.","Dikai Liu, Tianwei Zhang, Jianxiong Yin, Simon See",2025-03-12T02:15:13Z,2025-08-03T13:21:45Z,http://arxiv.org/abs/2503.08997v2,http://arxiv.org/pdf/2503.08997v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
MoE-Loco: Mixture of Experts for Multitask Locomotion,"We present MoE-Loco, a Mixture of Experts (MoE) framework for multitask
locomotion for legged robots. Our method enables a single policy to handle
diverse terrains, including bars, pits, stairs, slopes, and baffles, while
supporting quadrupedal and bipedal gaits. Using MoE, we mitigate the gradient
conflicts that typically arise in multitask reinforcement learning, improving
both training efficiency and performance. Our experiments demonstrate that
different experts naturally specialize in distinct locomotion behaviors, which
can be leveraged for task migration and skill composition. We further validate
our approach in both simulation and real-world deployment, showcasing its
robustness and adaptability.","Runhan Huang, Shaoting Zhu, Yilun Du, Hang Zhao",2025-03-11T15:53:54Z,2025-05-21T02:51:53Z,http://arxiv.org/abs/2503.08564v2,http://arxiv.org/pdf/2503.08564v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Distillation-PPO: A Novel Two-Stage Reinforcement Learning Framework for
  Humanoid Robot Perceptive Locomotion","In recent years, humanoid robots have garnered significant attention from
both academia and industry due to their high adaptability to environments and
human-like characteristics. With the rapid advancement of reinforcement
learning, substantial progress has been made in the walking control of humanoid
robots. However, existing methods still face challenges when dealing with
complex environments and irregular terrains. In the field of perceptive
locomotion, existing approaches are generally divided into two-stage methods
and end-to-end methods. Two-stage methods first train a teacher policy in a
simulated environment and then use distillation techniques, such as DAgger, to
transfer the privileged information learned as latent features or actions to
the student policy. End-to-end methods, on the other hand, forgo the learning
of privileged information and directly learn policies from a partially
observable Markov decision process (POMDP) through reinforcement learning.
However, due to the lack of supervision from a teacher policy, end-to-end
methods often face difficulties in training and exhibit unstable performance in
real-world applications. This paper proposes an innovative two-stage perceptive
locomotion framework that combines the advantages of teacher policies learned
in a fully observable Markov decision process (MDP) to regularize and supervise
the student policy. At the same time, it leverages the characteristics of
reinforcement learning to ensure that the student policy can continue to learn
in a POMDP, thereby enhancing the model's upper bound. Our experimental results
demonstrate that our two-stage training framework achieves higher training
efficiency and stability in simulated environments, while also exhibiting
better robustness and generalization capabilities in real-world applications.","Qiang Zhang, Gang Han, Jingkai Sun, Wen Zhao, Chenghao Sun, Jiahang Cao, Jiaxu Wang, Yijie Guo, Renjing Xu",2025-03-11T11:10:33Z,2025-03-11T11:10:33Z,http://arxiv.org/abs/2503.08299v1,http://arxiv.org/pdf/2503.08299v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
A Decapod Robot with Rotary Bellows-Enclosed Soft Transmissions,"Soft crawling robots exhibit efficient locomotion across various terrains and
demonstrate robustness to diverse environmental conditions. Here, we propose a
valveless soft-legged robot that integrates a pair of rotary bellows-enclosed
soft transmission systems (R-BESTS). The proposed R-BESTS can directly transmit
the servo rotation into leg swing motion. A timing belt controls the pair of
R-BESTS to maintain synchronous rotation in opposite phases, realizing
alternating tripod gaits of walking and turning. We explored several designs to
understand the role of a reinforcement skeleton in twisting the R-BESTS' input
bellows units. The bending sequences of the robot legs are controlled through
structural design for the output bellows units. Finally, we demonstrate
untethered locomotion with the soft robotic decapod. Experimental results show
that our robot can walk at 1.75 centimeters per second (0.07 body length per
second) for 90 min, turn with a 15-centimeter (0.6 BL) radius, carry a payload
of 200 g, and adapt to different terrains.","Yiming He, Yuchen Wang, Yunjia Zhang, Shuguang Li",2025-03-10T13:38:20Z,2025-03-10T13:38:20Z,http://arxiv.org/abs/2503.07321v1,http://arxiv.org/pdf/2503.07321v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"WHERE-Bot: a Wheel-less Helical-ring Everting Robot Capable of
  Omnidirectional Locomotion","Compared to conventional wheeled transportation systems designed for flat
surfaces, soft robots exhibit exceptional adaptability to various terrains,
enabling stable movement in complex environments. However, due to the risk of
collision with obstacles and barriers, most soft robots rely on sensors for
navigation in unstructured environments with uncertain boundaries. In this
work, we present the WHERE-Bot, a wheel-less everting soft robot capable of
omnidirectional locomotion. Our WHERE-Bot can navigate through unstructured
environments by leveraging its structural and motion advantages rather than
relying on sensors for boundary detection. By configuring a spring toy
``Slinky'' into a loop shape, the WHERE-Bot performs multiple rotational
motions: spiral-rotating along the hub circumference, self-rotating around the
hub's center, and orbiting around a certain point. The robot's trajectories can
be reprogrammed by actively altering its mass distribution. The WHERE-Bot shows
significant potential for boundary exploration in unstructured environments.","Siyuan Feng, Dengfeng Yan, Jin Liu, Haotong Han, Alexandra Kühl, Shuguang Li",2025-03-10T12:30:23Z,2025-03-10T12:30:23Z,http://arxiv.org/abs/2503.07245v1,http://arxiv.org/pdf/2503.07245v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"VMTS: Vision-Assisted Teacher-Student Reinforcement Learning for
  Multi-Terrain Locomotion in Bipedal Robots","Bipedal robots, due to their anthropomorphic design, offer substantial
potential across various applications, yet their control is hindered by the
complexity of their structure. Currently, most research focuses on
proprioception-based methods, which lack the capability to overcome complex
terrain. While visual perception is vital for operation in human-centric
environments, its integration complicates control further. Recent reinforcement
learning (RL) approaches have shown promise in enhancing legged robot
locomotion, particularly with proprioception-based methods. However, terrain
adaptability, especially for bipedal robots, remains a significant challenge,
with most research focusing on flat-terrain scenarios. In this paper, we
introduce a novel mixture of experts teacher-student network RL strategy, which
enhances the performance of teacher-student policies based on visual inputs
through a simple yet effective approach. Our method combines terrain selection
strategies with the teacher policy, resulting in superior performance compared
to traditional models. Additionally, we introduce an alignment loss between the
teacher and student networks, rather than enforcing strict similarity, to
improve the student's ability to navigate diverse terrains. We validate our
approach experimentally on the Limx Dynamic P1 bipedal robot, demonstrating its
feasibility and robustness across multiple terrain types.","Fu Chen, Rui Wan, Peidong Liu, Nanxing Zheng, Bo Zhou",2025-03-10T08:35:38Z,2025-07-18T07:48:59Z,http://arxiv.org/abs/2503.07049v2,http://arxiv.org/pdf/2503.07049v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Infinite Leagues Under the Sea: Photorealistic 3D Underwater Terrain
  Generation by Latent Fractal Diffusion Models","This paper tackles the problem of generating representations of underwater 3D
terrain. Off-the-shelf generative models, trained on Internet-scale data but
not on specialized underwater images, exhibit downgraded realism, as images of
the seafloor are relatively uncommon. To this end, we introduce DreamSea, a
generative model to generate hyper-realistic underwater scenes. DreamSea is
trained on real-world image databases collected from underwater robot surveys.
Images from these surveys contain massive real seafloor observations and
covering large areas, but are prone to noise and artifacts from the real world.
We extract 3D geometry and semantics from the data with visual foundation
models, and train a diffusion model that generates realistic seafloor images in
RGBD channels, conditioned on novel fractal distribution-based latent
embeddings. We then fuse the generated images into a 3D map, building a 3DGS
model supervised by 2D diffusion priors which allows photorealistic novel view
rendering. DreamSea is rigorously evaluated, demonstrating the ability to
robustly generate large-scale underwater scenes that are consistent, diverse,
and photorealistic. Our work drives impact in multiple domains, spanning
filming, gaming, and robot simulation.","Tianyi Zhang, Weiming Zhi, Joshua Mangelson, Matthew Johnson-Roberson",2025-03-09T21:43:37Z,2025-03-09T21:43:37Z,http://arxiv.org/abs/2503.06784v1,http://arxiv.org/pdf/2503.06784v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"T-CBF: Traversability-based Control Barrier Function to Navigate
  Vertically Challenging Terrain","Safety has been of paramount importance in motion planning and control
techniques and is an active area of research in the past few years. Most safety
research for mobile robots target at maintaining safety with the notion of
collision avoidance. However, safety goes beyond just avoiding collisions,
especially when robots have to navigate unstructured, vertically challenging,
off-road terrain, where vehicle rollover and immobilization is as critical as
collisions. In this work, we introduce a novel Traversability-based Control
Barrier Function (T-CBF), in which we use neural Control Barrier Functions
(CBFs) to achieve safety beyond collision avoidance on unstructured vertically
challenging terrain by reasoning about new safety aspects in terms of
traversability. The neural T-CBF trained on safe and unsafe observations
specific to traversability safety is then used to generate safe trajectories.
Furthermore, we present experimental results in simulation and on a physical
Verti-4 Wheeler (V4W) platform, demonstrating that T-CBF can provide
traversability safety while reaching the goal position. T-CBF planner
outperforms previously developed planners by 30\% in terms of keeping the robot
safe and mobile when navigating on real world vertically challenging terrain.","Manas Gupta, Xuesu Xiao",2025-03-08T06:12:38Z,2025-08-02T18:44:19Z,http://arxiv.org/abs/2503.06083v2,http://arxiv.org/pdf/2503.06083v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Words to Wheels: Vision-Based Autonomous Driving Understanding Human
  Language Instructions Using Foundation Models","This paper introduces an innovative application of foundation models,
enabling Unmanned Ground Vehicles (UGVs) equipped with an RGB-D camera to
navigate to designated destinations based on human language instructions.
Unlike learning-based methods, this approach does not require prior training
but instead leverages existing foundation models, thus facilitating
generalization to novel environments. Upon receiving human language
instructions, these are transformed into a 'cognitive route description' using
a large language model (LLM)-a detailed navigation route expressed in human
language. The vehicle then decomposes this description into landmarks and
navigation maneuvers. The vehicle also determines elevation costs and
identifies navigability levels of different regions through a terrain
segmentation model, GANav, trained on open datasets. Semantic elevation costs,
which take both elevation and navigability levels into account, are estimated
and provided to the Model Predictive Path Integral (MPPI) planner, responsible
for local path planning. Concurrently, the vehicle searches for target
landmarks using foundation models, including YOLO-World and EfficientViT-SAM.
Ultimately, the vehicle executes the navigation commands to reach the
designated destination, the final landmark. Our experiments demonstrate that
this application successfully guides UGVs to their destinations following human
language instructions in novel environments, such as unfamiliar terrain or
urban settings.","Chanhoe Ryu, Hyunki Seong, Daegyu Lee, Seongwoo Moon, Sungjae Min, D. Hyunchul Shim",2024-10-14T14:51:27Z,2024-10-14T14:51:27Z,http://arxiv.org/abs/2410.10577v1,http://arxiv.org/pdf/2410.10577v1.pdf,all:terrain AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2024
"Competency-Aware Planning for Probabilistically Safe Navigation Under
  Perception Uncertainty","Perception-based navigation systems are useful for unmanned ground vehicle
(UGV) navigation in complex terrains, where traditional depth-based navigation
schemes are insufficient. However, these data-driven methods are highly
dependent on their training data and can fail in surprising and dramatic ways
with little warning. To ensure the safety of the vehicle and the surrounding
environment, it is imperative that the navigation system is able to recognize
the predictive uncertainty of the perception model and respond safely and
effectively in the face of uncertainty. In an effort to enable safe navigation
under perception uncertainty, we develop a probabilistic and
reconstruction-based competency estimation (PaRCE) method to estimate the
model's level of familiarity with an input image as a whole and with specific
regions in the image. We find that the overall competency score can correctly
predict correctly classified, misclassified, and out-of-distribution (OOD)
samples. We also confirm that the regional competency maps can accurately
distinguish between familiar and unfamiliar regions across images. We then use
this competency information to develop a planning and control scheme that
enables effective navigation while maintaining a low probability of error. We
find that the competency-aware scheme greatly reduces the number of collisions
with unfamiliar obstacles, compared to a baseline controller with no competency
awareness. Furthermore, the regional competency information is very valuable in
enabling efficient navigation.","Sara Pohland, Claire Tomlin",2024-09-09T23:34:24Z,2025-07-26T17:14:53Z,http://arxiv.org/abs/2409.06111v5,http://arxiv.org/pdf/2409.06111v5.pdf,all:terrain AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2024
"Wildfire Autonomous Response and Prediction Using Cellular Automata
  (WARP-CA)","Wildfires pose a severe challenge to ecosystems and human settlements,
exacerbated by climate change and environmental factors. Traditional wildfire
modeling, while useful, often fails to adapt to the rapid dynamics of such
events. This report introduces the (Wildfire Autonomous Response and Prediction
Using Cellular Automata) WARP-CA model, a novel approach that integrates
terrain generation using Perlin noise with the dynamism of Cellular Automata
(CA) to simulate wildfire spread. We explore the potential of Multi-Agent
Reinforcement Learning (MARL) to manage wildfires by simulating autonomous
agents, such as UAVs and UGVs, within a collaborative framework. Our
methodology combines world simulation techniques and investigates emergent
behaviors in MARL, focusing on efficient wildfire suppression and considering
critical environmental factors like wind patterns and terrain features.",Abdelrahman Ramadan,2024-07-02T19:01:59Z,2024-07-02T19:01:59Z,http://arxiv.org/abs/2407.02613v1,http://arxiv.org/pdf/2407.02613v1.pdf,all:terrain AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2024
"Voxel Map to Occupancy Map Conversion Using Free Space Projection for
  Efficient Map Representation for Aerial and Ground Robots","This article introduces a novel method for converting 3D voxel maps, commonly
utilized by robots for localization and navigation, into 2D occupancy maps for
both unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs). The
generated 2D maps can be used for more efficient global navigation for both
UAVs and UGVs, in enabling algorithms developed for 2D maps to be useful in 3D
applications, and allowing for faster transfer of maps between multiple agents
in bandwidth-limited scenarios. The proposed method uses the free space
representation in the UFOMap mapping solution to generate 2D occupancy maps.
During the 3D to 2D map conversion, the method conducts safety checks and
eliminates free spaces in the map with dimensions (in the height axis) lower
than the robot's safety margins. This ensures that an aerial or ground robot
can navigate safely, relying primarily on the 2D map generated by the method.
Additionally, the method extracts the height of navigable free space and a
local estimate of the slope of the floor from the 3D voxel map. The height data
is utilized in converting paths generated using the 2D map into paths in 3D
space for both UAVs and UGVs. The slope data identifies areas too steep for a
ground robot to traverse, marking them as occupied, thus enabling a more
accurate representation of the terrain for ground robots. The effectiveness of
the proposed method in enabling computationally efficient navigation for both
aerial and ground robots is validated in two different environments, over both
static maps and in online implementation in an exploration mission. The methods
proposed within this article have been implemented in the popular robotics
framework ROS and are open-sourced. The code is available at:
https://github.com/LTU-RAI/Map-Conversion-3D-Voxel-Map-to-2D-Occupancy-Map.","Scott Fredriksson, Akshit Saradagi, George Nikolakopoulos",2024-06-11T13:55:37Z,2024-07-21T10:38:22Z,http://arxiv.org/abs/2406.07270v2,http://arxiv.org/pdf/2406.07270v2.pdf,all:terrain AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2024
"A Safety-Critical Framework for UGVs in Complex Environments: A
  Data-Driven Discrepancy-Aware Approach","This work presents a novel data-driven multi-layered planning and control
framework for the safe navigation of a class of unmanned ground vehicles (UGVs)
in the presence of unknown stationary obstacles and additive modeling
uncertainties. The foundation of this framework is a novel robust model
predictive planner, designed to generate optimal collision-free trajectories
given an occupancy grid map, and a paired ancillary controller, augmented to
provide robustness against model uncertainties extracted from learning data.
  To tackle modeling discrepancies, we identify both matched (input
discrepancies) and unmatched model residuals between the true and the nominal
reduced-order models using closed-loop tracking errors as training data.
Utilizing conformal prediction, we extract probabilistic upper bounds for the
unknown model residuals, which serve to construct a robustifying ancillary
controller. Further, we also determine maximum tracking discrepancies, also
known as the robust control invariance tube, under the augmented policy,
formulating them as collision buffers. Employing a LiDAR-based occupancy map to
characterize the environment, we construct a discrepancy-aware cost map that
incorporates these collision buffers. This map is then integrated into a
sampling-based model predictive path planner that generates optimal and safe
trajectories that can be robustly tracked by the augmented ancillary controller
in the presence of model mismatches.
  The effectiveness of the framework is experimentally validated for autonomous
high-speed trajectory tracking in a cluttered environment with four different
vehicle-terrain configurations. We also showcase the framework's versatility by
reformulating it as a driver-assist program, providing collision avoidance
corrections based on user joystick commands.","Skylar X. Wei, Lu Gan, Joel W. Burdick",2024-03-05T18:58:39Z,2024-03-05T18:58:39Z,http://arxiv.org/abs/2403.03215v1,http://arxiv.org/pdf/2403.03215v1.pdf,all:terrain AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2024
"Physics-Informed LSTM-Based Delay Compensation Framework for
  Teleoperated UGVs","Bilateral teleoperation of low-speed Unmanned Ground Vehicles (UGVs) on soft
terrains is crucial for applications like lunar exploration, offering effective
control of terrain-induced longitudinal slippage. However, latency arising from
transmission delays over a network presents a challenge in maintaining
high-fidelity closed-loop integration, potentially hindering UGV controls and
leading to poor command-tracking performance. To address this challenge, this
paper proposes a novel predictor framework that employs a Physics-informed Long
Short-Term Memory (PiLSTM) network for designing bilateral teleoperator
controls that effectively compensate for large delays. Contrasting with
conventional model-free predictor frameworks, which are limited by their linear
nature in capturing nonlinear and temporal dynamic behaviors, our approach
integrates the LSTM structure with physical constraints for enhanced
performance and better generalization across varied scenarios. Specifically,
four distinct predictors were employed in the framework: two compensate for
forward delays, while the other two compensate for backward delays. Due to
their effectiveness in learning from temporal data, the proposed PiLSTM
framework demonstrates a 26.1\ improvement in delay compensation over the
conventional model-free predictors for large delays in open-loop case studies.
Subsequently, experiments were conducted to validate the efficacy of the
framework in close-loop scenarios, particularly to compensate for the
real-network delays experienced by teleoperated UGVs coupled with longitudinal
slippage. The results confirm the proposed framework is effective in restoring
the fidelity of the closed-loop integration. This improvement is showcased
through improved performance and transparency, which leads to excellent
command-tracking performance.","Ahmad Abubakar, Yahya Zweiri, AbdelGafoor Haddad, Mubarak Yakubu, Ruqayya Alhammadi, Lakmal Seneviratne",2024-02-26T14:10:26Z,2024-02-26T14:10:26Z,http://arxiv.org/abs/2402.16587v1,http://arxiv.org/pdf/2402.16587v1.pdf,all:terrain AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2024
DRIVE: Data-driven Robot Input Vector Exploration,"An accurate motion model is a fundamental component of most autonomous
navigation systems. While much work has been done on improving model
formulation, no standard protocol exists for gathering empirical data required
to train models. In this work, we address this issue by proposing Data-driven
Robot Input Vector Exploration (DRIVE), a protocol that enables characterizing
uncrewed ground vehicles (UGVs) input limits and gathering empirical model
training data. We also propose a novel learned slip approach outperforming
similar acceleration learning approaches. Our contributions are validated
through an extensive experimental evaluation, cumulating over 7 km and 1.8 h of
driving data over three distinct UGVs and four terrain types. We show that our
protocol offers increased predictive performance over common human-driven
data-gathering protocols. Furthermore, our protocol converges with 46 s of
training data, almost four times less than the shortest human dataset gathering
protocol. We show that the operational limit for our model is reached in
extreme slip conditions encountered on surfaced ice. DRIVE is an efficient way
of characterizing UGV motion in its operational conditions. Our code and
dataset are both available online at this link:
https://github.com/norlab-ulaval/DRIVE.","Dominic Baril, Simon-Pierre Deschênes, Luc Coupal, Cyril Goffin, Julien Lépine, Philippe Giguère, François Pomerleau",2023-09-19T16:02:23Z,2024-03-27T13:54:19Z,http://arxiv.org/abs/2309.10718v2,http://arxiv.org/pdf/2309.10718v2.pdf,all:terrain AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2023
"Exploration Without Maps via Zero-Shot Out-of-Distribution Deep
  Reinforcement Learning","Operation of Autonomous Mobile Robots (AMRs) of all forms that include
wheeled ground vehicles, quadrupeds and humanoids in dynamically changing GPS
denied environments without a-priori maps, exclusively using onboard sensors,
is an unsolved problem that has potential to transform the economy, and vastly
improve humanity's capabilities with improvements to agriculture,
manufacturing, disaster response, military and space exploration. Conventional
AMR automation approaches are modularized into perception, motion planning and
control which is computationally inefficient, and requires explicit feature
extraction and engineering, that inhibits generalization, and deployment at
scale. Few works have focused on real-world end-to-end approaches that directly
map sensor inputs to control outputs due to the large amount of well curated
training data required for supervised Deep Learning (DL) which is time
consuming and labor intensive to collect and label, and sample inefficiency and
challenges to bridging the simulation to reality gap using Deep Reinforcement
Learning (DRL). This paper presents a novel method to efficiently train DRL for
robust end-to-end AMR exploration, in a constrained environment at physical
limits in simulation, transferred zero-shot to the real-world. The
representation learned in a compact parameter space with 2 fully connected
layers with 64 nodes each is demonstrated to exhibit emergent behavior for
out-of-distribution generalization to navigation in new environments that
include unstructured terrain without maps, and dynamic obstacle avoidance. The
learned policy outperforms conventional navigation algorithms while consuming a
fraction of the computation resources, enabling execution on a range of AMR
forms with varying embedded computer payloads.","Shathushan Sivashangaran, Apoorva Khairnar, Azim Eskandarian",2024-02-07T18:17:54Z,2024-02-07T18:17:54Z,http://arxiv.org/abs/2402.05066v1,http://arxiv.org/pdf/2402.05066v1.pdf,all:terrain AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"ARTPS: Depth-Enhanced Hybrid Anomaly Detection and Learnable Curiosity
  Score for Autonomous Rover Target Prioritization","We present ARTPS (Autonomous Rover Target Prioritization System), a novel
hybrid AI system that combines depth estimation, anomaly detection, and
learnable curiosity scoring for autonomous exploration of planetary surfaces.
Our approach integrates monocular depth estimation using Vision Transformers
with multi-component anomaly detection and a weighted curiosity score that
balances known value, anomaly signals, depth variance, and surface roughness.
The system achieves state-of-the-art performance with AUROC of 0.94, AUPRC of
0.89, and F1-Score of 0.87 on Mars rover datasets. We demonstrate significant
improvements in target prioritization accuracy through ablation studies and
provide comprehensive analysis of component contributions. The hybrid fusion
approach reduces false positives by 23% while maintaining high detection
sensitivity across diverse terrain types.",Poyraz Baydemir,2025-08-23T17:37:05Z,2025-08-23T17:37:05Z,http://arxiv.org/abs/2509.00042v1,http://arxiv.org/pdf/2509.00042v1.pdf,all:terrain AND all:rover AND submittedDate:[202309062228 TO 202509052228],2025
Enhancing Martian Terrain Recognition with Deep Constrained Clustering,"Martian terrain recognition is pivotal for advancing our understanding of
topography, geomorphology, paleoclimate, and habitability. While deep
clustering methods have shown promise in learning semantically homogeneous
feature embeddings from Martian rover imagery, the natural variations in
intensity, scale, and rotation pose significant challenges for accurate terrain
classification. To address these limitations, we propose Deep Constrained
Clustering with Metric Learning (DCCML), a novel algorithm that leverages
multiple constraint types to guide the clustering process. DCCML incorporates
soft must-link constraints derived from spatial and depth similarities between
neighboring patches, alongside hard constraints from stereo camera pairs and
temporally adjacent images. Experimental evaluation on the Curiosity rover
dataset (with 150 clusters) demonstrates that DCCML increases homogeneous
clusters by 16.7 percent while reducing the Davies-Bouldin Index from 3.86 to
1.82 and boosting retrieval accuracy from 86.71 percent to 89.86 percent. This
improvement enables more precise classification of Martian geological features,
advancing our capacity to analyze and understand the planet's landscape.","Tejas Panambur, Mario Parente",2025-03-22T03:38:16Z,2025-03-22T03:38:16Z,http://arxiv.org/abs/2503.17633v1,http://arxiv.org/pdf/2503.17633v1.pdf,all:terrain AND all:rover AND submittedDate:[202309062228 TO 202509052228],2025
MarsLGPR: Mars Rover Localization with Ground Penetrating Radar,"In this work, we propose the use of Ground Penetrating Radar (GPR) for rover
localization on Mars. Precise pose estimation is an important task for mobile
robots exploring planetary surfaces, as they operate in GPS-denied
environments. Although visual odometry provides accurate localization, it is
computationally expensive and can fail in dim or high-contrast lighting. Wheel
encoders can also provide odometry estimation, but are prone to slipping on the
sandy terrain encountered on Mars. Although traditionally a scientific
surveying sensor, GPR has been used on Earth for terrain classification and
localization through subsurface feature matching. The Perseverance rover and
the upcoming ExoMars rover have GPR sensors already equipped to aid in the
search of water and mineral resources. We propose to leverage GPR to aid in
Mars rover localization. Specifically, we develop a novel GPR-based deep
learning model that predicts 1D relative pose translation. We fuse our GPR pose
prediction method with inertial and wheel encoder data in a filtering framework
to output rover localization. We perform experiments in a Mars analog
environment and demonstrate that our GPR-based displacement predictions both
outperform wheel encoders and improve multi-modal filtering estimates in
high-slip environments. Lastly, we present the first dataset aimed at GPR-based
localization in Mars analog environments, which will be made publicly available
upon publication.","Anja Sheppard, Katherine A. Skinner",2025-03-06T20:19:21Z,2025-03-06T20:19:21Z,http://arxiv.org/abs/2503.04944v1,http://arxiv.org/pdf/2503.04944v1.pdf,all:terrain AND all:rover AND submittedDate:[202309062228 TO 202509052228],2025
Using MRNet to Predict Lunar Rock Categories Detected by Chang'e 5 Probe,"China's Chang'e 5 mission has been a remarkable success, with the chang'e 5
lander traveling on the Oceanus Procellarum to collect images of the lunar
surface. Over the past half century, people have brought back some lunar rock
samples, but its quantity does not meet the need for research. Under current
circumstances, people still mainly rely on the analysis of rocks on the lunar
surface through the detection of lunar rover. The Oceanus Procellarum, chosen
by Chang'e 5 mission, contains various kind of rock species. Therefore, we
first applied to the National Astronomical Observatories of the China under the
Chinese Academy of Sciences for the Navigation and Terrain Camera (NaTeCam) of
the lunar surface image, and established a lunar surface rock image data set
CE5ROCK. The data set contains 100 images, which randomly divided into
training, validation and test set. Experimental results show that the
identification accuracy testing on convolutional neural network (CNN) models
like AlexNet or MobileNet is about to 40.0%. In order to make full use of the
global information in Moon images, this paper proposes the MRNet (MoonRockNet)
network architecture. The encoding structure of the network uses VGG16 for
feature extraction, and the decoding part adds dilated convolution and commonly
used U-Net structure on the original VGG16 decoding structure, which is more
conducive to identify more refined but more sparsely distributed types of lunar
rocks. We have conducted extensive experiments on the established CE5ROCK data
set, and the experimental results show that MRNet can achieve more accurate
rock type identification, and outperform other existing mainstream algorithms
in the identification performance.","Jin Cui, Yifei Zou, Siyuan Zhang",2025-02-14T07:12:19Z,2025-02-14T07:12:19Z,http://arxiv.org/abs/2502.09952v1,http://arxiv.org/pdf/2502.09952v1.pdf,all:terrain AND all:rover AND submittedDate:[202309062228 TO 202509052228],2025
Resultant force on grains of a real sand dune: How to measure it?,"Dunes are bedforms found on sandy terrains shaped by fluid flow on Earth,
Mars, and other celestial bodies. Despite their prevalence, understanding dune
dynamics at the grain scale is challenging due to the vast number of grains
involved. In this study, we demonstrate a novel approach to estimate the forces
acting on individual dune grains using images. By combining subaqueous
experiments, high-speed camera recordings, discrete numerical simulations, and
a specially trained convolutional neural network, we can quantify these forces
with high accuracy. This method represents a breakthrough in studying granular
dynamics, offering a new way to measure forces not only on dune grains but also
on smaller objects, such as rocks, boulders, rovers, and man-made structures,
observed in satellite images of both Earth and Mars. This technique expands our
ability to analyze and understand fluid-grain interactions in diverse
environments.","Renato F. Miotto, Carlos A. Alvarez, Danilo S. Borges, William R. Wolf, Erick M. Franklin",2025-01-10T11:10:26Z,2025-01-10T11:10:26Z,http://arxiv.org/abs/2501.05869v1,http://arxiv.org/pdf/2501.05869v1.pdf,all:terrain AND all:rover AND submittedDate:[202309062228 TO 202509052228],2025
"A physics-based sensor simulation environment for lunar ground
  operations","This contribution reports on a software framework that uses physically-based
rendering to simulate camera operation in lunar conditions. The focus is on
generating synthetic images qualitatively similar to those produced by an
actual camera operating on a vehicle traversing and/or actively interacting
with lunar terrain, e.g., for construction operations. The highlights of this
simulator are its ability to capture (i) light transport in lunar conditions
and (ii) artifacts related to the vehicle-terrain interaction, which might
include dust formation and transport. The simulation infrastructure is built
within an in-house developed physics engine called Chrono, which simulates the
dynamics of the deformable terrain-vehicle interaction, as well as fallout of
this interaction. The Chrono::Sensor camera model draws on ray tracing and
Hapke Photometric Functions. We analyze the performance of the simulator using
two virtual experiments featuring digital twins of NASA's VIPER rover
navigating a lunar environment, and of the NASA's RASSOR excavator engaged into
a digging operation. The sensor simulation solution presented can be used for
the design and testing of perception algorithms, or as a component of in-silico
experiments that pertain to large lunar operations, e.g., traversability,
construction tasks.","Nevindu M. Batagoda, Bo-Hsun Chen, Harry Zhang, Radu Serban, Dan Negrut",2024-10-06T06:18:02Z,2024-10-06T06:18:02Z,http://arxiv.org/abs/2410.04371v1,http://arxiv.org/pdf/2410.04371v1.pdf,all:terrain AND all:rover AND submittedDate:[202309062228 TO 202509052228],2024
"SlipNet: Enhancing Slip Cost Mapping for Autonomous Navigation on
  Heterogeneous and Deformable Terrains","Autonomous space rovers face significant challenges when navigating
deformable and heterogeneous terrains due to variability in soil properties,
which can lead to severe wheel slip, compromising navigation efficiency and
increasing the risk of entrapment. To address this problem, we introduce
SlipNet, a novel approach for predicting wheel slip in segmented regions of
diverse terrain surfaces without relying on prior terrain classification.
SlipNet employs dynamic terrain segmentation and slip assignment techniques on
previously unseen data, enhancing rover navigation capabilities in uncertain
environments. We developed a synthetic data generation framework using the
high-fidelity Vortex Studio simulator to create realistic datasets that
replicate a wide range of deformable terrain conditions for training and
evaluation. Extensive simulation results demonstrate that our model, combining
DeepLab v3+ with SlipNet, significantly outperforms the state-of-the-art
TerrainNet method, achieving lower mean absolute error (MAE) across five
distinct terrain samples. These findings highlight the effectiveness of SlipNet
in improving rover navigation in challenging terrains.","Mubarak Yakubu, Yahya Zweiri, Ahmad Abubakar, Rana Azzam, Ruqayya Alhammadi, Lakmal Seneviratne",2024-09-03T20:09:07Z,2024-09-22T18:03:33Z,http://arxiv.org/abs/2409.02273v2,http://arxiv.org/pdf/2409.02273v2.pdf,all:terrain AND all:rover AND submittedDate:[202309062228 TO 202509052228],2024
"Deep Probabilistic Traversability with Test-time Adaptation for
  Uncertainty-aware Planetary Rover Navigation","Traversability assessment of deformable terrain is vital for safe rover
navigation on planetary surfaces. Machine learning (ML) is a powerful tool for
traversability prediction but faces predictive uncertainty. This uncertainty
leads to prediction errors, increasing the risk of wheel slips and
immobilization for planetary rovers. To address this issue, we integrate
principal approaches to uncertainty handling -- quantification, exploitation,
and adaptation -- into a single learning and planning framework for rover
navigation. The key concept is \emph{deep probabilistic traversability},
forming the basis of an end-to-end probabilistic ML model that predicts slip
distributions directly from rover traverse observations. This probabilistic
model quantifies uncertainties in slip prediction and exploits them as
traversability costs in path planning. Its end-to-end nature also allows
adaptation of pre-trained models with in-situ traverse experience to reduce
uncertainties. We perform extensive simulations in synthetic environments that
pose representative uncertainties in planetary analog terrains. Experimental
results show that our method achieves more robust path planning under novel
environmental conditions than existing approaches.","Masafumi Endo, Tatsunori Taniai, Genya Ishigami",2024-09-01T07:21:12Z,2024-09-01T07:21:12Z,http://arxiv.org/abs/2409.00641v1,http://arxiv.org/pdf/2409.00641v1.pdf,all:terrain AND all:rover AND submittedDate:[202309062228 TO 202509052228],2024
"Synthetic Lunar Terrain: A Multimodal Open Dataset for Training and
  Evaluating Neuromorphic Vision Algorithms","Synthetic Lunar Terrain (SLT) is an open dataset collected from an analogue
test site for lunar missions, featuring synthetic craters in a high-contrast
lighting setup. It includes several side-by-side captures from event-based and
conventional RGB cameras, supplemented with a high-resolution 3D laser scan for
depth estimation. The event-stream recorded from the neuromorphic vision sensor
of the event-based camera is of particular interest as this emerging technology
provides several unique advantages, such as high data rates, low energy
consumption and resilience towards scenes of high dynamic range. SLT provides a
solid foundation to analyse the limits of RGB-cameras and potential advantages
or synergies in utilizing neuromorphic visions with the goal of enabling and
improving lunar specific applications like rover navigation, landing in
cratered environments or similar.","Marcus Märtens, Kevin Farries, John Culton, Tat-Jun Chin",2024-08-30T02:14:33Z,2024-08-30T02:14:33Z,http://arxiv.org/abs/2408.16971v1,http://arxiv.org/pdf/2408.16971v1.pdf,all:terrain AND all:rover AND submittedDate:[202309062228 TO 202509052228],2024
"Safety Enhancement in Planetary Rovers: Early Detection of Tip-over
  Risks Using Autoencoders","Autonomous robots consistently encounter unforeseen dangerous situations
during exploration missions. The characteristic rimless wheels in the AsguardIV
rover allow it to overcome challenging terrains. However, steep slopes or
difficult maneuvers can cause the rover to tip over and threaten the completion
of a mission. This work focuses on identifying early signs or initial stages
for potential tip-over events to predict and detect these critical moments
before they fully occur, possibly preventing accidents and enhancing the safety
and stability of the rover during its exploration mission. Inertial Measurement
Units (IMU) readings are used to develop compact, robust, and efficient
Autoencoders that combine the power of sequence processing of Long Short-Term
Memory Networks (LSTM). By leveraging LSTM-based Autoencoders, this work
contributes predictive capabilities for detecting tip-over risks and developing
safety measures for more reliable exploration missions.",Mariela De Lucas Alvarez,2024-08-10T17:34:48Z,2024-08-10T17:34:48Z,http://arxiv.org/abs/2408.05602v1,http://arxiv.org/pdf/2408.05602v1.pdf,all:terrain AND all:rover AND submittedDate:[202309062228 TO 202509052228],2024
Opening the Black Box of 3D Reconstruction Error Analysis with VECTOR,"Reconstruction of 3D scenes from 2D images is a technical challenge that
impacts domains from Earth and planetary sciences and space exploration to
augmented and virtual reality. Typically, reconstruction algorithms first
identify common features across images and then minimize reconstruction errors
after estimating the shape of the terrain. This bundle adjustment (BA) step
optimizes around a single, simplifying scalar value that obfuscates many
possible causes of reconstruction errors (e.g., initial estimate of the
position and orientation of the camera, lighting conditions, ease of feature
detection in the terrain). Reconstruction errors can lead to inaccurate
scientific inferences or endanger a spacecraft exploring a remote environment.
To address this challenge, we present VECTOR, a visual analysis tool that
improves error inspection for stereo reconstruction BA. VECTOR provides
analysts with previously unavailable visibility into feature locations, camera
pose, and computed 3D points. VECTOR was developed in partnership with the
Perseverance Mars Rover and Ingenuity Mars Helicopter terrain reconstruction
team at the NASA Jet Propulsion Laboratory. We report on how this tool was used
to debug and improve terrain reconstruction for the Mars 2020 mission.","Racquel Fygenson, Kazi Jawad, Isabel Li, Francois Ayoub, Robert G. Deen, Scott Davidoff, Dominik Moritz, Mauricio Hess-Flores",2024-08-07T02:03:32Z,2024-08-07T02:03:32Z,http://arxiv.org/abs/2408.03503v1,http://arxiv.org/pdf/2408.03503v1.pdf,all:terrain AND all:rover AND submittedDate:[202309062228 TO 202509052228],2024
"Using high-fidelity discrete element simulation to calibrate an
  expeditious terramechanics model in a multibody dynamics framework","The wheel-soil interaction has great impact on the dynamics of off-road
vehicles in terramechanics applications. The Soil Contact Model (SCM), which
anchors an empirical method to characterize the frictional contact between a
wheel and soil, has been widely used in off-road vehicle dynamics simulations
because it quickly produces adequate results for many terramechanics
applications. The SCM approach calls for a set of model parameters that are
obtained via a bevameter test. This test is expensive and time consuming to
carry out, and in some cases difficult to set up, e.g., in extraterrestrial
applications. We propose an approach to address these concerns by conducting
the bevameter test in simulation, using a model that captures the physics of
the actual experiment with high fidelity. To that end, we model the bevameter
test rig as a multibody system, while the dynamics of the soil is captured
using a discrete element model (DEM). The multibody dynamics--soil dynamics
co-simulation is used to replicate the bevameter test, producing high-fidelity
ground truth test data that is subsequently used to calibrate the SCM
parameters within a Bayesian inference framework. To test the accuracy of the
resulting SCM terramechanics, we run single wheel and full rover simulations
using both DEM and SCM terrains. The SCM results match well with those produced
by the DEM solution, and the simulation time for SCM is two to three orders of
magnitude lower than that of DEM. All simulations in this work are performed
using Chrono, an open-source, publicly available simulator. The scripts and
models used are available in a public repository for reproducibility studies
and further research.","Yuemin Zhang, Junpeng Dai, Wei Hu, Dan Negrut",2024-07-26T17:56:13Z,2024-07-26T17:56:13Z,http://arxiv.org/abs/2407.18903v1,http://arxiv.org/pdf/2407.18903v1.pdf,all:terrain AND all:rover AND submittedDate:[202309062228 TO 202509052228],2024
"PWTO: A Heuristic Approach for Trajectory Optimization in Complex
  Terrains","This paper considers a trajectory planning problem for a robot navigating
complex terrains, which arises in applications ranging from autonomous mining
vehicles to planetary rovers. The problem seeks to find a low-cost dynamically
feasible trajectory for the robot. The problem is challenging as it requires
solving a non-linear optimization problem that often has many local minima due
to the complex terrain. To address the challenge, we propose a method called
Pareto-optimal Warm-started Trajectory Optimization (PWTO) that attempts to
combine the benefits of graph search and trajectory optimization, two very
different approaches to planning. PWTO first creates a state lattice using
simplified dynamics of the robot and leverages a multi-objective graph search
method to obtain a set of paths. Each of the paths is then used to warm-start a
local trajectory optimization process, so that different local minima are
explored to find a globally low-cost solution. In our tests, the solution cost
computed by PWTO is often less than half of the costs computed by the
baselines. In addition, we verify the trajectories generated by PWTO in Gazebo
simulation in complex terrains with both wheeled and quadruped robots. The code
of this paper is open sourced and can be found at
https://github.com/rap-lab-org/public_pwto.","Yilin Cai, Zhongqiang Ren",2024-07-03T01:43:16Z,2024-07-03T01:43:16Z,http://arxiv.org/abs/2407.02745v1,http://arxiv.org/pdf/2407.02745v1.pdf,all:terrain AND all:rover AND submittedDate:[202309062228 TO 202509052228],2024
"Autonomous Control of a Novel Closed Chain Five Bar Active Suspension
  via Deep Reinforcement Learning","Planetary exploration requires traversal in environments with rugged
terrains. In addition, Mars rovers and other planetary exploration robots often
carry sensitive scientific experiments and components onboard, which must be
protected from mechanical harm. This paper deals with an active suspension
system focused on chassis stabilisation and an efficient traversal method while
encountering unavoidable obstacles. Soft Actor-Critic (SAC) was applied along
with Proportional Integral Derivative (PID) control to stabilise the chassis
and traverse large obstacles at low speeds. The model uses the rover's distance
from surrounding obstacles, the height of the obstacle, and the chassis'
orientation to actuate the control links of the suspension accurately.
Simulations carried out in the Gazebo environment are used to validate the
proposed active system.","Nishesh Singh, Sidharth Ramesh, Abhishek Shankar, Jyotishka Duttagupta, Leander Stephen D'Souza, Sanjay Singh",2024-06-27T05:27:39Z,2024-07-04T04:12:25Z,http://arxiv.org/abs/2406.18899v3,http://arxiv.org/pdf/2406.18899v3.pdf,all:terrain AND all:rover AND submittedDate:[202309062228 TO 202509052228],2024
"Traversing Mars: Cooperative Informative Path Planning to Efficiently
  Navigate Unknown Scenes","The ability to traverse an unknown environment is crucial for autonomous
robot operations. However, due to the limited sensing capabilities and system
constraints, approaching this problem with a single robot agent can be slow,
costly, and unsafe. For example, in planetary exploration missions, the wear on
the wheels of a rover from abrasive terrain should be minimized at all costs as
reparations are infeasible. On the other hand, utilizing a scouting robot such
as a micro aerial vehicle (MAV) has the potential to reduce wear and time costs
and increasing safety of a follower robot. This work proposes a novel
cooperative IPP framework that allows a scout (e.g., an MAV) to efficiently
explore the minimum-cost-path for a follower (e.g., a rover) to reach the goal.
We derive theoretic guarantees for our algorithm, and prove that the algorithm
always terminates, always finds the optimal path if it exists, and terminates
early when the found path is shown to be optimal or infeasible. We show in
thorough experimental evaluation that the guarantees hold in practice, and that
our algorithm is 22.5% quicker to find the optimal path and 15% quicker to
terminate compared to existing methods.","Friedrich M. Rockenbauer, Jaeyoung Lim, Marcus G. Müller, Roland Siegwart, Lukas Schmid",2024-06-08T01:10:22Z,2024-06-12T11:41:17Z,http://arxiv.org/abs/2406.05313v2,http://arxiv.org/pdf/2406.05313v2.pdf,all:terrain AND all:rover AND submittedDate:[202309062228 TO 202509052228],2024
"A Novel Methodology for Autonomous Planetary Exploration Using
  Multi-Robot Teams","One of the fundamental limiting factors in planetary exploration is the
autonomous capabilities of planetary exploration rovers. This study proposes a
novel methodology for trustworthy autonomous multi-robot teams which
incorporates data from multiple sources (HiRISE orbiter imaging, probability
distribution maps, and on-board rover sensors) to find efficient exploration
routes in Jezero crater. A map is generated, consisting of a 3D terrain model,
traversability analysis, and probability distribution map of points of
scientific interest. A three-stage mission planner generates an efficient
route, which maximises the accumulated probability of identifying points of
interest. A 4D RRT* algorithm is used to determine smooth, flat paths, and
prioritised planning is used to coordinate a safe set of paths. The above
methodology is shown to coordinate safe and efficient rover paths, which ensure
the rovers remain within their nominal pitch and roll limits throughout
operation.","Sarah Swinton, Jan-Hendrik Ewers, Euan McGookin, David Anderson, Douglas Thomson",2024-05-21T13:40:54Z,2024-05-21T13:40:54Z,http://arxiv.org/abs/2405.12790v1,http://arxiv.org/pdf/2405.12790v1.pdf,all:terrain AND all:rover AND submittedDate:[202309062228 TO 202509052228],2024
"Immersive Rover Control and Obstacle Detection based on Extended Reality
  and Artificial Intelligence","Lunar exploration has become a key focus, driving scientific and
technological advances. Ongoing missions are deploying rovers to the surface of
the Moon, targeting the far side and south pole. However, these terrains pose
challenges, emphasizing the need for precise obstacles and resource detection
to avoid mission risks. This work proposes a novel system that integrates
eXtended Reality (XR) and Artificial Intelligence (AI) to teleoperate lunar
rovers. It is capable of autonomously detecting rocks and recreating an
immersive 3D virtual environment of the location of the robot. This system has
been validated in a lunar laboratory to observe its advantages over traditional
2D-based teleoperation approaches","Sofía Coloma, Alexandre Frantz, Dave van der Meer, Ernest Skrzypczyk, Andrej Orsula, Miguel Olivares-Mendez",2024-04-22T11:28:34Z,2024-04-22T11:28:34Z,http://arxiv.org/abs/2404.14095v1,http://arxiv.org/pdf/2404.14095v1.pdf,all:terrain AND all:rover AND submittedDate:[202309062228 TO 202509052228],2024
Federated Multi-Agent Mapping for Planetary Exploration,"Multi-agent robotic exploration stands to play an important role in space
exploration as the next generation of robotic systems ventures to far-flung
environments. A key challenge in this new paradigm will be to effectively share
and utilize the vast amount of data generated onboard while operating in
bandwidth-constrained regimes typical of space missions. Federated learning
(FL) is a promising tool for bridging this gap. Drawing inspiration from the
upcoming CADRE Lunar rover mission, we propose a federated multi-agent mapping
approach that jointly trains a global map model across agents without
transmitting raw data. Our method leverages implicit neural mapping to generate
parsimonious, adaptable representations, reducing data transmission by up to
93.8% compared to raw maps. Furthermore, we enhance this approach with
meta-initialization on Earth-based traversability datasets to significantly
accelerate map convergence; reducing iterations required to reach target
performance by 80% compared to random initialization. We demonstrate the
efficacy of our approach on Martian terrains and glacier datasets, achieving
downstream path planning F1 scores as high as 0.95 while outperforming on map
reconstruction losses.","Tiberiu-Ioan Szatmari, Abhishek Cauligi",2024-04-02T20:32:32Z,2025-03-06T22:11:55Z,http://arxiv.org/abs/2404.02289v3,http://arxiv.org/pdf/2404.02289v3.pdf,all:terrain AND all:rover AND submittedDate:[202309062228 TO 202509052228],2024
"Safe Mission-Level Path Planning for Exploration of Lunar Shadowed
  Regions by a Solar-Powered Rover","Exploration of the lunar south pole with a solar-powered rover is challenging
due to the highly dynamic solar illumination conditions and the presence of
permanently shadowed regions (PSRs). In turn, careful planning in space and
time is essential. Mission-level path planning is a global, spatiotemporal
paradigm that addresses this challenge, taking into account rover resources and
mission requirements. However, existing approaches do not proactively account
for random disturbances, such as recurring faults, that may temporarily delay
rover traverse progress. In this paper, we formulate a chance-constrained
mission-level planning problem for the exploration of PSRs by a solar-powered
rover affected by random faults. The objective is to find a policy that visits
as many waypoints of scientific interest as possible while respecting an upper
bound on the probability of mission failure.
  Our approach assumes that faults occur randomly, but at a known, constant
average rate. Each fault is resolved within a fixed time, simulating the
recovery period of an autonomous system or the time required for a team of
human operators to intervene. Unlike solutions based upon dynamic programming
alone, our method breaks the chance-constrained optimization problem into
smaller offline and online subtasks to make the problem computationally
tractable. Specifically, our solution combines existing mission-level path
planning techniques with a stochastic reachability analysis component. We find
mission plans that remain within reach of safety throughout large state spaces.
To empirically validate our algorithm, we simulate mission scenarios using
orbital terrain and illumination maps of Cabeus Crater. Results from
simulations of multi-day, long-range drives in the LCROSS impact region are
also presented.","Olivier Lamarre, Shantanu Malhotra, Jonathan Kelly",2024-01-16T18:38:04Z,2025-01-03T06:04:21Z,http://arxiv.org/abs/2401.08558v2,http://arxiv.org/pdf/2401.08558v2.pdf,all:terrain AND all:rover AND submittedDate:[202309062228 TO 202509052228],2024
Hyperuniformity on Mars: Pebbles scattered on sand,"In Gale Crater near Mars' equator, dunes and ripples of sand stand out from
the general orderless, rocky terrain. In addition, images from Curiosity, the
Mars Science Laboratory rover, reveal more subtle orderly forms: widespread,
meter-scale domains of evenly spaced, pebble-size rocks (termed clasts) on
wind-blown sand in scattered locations. Here, we examine quantitatively several
clast domains on both Mars and Earth, and compare their geometry with that of
random points. The clast distributions are more orderly than expected by
chance; they differ significantlty from those associated with uniform (Poisson)
random processes. Moreover, they are hyperuniform, a self-organized state
recently recognized in diverse active materials and biological systems but that
appears novel for planetary surfaces. These patches are often surrounded by
recent wind-borne ripples, suggesting an interplay between sand transport,
ripple activity and clasts. Using numerical simulations, we show that clast
displacements induced by gravity, combined with the evolution of the sand
surface caused by aeolian sand transport and ripple migration, can produce
realistic hyperuniform and random clast distributions, as well as distinct
clast alignements. Our findings highlight the existence of easily overlooked
disordered hyperuniform states on ground surfaces, suggesting novel
self-organized states beyond distinct geometric patterns.","Zheng Zhu, Bernard Hallet, András A. Sipos, Gábor Domokos, Quan-Xing Liu",2023-12-21T13:09:30Z,2024-05-02T14:17:10Z,http://arxiv.org/abs/2312.13818v2,http://arxiv.org/pdf/2312.13818v2.pdf,all:terrain AND all:rover AND submittedDate:[202309062228 TO 202509052228],2023
"Dynamic modeling of wing-assisted inclined running with a morphing
  multi-modal robot","Robot designs can take many inspirations from nature, where there are many
examples of highly resilient and fault-tolerant locomotion strategies to
navigate complex terrains by using multi-functional appendages. For example,
Chukar and Hoatzin birds can repurpose their wings for quadrupedal walking and
wing-assisted incline running (WAIR) to climb steep surfaces. We took
inspiration from nature and designed a morphing robot with multi-functional
thruster-wheel appendages that allows the robot to change its mode of
locomotion by transforming into a rover, quad-rotor, mobile inverted pendulum
(MIP), and other modes. In this work, we derive a dynamic model and formulate a
nonlinear model predictive controller to perform WAIR to showcase the unique
capabilities of our robot. We implemented the model and controller in a
numerical simulation and experiments to show their feasibility and the
capabilities of our transforming multi-modal robot.","Eric Sihite, Alireza Ramezani, Morteza Gharib",2023-11-16T15:35:47Z,2023-11-16T15:35:47Z,http://arxiv.org/abs/2311.09963v1,http://arxiv.org/pdf/2311.09963v1.pdf,all:terrain AND all:rover AND submittedDate:[202309062228 TO 202509052228],2023
"Chrono DEM-Engine: A Discrete Element Method dual-GPU simulator with
  customizable contact forces and element shape","This paper introduces DEM-Engine, a new submodule of Project Chrono, that is
designed to carry out Discrete Element Method (DEM) simulations. Based on
spherical primitive shapes, DEM-Engine can simulate polydisperse granular
materials and handle complex shapes generated as assemblies of primitives,
referred to as clumps. DEM-Engine has a multi-tier parallelized structure that
is optimized to operate simultaneously on two GPUs. The code uses
custom-defined data types to reduce memory footprint and increase bandwidth. A
novel ""delayed contact detection"" algorithm allows the decoupling of the
contact detection and force computation, thus splitting the workload into two
asynchronous GPU streams. DEM-Engine uses just-in-time compilation to support
user-defined contact force models. This paper discusses its C++ and Python
interfaces and presents a variety of numerical tests, in which impact forces,
complex-shaped particle flows, and a custom force model are validated
considering well-known benchmark cases. Additionally, the full potential of the
simulator is demonstrated for the investigation of extraterrestrial rover
mobility on granular terrain. The chosen case study demonstrates that
large-scale co-simulations (comprising 11 million elements) spanning 15
seconds, in conjunction with an external multi-body dynamics system, can be
efficiently executed within a day. Lastly, a performance test suggests that
DEM-Engine displays linear scaling up to 150 million elements on two NVIDIA
A100 GPUs.","Ruochun Zhang, Bonaventura Tagliafierro, Colin Vanden Heuvel, Shlok Sabarwal, Luning Bakke, Yulong Yue, Xin Wei, Radu Serban, Dan Negrut",2023-11-08T12:48:35Z,2023-11-09T13:57:50Z,http://arxiv.org/abs/2311.04648v2,http://arxiv.org/pdf/2311.04648v2.pdf,all:terrain AND all:rover AND submittedDate:[202309062228 TO 202509052228],2023
"Martian Lava Tube Exploration Using Jumping Legged Robots: A Concept
  Study","In recent years, robotic exploration has become increasingly important in
planetary exploration. One area of particular interest for exploration is
Martian lava tubes, which have several distinct features of interest. First, it
is theorized that they contain more easily accessible resources such as water
ice, needed for in-situ utilization on Mars. Second, lava tubes of significant
size can provide radiation and impact shelter for possible future human
missions to Mars. Third, lava tubes may offer a protected and preserved view
into Mars' geological and possible biological past. However, exploration of
these lava tubes poses significant challenges due to their sheer size,
geometric complexity, uneven terrain, steep slopes, collapsed sections,
significant obstacles, and unstable surfaces. Such challenges may hinder
traditional wheeled rover exploration. To overcome these challenges, legged
robots and particularly jumping systems have been proposed as potential
solutions. Jumping legged robots utilize legs to both walk and jump. This
allows them to traverse uneven terrain and steep slopes more easily compared to
wheeled or tracked systems. In the context of Martian lava tube exploration,
jumping legged robots would be particularly useful due to their ability to jump
over big boulders, gaps, and obstacles, as well as to descend and climb steep
slopes. This would allow them to explore and map such caves, and possibly
collect samples from areas that may otherwise be inaccessible. This paper
presents the specifications, design, capabilities, and possible mission
profiles for state-of-the-art legged robots tailored to space exploration.
Additionally, it presents the design, capabilities, and possible mission
profiles of a new jumping legged robot for Martian lava tube exploration that
is being developed at the Norwegian University of Science and Technology.","Jørgen Anker Olsen, Kostas Alexis",2023-10-23T12:47:00Z,2023-10-23T12:47:00Z,http://arxiv.org/abs/2310.14876v1,http://arxiv.org/pdf/2310.14876v1.pdf,all:terrain AND all:rover AND submittedDate:[202309062228 TO 202509052228],2023
Automatic Data Processing for Space Robotics Machine Learning,"Autonomous terrain classification is an important problem in planetary
navigation, whether the goal is to identify scientific sites of interest or to
traverse treacherous areas safely. Past Martian rovers have relied on human
operators to manually identify a navigable path from transmitted imagery. Our
goals on Mars in the next few decades will eventually require rovers that can
autonomously move farther, faster, and through more dangerous
landscapes--demonstrating a need for improved terrain classification for
traversability. Autonomous navigation through extreme environments will enable
the search for water on the Moon and Mars as well as preparations for human
habitats. Advancements in machine learning techniques have demonstrated
potential to improve terrain classification capabilities for ground vehicles on
Earth. However, classification results for space applications are limited by
the availability of training data suitable for supervised learning methods.
This paper contributes an open source automatic data processing pipeline that
uses camera geometry to co-locate Curiosity and Perseverance Mastcam image
products with Mars overhead maps via ray projection over a terrain model. In
future work, this automated data processing pipeline will be leveraged for
development of machine learning methods for terrain classification.","Anja Sheppard, Katherine A. Skinner",2023-10-03T10:15:00Z,2023-10-03T10:15:00Z,http://arxiv.org/abs/2310.01932v1,http://arxiv.org/pdf/2310.01932v1.pdf,all:terrain AND all:rover AND submittedDate:[202309062228 TO 202509052228],2023
"Learning manipulation of steep granular slopes for fast Mini Rover
  turning","Future planetary exploration missions will require reaching challenging
regions such as craters and steep slopes. Such regions are ubiquitous and
present science-rich targets potentially containing information regarding the
planet's internal structure. Steep slopes consisting of low-cohesion regolith
are prone to flow downward under small disturbances, making it very challenging
for autonomous rovers to traverse. Moreover, the navigation trajectories of
rovers are heavily limited by the terrain topology and future systems will need
to maneuver on flowable surfaces without getting trapped, allowing them to
further expand their reach and increase mission efficiency.
  In this work, we used a laboratory-scale rover robot and performed
maneuvering experiments on a steep granular slope of poppy seeds to explore the
rover's turning capabilities. The rover is capable of lifting, sweeping, and
spinning its wheels, allowing it to execute leg-like gait patterns. The
high-dimensional actuation capabilities of the rover facilitate effective
manipulation of the underlying granular surface. We used Bayesian Optimization
(BO) to gain insight into successful turning gaits in high dimensional search
space and found strategies such as differential wheel spinning and pivoting
around a single sweeping wheel. We then used these insights to further
fine-tune the turning gait, enabling the rover to turn 90 degrees at just above
4 seconds with minimal slip. Combining gait optimization and human-tuning
approaches, we found that fast turning is empowered by creating anisotropic
torques with the sweeping wheel.","Deniz Kerimoglu, Daniel Soto, Malone Lincoln Hemsley, Joseph Brunner, Sehoon Ha, Tingnan Zhang, Daniel I. Goldman",2023-10-02T15:18:39Z,2023-10-02T15:18:39Z,http://arxiv.org/abs/2310.01273v1,http://arxiv.org/pdf/2310.01273v1.pdf,all:terrain AND all:rover AND submittedDate:[202309062228 TO 202509052228],2023
"Teacher-Student Reinforcement Learning for Mapless Navigation using a
  Planetary Space Rover","We address the challenge of enhancing navigation autonomy for planetary space
rovers using reinforcement learning (RL). The ambition of future space missions
necessitates advanced autonomous navigation capabilities for rovers to meet
mission objectives. RL's potential in robotic autonomy is evident, but its
reliance on simulations poses a challenge. Transferring policies to real-world
scenarios often encounters the ""reality gap"", disrupting the transition from
virtual to physical environments. The reality gap is exacerbated in the context
of mapless navigation on Mars and Moon-like terrains, where unpredictable
terrains and environmental factors play a significant role. Effective
navigation requires a method attuned to these complexities and real-world data
noise. We introduce a novel two-stage RL approach using offline noisy data. Our
approach employs a teacher-student policy learning paradigm, inspired by the
""learning by cheating"" method. The teacher policy is trained in simulation.
Subsequently, the student policy is trained on noisy data, aiming to mimic the
teacher's behaviors while being more robust to real-world uncertainties. Our
policies are transferred to a custom-designed rover for real-world testing.
Comparative analyses between the teacher and student policies reveal that our
approach offers improved behavioral performance, heightened noise resilience,
and more effective sim-to-real transfer.","Anton Bjørndahl Mortensen, Emil Tribler Pedersen, Laia Vives Benedicto, Lionel Burg, Mads Rossen Madsen, Simon Bøgh",2023-09-22T11:39:50Z,2023-09-22T11:39:50Z,http://arxiv.org/abs/2309.12807v1,http://arxiv.org/pdf/2309.12807v1.pdf,all:terrain AND all:rover AND submittedDate:[202309062228 TO 202509052228],2023
"POLAR-Sim: Augmenting NASA's POLAR Dataset for Data-Driven Lunar
  Perception and Rover Simulation","NASA's POLAR dataset contains approximately 2,600 pairs of high dynamic range
stereo photos captured across 13 varied terrain scenarios, including areas with
sparse or dense rock distributions, craters, and rocks of different sizes. The
purpose of these photos is to spur development in robotics, AI-based
perception, and autonomous navigation. Acknowledging a scarcity of lunar images
from around the lunar poles, NASA Ames produced on Earth but in controlled
conditions images that resemble rover operating conditions from these regions
of the Moon. We report on the outcomes of an effort aimed at accomplishing two
tasks. In Task 1, we provided bounding boxes and semantic segmentation
information for all the images in NASA's POLAR dataset. This effort resulted in
23,000 labels and semantic segmentation annotations pertaining to rocks,
shadows, and craters. In Task 2, we generated the digital twins of the 13
scenarios that have been used to produce all the photos in the POLAR dataset.
Specifically, for each of these scenarios, we produced individual meshes,
texture information, and material properties associated with the ground and the
rocks in each scenario. This allows anyone with a camera model to synthesize
images associated with any of the 13 scenarios of the POLAR dataset.
Effectively, one can generate as many semantically labeled synthetic images as
desired -- with different locations and exposure values in the scene, for
different positions of the sun, with or without the presence of active
illumination, etc. The benefit of this work is twofold. Using outcomes of Task
1, one can train and/or test perception algorithms that deal with Moon images.
For Task 2, one can produce as much data as desired to train and test AI
algorithms that are anticipated to work in lunar conditions. All the outcomes
of this work are available in a public repository for unfettered use and
distribution.","Bo-Hsun Chen, Peter Negrut, Thomas Liang, Nevindu Batagoda, Harry Zhang, Dan Negrut",2023-09-21T18:00:34Z,2025-01-23T22:17:17Z,http://arxiv.org/abs/2309.12397v2,http://arxiv.org/pdf/2309.12397v2.pdf,all:terrain AND all:rover AND submittedDate:[202309062228 TO 202509052228],2023
