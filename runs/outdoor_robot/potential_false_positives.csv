title,summary,authors,published,updated,url,pdf_url,matched_query,year
"Lang2Lift: A Framework for Language-Guided Pallet Detection and Pose
  Estimation Integrated in Autonomous Outdoor Forklift Operation","The logistics and construction industries face persistent challenges in
automating pallet handling, especially in outdoor environments with variable
payloads, inconsistencies in pallet quality and dimensions, and unstructured
surroundings. In this paper, we tackle automation of a critical step in pallet
transport: the pallet pick-up operation. Our work is motivated by labor
shortages, safety concerns, and inefficiencies in manually locating and
retrieving pallets under such conditions. We present Lang2Lift, a framework
that leverages foundation models for natural language-guided pallet detection
and 6D pose estimation, enabling operators to specify targets through intuitive
commands such as ""pick up the steel beam pallet near the crane."" The perception
pipeline integrates Florence-2 and SAM-2 for language-grounded segmentation
with FoundationPose for robust pose estimation in cluttered, multi-pallet
outdoor scenes under variable lighting. The resulting poses feed into a motion
planning module for fully autonomous forklift operation. We validate Lang2Lift
on the ADAPT autonomous forklift platform, achieving 0.76 mIoU pallet
segmentation accuracy on a real-world test dataset. Timing and error analysis
demonstrate the system's robustness and confirm its feasibility for deployment
in operational logistics and construction environments. Video demonstrations
are available at https://eric-nguyen1402.github.io/lang2lift.github.io/","Huy Hoang Nguyen, Johannes Huemer, Markus Murschitz, Tobias Glueck, Minh Nhat Vu, Andreas Kugi",2025-08-21T10:28:39Z,2025-08-21T10:28:39Z,http://arxiv.org/abs/2508.15427v1,http://arxiv.org/pdf/2508.15427v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"EAROL: Environmental Augmented Perception-Aware Planning and Robust
  Odometry via Downward-Mounted Tilted LiDAR","To address the challenges of localization drift and perception-planning
coupling in unmanned aerial vehicles (UAVs) operating in open-top scenarios
(e.g., collapsed buildings, roofless mazes), this paper proposes EAROL, a novel
framework with a downward-mounted tilted LiDAR configuration (20{\deg}
inclination), integrating a LiDAR-Inertial Odometry (LIO) system and a
hierarchical trajectory-yaw optimization algorithm. The hardware innovation
enables constraint enhancement via dense ground point cloud acquisition and
forward environmental awareness for dynamic obstacle detection. A
tightly-coupled LIO system, empowered by an Iterative Error-State Kalman Filter
(IESKF) with dynamic motion compensation, achieves high level 6-DoF
localization accuracy in feature-sparse environments. The planner, augmented by
environment, balancing environmental exploration, target tracking precision,
and energy efficiency. Physical experiments demonstrate 81% tracking error
reduction, 22% improvement in perceptual coverage, and near-zero vertical drift
across indoor maze and 60-meter-scale outdoor scenarios. This work proposes a
hardware-algorithm co-design paradigm, offering a robust solution for UAV
autonomy in post-disaster search and rescue missions. We will release our
software and hardware as an open-source package for the community. Video:
https://youtu.be/7av2ueLSiYw.","Xinkai Liang, Yigu Ge, Yangxi Shi, Haoyu Yang, Xu Cao, Hao Fang",2025-08-20T09:16:29Z,2025-08-20T09:16:29Z,http://arxiv.org/abs/2508.14554v1,http://arxiv.org/pdf/2508.14554v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"D$^2$-LIO: Enhanced Optimization for LiDAR-IMU Odometry Considering
  Directional Degeneracy","LiDAR-inertial odometry (LIO) plays a vital role in achieving accurate
localization and mapping, especially in complex environments. However, the
presence of LiDAR feature degeneracy poses a major challenge to reliable state
estimation. To overcome this issue, we propose an enhanced LIO framework that
integrates adaptive outlier-tolerant correspondence with a scan-to-submap
registration strategy. The core contribution lies in an adaptive outlier
removal threshold, which dynamically adjusts based on point-to-sensor distance
and the motion amplitude of platform. This mechanism improves the robustness of
feature matching in varying conditions. Moreover, we introduce a flexible
scan-to-submap registration method that leverages IMU data to refine pose
estimation, particularly in degenerate geometric configurations. To further
enhance localization accuracy, we design a novel weighting matrix that fuses
IMU preintegration covariance with a degeneration metric derived from the
scan-to-submap process. Extensive experiments conducted in both indoor and
outdoor environments-characterized by sparse or degenerate features-demonstrate
that our method consistently outperforms state-of-the-art approaches in terms
of both robustness and accuracy.","Guodong Yao, Hao Wang, Qing Chang",2025-08-20T01:51:53Z,2025-08-20T01:51:53Z,http://arxiv.org/abs/2508.14355v1,http://arxiv.org/pdf/2508.14355v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Towards Fully Onboard State Estimation and Trajectory Tracking for UAVs
  with Suspended Payloads","This paper addresses the problem of tracking the position of a
cable-suspended payload carried by an unmanned aerial vehicle, with a focus on
real-world deployment and minimal hardware requirements. In contrast to many
existing approaches that rely on motion-capture systems, additional onboard
cameras, or instrumented payloads, we propose a framework that uses only
standard onboard sensors--specifically, real-time kinematic global navigation
satellite system measurements and data from the onboard inertial measurement
unit--to estimate and control the payload's position. The system models the
full coupled dynamics of the aerial vehicle and payload, and integrates a
linear Kalman filter for state estimation, a model predictive contouring
control planner, and an incremental model predictive controller. The control
architecture is designed to remain effective despite sensing limitations and
estimation uncertainty. Extensive simulations demonstrate that the proposed
system achieves performance comparable to control based on ground-truth
measurements, with only minor degradation (< 6%). The system also shows strong
robustness to variations in payload parameters. Field experiments further
validate the framework, confirming its practical applicability and reliable
performance in outdoor environments using only off-the-shelf aerial vehicle
hardware.","Martin Jiroušek, Tomáš Báča, Martin Saska",2025-08-15T15:48:42Z,2025-08-15T15:48:42Z,http://arxiv.org/abs/2508.11547v1,http://arxiv.org/pdf/2508.11547v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Multi-view Normal and Distance Guidance Gaussian Splatting for Surface
  Reconstruction","3D Gaussian Splatting (3DGS) achieves remarkable results in the field of
surface reconstruction. However, when Gaussian normal vectors are aligned
within the single-view projection plane, while the geometry appears reasonable
in the current view, biases may emerge upon switching to nearby views. To
address the distance and global matching challenges in multi-view scenes, we
design multi-view normal and distance-guided Gaussian splatting. This method
achieves geometric depth unification and high-accuracy reconstruction by
constraining nearby depth maps and aligning 3D normals. Specifically, for the
reconstruction of small indoor and outdoor scenes, we propose a multi-view
distance reprojection regularization module that achieves multi-view Gaussian
alignment by computing the distance loss between two nearby views and the same
Gaussian surface. Additionally, we develop a multi-view normal enhancement
module, which ensures consistency across views by matching the normals of pixel
points in nearby views and calculating the loss. Extensive experimental results
demonstrate that our method outperforms the baseline in both quantitative and
qualitative evaluations, significantly enhancing the surface reconstruction
capability of 3DGS. Our code will be made publicly available at
(https://github.com/Bistu3DV/MND-GS/).","Bo Jia, Yanan Guo, Ying Chang, Benkui Zhang, Ying Xie, Kangning Du, Lin Cao",2025-08-11T07:25:13Z,2025-08-13T15:51:51Z,http://arxiv.org/abs/2508.07701v2,http://arxiv.org/pdf/2508.07701v2.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"ReNiL: Relative Neural Inertial Locator with Any-Scale Bayesian
  Inference","Pedestrian inertial localization is key for mobile and IoT services because
it provides infrastructure-free positioning. Yet most learning-based methods
depend on fixed sliding-window integration, struggle to adapt to diverse motion
scales and cadences, and yield inconsistent uncertainty, limiting real-world
use. We present ReNiL, a Bayesian deep-learning framework for accurate,
efficient, and uncertainty-aware pedestrian localization. ReNiL introduces
Inertial Positioning Demand Points (IPDPs) to estimate motion at contextually
meaningful waypoints instead of dense tracking, and supports inference on IMU
sequences at any scale so cadence can match application needs. It couples a
motion-aware orientation filter with an Any-Scale Laplace Estimator (ASLE), a
dual-task network that blends patch-based self-supervision with Bayesian
regression. By modeling displacements with a Laplace distribution, ReNiL
provides homogeneous Euclidean uncertainty that integrates cleanly with other
sensors. A Bayesian inference chain links successive IPDPs into consistent
trajectories. On RoNIN-ds and a new WUDataset covering indoor and outdoor
motion from 28 participants, ReNiL achieves state-of-the-art displacement
accuracy and uncertainty consistency, outperforming TLIO, CTIN, iMoT, and RoNIN
variants while reducing computation. Application studies further show
robustness and practicality for mobile and IoT localization, making ReNiL a
scalable, uncertainty-aware foundation for next-generation positioning.","Kaixuan Wu, Yuanzhuo Xu, Zejun Zhang, Weiping Zhu, Steve Drew, Xiaoguang Niu",2025-08-08T06:21:23Z,2025-08-12T11:18:57Z,http://arxiv.org/abs/2508.06053v2,http://arxiv.org/pdf/2508.06053v2.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"mmWave Radar-Based Non-Line-of-Sight Pedestrian Localization at
  T-Junctions Utilizing Road Layout Extraction via Camera","Pedestrians Localization in Non-Line-of-Sight (NLoS) regions within urban
environments poses a significant challenge for autonomous driving systems.
While mmWave radar has demonstrated potential for detecting objects in such
scenarios, the 2D radar point cloud (PCD) data is susceptible to distortions
caused by multipath reflections, making accurate spatial inference difficult.
Additionally, although camera images provide high-resolution visual
information, they lack depth perception and cannot directly observe objects in
NLoS regions. In this paper, we propose a novel framework that interprets radar
PCD through road layout inferred from camera for localization of NLoS
pedestrians. The proposed method leverages visual information from the camera
to interpret 2D radar PCD, enabling spatial scene reconstruction. The
effectiveness of the proposed approach is validated through experiments
conducted using a radar-camera system mounted on a real vehicle. The
localization performance is evaluated using a dataset collected in outdoor NLoS
driving environments, demonstrating the practical applicability of the method.","Byeonggyu Park, Hee-Yeun Kim, Byonghyok Choi, Hansang Cho, Byungkwan Kim, Soomok Lee, Mingu Jeon, Seong-Woo Kim",2025-08-04T12:31:11Z,2025-08-04T12:31:11Z,http://arxiv.org/abs/2508.02348v1,http://arxiv.org/pdf/2508.02348v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding,"Visual grounding aims to identify objects or regions in a scene based on
natural language descriptions, essential for spatially aware perception in
autonomous driving. However, existing visual grounding tasks typically depend
on bounding boxes that often fail to capture fine-grained details. Not all
voxels within a bounding box are occupied, resulting in inaccurate object
representations. To address this, we introduce a benchmark for 3D occupancy
grounding in challenging outdoor scenes. Built on the nuScenes dataset, it
integrates natural language with voxel-level occupancy annotations, offering
more precise object perception compared to the traditional grounding task.
Moreover, we propose GroundingOcc, an end-to-end model designed for 3D
occupancy grounding through multi-modal learning. It combines visual, textual,
and point cloud features to predict object location and occupancy information
from coarse to fine. Specifically, GroundingOcc comprises a multimodal encoder
for feature extraction, an occupancy head for voxel-wise predictions, and a
grounding head to refine localization. Additionally, a 2D grounding module and
a depth estimation module enhance geometric understanding, thereby boosting
model performance. Extensive experiments on the benchmark demonstrate that our
method outperforms existing baselines on 3D occupancy grounding. The dataset is
available at https://github.com/RONINGOD/GroundingOcc.","Zhan Shi, Song Wang, Junbo Chen, Jianke Zhu",2025-08-02T05:05:50Z,2025-09-03T12:05:59Z,http://arxiv.org/abs/2508.01197v2,http://arxiv.org/pdf/2508.01197v2.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes,"3D Gaussian Splatting (3DGS) has recently gained popularity in SLAM
applications due to its fast rendering and high-fidelity representation.
However, existing 3DGS-SLAM systems have predominantly focused on indoor
environments and relied on active depth sensors, leaving a gap for large-scale
outdoor applications. We present BGS-SLAM, the first binocular 3D Gaussian
Splatting SLAM system designed for outdoor scenarios. Our approach uses only
RGB stereo pairs without requiring LiDAR or active sensors. BGS-SLAM leverages
depth estimates from pre-trained deep stereo networks to guide 3D Gaussian
optimization with a multi-loss strategy enhancing both geometric consistency
and visual quality. Experiments on multiple datasets demonstrate that BGS-SLAM
achieves superior tracking accuracy and mapping performance compared to other
3DGS-based solutions in complex outdoor environments.","Xiaohan Li, Ziren Gong, Fabio Tosi, Matteo Poggi, Stefano Mattoccia, Dong Liu, Jun Wu",2025-07-31T15:54:51Z,2025-07-31T15:54:51Z,http://arxiv.org/abs/2507.23677v1,http://arxiv.org/pdf/2507.23677v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic
  Expansive Scenarios","LiDAR-based localization serves as a critical component in autonomous
systems, yet existing approaches face persistent challenges in balancing
repeatability, accuracy, and environmental adaptability. Traditional point
cloud registration methods relying solely on offline maps often exhibit limited
robustness against long-term environmental changes, leading to localization
drift and reliability degradation in dynamic real-world scenarios. To address
these challenges, this paper proposes DuLoc, a robust and accurate localization
method that tightly couples LiDAR-inertial odometry with offline map-based
localization, incorporating a constant-velocity motion model to mitigate
outlier noise in real-world scenarios. Specifically, we develop a LiDAR-based
localization framework that seamlessly integrates a prior global map with
dynamic real-time local maps, enabling robust localization in unbounded and
changing environments. Extensive real-world experiments in ultra unbounded port
that involve 2,856 hours of operational data across 32 Intelligent Guided
Vehicles (IGVs) are conducted and reported in this study. The results attained
demonstrate that our system outperforms other state-of-the-art LiDAR
localization systems in large-scale changing outdoor environments.","Haoxuan Jiang, Peicong Qian, Yusen Xie, Xiaocong Li, Ming Liu, Jun Ma",2025-07-31T15:38:50Z,2025-07-31T15:38:50Z,http://arxiv.org/abs/2507.23660v1,http://arxiv.org/pdf/2507.23660v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Vision-Language Fusion for Real-Time Autonomous Driving: Goal-Centered
  Cross-Attention of Camera, HD-Map, & Waypoints","Autonomous cars need geometric accuracy and semantic understanding to
navigate complex environments, yet most stacks handle them separately. We
present XYZ-Drive, a single vision-language model that reads a front-camera
frame, a 25m $\times$ 25m overhead map, and the next waypoint, then outputs
steering and speed. A lightweight goal-centered cross-attention layer lets
waypoint tokens highlight relevant image and map patches, supporting both
action and textual explanations, before the fused tokens enter a partially
fine-tuned LLaMA-3.2 11B model.
  On the MD-NEX Outdoor-Driving benchmark XYZ-Drive attains 95% success and
0.80 Success weighted by Path Length (SPL), surpassing PhysNav-DG by 15%. and
halving collisions, all while significantly improving efficiency by using only
a single branch. Sixteen ablations explain the gains. Removing any modality
(vision, waypoint, map) drops success by up to 11%, confirming their
complementary roles and rich connections. Replacing goal-centered attention
with simple concatenation cuts 3% in performance, showing query-based fusion
injects map knowledge more effectively. Keeping the transformer frozen loses
5%, showing the importance of fine-tuning when applying VLMs for specific tasks
such as autonomous driving. Coarsening map resolution from 10 cm to 40 cm blurs
lane edges and raises crash rate.
  Overall, these results demonstrate that early, token-level fusion of intent
and map layout enables accurate, transparent, real-time driving.","Santosh Patapati, Trisanth Srinivasan, Murari Ambati",2025-07-30T19:51:23Z,2025-08-05T02:56:37Z,http://arxiv.org/abs/2507.23064v2,http://arxiv.org/pdf/2507.23064v2.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Early Goal-Guided Multi-Scale Fusion for Real-Time Vision-Language
  Driving","Autonomous vehicles must react in milliseconds while reasoning about road
geometry and traffic intent to navigate complex situations. We introduce
NovaDrive, a single-branch vision-language architecture that processes
front-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a
single branch. A lightweight, two-stage cross-attention block first aligns
waypoint tokens with the HD map, then refines attention over fine-grained image
and depth patches. Coupled with a novel smoothness loss that discourages abrupt
steering and speed changes, this design eliminates the need for recurrent
memory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language
backbone, enabling real-time inference. On the nuScenes / Waymo subset of the
MD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts
path-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from
2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations
confirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention
fusion each contribute the most to these gains. Beyond safety, NovaDrive's
shorter routes (resulting from the novel smoothness loss) translate to lower
fuel or battery usage, pointing toward leaner, more easily updated driving
stacks. NovaDrive can be extended to other embodied-AI domains as well.","Santosh Patapati, Trisanth Srinivasan",2025-07-30T19:12:42Z,2025-07-30T19:12:42Z,http://arxiv.org/abs/2507.23042v1,http://arxiv.org/pdf/2507.23042v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Monocular Semantic Scene Completion via Masked Recurrent Networks,"Monocular Semantic Scene Completion (MSSC) aims to predict the voxel-wise
occupancy and semantic category from a single-view RGB image. Existing methods
adopt a single-stage framework that aims to simultaneously achieve visible
region segmentation and occluded region hallucination, while also being
affected by inaccurate depth estimation. Such methods often achieve suboptimal
performance, especially in complex scenes. We propose a novel two-stage
framework that decomposes MSSC into coarse MSSC followed by the Masked
Recurrent Network. Specifically, we propose the Masked Sparse Gated Recurrent
Unit (MS-GRU) which concentrates on the occupied regions by the proposed mask
updating mechanism, and a sparse GRU design is proposed to reduce the
computation cost. Additionally, we propose the distance attention projection to
reduce projection errors by assigning different attention scores according to
the distance to the observed surface. Experimental results demonstrate that our
proposed unified framework, MonoMRN, effectively supports both indoor and
outdoor scenes and achieves state-of-the-art performance on the NYUv2 and
SemanticKITTI datasets. Furthermore, we conduct robustness analysis under
various disturbances, highlighting the role of the Masked Recurrent Network in
enhancing the model's resilience to such challenges. The source code is
publicly available.","Xuzhi Wang, Xinran Wu, Song Wang, Lingdong Kong, Ziping Zhao",2025-07-23T16:29:45Z,2025-07-23T16:29:45Z,http://arxiv.org/abs/2507.17661v1,http://arxiv.org/pdf/2507.17661v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Towards foundational LiDAR world models with efficient latent flow
  matching","LiDAR-based world models offer more structured and geometry-aware
representations than their image-based counterparts. However, existing LiDAR
world models are narrowly trained; each model excels only in the domain for
which it was built. Can we develop LiDAR world models that exhibit strong
transferability across multiple domains? We conduct the first systematic domain
transfer study across three demanding scenarios: (i) outdoor to indoor
generalization, (ii) sparse-beam \& dense-beam adaptation, and (iii)
non-semantic to semantic transfer. Given different amounts of fine-tuning data,
our experiments show that a single pre-trained model can achieve up to 11%
absolute improvement (83\% relative) over training from scratch and outperforms
training from scratch in 30/36 of our comparisons. This transferability of
dynamic learning significantly reduces the reliance on manually annotated data
for semantic occupancy forecasting: our method exceed the previous semantic
occupancy forecasting models with only 5% of the labeled training data required
by prior models. We also observed inefficiencies of current LiDAR world models,
mainly through their under-compression of LiDAR data and inefficient training
objectives. To address this, we propose a latent conditional flow matching
(CFM)-based frameworks that achieves state-of-the-art reconstruction accuracy
using only half the training data and a compression ratio 6 times higher than
that of prior methods. Our model achieves SOTA performance on
future-trajectory-conditioned semantic occupancy forecasting while being 23x
more computationally efficient (a 28x FPS speedup); and achieves SOTA
performance on semantic occupancy forecasting while being 2x more
computationally efficient (a 1.1x FPS speedup).","Tianran Liu, Shengwen Zhao, Nicholas Rhinehart",2025-06-30T00:16:55Z,2025-06-30T00:16:55Z,http://arxiv.org/abs/2506.23434v1,http://arxiv.org/pdf/2506.23434v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Cooperative Circumnavigation for Multi-Quadrotor Systems via Onboard
  Sensing","A cooperative circumnavigation framework is proposed for multi-quadrotor
systems to enclose and track a moving target without reliance on external
localization systems. The distinct relationships between quadrotor-quadrotor
and quadrotor-target interactions are evaluated using a heterogeneous
perception strategy and corresponding state estimation algorithms. A modified
Kalman filter is developed to fuse visual-inertial odometry with range
measurements to enhance the accuracy of inter-quadrotor relative localization.
An event-triggered distributed Kalman filter is designed to achieve robust
target state estimation under visual occlusion by incorporating neighbor
measurements and estimated inter-quadrotor relative positions. Using the
estimation results, a cooperative circumnavigation controller is constructed,
leveraging an oscillator-based autonomous formation flight strategy. We conduct
extensive indoor and outdoor experiments to validate the efficiency of the
proposed circumnavigation framework in occluded environments. Furthermore, a
quadrotor failure experiment highlights the inherent fault tolerance property
of the proposed framework, underscoring its potential for deployment in
search-and-rescue operations.","Xueming Liu, Lin Li, Xiang Zhou, Qingrui Zhang, Tianjiang Hu",2025-06-26T02:41:55Z,2025-06-26T02:41:55Z,http://arxiv.org/abs/2506.20954v1,http://arxiv.org/pdf/2506.20954v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Look to Locate: Vision-Based Multisensory Navigation with 3-D Digital
  Maps for GNSS-Challenged Environments","In Global Navigation Satellite System (GNSS)-denied environments such as
indoor parking structures or dense urban canyons, achieving accurate and robust
vehicle positioning remains a significant challenge. This paper proposes a
cost-effective, vision-based multi-sensor navigation system that integrates
monocular depth estimation, semantic filtering, and visual map registration
(VMR) with 3-D digital maps. Extensive testing in real-world indoor and outdoor
driving scenarios demonstrates the effectiveness of the proposed system,
achieving sub-meter accuracy of 92% indoors and more than 80% outdoors, with
consistent horizontal positioning and heading average root mean-square errors
of approximately 0.98 m and 1.25 {\deg}, respectively. Compared to the
baselines examined, the proposed solution significantly reduced drift and
improved robustness under various conditions, achieving positioning accuracy
improvements of approximately 88% on average. This work highlights the
potential of cost-effective monocular vision systems combined with 3D maps for
scalable, GNSS-independent navigation in land vehicles.","Ola Elmaghraby, Eslam Mounier, Paulo Ricardo Marques de Araujo, Aboelmagd Noureldin",2025-06-24T17:44:03Z,2025-06-24T17:44:03Z,http://arxiv.org/abs/2506.19827v1,http://arxiv.org/pdf/2506.19827v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit
  Neural Scene Representation","Neural implicit scene representations have recently shown promising results
in dense visual SLAM. However, existing implicit SLAM algorithms are
constrained to single-agent scenarios, and fall difficulties in large-scale
scenes and long sequences. Existing NeRF-based multi-agent SLAM frameworks
cannot meet the constraints of communication bandwidth. To this end, we propose
the first distributed multi-agent collaborative neural SLAM framework with
hybrid scene representation, distributed camera tracking, intra-to-inter loop
closure, and online distillation for multiple submap fusion. A novel
triplane-grid joint scene representation method is proposed to improve scene
reconstruction. A novel intra-to-inter loop closure method is designed to
achieve local (single-agent) and global (multi-agent) consistency. We also
design a novel online distillation method to fuse the information of different
submaps to achieve global consistency. Furthermore, to the best of our
knowledge, there is no real-world dataset for NeRF-based/GS-based SLAM that
provides both continuous-time trajectories groundtruth and high-accuracy 3D
meshes groundtruth. To this end, we propose the first real-world Dense slam
(DES) dataset covering both single-agent and multi-agent scenarios, ranging
from small rooms to large-scale outdoor scenes, with high-accuracy ground truth
for both 3D mesh and continuous-time camera trajectory. This dataset can
advance the development of the research in both SLAM, 3D reconstruction, and
visual foundation model. Experiments on various datasets demonstrate the
superiority of the proposed method in both mapping, tracking, and
communication. The dataset and code will open-source on
https://github.com/dtc111111/mcnslam.","Tianchen Deng, Guole Shen, Xun Chen, Shenghai Yuan, Hongming Shen, Guohao Peng, Zhenyu Wu, Jingchuan Wang, Lihua Xie, Danwei Wang, Hesheng Wang, Weidong Chen",2025-06-23T14:22:29Z,2025-08-19T06:02:44Z,http://arxiv.org/abs/2506.18678v2,http://arxiv.org/pdf/2506.18678v2.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Crowdsourcing Ubiquitous Indoor Localization with Non-Cooperative Wi-Fi
  Ranging","Indoor localization opens the path to potentially transformative
applications. Although many indoor localization methods have been proposed over
the years, they remain too impractical for widespread deployment in the real
world. In this paper, we introduce PeepLoc, a deployable and scalable
Wi-Fi-based solution for indoor localization that relies only on pre-existing
devices and infrastructure. Specifically, PeepLoc works on any mobile device
with an unmodified Wi-Fi transceiver and in any indoor environment with a
sufficient number of Wi-Fi access points (APs) and pedestrian traffic. At the
core of PeepLoc is (a) a mechanism which allows any Wi-Fi device to obtain
non-cooperative time-of-flight (ToF) to any Wi-Fi AP and (b) a novel
bootstrapping mechanism that relies on pedestrian dead reckoning (PDR) and
crowdsourcing to opportunistically initialize pre-existing APs as anchor points
within an environment. We implement PeepLoc using commodity hardware and
evaluate it extensively across 4 campus buildings. We show PeepLoc leads to a
mean and median positional error of 3.41 m and 3.06 m respectively, which is
superior to existing deployed indoor localization systems and is competitive
with commodity GPS in outdoor environments.","Emerson Sie, Enguang Fan, Federico Cifuentes-Urtubey, Deepak Vasisht",2025-06-23T06:04:45Z,2025-07-05T07:44:19Z,http://arxiv.org/abs/2506.18317v2,http://arxiv.org/pdf/2506.18317v2.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Embodied Web Agents: Bridging Physical-Digital Realms for Integrated
  Agent Intelligence","AI agents today are mostly siloed - they either retrieve and reason over vast
amount of digital information and knowledge obtained online; or interact with
the physical world through embodied perception, planning and action - but
rarely both. This separation limits their ability to solve tasks that require
integrated physical and digital intelligence, such as cooking from online
recipes, navigating with dynamic map data, or interpreting real-world landmarks
using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI
agents that fluidly bridge embodiment and web-scale reasoning. To
operationalize this concept, we first develop the Embodied Web Agents task
environments, a unified simulation platform that tightly integrates realistic
3D indoor and outdoor environments with functional web interfaces. Building
upon this platform, we construct and release the Embodied Web Agents Benchmark,
which encompasses a diverse suite of tasks including cooking, navigation,
shopping, tourism, and geolocation - all requiring coordinated reasoning across
physical and digital realms for systematic assessment of cross-domain
intelligence. Experimental results reveal significant performance gaps between
state-of-the-art AI systems and human capabilities, establishing both
challenges and opportunities at the intersection of embodied cognition and
web-scale knowledge access. All datasets, codes and websites are publicly
available at our project page https://embodied-web-agent.github.io/.","Yining Hong, Rui Sun, Bingxuan Li, Xingcheng Yao, Maxine Wu, Alexander Chien, Da Yin, Ying Nian Wu, Zhecan James Wang, Kai-Wei Chang",2025-06-18T17:58:17Z,2025-07-29T22:40:49Z,http://arxiv.org/abs/2506.15677v3,http://arxiv.org/pdf/2506.15677v3.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Time-Optimized Safe Navigation in Unstructured Environments through
  Learning Based Depth Completion","Quadrotors hold significant promise for several applications such as
agriculture, search and rescue, and infrastructure inspection. Achieving
autonomous operation requires systems to navigate safely through complex and
unfamiliar environments. This level of autonomy is particularly challenging due
to the complexity of such environments and the need for real-time decision
making especially for platforms constrained by size, weight, and power (SWaP),
which limits flight time and precludes the use of bulky sensors like Light
Detection and Ranging (LiDAR) for mapping. Furthermore, computing globally
optimal, collision-free paths and translating them into time-optimized, safe
trajectories in real time adds significant computational complexity. To address
these challenges, we present a fully onboard, real-time navigation system that
relies solely on lightweight onboard sensors. Our system constructs a dense 3D
map of the environment using a novel visual depth estimation approach that
fuses stereo and monocular learning-based depth, yielding longer-range, denser,
and less noisy depth maps than conventional stereo methods. Building on this
map, we introduce a novel planning and trajectory generation framework capable
of rapidly computing time-optimal global trajectories. As the map is
incrementally updated with new depth information, our system continuously
refines the trajectory to maintain safety and optimality. Both our planner and
trajectory generator outperforms state-of-the-art methods in terms of
computational efficiency and guarantee obstacle-free trajectories. We validate
our system through robust autonomous flight experiments in diverse indoor and
outdoor environments, demonstrating its effectiveness for safe navigation in
previously unknown settings.","Jeffrey Mao, Raghuram Cauligi Srinivas, Steven Nogar, Giuseppe Loianno",2025-06-17T21:01:05Z,2025-06-17T21:01:05Z,http://arxiv.org/abs/2506.14975v1,http://arxiv.org/pdf/2506.14975v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Towards Perception-based Collision Avoidance for UAVs when Guiding the
  Visually Impaired","Autonomous navigation by drones using onboard sensors combined with machine
learning and computer vision algorithms is impacting a number of domains,
including agriculture, logistics, and disaster management. In this paper, we
examine the use of drones for assisting visually impaired people (VIPs) in
navigating through outdoor urban environments. Specifically, we present a
perception-based path planning system for local planning around the
neighborhood of the VIP, integrated with a global planner based on GPS and maps
for coarse planning. We represent the problem using a geometric formulation and
propose a multi DNN based framework for obstacle avoidance of the UAV as well
as the VIP. Our evaluations conducted on a drone human system in a university
campus environment verifies the feasibility of our algorithms in three
scenarios; when the VIP walks on a footpath, near parked vehicles, and in a
crowded street.","Suman Raj, Swapnil Padhi, Ruchi Bhoot, Prince Modi, Yogesh Simmhan",2025-06-17T09:08:30Z,2025-06-17T09:08:30Z,http://arxiv.org/abs/2506.14857v1,http://arxiv.org/pdf/2506.14857v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"LogoSP: Local-global Grouping of Superpoints for Unsupervised Semantic
  Segmentation of 3D Point Clouds","We study the problem of unsupervised 3D semantic segmentation on raw point
clouds without needing human labels in training. Existing methods usually
formulate this problem into learning per-point local features followed by a
simple grouping strategy, lacking the ability to discover additional and
possibly richer semantic priors beyond local features. In this paper, we
introduce LogoSP to learn 3D semantics from both local and global point
features. The key to our approach is to discover 3D semantic information by
grouping superpoints according to their global patterns in the frequency
domain, thus generating highly accurate semantic pseudo-labels for training a
segmentation network. Extensive experiments on two indoor and an outdoor
datasets show that our LogoSP surpasses all existing unsupervised methods by
large margins, achieving the state-of-the-art performance for unsupervised 3D
semantic segmentation. Notably, our investigation into the learned global
patterns reveals that they truly represent meaningful 3D semantics in the
absence of human labels during training.","Zihui Zhang, Weisheng Dai, Hongtao Wen, Bo Yang",2025-06-09T15:21:37Z,2025-06-09T15:21:37Z,http://arxiv.org/abs/2506.07857v1,http://arxiv.org/pdf/2506.07857v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
FlySearch: Exploring how vision-language models explore,"The real world is messy and unstructured. Uncovering critical information
often requires active, goal-driven exploration. It remains to be seen whether
Vision-Language Models (VLMs), which recently emerged as a popular zero-shot
tool in many difficult tasks, can operate effectively in such conditions. In
this paper, we answer this question by introducing FlySearch, a 3D, outdoor,
photorealistic environment for searching and navigating to objects in complex
scenes. We define three sets of scenarios with varying difficulty and observe
that state-of-the-art VLMs cannot reliably solve even the simplest exploration
tasks, with the gap to human performance increasing as the tasks get harder. We
identify a set of central causes, ranging from vision hallucination, through
context misunderstanding, to task planning failures, and we show that some of
them can be addressed by finetuning. We publicly release the benchmark,
scenarios, and the underlying codebase.","Adam Pardyl, Dominik Matuszek, Mateusz Przebieracz, Marek Cygan, Bartosz Zieliński, Maciej Wołczyk",2025-06-03T14:03:42Z,2025-06-04T09:32:09Z,http://arxiv.org/abs/2506.02896v2,http://arxiv.org/pdf/2506.02896v2.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"AniTrack: A Power-Efficient, Time-Slotted and Robust UWB Localization
  System for Animal Tracking in a Controlled Setting","Accurate localization is essential for a wide range of applications,
including asset tracking, smart agriculture, and animal monitoring. While
traditional localization methods, such as Global Navigation Satellite System
(GNSS), Wi-Fi, and Bluetooth Low Energy (BLE), offer varying levels of accuracy
and coverage, they have drawbacks regarding power consumption, infrastructure
requirements, and deployment flexibility. Ultra-Wideband (UWB) is emerging as
an alternative, offering centimeter-level accuracy and energy efficiency,
especially suitable for medium to large field monitoring with capabilities to
work indoors and outdoors. However, existing UWB localization systems require
infrastructure with mains power to supply the anchors, which impedes their
scalability and ease of deployment. This underscores the need for a fully
battery-powered and energy-efficient localization system. This paper presents
an energy-optimized, battery-operated UWB localization system that leverages
Long Range Wide Area Network (LoRaWAN) for data transmission to a server
backend. By employing single-sided two-way ranging (SS-TWR) in a time-slotted
localization approach, the power consumption both on the anchor and the tag is
reduced, while maintaining high accuracy. With a low average power consumption
of 20.44 mW per anchor and 7.19 mW per tag, the system allows fully
battery-powered operation for up to 25 days, achieving average accuracy of
13.96 cm with self-localizing anchors on a 600 m2 testing ground. To validate
its effectiveness and ease of installation in a challenging application
scenario, ten anchors and two tags were successfully deployed in a tropical
zoological biome where they could be used to track Aldabra Giant Tortoises
(Aldabrachelys gigantea).","Victor Luder, Lukas Schulthess, Silvano Cortesi, Leyla Rivero Davis, Michele Magno",2025-05-30T20:42:40Z,2025-05-30T20:42:40Z,http://arxiv.org/abs/2506.00216v1,http://arxiv.org/pdf/2506.00216v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
LiDAR Based Semantic Perception for Forklifts in Outdoor Environments,"In this study, we present a novel LiDAR-based semantic segmentation framework
tailored for autonomous forklifts operating in complex outdoor environments.
Central to our approach is the integration of a dual LiDAR system, which
combines forward-facing and downward-angled LiDAR sensors to enable
comprehensive scene understanding, specifically tailored for industrial
material handling tasks. The dual configuration improves the detection and
segmentation of dynamic and static obstacles with high spatial precision. Using
high-resolution 3D point clouds captured from two sensors, our method employs a
lightweight yet robust approach that segments the point clouds into
safety-critical instance classes such as pedestrians, vehicles, and forklifts,
as well as environmental classes such as driveable ground, lanes, and
buildings. Experimental validation demonstrates that our approach achieves high
segmentation accuracy while satisfying strict runtime requirements,
establishing its viability for safety-aware, fully autonomous forklift
navigation in dynamic warehouse and yard environments.","Benjamin Serfling, Hannes Reichert, Lorenzo Bayerlein, Konrad Doll, Kati Radkhah-Lens",2025-05-28T11:45:14Z,2025-05-28T11:45:14Z,http://arxiv.org/abs/2505.22258v1,http://arxiv.org/pdf/2505.22258v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Real-World Deployment of Cloud Autonomous Mobility System Using 5G
  Networks for Outdoor and Indoor Environments","The growing complexity of both outdoor and indoor mobility systems demands
scalable, cost-effective, and reliable perception and communication frameworks.
This work presents the real-world deployment and evaluation of a Cloud
Autonomous Mobility (CAM) system that leverages distributed sensor nodes
connected via 5G networks, which integrates LiDAR- and camera-based perception
at infrastructure units, cloud computing for global information fusion, and
Ultra-Reliable Low Latency Communications (URLLC) to enable real-time
situational awareness and autonomous operation. The CAM system is deployed in
two distinct environments: a dense urban roundabout and a narrow indoor
hospital corridor. Field experiments show improved traffic monitoring, hazard
detection, and asset management capabilities. The paper also discusses
practical deployment challenges and shares key insights for scaling CAM
systems. The results highlight the potential of cloud-based infrastructure
perception to advance both outdoor and indoor intelligent transportation
systems.","Yufeng Yang, Minghao Ning, Keqi Shu, Aladdin Saleh, Ehsan Hashemi, Amir Khajepour",2025-05-27T18:51:45Z,2025-05-27T18:51:45Z,http://arxiv.org/abs/2505.21676v1,http://arxiv.org/pdf/2505.21676v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Distance Estimation in Outdoor Driving Environments Using Phase-only
  Correlation Method with Event Cameras","With the growing adoption of autonomous driving, the advancement of sensor
technology is crucial for ensuring safety and reliable operation. Sensor fusion
techniques that combine multiple sensors such as LiDAR, radar, and cameras have
proven effective, but the integration of multiple devices increases both
hardware complexity and cost. Therefore, developing a single sensor capable of
performing multiple roles is highly desirable for cost-efficient and scalable
autonomous driving systems.
  Event cameras have emerged as a promising solution due to their unique
characteristics, including high dynamic range, low latency, and high temporal
resolution. These features enable them to perform well in challenging lighting
conditions, such as low-light or backlit environments. Moreover, their ability
to detect fine-grained motion events makes them suitable for applications like
pedestrian detection and vehicle-to-infrastructure communication via visible
light.
  In this study, we present a method for distance estimation using a monocular
event camera and a roadside LED bar. By applying a phase-only correlation
technique to the event data, we achieve sub-pixel precision in detecting the
spatial shift between two light sources. This enables accurate
triangulation-based distance estimation without requiring stereo vision. Field
experiments conducted in outdoor driving scenarios demonstrated that the
proposed approach achieves over 90% success rate with less than 0.5-meter error
for distances ranging from 20 to 60 meters.
  Future work includes extending this method to full position estimation by
leveraging infrastructure such as smart poles equipped with LEDs, enabling
event-camera-based vehicles to determine their own position in real time. This
advancement could significantly enhance navigation accuracy, route
optimization, and integration into intelligent transportation systems.","Masataka Kobayashi, Shintaro Shiba, Quan Kong, Norimasa Kobori, Tsukasa Shimizu, Shan Lu, Takaya Yamazato",2025-05-23T07:44:33Z,2025-05-23T07:44:33Z,http://arxiv.org/abs/2505.17582v1,http://arxiv.org/pdf/2505.17582v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"R3GS: Gaussian Splatting for Robust Reconstruction and Relocalization in
  Unconstrained Image Collections","We propose R3GS, a robust reconstruction and relocalization framework
tailored for unconstrained datasets. Our method uses a hybrid representation
during training. Each anchor combines a global feature from a convolutional
neural network (CNN) with a local feature encoded by the multiresolution hash
grids [2]. Subsequently, several shallow multi-layer perceptrons (MLPs) predict
the attributes of each Gaussians, including color, opacity, and covariance. To
mitigate the adverse effects of transient objects on the reconstruction
process, we ffne-tune a lightweight human detection network. Once ffne-tuned,
this network generates a visibility map that efffciently generalizes to other
transient objects (such as posters, banners, and cars) with minimal need for
further adaptation. Additionally, to address the challenges posed by sky
regions in outdoor scenes, we propose an effective sky-handling technique that
incorporates a depth prior as a constraint. This allows the inffnitely distant
sky to be represented on the surface of a large-radius sky sphere,
signiffcantly reducing ffoaters caused by errors in sky reconstruction.
Furthermore, we introduce a novel relocalization method that remains robust to
changes in lighting conditions while estimating the camera pose of a given
image within the reconstructed 3DGS scene. As a result, R3GS significantly
enhances rendering ffdelity, improves both training and rendering efffciency,
and reduces storage requirements. Our method achieves state-of-the-art
performance compared to baseline methods on in-the-wild datasets. The code will
be made open-source following the acceptance of the paper.","Xu yan, Zhaohui Wang, Rong Wei, Jingbo Yu, Dong Li, Xiangde Liu",2025-05-21T09:25:22Z,2025-05-21T09:25:22Z,http://arxiv.org/abs/2505.15294v1,http://arxiv.org/pdf/2505.15294v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Technical Report for ICRA 2025 GOOSE 2D Semantic Segmentation Challenge:
  Boosting Off-Road Segmentation via Photometric Distortion and Exponential
  Moving Average","We report on the application of a high-capacity semantic segmentation
pipeline to the GOOSE 2D Semantic Segmentation Challenge for unstructured
off-road environments. Using a FlashInternImage-B backbone together with a
UPerNet decoder, we adapt established techniques, rather than designing new
ones, to the distinctive conditions of off-road scenes. Our training recipe
couples strong photometric distortion augmentation (to emulate the wide
lighting variations of outdoor terrain) with an Exponential Moving Average
(EMA) of weights for better generalization. Using only the GOOSE training
dataset, we achieve 88.8\% mIoU on the validation set.","Wonjune Kim, Lae-kyoung Lee, Su-Yong An",2025-05-17T00:29:17Z,2025-05-17T00:29:17Z,http://arxiv.org/abs/2505.11769v1,http://arxiv.org/pdf/2505.11769v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Large-Scale Gaussian Splatting SLAM,"The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian
Splatting (3DGS) have shown encouraging and impressive results for visual SLAM.
However, most representative methods require RGBD sensors and are only
available for indoor environments. The robustness of reconstruction in
large-scale outdoor scenarios remains unexplored. This paper introduces a
large-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The
proposed LSG-SLAM employs a multi-modality strategy to estimate prior poses
under large view changes. In tracking, we introduce feature-alignment warping
constraints to alleviate the adverse effects of appearance similarity in
rendering losses. For the scalability of large-scale scenarios, we introduce
continuous Gaussian Splatting submaps to tackle unbounded scenes with limited
memory. Loops are detected between GS submaps by place recognition and the
relative pose between looped keyframes is optimized utilizing rendering and
feature warping losses. After the global optimization of camera poses and
Gaussian points, a structure refinement module enhances the reconstruction
quality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM
achieves superior performance over existing Neural, 3DGS-based, and even
traditional approaches. Project page: https://lsg-slam.github.io.","Zhe Xin, Chenyang Wu, Penghui Huang, Yanyong Zhang, Yinian Mao, Guoquan Huang",2025-05-15T03:00:32Z,2025-05-15T03:00:32Z,http://arxiv.org/abs/2505.09915v1,http://arxiv.org/pdf/2505.09915v1.pdf,all:outdoor AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Population-aware Online Mirror Descent for Mean-Field Games with Common
  Noise by Deep Reinforcement Learning","Mean Field Games (MFGs) offer a powerful framework for studying large-scale
multi-agent systems. Yet, learning Nash equilibria in MFGs remains a
challenging problem, particularly when the initial distribution is unknown or
when the population is subject to common noise. In this paper, we introduce an
efficient deep reinforcement learning (DRL) algorithm designed to achieve
population-dependent Nash equilibria without relying on averaging or historical
sampling, inspired by Munchausen RL and Online Mirror Descent. The resulting
policy is adaptable to various initial distributions and sources of common
noise. Through numerical experiments on seven canonical examples, we
demonstrate that our algorithm exhibits superior convergence properties
compared to state-of-the-art algorithms, particularly a DRL version of
Fictitious Play for population-dependent policies. The performance in the
presence of common noise underscores the robustness and adaptability of our
approach.","Zida Wu, Mathieu Lauriere, Matthieu Geist, Olivier Pietquin, Ankur Mehta",2025-09-03T05:33:46Z,2025-09-03T05:33:46Z,http://arxiv.org/abs/2509.03030v1,http://arxiv.org/pdf/2509.03030v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"TransForSeg: A Multitask Stereo ViT for Joint Stereo Segmentation and 3D
  Force Estimation in Catheterization","Recently, the emergence of multitask deep learning models has enhanced
catheterization procedures by providing tactile and visual perception data
through an end-to-end architecture. This information is derived from a
segmentation and force estimation head, which localizes the catheter in X-ray
images and estimates the applied pressure based on its deflection within the
image. These stereo vision architectures incorporate a CNN-based
encoder-decoder that captures the dependencies between X-ray images from two
viewpoints, enabling simultaneous 3D force estimation and stereo segmentation
of the catheter. With these tasks in mind, this work approaches the problem
from a new perspective. We propose a novel encoder-decoder Vision Transformer
model that processes two input X-ray images as separate sequences. Given
sequences of X-ray patches from two perspectives, the transformer captures
long-range dependencies without the need to gradually expand the receptive
field for either image. The embeddings generated by both the encoder and
decoder are fed into two shared segmentation heads, while a regression head
employs the fused information from the decoder for 3D force estimation. The
proposed model is a stereo Vision Transformer capable of simultaneously
segmenting the catheter from two angles while estimating the generated forces
at its tip in 3D. This model has undergone extensive experiments on synthetic
X-ray images with various noise levels and has been compared against
state-of-the-art pure segmentation models, vision-based catheter force
estimation methods, and a multitask catheter segmentation and force estimation
approach. It outperforms existing models, setting a new state-of-the-art in
both catheter segmentation and force estimation.","Pedram Fekri, Mehrdad Zadeh, Javad Dargahi",2025-09-01T16:36:23Z,2025-09-01T16:36:23Z,http://arxiv.org/abs/2509.01605v1,http://arxiv.org/pdf/2509.01605v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"FGO-SLAM: Enhancing Gaussian SLAM with Globally Consistent Opacity
  Radiance Field","Visual SLAM has regained attention due to its ability to provide perceptual
capabilities and simulation test data for Embodied AI. However, traditional
SLAM methods struggle to meet the demands of high-quality scene reconstruction,
and Gaussian SLAM systems, despite their rapid rendering and high-quality
mapping capabilities, lack effective pose optimization methods and face
challenges in geometric reconstruction. To address these issues, we introduce
FGO-SLAM, a Gaussian SLAM system that employs an opacity radiance field as the
scene representation to enhance geometric mapping performance. After initial
pose estimation, we apply global adjustment to optimize camera poses and sparse
point cloud, ensuring robust tracking of our approach. Additionally, we
maintain a globally consistent opacity radiance field based on 3D Gaussians and
introduce depth distortion and normal consistency terms to refine the scene
representation. Furthermore, after constructing tetrahedral grids, we identify
level sets to directly extract surfaces from 3D Gaussians. Results across
various real-world and large-scale synthetic datasets demonstrate that our
method achieves state-of-the-art tracking accuracy and mapping performance.","Fan Zhu, Yifan Zhao, Ziyu Chen, Biao Yu, Hui Zhu",2025-09-01T15:20:41Z,2025-09-01T15:20:41Z,http://arxiv.org/abs/2509.01547v1,http://arxiv.org/pdf/2509.01547v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"A Reactive Grasping Framework for Multi-DoF Grippers via Task Space
  Velocity Fields and Joint Space QP","We present a fast and reactive grasping framework for multi-DoF grippers that
combines task-space velocity fields with a joint-space Quadratic Program (QP)
in a hierarchical structure. Reactive, collision-free global motion planning is
particularly challenging for high-DoF systems, since simultaneous increases in
state dimensionality and planning horizon trigger a combinatorial explosion of
the search space, making real-time planning intractable. To address this, we
plan globally in a lower-dimensional task space, such as fingertip positions,
and track locally in the full joint space while enforcing all constraints. This
approach is realized by constructing velocity fields in multiple task-space
coordinates (or in some cases a subset of joint coordinates) and solving a
weighted joint-space QP to compute joint velocities that track these fields
with appropriately assigned priorities. Through simulation experiments with
privileged knowledge and real-world tests using the recent pose-tracking
algorithm FoundationPose, we verify that our method enables high-DoF arm-hand
systems to perform real-time, collision-free reaching motions while adapting to
dynamic environments and external disturbances.","Yonghyeon Lee, Tzu-Yuan Lin, Alexander Alexiev, Sangbae Kim",2025-09-01T00:52:01Z,2025-09-01T00:52:01Z,http://arxiv.org/abs/2509.01044v1,http://arxiv.org/pdf/2509.01044v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Enhanced Mean Field Game for Interactive Decision-Making with Varied
  Stylish Multi-Vehicles","This paper presents an MFG-based decision-making framework for autonomous
driving in heterogeneous traffic. To capture diverse human behaviors, we
propose a quantitative driving style representation that maps abstract traits
to parameters such as speed, safety factors, and reaction time. These
parameters are embedded into the MFG through a spatial influence field model.
To ensure safe operation in dense traffic, we introduce a safety-critical
lane-changing algorithm that leverages dynamic safety margins,
time-to-collision analysis, and multi-layered constraints. Real-world NGSIM
data is employed for style calibration and empirical validation. Experimental
results demonstrate zero collisions across six style combinations, two
15-vehicle scenarios, and NGSIM-based trials, consistently outperforming
conventional game-theoretic baselines. Overall, our approach provides a
scalable, interpretable, and behavior-aware planning framework for real-world
autonomous driving applications.","Liancheng Zheng, Zhen Tian, Yangfan He, Shuo Liu, Ke Gong, Huilin Chen, Zhihao Lin",2025-08-31T20:24:53Z,2025-08-31T20:24:53Z,http://arxiv.org/abs/2509.00981v1,http://arxiv.org/pdf/2509.00981v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"A Risk-aware Spatial-temporal Trajectory Planning Framework for
  Autonomous Vehicles Using QP-MPC and Dynamic Hazard Fields","Trajectory planning is a critical component in ensuring the safety,
stability, and efficiency of autonomous vehicles. While existing trajectory
planning methods have achieved progress, they often suffer from high
computational costs, unstable performance in dynamic environments, and limited
validation across diverse scenarios. To overcome these challenges, we propose
an enhanced QP-MPC-based framework that incorporates three key innovations: (i)
a novel cost function designed with a dynamic hazard field, which explicitly
balances safety, efficiency, and comfort; (ii) seamless integration of this
cost function into the QP-MPC formulation, enabling direct optimization of
desired driving behaviors; and (iii) extensive validation of the proposed
framework across complex tasks. The spatial safe planning is guided by a
dynamic hazard field (DHF) for risk assessment, while temporal safe planning is
based on a space-time graph. Besides, the quintic polynomial sampling and
sub-reward of comforts are used to ensure comforts during lane-changing. The
sub-reward of efficiency is used to maintain driving efficiency. Finally, the
proposed DHF-enhanced objective function integrates multiple objectives,
providing a proper optimization tasks for QP-MPC. Extensive simulations
demonstrate that the proposed framework outperforms benchmark optimization
methods in terms of efficiency, stability, and comfort across a variety of
scenarios likes lane-changing, overtaking, and crossing intersections.","Zhen Tian, Zhihao Lin, Dezong Zhao, Christos Anagnostopoulos, Qiyuan Wang, Wenjing Zhao, Xiaodan Wang, Chongfeng Wei",2025-08-31T00:35:55Z,2025-08-31T00:35:55Z,http://arxiv.org/abs/2509.00643v1,http://arxiv.org/pdf/2509.00643v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Vehicle-in-Virtual-Environment (VVE) Method for Developing and
  Evaluating VRU Safety of Connected and Autonomous Driving with Focus on
  Bicyclist Safety","Extensive research has already been conducted in the autonomous driving field
to help vehicles navigate safely and efficiently. At the same time, plenty of
current research on vulnerable road user (VRU) safety is performed which
largely concentrates on perception, localization, or trajectory prediction of
VRUs. However, existing research still exhibits several gaps, including the
lack of a unified planning and collision avoidance system for autonomous
vehicles, limited investigation into delay tolerant control strategies, and the
absence of an efficient and standardized testing methodology. Ensuring VRU
safety remains one of the most pressing challenges in autonomous driving,
particularly in dynamic and unpredictable environments. In this two year
project, we focused on applying the Vehicle in Virtual Environment (VVE) method
to develop, evaluate, and demonstrate safety functions for Vulnerable Road
Users (VRUs) using automated steering and braking of ADS. In this current
second year project report, our primary focus was on enhancing the previous
year results while also considering bicyclist safety.","Haochong Chen, Xincheng Cao, Bilin Aksun-Guvenc, Levent Guvenc",2025-08-30T22:43:14Z,2025-08-30T22:43:14Z,http://arxiv.org/abs/2509.00624v1,http://arxiv.org/pdf/2509.00624v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Domain Adaptation-Based Crossmodal Knowledge Distillation for 3D
  Semantic Segmentation","Semantic segmentation of 3D LiDAR data plays a pivotal role in autonomous
driving. Traditional approaches rely on extensive annotated data for point
cloud analysis, incurring high costs and time investments. In contrast,
realworld image datasets offer abundant availability and substantial scale. To
mitigate the burden of annotating 3D LiDAR point clouds, we propose two
crossmodal knowledge distillation methods: Unsupervised Domain Adaptation
Knowledge Distillation (UDAKD) and Feature and Semantic-based Knowledge
Distillation (FSKD). Leveraging readily available spatio-temporally
synchronized data from cameras and LiDARs in autonomous driving scenarios, we
directly apply a pretrained 2D image model to unlabeled 2D data. Through
crossmodal knowledge distillation with known 2D-3D correspondence, we actively
align the output of the 3D network with the corresponding points of the 2D
network, thereby obviating the necessity for 3D annotations. Our focus is on
preserving modality-general information while filtering out modality-specific
details during crossmodal distillation. To achieve this, we deploy
self-calibrated convolution on 3D point clouds as the foundation of our domain
adaptation module. Rigorous experimentation validates the effectiveness of our
proposed methods, consistently surpassing the performance of state-of-the-art
approaches in the field.","Jialiang Kang, Jiawen Wang, Dingsheng Luo",2025-08-30T06:34:39Z,2025-08-30T06:34:39Z,http://arxiv.org/abs/2509.00379v1,http://arxiv.org/pdf/2509.00379v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Cooperative Sensing Enhanced UAV Path-Following and Obstacle Avoidance
  with Variable Formation","The high mobility of unmanned aerial vehicles (UAVs) enables them to be used
in various civilian fields, such as rescue and cargo transport. Path-following
is a crucial way to perform these tasks while sensing and collision avoidance
are essential for safe flight. In this paper, we investigate how to efficiently
and accurately achieve path-following, obstacle sensing and avoidance subtasks,
as well as their conflict-free fusion scheduling. Firstly, a high precision
deep reinforcement learning (DRL)-based UAV formation path-following model is
developed, and the reward function with adaptive weights is designed from the
perspective of distance and velocity errors. Then, we use integrated sensing
and communication (ISAC) signals to detect the obstacle and derive the
Cramer-Rao lower bound (CRLB) for obstacle sensing by information-level fusion,
based on which we propose the variable formation enhanced obstacle position
estimation (VFEO) algorithm. In addition, an online obstacle avoidance scheme
without pretraining is designed to solve the sparse reward. Finally, with the
aid of null space based (NSB) behavioral method, we present a hierarchical
subtasks fusion strategy. Simulation results demonstrate the effectiveness and
superiority of the subtask algorithms and the hierarchical fusion strategy.","Changheng Wang, Zhiqing Wei, Wangjun Jiang, Haoyue Jiang, Zhiyong Feng",2025-08-29T02:48:41Z,2025-08-29T02:48:41Z,http://arxiv.org/abs/2508.21316v1,http://arxiv.org/pdf/2508.21316v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"TRUST-Planner: Topology-guided Robust Trajectory Planner for AAVs with
  Uncertain Obstacle Spatial-temporal Avoidance","Despite extensive developments in motion planning of autonomous aerial
vehicles (AAVs), existing frameworks faces the challenges of local minima and
deadlock in complex dynamic environments, leading to increased collision risks.
To address these challenges, we present TRUST-Planner, a topology-guided
hierarchical planning framework for robust spatial-temporal obstacle avoidance.
In the frontend, a dynamic enhanced visible probabilistic roadmap (DEV-PRM) is
proposed to rapidly explore topological paths for global guidance. The backend
utilizes a uniform terminal-free minimum control polynomial (UTF-MINCO) and
dynamic distance field (DDF) to enable efficient predictive obstacle avoidance
and fast parallel computation. Furthermore, an incremental multi-branch
trajectory management framework is introduced to enable spatio-temporal
topological decision-making, while efficiently leveraging historical
information to reduce replanning time. Simulation results show that
TRUST-Planner outperforms baseline competitors, achieving a 96\% success rate
and millisecond-level computation efficiency in tested complex environments.
Real-world experiments further validate the feasibility and practicality of the
proposed method.","Junzhi Li, Teng Long, Jingliang Sun, Jianxin Zhong",2025-08-20T10:52:28Z,2025-08-20T10:52:28Z,http://arxiv.org/abs/2508.14610v1,http://arxiv.org/pdf/2508.14610v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Dimension-Decomposed Learning for Quadrotor Geometric Attitude Control
  with Almost Global Exponential Convergence on SO(3)","This paper introduces a lightweight and interpretable online learning
approach called Dimension-Decomposed Learning (DiD-L) for disturbance
identification in quadrotor geometric attitude control. As a module instance of
DiD-L, we propose the Sliced Adaptive-Neuro Mapping (SANM). Specifically, to
address underlying underfitting problems, the high-dimensional mapping for
online identification is axially ``sliced"" into multiple low-dimensional
submappings (slices). In this way, the complex high-dimensional problem is
decomposed into a set of simple low-dimensional subtasks addressed by shallow
neural networks and adaptive laws. These neural networks and adaptive laws are
updated online via Lyapunov-based adaptation without the persistent excitation
(PE) condition. To enhance the interpretability of the proposed approach, we
prove that the state solution of the rotational error dynamics exponentially
converges into an arbitrarily small ball within an almost global attraction
domain, despite time-varying disturbances and inertia uncertainties. This
result is novel as it demonstrates exponential convergence without requiring
pre-training for unseen disturbances and specific knowledge of the model. To
our knowledge in the quadrotor control field, DiD-L is the first online
learning approach that is lightweight enough to run in real-time at 400 Hz on
microcontroller units (MCUs) such as STM32, and has been validated through
real-world experiments.","Tianhua Gao, Masashi Izumita, Kohji Tomita, Akiya Kamimura",2025-08-20T04:41:42Z,2025-08-28T11:58:55Z,http://arxiv.org/abs/2508.14422v2,http://arxiv.org/pdf/2508.14422v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"2COOOL: 2nd Workshop on the Challenge Of Out-Of-Label Hazards in
  Autonomous Driving","As the computer vision community advances autonomous driving algorithms,
integrating vision-based insights with sensor data remains essential for
improving perception, decision making, planning, prediction, simulation, and
control. Yet we must ask: Why don't we have entirely safe self-driving cars
yet? A key part of the answer lies in addressing novel scenarios, one of the
most critical barriers to real-world deployment. Our 2COOOL workshop provides a
dedicated forum for researchers and industry experts to push the state of the
art in novelty handling, including out-of-distribution hazard detection,
vision-language models for hazard understanding, new benchmarking and
methodologies, and safe autonomous driving practices. The 2nd Workshop on the
Challenge of Out-of-Label Hazards in Autonomous Driving (2COOOL) will be held
at the International Conference on Computer Vision (ICCV) 2025 in Honolulu,
Hawaii, on October 19, 2025. We aim to inspire the development of new
algorithms and systems for hazard avoidance, drawing on ideas from anomaly
detection, open-set recognition, open-vocabulary modeling, domain adaptation,
and related fields. Building on the success of its inaugural edition at the
Winter Conference on Applications of Computer Vision (WACV) 2025, the workshop
will feature a mix of academic and industry participation.","Ali K. AlShami, Ryan Rabinowitz, Maged Shoman, Jianwu Fang, Lukas Picek, Shao-Yuan Lo, Steve Cruz, Khang Nhut Lam, Nachiket Kamod, Lei-Lei Li, Jugal Kalita, Terrance E. Boult",2025-08-18T18:55:54Z,2025-08-18T18:55:54Z,http://arxiv.org/abs/2508.21080v1,http://arxiv.org/pdf/2508.21080v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Recent Advances in Transformer and Large Language Models for UAV
  Applications","The rapid advancement of Transformer-based models has reshaped the landscape
of uncrewed aerial vehicle (UAV) systems by enhancing perception,
decision-making, and autonomy. This review paper systematically categorizes and
evaluates recent developments in Transformer architectures applied to UAVs,
including attention mechanisms, CNN-Transformer hybrids, reinforcement learning
Transformers, and large language models (LLMs). Unlike previous surveys, this
work presents a unified taxonomy of Transformer-based UAV models, highlights
emerging applications such as precision agriculture and autonomous navigation,
and provides comparative analyses through structured tables and performance
benchmarks. The paper also reviews key datasets, simulators, and evaluation
metrics used in the field. Furthermore, it identifies existing gaps in the
literature, outlines critical challenges in computational efficiency and
real-time deployment, and offers future research directions. This comprehensive
synthesis aims to guide researchers and practitioners in understanding and
advancing Transformer-driven UAV technologies.","Hamza Kheddar, Yassine Habchi, Mohamed Chahine Ghanem, Mustapha Hemis, Dusit Niyato",2025-08-15T22:56:37Z,2025-08-15T22:56:37Z,http://arxiv.org/abs/2508.11834v1,http://arxiv.org/pdf/2508.11834v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Nominal Evaluation Of Automatic Multi-Sections Control Potential In
  Comparison To A Simpler One- Or Two-Sections Alternative With Predictive
  Spray Switching","Automatic Section Control (ASC) is a long-standing trend for spraying in
agriculture. It promises to minimise spray overlap areas. The core idea is to
(i) switch off spray nozzles on areas that have already been sprayed, and (ii)
to dynamically adjust nozzle flow rates along the boom bar that holds the spray
nozzles when velocities of boom sections vary during turn maneuvers. ASC is not
possible without sensors, in particular for accurate positioning data. Spraying
and the movement of modern wide boom bars are highly dynamic processes. In
addition, many uncertainty factors have an effect such as cross wind drift,
boom height, nozzle clogging in open-field conditions, and so forth. In view of
this complexity, the natural question arises if a simpler alternative exist.
Therefore, an Automatic Multi-Sections Control method is compared to a proposed
simpler one- or two-sections alternative that uses predictive spray switching.
The comparison is provided under nominal conditions. Agricultural spraying is
intrinsically linked to area coverage path planning and spray switching logic.
Combinations of two area coverage path planning and switching logics as well as
three sections-setups are compared. The three sections-setups differ by
controlling 48 sections, 2 sections or controlling all nozzles uniformly with
the same control signal as one single section. Methods are evaluated on 10
diverse real-world field examples, including non-convex field contours,
freeform mainfield lanes and multiple obstacle areas. A preferred method is
suggested that (i) minimises area coverage pathlength, (ii) offers intermediate
overlap, (iii) is suitable for manual driving by following a pre-planned
predictive spray switching logic for an area coverage path plan, and (iv) and
in contrast to ASC can be implemented sensor-free and therefore at low cost.",Mogens Plessen,2025-08-15T16:27:44Z,2025-08-15T16:27:44Z,http://arxiv.org/abs/2508.11573v1,http://arxiv.org/pdf/2508.11573v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"RayletDF: Raylet Distance Fields for Generalizable 3D Surface
  Reconstruction from Point Clouds or Gaussians","In this paper, we present a generalizable method for 3D surface
reconstruction from raw point clouds or pre-estimated 3D Gaussians by 3DGS from
RGB images. Unlike existing coordinate-based methods which are often
computationally intensive when rendering explicit surfaces, our proposed
method, named RayletDF, introduces a new technique called raylet distance
field, which aims to directly predict surface points from query rays. Our
pipeline consists of three key modules: a raylet feature extractor, a raylet
distance field predictor, and a multi-raylet blender. These components work
together to extract fine-grained local geometric features, predict raylet
distances, and aggregate multiple predictions to reconstruct precise surface
points. We extensively evaluate our method on multiple public real-world
datasets, demonstrating superior performance in surface reconstruction from
point clouds or 3D Gaussians. Most notably, our method achieves exceptional
generalization ability, successfully recovering 3D surfaces in a single-forward
pass across unseen datasets in testing.","Shenxing Wei, Jinxi Li, Yafei Yang, Siyuan Zhou, Bo Yang",2025-08-13T14:05:21Z,2025-08-13T14:05:21Z,http://arxiv.org/abs/2508.09830v1,http://arxiv.org/pdf/2508.09830v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Surg-InvNeRF: Invertible NeRF for 3D tracking and reconstruction in
  surgical vision","We proposed a novel test-time optimisation (TTO) approach framed by a
NeRF-based architecture for long-term 3D point tracking. Most current methods
in point tracking struggle to obtain consistent motion or are limited to 2D
motion. TTO approaches frame the solution for long-term tracking as optimising
a function that aggregates correspondences from other specialised
state-of-the-art methods. Unlike the state-of-the-art on TTO, we propose
parametrising such a function with our new invertible Neural Radiance Field
(InvNeRF) architecture to perform both 2D and 3D tracking in surgical
scenarios. Our approach allows us to exploit the advantages of a
rendering-based approach by supervising the reprojection of pixel
correspondences. It adapts strategies from recent rendering-based methods to
obtain a bidirectional deformable-canonical mapping, to efficiently handle a
defined workspace, and to guide the rays' density. It also presents our
multi-scale HexPlanes for fast inference and a new algorithm for efficient
pixel sampling and convergence criteria. We present results in the STIR and
SCARE datasets, for evaluating point tracking and testing the integration of
kinematic data in our pipeline, respectively. In 2D point tracking, our
approach surpasses the precision and accuracy of the TTO state-of-the-art
methods by nearly 50% on average precision, while competing with other
approaches. In 3D point tracking, this is the first TTO approach, surpassing
feed-forward methods while incorporating the benefits of a deformable
NeRF-based reconstruction.","Gerardo Loza, Junlei Hu, Dominic Jones, Sharib Ali, Pietro Valdastri",2025-08-13T10:20:24Z,2025-08-13T10:20:24Z,http://arxiv.org/abs/2508.09681v1,http://arxiv.org/pdf/2508.09681v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Distilling LLM Prior to Flow Model for Generalizable Agent's Imagination
  in Object Goal Navigation","The Object Goal Navigation (ObjectNav) task challenges agents to locate a
specified object in an unseen environment by imagining unobserved regions of
the scene. Prior approaches rely on deterministic and discriminative models to
complete semantic maps, overlooking the inherent uncertainty in indoor layouts
and limiting their ability to generalize to unseen environments. In this work,
we propose GOAL, a generative flow-based framework that models the semantic
distribution of indoor environments by bridging observed regions with
LLM-enriched full-scene semantic maps. During training, spatial priors inferred
from large language models (LLMs) are encoded as two-dimensional Gaussian
fields and injected into target maps, distilling rich contextual knowledge into
the flow model and enabling more generalizable completions. Extensive
experiments demonstrate that GOAL achieves state-of-the-art performance on MP3D
and Gibson, and shows strong generalization in transfer settings to HM3D. Codes
and pretrained models are available at https://github.com/Badi-Li/GOAL.","Badi Li, Ren-jie Lu, Yu Zhou, Jingke Meng, Wei-shi Zheng",2025-08-13T01:57:48Z,2025-08-13T01:57:48Z,http://arxiv.org/abs/2508.09423v1,http://arxiv.org/pdf/2508.09423v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Decision-Making-Based Path Planning for Autonomous UAVs: A Survey,"One of the most critical features for the successful operation of autonomous
UAVs is the ability to make decisions based on the information acquired from
their surroundings. Each UAV must be able to make decisions during the flight
in order to deal with uncertainties in its system and the environment, and to
further act upon the information being received. Such decisions influence the
future behavior of the UAV, which is expressed as the path plan. Thus,
decision-making in path planning is an enabling technique for deploying
autonomous UAVs in real-world applications. This survey provides an overview of
existing studies that use aspects of decision-making in path planning,
presenting the research strands for Exploration Path Planning and Informative
Path Planning, and focusing on characteristics of how data have been modeled
and understood. Finally, we highlight the existing challenges for relevant
topics in this field.","Kelen C. Teixeira Vivaldini, Robert Pěnička, Martin Saska",2025-08-12T19:38:33Z,2025-08-12T19:38:33Z,http://arxiv.org/abs/2508.09304v1,http://arxiv.org/pdf/2508.09304v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Risk Map As Middleware: Towards Interpretable Cooperative End-to-end
  Autonomous Driving for Risk-Aware Planning","End-to-end paradigm has emerged as a promising approach to autonomous
driving. However, existing single-agent end-to-end pipelines are often
constrained by occlusion and limited perception range, resulting in hazardous
driving. Furthermore, their black-box nature prevents the interpretability of
the driving behavior, leading to an untrustworthiness system. To address these
limitations, we introduce Risk Map as Middleware (RiskMM) and propose an
interpretable cooperative end-to-end driving framework. The risk map learns
directly from the driving data and provides an interpretable spatiotemporal
representation of the scenario from the upstream perception and the
interactions between the ego vehicle and the surrounding environment for
downstream planning. RiskMM first constructs a multi-agent spatiotemporal
representation with unified Transformer-based architecture, then derives
risk-aware representations by modeling interactions among surrounding
environments with attention. These representations are subsequently fed into a
learning-based Model Predictive Control (MPC) module. The MPC planner
inherently accommodates physical constraints and different vehicle types and
can provide interpretation by aligning learned parameters with explicit MPC
elements. Evaluations conducted on the real-world V2XPnP-Seq dataset confirm
that RiskMM achieves superior and robust performance in risk-aware trajectory
planning, significantly enhancing the interpretability of the cooperative
end-to-end driving framework. The codebase will be released to facilitate
future research in this field.","Mingyue Lei, Zewei Zhou, Hongchen Li, Jiaqi Ma, Jia Hu",2025-08-11T07:00:52Z,2025-08-11T07:00:52Z,http://arxiv.org/abs/2508.07686v1,http://arxiv.org/pdf/2508.07686v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Noise-Aware Generative Microscopic Traffic Simulation,"Accurately modeling individual vehicle behavior in microscopic traffic
simulation remains a key challenge in intelligent transportation systems, as it
requires vehicles to realistically generate and respond to complex traffic
phenomena such as phantom traffic jams. While traditional human driver
simulation models offer computational tractability, they do so by abstracting
away the very complexity that defines human driving. On the other hand, recent
advances in infrastructure-mounted camera-based roadway sensing have enabled
the extraction of vehicle trajectory data, presenting an opportunity to shift
toward generative, agent-based models. Yet, a major bottleneck remains: most
existing datasets are either overly sanitized or lack standardization, failing
to reflect the noisy, imperfect nature of real-world sensing. Unlike data from
vehicle-mounted sensors-which can mitigate sensing artifacts like occlusion
through overlapping fields of view and sensor fusion-infrastructure-based
sensors surface a messier, more practical view of challenges that traffic
engineers encounter. To this end, we present the I-24 MOTION Scenario Dataset
(I24-MSD)-a standardized, curated dataset designed to preserve a realistic
level of sensor imperfection, embracing these errors as part of the learning
problem rather than an obstacle to overcome purely from preprocessing. Drawing
from noise-aware learning strategies in computer vision, we further adapt
existing generative models in the autonomous driving community for I24-MSD with
noise-aware loss functions. Our results show that such models not only
outperform traditional baselines in realism but also benefit from explicitly
engaging with, rather than suppressing, data imperfection. We view I24-MSD as a
stepping stone toward a new generation of microscopic traffic simulation that
embraces the real-world challenges and is better aligned with practical needs.","Vindula Jayawardana, Catherine Tang, Junyi Ji, Jonah Philion, Xue Bin Peng, Cathy Wu",2025-08-10T18:41:49Z,2025-08-10T18:41:49Z,http://arxiv.org/abs/2508.07453v1,http://arxiv.org/pdf/2508.07453v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"OmniShape: Zero-Shot Multi-Hypothesis Shape and Pose Estimation in the
  Real World","We would like to estimate the pose and full shape of an object from a single
observation, without assuming known 3D model or category. In this work, we
propose OmniShape, the first method of its kind to enable probabilistic pose
and shape estimation. OmniShape is based on the key insight that shape
completion can be decoupled into two multi-modal distributions: one capturing
how measurements project into a normalized object reference frame defined by
the dataset and the other modelling a prior over object geometries represented
as triplanar neural fields. By training separate conditional diffusion models
for these two distributions, we enable sampling multiple hypotheses from the
joint pose and shape distribution. OmniShape demonstrates compelling
performance on challenging real world datasets. Project website:
https://tri-ml.github.io/omnishape","Katherine Liu, Sergey Zakharov, Dian Chen, Takuya Ikeda, Greg Shakhnarovich, Adrien Gaidon, Rares Ambrus",2025-08-05T17:30:41Z,2025-08-05T17:30:41Z,http://arxiv.org/abs/2508.03669v1,http://arxiv.org/pdf/2508.03669v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Vision-based Navigation of Unmanned Aerial Vehicles in Orchards: An
  Imitation Learning Approach","Autonomous unmanned aerial vehicle (UAV) navigation in orchards presents
significant challenges due to obstacles and GPS-deprived environments. In this
work, we introduce a learning-based approach to achieve vision-based navigation
of UAVs within orchard rows. Our method employs a variational autoencoder
(VAE)-based controller, trained with an intervention-based learning framework
that allows the UAV to learn a visuomotor policy from human experience. We
validate our approach in real orchard environments with a custom-built
quadrotor platform. Field experiments demonstrate that after only a few
iterations of training, the proposed VAE-based controller can autonomously
navigate the UAV based on a front-mounted camera stream. The controller
exhibits strong obstacle avoidance performance, achieves longer flying
distances with less human assistance, and outperforms existing algorithms.
Furthermore, we show that the policy generalizes effectively to novel
environments and maintains competitive performance across varying conditions
and speeds. This research not only advances UAV autonomy but also holds
significant potential for precision agriculture, improving efficiency in
orchard monitoring and management.","Peng Wei, Prabhash Ragbir, Stavros G. Vougioukas, Zhaodan Kong",2025-08-04T17:06:04Z,2025-08-04T17:06:04Z,http://arxiv.org/abs/2508.02617v1,http://arxiv.org/pdf/2508.02617v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"MUTE-DSS: A Digital-Twin-Based Decision Support System for Minimizing
  Underwater Radiated Noise in Ship Voyage Planning","We present a novel MUTE-DSS, a digital-twin-based decision support system for
minimizing underwater radiated noise (URN) during ship voyage planning. It is a
ROS2-centric framework that integrates state-of-the-art acoustic models
combining a semi-empirical reference spectrum for near-field modeling with 3D
ray tracing for propagation losses for far-field modeling, offering real-time
computation of the ship noise signature, alongside a data-driven Southern
resident killer whale distribution model. The proposed DSS performs a two-stage
optimization pipeline: Batch Informed Trees for collision-free ship routing and
a genetic algorithm for adaptive ship speed profiling under voyage constraints
that minimizes cumulative URN exposure to marine mammals. The effectiveness of
MUTE-DSS is demonstrated through case studies of ships operating between the
Strait of Georgia and the Strait of Juan de Fuca, comparing optimized voyages
against baseline trajectories derived from automatic identification system
data. Results show substantial reductions in noise exposure level, up to 7.14
dB, corresponding to approximately an 80.68% reduction in a simplified
scenario, and an average 4.90 dB reduction, corresponding to approximately a
67.6% reduction in a more realistic dynamic setting. These results illustrate
the adaptability and practical utility of the proposed decision support system.","Akash Venkateshwaran, Indu Kant Deo, Rajeev K. Jaiman",2025-08-03T20:02:56Z,2025-08-03T20:02:56Z,http://arxiv.org/abs/2508.01907v1,http://arxiv.org/pdf/2508.01907v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Reducing the gap between general purpose data and aerial images in
  concentrated solar power plants","In the context of Concentrated Solar Power (CSP) plants, aerial images
captured by drones present a unique set of challenges. Unlike urban or natural
landscapes commonly found in existing datasets, solar fields contain highly
reflective surfaces, and domain-specific elements that are uncommon in
traditional computer vision benchmarks. As a result, machine learning models
trained on generic datasets struggle to generalize to this setting without
extensive retraining and large volumes of annotated data. However, collecting
and labeling such data is costly and time-consuming, making it impractical for
rapid deployment in industrial applications.
  To address this issue, we propose a novel approach: the creation of
AerialCSP, a virtual dataset that simulates aerial imagery of CSP plants. By
generating synthetic data that closely mimic real-world conditions, our
objective is to facilitate pretraining of models before deployment,
significantly reducing the need for extensive manual labeling. Our main
contributions are threefold: (1) we introduce AerialCSP, a high-quality
synthetic dataset for aerial inspection of CSP plants, providing annotated data
for object detection and image segmentation; (2) we benchmark multiple models
on AerialCSP, establishing a baseline for CSP-related vision tasks; and (3) we
demonstrate that pretraining on AerialCSP significantly improves real-world
fault detection, particularly for rare and small defects, reducing the need for
extensive manual labeling. AerialCSP is made publicly available at
https://mpcutino.github.io/aerialcsp/.","M. A. Pérez-Cutiño, J. Valverde, J. Capitán, J. M. Díaz-Báñez",2025-08-01T08:57:02Z,2025-08-01T08:57:02Z,http://arxiv.org/abs/2508.00440v1,http://arxiv.org/pdf/2508.00440v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Online Estimation of Table-Top Grown Strawberry Mass in Field Conditions
  with Occlusions","Accurate mass estimation of table-top grown strawberries under field
conditions remains challenging due to frequent occlusions and pose variations.
This study proposes a vision-based pipeline integrating RGB-D sensing and deep
learning to enable non-destructive, real-time and online mass estimation. The
method employed YOLOv8-Seg for instance segmentation, Cycle-consistent
generative adversarial network (CycleGAN) for occluded region completion, and
tilt-angle correction to refine frontal projection area calculations. A
polynomial regression model then mapped the geometric features to mass.
Experiments demonstrated mean mass estimation errors of 8.11% for isolated
strawberries and 10.47% for occluded cases. CycleGAN outperformed large mask
inpainting (LaMa) model in occlusion recovery, achieving superior pixel area
ratios (PAR) (mean: 0.978 vs. 1.112) and higher intersection over union (IoU)
scores (92.3% vs. 47.7% in the [0.9-1] range). This approach addresses critical
limitations of traditional methods, offering a robust solution for automated
harvesting and yield monitoring with complex occlusion patterns.","Jinshan Zhen, Yuanyue Ge, Tianxiao Zhu, Hui Zhao, Ya Xiong",2025-07-31T12:10:23Z,2025-07-31T12:10:23Z,http://arxiv.org/abs/2507.23487v1,http://arxiv.org/pdf/2507.23487v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Optimizing Spreading Factor Selection for Mobile LoRa Gateways Using
  Single-Channel Hardware","The deployment of mobile LoRa gateways using low-cost single-channel hardware
presents a significant challenge in maintaining reliable communication due to
the lack of dynamic configuration support. In traditional LoRaWAN networks,
Adaptive Data Rate (ADR) mechanisms optimize communication parameters in real
time. However, such features are typically supported only by expensive
multi-channel gateways. This study proposes a cost-effective and
energy-efficient solution by statically selecting the optimal Spreading Factor
(SF) using a two-phase algorithm. The method first applies rule-based exclusion
to eliminate SFs that violate constraints related to distance, data rate, link
margin, and regulatory limits. Remaining candidates are then evaluated using a
weighted scoring model incorporating Time-on-Air, energy consumption, data
rate, and link robustness. The proposed algorithm was validated through
extensive field tests and NS-3 simulations under line-of-sight conditions.
Results demonstrate that the selected SF matched the optimal SF in over 92% of
cases across 672 simulated scenarios, confirming the algorithm's effectiveness.
This approach offers a scalable alternative to dynamic protocols, enabling
reliable mobile LoRa deployments in cost-sensitive environments such as
agriculture and rural sensing applications.",W. A. Sasindu Wijesuriya,2025-07-26T12:54:11Z,2025-07-26T12:54:11Z,http://arxiv.org/abs/2507.19938v1,http://arxiv.org/pdf/2507.19938v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"SmartPNT-MSF: A Multi-Sensor Fusion Dataset for Positioning and
  Navigation Research","High-precision navigation and positioning systems are critical for
applications in autonomous vehicles and mobile mapping, where robust and
continuous localization is essential. To test and enhance the performance of
algorithms, some research institutions and companies have successively
constructed and publicly released datasets. However, existing datasets still
suffer from limitations in sensor diversity and environmental coverage. To
address these shortcomings and advance development in related fields, the
SmartPNT Multisource Integrated Navigation, Positioning, and Attitude Dataset
has been developed. This dataset integrates data from multiple sensors,
including Global Navigation Satellite Systems (GNSS), Inertial Measurement
Units (IMU), optical cameras, and LiDAR, to provide a rich and versatile
resource for research in multi-sensor fusion and high-precision navigation. The
dataset construction process is thoroughly documented, encompassing sensor
configurations, coordinate system definitions, and calibration procedures for
both cameras and LiDAR. A standardized framework for data collection and
processing ensures consistency and scalability, enabling large-scale analysis.
Validation using state-of-the-art Simultaneous Localization and Mapping (SLAM)
algorithms, such as VINS-Mono and LIO-SAM, demonstrates the dataset's
applicability for advanced navigation research. Covering a wide range of
real-world scenarios, including urban areas, campuses, tunnels, and suburban
environments, the dataset offers a valuable tool for advancing navigation
technologies and addressing challenges in complex environments. By providing a
publicly accessible, high-quality dataset, this work aims to bridge gaps in
sensor diversity, data accessibility, and environmental representation,
fostering further innovation in the field.","Feng Zhu, Zihang Zhang, Kangcheng Teng, Abduhelil Yakup, Xiaohong Zhang",2025-07-25T09:06:11Z,2025-07-31T05:59:58Z,http://arxiv.org/abs/2507.19079v2,http://arxiv.org/pdf/2507.19079v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Equivariant Volumetric Grasping,"We propose a new volumetric grasp model that is equivariant to rotations
around the vertical axis, leading to a significant improvement in sample
efficiency. Our model employs a tri-plane volumetric feature representation --
i.e., the projection of 3D features onto three canonical planes. We introduce a
novel tri-plane feature design in which features on the horizontal plane are
equivariant to 90{\deg} rotations, while the sum of features from the other two
planes remains invariant to the same transformations. This design is enabled by
a new deformable steerable convolution, which combines the adaptability of
deformable convolutions with the rotational equivariance of steerable ones.
This allows the receptive field to adapt to local object geometry while
preserving equivariance properties. We further develop equivariant adaptations
of two state-of-the-art volumetric grasp planners, GIGA and IGD. Specifically,
we derive a new equivariant formulation of IGD's deformable attention mechanism
and propose an equivariant generative model of grasp orientations based on flow
matching. We provide a detailed analytical justification of the proposed
equivariance properties and validate our approach through extensive simulated
and real-world experiments. Our results demonstrate that the proposed
projection-based design significantly reduces both computational and memory
costs. Moreover, the equivariant grasp models built on top of our tri-plane
features consistently outperform their non-equivariant counterparts, achieving
higher performance with only a modest computational overhead. Video and code
can be viewed in: https://mousecpn.github.io/evg-page/","Pinhao Song, Yutong Hu, Pengteng Li, Renaud Detry",2025-07-24T23:18:32Z,2025-08-05T12:56:02Z,http://arxiv.org/abs/2507.18847v2,http://arxiv.org/pdf/2507.18847v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
SaLF: Sparse Local Fields for Multi-Sensor Rendering in Real-Time,"High-fidelity sensor simulation of light-based sensors such as cameras and
LiDARs is critical for safe and accurate autonomy testing. Neural radiance
field (NeRF)-based methods that reconstruct sensor observations via ray-casting
of implicit representations have demonstrated accurate simulation of driving
scenes, but are slow to train and render, hampering scale. 3D Gaussian
Splatting (3DGS) has demonstrated faster training and rendering times through
rasterization, but is primarily restricted to pinhole camera sensors,
preventing usage for realistic multi-sensor autonomy evaluation. Moreover, both
NeRF and 3DGS couple the representation with the rendering procedure (implicit
networks for ray-based evaluation, particles for rasterization), preventing
interoperability, which is key for general usage. In this work, we present
Sparse Local Fields (SaLF), a novel volumetric representation that supports
rasterization and raytracing. SaLF represents volumes as a sparse set of 3D
voxel primitives, where each voxel is a local implicit field. SaLF has fast
training (<30 min) and rendering capabilities (50+ FPS for camera and 600+ FPS
LiDAR), has adaptive pruning and densification to easily handle large scenes,
and can support non-pinhole cameras and spinning LiDARs. We demonstrate that
SaLF has similar realism as existing self-driving sensor simulation methods
while improving efficiency and enhancing capabilities, enabling more scalable
simulation. https://waabi.ai/salf/","Yun Chen, Matthew Haines, Jingkang Wang, Krzysztof Baron-Lis, Sivabalan Manivasagam, Ze Yang, Raquel Urtasun",2025-07-24T18:01:22Z,2025-07-24T18:01:22Z,http://arxiv.org/abs/2507.18713v1,http://arxiv.org/pdf/2507.18713v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Autonomous UAV Navigation for Search and Rescue Missions Using Computer
  Vision and Convolutional Neural Networks","In this paper, we present a subsystem, using Unmanned Aerial Vehicles (UAV),
for search and rescue missions, focusing on people detection, face recognition
and tracking of identified individuals. The proposed solution integrates a UAV
with ROS2 framework, that utilizes multiple convolutional neural networks (CNN)
for search missions. System identification and PD controller deployment are
performed for autonomous UAV navigation. The ROS2 environment utilizes the
YOLOv11 and YOLOv11-pose CNNs for tracking purposes, and the dlib library CNN
for face recognition. The system detects a specific individual, performs face
recognition and starts tracking. If the individual is not yet known, the UAV
operator can manually locate the person, save their facial image and
immediately initiate the tracking process. The tracking process relies on
specific keypoints identified on the human body using the YOLOv11-pose CNN
model. These keypoints are used to track a specific individual and maintain a
safe distance. To enhance accurate tracking, system identification is
performed, based on measurement data from the UAVs IMU. The identified system
parameters are used to design PD controllers that utilize YOLOv11-pose to
estimate the distance between the UAVs camera and the identified individual.
The initial experiments, conducted on 14 known individuals, demonstrated that
the proposed subsystem can be successfully used in real time. The next step
involves implementing the system on a large experimental UAV for field use and
integrating autonomous navigation with GPS-guided control for rescue operations
planning.","Luka Šiktar, Branimir Ćaran, Bojan Šekoranja, Marko Švaco",2025-07-24T07:54:45Z,2025-07-24T07:54:45Z,http://arxiv.org/abs/2507.18160v1,http://arxiv.org/pdf/2507.18160v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"CA-Cut: Crop-Aligned Cutout for Data Augmentation to Learn More Robust
  Under-Canopy Navigation","State-of-the-art visual under-canopy navigation methods are designed with
deep learning-based perception models to distinguish traversable space from
crop rows. While these models have demonstrated successful performance, they
require large amounts of training data to ensure reliability in real-world
field deployment. However, data collection is costly, demanding significant
human resources for in-field sampling and annotation. To address this
challenge, various data augmentation techniques are commonly employed during
model training, such as color jittering, Gaussian blur, and horizontal flip, to
diversify training data and enhance model robustness. In this paper, we
hypothesize that utilizing only these augmentation techniques may lead to
suboptimal performance, particularly in complex under-canopy environments with
frequent occlusions, debris, and non-uniform spacing of crops. Instead, we
propose a novel augmentation method, so-called Crop-Aligned Cutout (CA-Cut)
which masks random regions out in input images that are spatially distributed
around crop rows on the sides to encourage trained models to capture high-level
contextual features even when fine-grained information is obstructed. Our
extensive experiments with a public cornfield dataset demonstrate that
masking-based augmentations are effective for simulating occlusions and
significantly improving robustness in semantic keypoint predictions for visual
navigation. In particular, we show that biasing the mask distribution toward
crop rows in CA-Cut is critical for enhancing both prediction accuracy and
generalizability across diverse environments achieving up to a 36.9% reduction
in prediction error. In addition, we conduct ablation studies to determine the
number of masks, the size of each mask, and the spatial distribution of masks
to maximize overall performance.","Robel Mamo, Taeyeong Choi",2025-07-23T17:41:55Z,2025-07-24T13:55:49Z,http://arxiv.org/abs/2507.17727v2,http://arxiv.org/pdf/2507.17727v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
IONext: Unlocking the Next Era of Inertial Odometry,"Researchers have increasingly adopted Transformer-based models for inertial
odometry. While Transformers excel at modeling long-range dependencies, their
limited sensitivity to local, fine-grained motion variations and lack of
inherent inductive biases often hinder localization accuracy and
generalization. Recent studies have shown that incorporating large-kernel
convolutions and Transformer-inspired architectural designs into CNN can
effectively expand the receptive field, thereby improving global motion
perception. Motivated by these insights, we propose a novel CNN-based module
called the Dual-wing Adaptive Dynamic Mixer (DADM), which adaptively captures
both global motion patterns and local, fine-grained motion features from
dynamic inputs. This module dynamically generates selective weights based on
the input, enabling efficient multi-scale feature aggregation. To further
improve temporal modeling, we introduce the Spatio-Temporal Gating Unit (STGU),
which selectively extracts representative and task-relevant motion features in
the temporal domain. This unit addresses the limitations of temporal modeling
observed in existing CNN approaches. Built upon DADM and STGU, we present a new
CNN-based inertial odometry backbone, named Next Era of Inertial Odometry
(IONext). Extensive experiments on six public datasets demonstrate that IONext
consistently outperforms state-of-the-art (SOTA) Transformer- and CNN-based
methods. For instance, on the RNIN dataset, IONext reduces the average ATE by
10% and the average RTE by 12% compared to the representative model iMOT.","Shanshan Zhang, Siyue Wang, Tianshui Wen, Qi Zhang, Ziheng Zhou, Lingxiang Zheng, Yu Yang",2025-07-23T00:09:36Z,2025-07-23T00:09:36Z,http://arxiv.org/abs/2507.17089v1,http://arxiv.org/pdf/2507.17089v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Distributed Oscillatory Guidance for Formation Flight of Fixed-Wing
  Drones","The autonomous formation flight of fixed-wing drones is hard when the
coordination requires the actuation over their speeds since they are critically
bounded and aircraft are mostly designed to fly at a nominal airspeed. This
paper proposes an algorithm to achieve formation flights of fixed-wing drones
without requiring any actuation over their speed. In particular, we guide all
the drones to travel over specific paths, e.g., parallel straight lines, and we
superpose an oscillatory behavior onto the guiding vector field that drives the
drones to the paths. This oscillation enables control over the average velocity
along the path, thereby facilitating inter-drone coordination. Each drone
adjusts its oscillation amplitude distributively in a closed-loop manner by
communicating with neighboring agents in an undirected and connected graph. A
novel consensus algorithm is introduced, leveraging a non-negative, asymmetric
saturation function. This unconventional saturation is justified since negative
amplitudes do not make drones travel backward or have a negative velocity along
the path. Rigorous theoretical analysis of the algorithm is complemented by
validation through numerical simulations and a real-world formation flight.","Yang Xu, Jesús Bautista, José Hinojosa, Héctor García de Marina",2025-07-22T11:00:31Z,2025-07-22T11:00:31Z,http://arxiv.org/abs/2507.16458v1,http://arxiv.org/pdf/2507.16458v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"DWSFormer: A Lightweight Inertial Odometry Network for Complex Motion
  Modeling","Inertial odometry (IO) directly estimates the position of a carrier from
inertial sensor measurements and serves as a core technology for the widespread
deployment of consumer grade localization systems. While existing IO methods
can accurately reconstruct simple and near linear motion trajectories, they
often fail to account for drift errors caused by complex motion patterns such
as turning. This limitation significantly degrades localization accuracy and
restricts the applicability of IO systems in real world scenarios. To address
these challenges, we propose a lightweight IO framework. Specifically, inertial
data is projected into a high dimensional implicit nonlinear feature space
using the Star Operation method, enabling the extraction of complex motion
features that are typically overlooked. We further introduce a collaborative
attention mechanism that jointly models global motion dynamics across both
channel and temporal dimensions. In addition, we design Multi Scale Gated
Convolution Units to capture fine grained dynamic variations throughout the
motion process, thereby enhancing the model's ability to learn rich and
expressive motion representations. Extensive experiments demonstrate that our
proposed method consistently outperforms SOTA baselines across six widely used
inertial datasets. Compared to baseline models on the RoNIN dataset, it
achieves reductions in ATE ranging from 2.26% to 65.78%, thereby establishing a
new benchmark in the field.","Shanshan Zhang, Qi Zhang, Siyue Wang, Tianshui Wen, Ziheng Zhou, Lingxiang Zheng, Yu Yang",2025-07-22T00:28:58Z,2025-07-22T00:28:58Z,http://arxiv.org/abs/2507.16121v1,http://arxiv.org/pdf/2507.16121v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Leveraging multi-source and heterogeneous signals for fatigue detection,"Fatigue detection plays a critical role in safety-critical applications such
as aviation, mining, and long-haul transport. However, most existing methods
rely on high-end sensors and controlled environments, limiting their
applicability in real world settings. This paper formally defines a practical
yet underexplored problem setting for real world fatigue detection, where
systems operating with context-appropriate sensors aim to leverage knowledge
from differently instrumented sources including those using impractical sensors
deployed in controlled environments. To tackle this challenge, we propose a
heterogeneous and multi-source fatigue detection framework that adaptively
utilizes the available modalities in the target domain while benefiting from
the diverse configurations present in source domains. Our experiments,
conducted using a realistic field-deployed sensor setup and two publicly
available datasets, demonstrate the practicality, robustness, and improved
generalization of our approach, paving the practical way for effective fatigue
monitoring in sensor-constrained scenarios.","Luobin Cui, Yanlai Wu, Tang Ying, Weikai Li",2025-07-21T17:22:18Z,2025-07-24T14:41:42Z,http://arxiv.org/abs/2507.16859v2,http://arxiv.org/pdf/2507.16859v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Improving Functional Reliability of Near-Field Monitoring for Emergency
  Braking in Autonomous Vehicles","Autonomous vehicles require reliable hazard detection. However, primary
sensor systems may miss near-field obstacles, resulting in safety risks.
Although a dedicated fast-reacting near-field monitoring system can mitigate
this, it typically suffers from false positives. To mitigate these, in this
paper, we introduce three monitoring strategies based on dynamic spatial
properties, relevant object sizes, and motion-aware prediction. In experiments
in a validated simulation, we compare the initial monitoring strategy against
the proposed improvements. The results demonstrate that the proposed strategies
can significantly improve the reliability of near-field monitoring systems.","Junnan Pan, Prodromos Sotiriadis, Vladislav Nenchev, Ferdinand Englberger",2025-07-21T13:17:44Z,2025-07-21T13:17:44Z,http://arxiv.org/abs/2507.15594v1,http://arxiv.org/pdf/2507.15594v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Analytical Formulation of Autonomous Vehicle Freeway Merging Control
  with State-Dependent Discharge Rates","The core of the freeway merging control problem lies in dynamic queue
propagation and dissipation linked to merging vehicle behavior. Traditionally,
queuing is modeled through demand-supply interactions with time varying demand
and fixed capacity. However, field observations show flow rates decrease during
congestion at freeway merges due to the impact of intersecting traffic, a
factor overlooked in fundamental diagrams. This manuscript introduces an
analytical approach to characterize and control the dynamic multi-stage merging
of autonomous vehicles, prioritizing traffic efficiency and safety. For the
first time, the effective discharge rate at the merging point, reduced by the
multi-stage dynamic merging process, is analytically derived using a closed
form formulation. Leveraging this expression, performance metrics such as queue
length and traffic delay are derived as the first objective. Additionally, a
crash risk function is established to quantitatively assess potential
collisions during the merging process, serving as the second objective.
Finally, the problem is formulated as a dynamic programming model to jointly
minimize delay and crash risk, with the merging location and speed as decision
variables. Given the terminal state, the ramp vehicle merging task is
formulated as a recursive optimization problem, employing backward induction to
find the minimum cost solution. Numerical experiments using the NGSIM dataset
validate the derived effective discharge rate. The results indicate that the
proposed model outperforms two benchmark algorithms, leading to a more
efficient and safer merging process.","Qing Tang, Xianbiao Hu",2025-07-20T19:19:38Z,2025-07-20T19:19:38Z,http://arxiv.org/abs/2507.16846v1,http://arxiv.org/pdf/2507.16846v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"A multi-strategy improved snake optimizer for three-dimensional UAV path
  planning and engineering problems","Metaheuristic algorithms have gained widespread application across various
fields owing to their ability to generate diverse solutions. One such algorithm
is the Snake Optimizer (SO), a progressive optimization approach. However, SO
suffers from the issues of slow convergence speed and susceptibility to local
optima. In light of these shortcomings, we propose a novel Multi-strategy
Improved Snake Optimizer (MISO). Firstly, we propose a new adaptive random
disturbance strategy based on sine function to alleviate the risk of getting
trapped in a local optimum. Secondly, we introduce adaptive Levy flight
strategy based on scale factor and leader and endow the male snake leader with
flight capability, which makes it easier for the algorithm to leap out of the
local optimum and find the global optimum. More importantly, we put forward a
position update strategy combining elite leadership and Brownian motion,
effectively accelerating the convergence speed while ensuring precision.
Finally, to demonstrate the performance of MISO, we utilize 30 CEC2017 test
functions and the CEC2022 test suite, comparing it with 11 popular algorithms
across different dimensions to validate its effectiveness. Moreover, Unmanned
Aerial Vehicle (UAV) has been widely used in various fields due to its
advantages of low cost, high mobility and easy operation. However, the UAV path
planning problem is crucial for flight safety and efficiency, and there are
still challenges in establishing and optimizing the path model. Therefore, we
apply MISO to the UAV 3D path planning problem as well as 6 engineering design
problems to assess its feasibility in practical applications. The experimental
results demonstrate that MISO exceeds other competitive algorithms in terms of
solution quality and stability, establishing its strong potential for
application.","Genliang Li, Yaxin Cui, Jinyu Su",2025-07-18T16:11:35Z,2025-08-13T14:12:28Z,http://arxiv.org/abs/2507.14043v2,http://arxiv.org/pdf/2507.14043v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"UniLGL: Learning Uniform Place Recognition for FOV-limited/Panoramic
  LiDAR Global Localization","Existing LGL methods typically consider only partial information (e.g.,
geometric features) from LiDAR observations or are designed for homogeneous
LiDAR sensors, overlooking the uniformity in LGL. In this work, a uniform LGL
method is proposed, termed UniLGL, which simultaneously achieves spatial and
material uniformity, as well as sensor-type uniformity. The key idea of the
proposed method is to encode the complete point cloud, which contains both
geometric and material information, into a pair of BEV images (i.e., a spatial
BEV image and an intensity BEV image). An end-to-end multi-BEV fusion network
is designed to extract uniform features, equipping UniLGL with spatial and
material uniformity. To ensure robust LGL across heterogeneous LiDAR sensors, a
viewpoint invariance hypothesis is introduced, which replaces the conventional
translation equivariance assumption commonly used in existing LPR networks and
supervises UniLGL to achieve sensor-type uniformity in both global descriptors
and local feature representations. Finally, based on the mapping between local
features on the 2D BEV image and the point cloud, a robust global pose
estimator is derived that determines the global minimum of the global pose on
SE(3) without requiring additional registration. To validate the effectiveness
of the proposed uniform LGL, extensive benchmarks are conducted in real-world
environments, and the results show that the proposed UniLGL is demonstratively
competitive compared to other State-of-the-Art LGL methods. Furthermore, UniLGL
has been deployed on diverse platforms, including full-size trucks and agile
Micro Aerial Vehicles (MAVs), to enable high-precision localization and mapping
as well as multi-MAV collaborative exploration in port and forest environments,
demonstrating the applicability of UniLGL in industrial and field scenarios.","Hongming Shen, Xun Chen, Yulin Hui, Zhenyu Wu, Wei Wang, Qiyang Lyu, Tianchen Deng, Danwei Wang",2025-07-16T12:45:56Z,2025-07-31T11:57:29Z,http://arxiv.org/abs/2507.12194v2,http://arxiv.org/pdf/2507.12194v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Physically Based Neural LiDAR Resimulation,"Methods for Novel View Synthesis (NVS) have recently found traction in the
field of LiDAR simulation and large-scale 3D scene reconstruction. While
solutions for faster rendering or handling dynamic scenes have been proposed,
LiDAR specific effects remain insufficiently addressed. By explicitly modeling
sensor characteristics such as rolling shutter, laser power variations, and
intensity falloff, our method achieves more accurate LiDAR simulation compared
to existing techniques. We demonstrate the effectiveness of our approach
through quantitative and qualitative comparisons with state-of-the-art methods,
as well as ablation studies that highlight the importance of each sensor model
component. Beyond that, we show that our approach exhibits advanced
resimulation capabilities, such as generating high resolution LiDAR scans in
the camera perspective.
  Our code and the resulting dataset are available at
https://github.com/richardmarcus/PBNLiDAR.","Richard Marcus, Marc Stamminger",2025-07-15T19:49:44Z,2025-07-15T19:49:44Z,http://arxiv.org/abs/2507.12489v1,http://arxiv.org/pdf/2507.12489v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Uncertainty Aware Mapping for Vision-Based Underwater Robots,"Vision-based underwater robots can be useful in inspecting and exploring
confined spaces where traditional sensors and preplanned paths cannot be
followed. Sensor noise and situational change can cause significant uncertainty
in environmental representation. Thus, this paper explores how to represent
mapping inconsistency in vision-based sensing and incorporate depth estimation
confidence into the mapping framework. The scene depth and the confidence are
estimated using the RAFT-Stereo model and are integrated into a voxel-based
mapping framework, Voxblox. Improvements in the existing Voxblox weight
calculation and update mechanism are also proposed. Finally, a qualitative
analysis of the proposed method is performed in a confined pool and in a pier
in the Trondheim fjord. Experiments using an underwater robot demonstrated the
change in uncertainty in the visualization.","Abhimanyu Bhowmik, Mohit Singh, Madhushree Sannigrahi, Martin Ludvigsen, Kostas Alexis",2025-07-15T05:09:36Z,2025-07-15T05:09:36Z,http://arxiv.org/abs/2507.10991v1,http://arxiv.org/pdf/2507.10991v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"A Learning Framework For Cooperative Collision Avoidance of UAV Swarms
  Leveraging Domain Knowledge","This paper presents a multi-agent reinforcement learning (MARL) framework for
cooperative collision avoidance of UAV swarms leveraging domain
knowledge-driven reward. The reward is derived from knowledge in the domain of
image processing, approximating contours on a two-dimensional field. By
modeling obstacles as maxima on the field, collisions are inherently avoided as
contours never go through peaks or intersect. Additionally, counters are smooth
and energy-efficient. Our framework enables training with large swarm sizes as
the agent interaction is minimized and the need for complex credit assignment
schemes or observation sharing mechanisms in state-of-the-art MARL approaches
are eliminated. Moreover, UAVs obtain the ability to adapt to complex
environments where contours may be non-viable or non-existent through intensive
training. Extensive experiments are conducted to evaluate the performances of
our framework against state-of-the-art MARL algorithms.","Shuangyao Huang, Haibo Zhang, Zhiyi Huang",2025-07-15T02:09:53Z,2025-07-15T02:09:53Z,http://arxiv.org/abs/2507.10913v1,http://arxiv.org/pdf/2507.10913v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"TOP: Trajectory Optimization via Parallel Optimization towards Constant
  Time Complexity","Optimization has been widely used to generate smooth trajectories for motion
planning. However, existing trajectory optimization methods show weakness when
dealing with large-scale long trajectories. Recent advances in parallel
computing have accelerated optimization in some fields, but how to efficiently
solve trajectory optimization via parallelism remains an open question. In this
paper, we propose a novel trajectory optimization framework based on the
Consensus Alternating Direction Method of Multipliers (CADMM) algorithm, which
decomposes the trajectory into multiple segments and solves the subproblems in
parallel. The proposed framework reduces the time complexity to O(1) per
iteration to the number of segments, compared to O(N) of the state-of-the-art
(SOTA) approaches. Furthermore, we introduce a closed-form solution that
integrates convex linear and quadratic constraints to speed up the
optimization, and we also present numerical solutions for general inequality
constraints. A series of simulations and experiments demonstrate that our
approach outperforms the SOTA approach in terms of efficiency and smoothness.
Especially for a large-scale trajectory, with one hundred segments, achieving
over a tenfold speedup. To fully explore the potential of our algorithm on
modern parallel computing architectures, we deploy our framework on a GPU and
show high performance with thousands of segments.","Jiajun Yu, Nanhe Chen, Guodong Liu, Chao Xu, Fei Gao, Yanjun Cao",2025-07-14T13:56:59Z,2025-07-16T20:42:16Z,http://arxiv.org/abs/2507.10290v2,http://arxiv.org/pdf/2507.10290v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"REACT: Real-time Entanglement-Aware Coverage Path Planning for Tethered
  Underwater Vehicles","Inspection of complex underwater structures with tethered underwater vehicles
is often hindered by the risk of tether entanglement. We propose REACT
(real-time entanglement-aware coverage path planning for tethered underwater
vehicles), a framework designed to overcome this limitation. REACT comprises a
fast geometry-based tether model using the signed distance field (SDF) map for
accurate, real-time simulation of taut tether configurations around arbitrary
structures in 3D. This model enables an efficient online replanning strategy by
enforcing a maximum tether length constraint, thereby actively preventing
entanglement. By integrating REACT into a coverage path planning framework, we
achieve safe and optimal inspection paths, previously challenging due to tether
constraints. The complete REACT framework's efficacy is validated in a pipe
inspection scenario, demonstrating safe, entanglement-free navigation and
full-coverage inspection. Simulation results show that REACT achieves complete
coverage while maintaining tether constraints and completing the total mission
20% faster than conventional planners, despite a longer inspection time due to
proactive avoidance of entanglement that eliminates extensive post-mission
disentanglement. Real-world experiments confirm these benefits, where REACT
completes the full mission, while the baseline planner fails due to physical
tether entanglement.","Abdelhakim Amer, Mohit Mehindratta, Yury Brodskiy, Bilal Wehbe, Erdal Kayacan",2025-07-14T12:18:01Z,2025-07-14T12:18:01Z,http://arxiv.org/abs/2507.10204v1,http://arxiv.org/pdf/2507.10204v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Real-Time Adaptive Motion Planning via Point Cloud-Guided, Energy-Based
  Diffusion and Potential Fields","Motivated by the problem of pursuit-evasion, we present a motion planning
framework that combines energy-based diffusion models with artificial potential
fields for robust real time trajectory generation in complex environments. Our
approach processes obstacle information directly from point clouds, enabling
efficient planning without requiring complete geometric representations. The
framework employs classifier-free guidance training and integrates local
potential fields during sampling to enhance obstacle avoidance. In dynamic
scenarios, the system generates initial trajectories using the diffusion model
and continuously refines them through potential field-based adaptation,
demonstrating effective performance in pursuit-evasion scenarios with partial
pursuer observability.","Wondmgezahu Teshome, Kian Behzad, Octavia Camps, Michael Everett, Milad Siami, Mario Sznaier",2025-07-12T19:42:07Z,2025-07-12T19:42:07Z,http://arxiv.org/abs/2507.09383v1,http://arxiv.org/pdf/2507.09383v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"DLBAcalib: Robust Extrinsic Calibration for Non-Overlapping LiDARs Based
  on Dual LBA","Accurate extrinsic calibration of multiple LiDARs is crucial for improving
the foundational performance of three-dimensional (3D) map reconstruction
systems. This paper presents a novel targetless extrinsic calibration framework
for multi-LiDAR systems that does not rely on overlapping fields of view or
precise initial parameter estimates. Unlike conventional calibration methods
that require manual annotations or specific reference patterns, our approach
introduces a unified optimization framework by integrating LiDAR bundle
adjustment (LBA) optimization with robust iterative refinement. The proposed
method constructs an accurate reference point cloud map via continuous scanning
from the target LiDAR and sliding-window LiDAR bundle adjustment, while
formulating extrinsic calibration as a joint LBA optimization problem. This
method effectively mitigates cumulative mapping errors and achieves
outlier-resistant parameter estimation through an adaptive weighting mechanism.
Extensive evaluations in both the CARLA simulation environment and real-world
scenarios demonstrate that our method outperforms state-of-the-art calibration
techniques in both accuracy and robustness. Experimental results show that for
non-overlapping sensor configurations, our framework achieves an average
translational error of 5 mm and a rotational error of 0.2{\deg}, with an
initial error tolerance of up to 0.4 m/30{\deg}. Moreover, the calibration
process operates without specialized infrastructure or manual parameter tuning.
The code is open source and available on GitHub
(\underline{https://github.com/Silentbarber/DLBAcalib})","Han Ye, Yuqiang Jin, Jinyuan Liu, Tao Li, Wen-An Zhang, Minglei Fu",2025-07-12T07:48:02Z,2025-07-12T07:48:02Z,http://arxiv.org/abs/2507.09176v1,http://arxiv.org/pdf/2507.09176v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Joint Optimization-based Targetless Extrinsic Calibration for Multiple
  LiDARs and GNSS-Aided INS of Ground Vehicles","Accurate extrinsic calibration between multiple LiDAR sensors and a
GNSS-aided inertial navigation system (GINS) is essential for achieving
reliable sensor fusion in intelligent mining environments. Such calibration
enables vehicle-road collaboration by aligning perception data from
vehicle-mounted sensors to a unified global reference frame. However, existing
methods often depend on artificial targets, overlapping fields of view, or
precise trajectory estimation, which are assumptions that may not hold in
practice. Moreover, the planar motion of mining vehicles leads to observability
issues that degrade calibration performance. This paper presents a targetless
extrinsic calibration method that aligns multiple onboard LiDAR sensors to the
GINS coordinate system without requiring overlapping sensor views or external
targets. The proposed approach introduces an observation model based on the
known installation height of the GINS unit to constrain unobservable
calibration parameters under planar motion. A joint optimization framework is
developed to refine both the extrinsic parameters and GINS trajectory by
integrating multiple constraints derived from geometric correspondences and
motion consistency. The proposed method is applicable to heterogeneous LiDAR
configurations, including both mechanical and solid-state sensors. Extensive
experiments on simulated and real-world datasets demonstrate the accuracy,
robustness, and practical applicability of the approach under diverse sensor
setups.","Junhui Wang, Yan Qiao, Chao Gao, Naiqi Wu",2025-07-11T06:48:53Z,2025-07-11T06:48:53Z,http://arxiv.org/abs/2507.08349v1,http://arxiv.org/pdf/2507.08349v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Improving AEBS Validation Through Objective Intervention Classification
  Leveraging the Prediction Divergence Principle","The safety validation of automatic emergency braking system (AEBS) requires
accurately distinguishing between false positive (FP) and true positive (TP)
system activations. While simulations allow straightforward differentiation by
comparing scenarios with and without interventions, analyzing activations from
open-loop resimulations - such as those from field operational testing (FOT) -
is more complex. This complexity arises from scenario parameter uncertainty and
the influence of driver interventions in the recorded data. Human labeling is
frequently used to address these challenges, relying on subjective assessments
of intervention necessity or situational criticality, potentially introducing
biases and limitations. This work proposes a rule-based classification approach
leveraging the Prediction Divergence Principle (PDP) to address those issues.
Applied to a simplified AEBS, the proposed method reveals key strengths,
limitations, and system requirements for effective implementation. The findings
suggest that combining this approach with human labeling may enhance the
transparency and consistency of classification, thereby improving the overall
validation process. While the rule set for classification derived in this work
adopts a conservative approach, the paper outlines future directions for
refinement and broader applicability. Finally, this work highlights the
potential of such methods to complement existing practices, paving the way for
more reliable and reproducible AEBS validation frameworks.","Daniel Betschinske, Steven Peters",2025-07-10T15:55:05Z,2025-07-21T12:23:53Z,http://arxiv.org/abs/2507.07872v2,http://arxiv.org/pdf/2507.07872v2.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Aerial Maritime Vessel Detection and Identification,"Autonomous maritime surveillance and target vessel identification in
environments where Global Navigation Satellite Systems (GNSS) are not available
is critical for a number of applications such as search and rescue and threat
detection. When the target vessel is only described by visual cues and its last
known position is not available, unmanned aerial vehicles (UAVs) must rely
solely on on-board vision to scan a large search area under strict
computational constraints. To address this challenge, we leverage the YOLOv8
object detection model to detect all vessels in the field of view. We then
apply feature matching and hue histogram distance analysis to determine whether
any detected vessel corresponds to the target. When found, we localize the
target using simple geometric principles. We demonstrate the proposed method in
real-world experiments during the MBZIRC2023 competition, integrated into a
fully autonomous system with GNSS-denied navigation. We also evaluate the
impact of perspective on detection accuracy and localization precision and
compare it with the oracle approach.","Antonella Barisic Kulas, Frano Petric, Stjepan Bogdan",2025-07-09T11:43:02Z,2025-07-09T11:43:02Z,http://arxiv.org/abs/2507.07153v1,http://arxiv.org/pdf/2507.07153v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"VisioPath: Vision-Language Enhanced Model Predictive Control for Safe
  Autonomous Navigation in Mixed Traffic","In this paper, we introduce VisioPath, a novel framework combining
vision-language models (VLMs) with model predictive control (MPC) to enable
safe autonomous driving in dynamic traffic environments. The proposed approach
leverages a bird's-eye view video processing pipeline and zero-shot VLM
capabilities to obtain structured information about surrounding vehicles,
including their positions, dimensions, and velocities. Using this rich
perception output, we construct elliptical collision-avoidance potential fields
around other traffic participants, which are seamlessly integrated into a
finite-horizon optimal control problem for trajectory planning. The resulting
trajectory optimization is solved via differential dynamic programming with an
adaptive regularization scheme and is embedded in an event-triggered MPC loop.
To ensure collision-free motion, a safety verification layer is incorporated in
the framework that provides an assessment of potential unsafe trajectories.
Extensive simulations in Simulation of Urban Mobility (SUMO) demonstrate that
VisioPath outperforms conventional MPC baselines across multiple metrics. By
combining modern AI-driven perception with the rigorous foundation of optimal
control, VisioPath represents a significant step forward in safe trajectory
planning for complex traffic systems.","Shanting Wang, Panagiotis Typaldos, Chenjun Li, Andreas A. Malikopoulos",2025-07-08T22:47:41Z,2025-07-08T22:47:41Z,http://arxiv.org/abs/2507.06441v1,http://arxiv.org/pdf/2507.06441v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Mapping the Catacombs: An Underwater Cave Segment of the Devil's Eye
  System","This paper presents a framework for mapping underwater caves. Underwater
caves are crucial for fresh water resource management, underwater archaeology,
and hydrogeology. Mapping the cave's outline and dimensions, as well as
creating photorealistic 3D maps, is critical for enabling a better
understanding of this underwater domain. In this paper, we present the mapping
of an underwater cave segment (the catacombs) of the Devil's Eye cave system at
Ginnie Springs, FL. We utilized a set of inexpensive action cameras in
conjunction with a dive computer to estimate the trajectories of the cameras
together with a sparse point cloud. The resulting reconstructions are utilized
to produce a one-dimensional retract of the cave passages in the form of the
average trajectory together with the boundaries (top, bottom, left, and right).
The use of the dive computer enables the observability of the z-dimension in
addition to the roll and pitch in a visual/inertial framework (SVIn2). In
addition, the keyframes generated by SVIn2 together with the estimated camera
poses for select areas are used as input to a global optimization (bundle
adjustment) framework -- COLMAP -- in order to produce a dense reconstruction
of those areas. The same cave segment is manually surveyed using the MNemo V2
instrument, providing an additional set of measurements validating the proposed
approach. It is worth noting that with the use of action cameras, the primary
components of a cave map can be constructed. Furthermore, with the utilization
of a global optimization framework guided by the results of VI-SLAM package
SVIn2, photorealistic dense 3D representations of selected areas can be
reconstructed.","Michalis Chatzispyrou, Luke Horgan, Hyunkil Hwang, Harish Sathishchandra, Monika Roznere, Alberto Quattrini Li, Philippos Mordohai, Ioannis Rekleitis",2025-07-08T21:03:35Z,2025-07-08T21:03:35Z,http://arxiv.org/abs/2507.06397v1,http://arxiv.org/pdf/2507.06397v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Implicit Dual-Control for Visibility-Aware Navigation in Unstructured
  Environments","Navigating complex, cluttered, and unstructured environments that are a
priori unknown presents significant challenges for autonomous ground vehicles,
particularly when operating with a limited field of view(FOV) resulting in
frequent occlusion and unobserved space. This paper introduces a novel
visibility-aware model predictive path integral framework(VA-MPPI). Formulated
as a dual control problem where perceptual uncertainties and control decisions
are intertwined, it reasons over perception uncertainty evolution within a
unified planning and control pipeline. Unlike traditional methods that rely on
explicit uncertainty objectives, the VA-MPPI controller implicitly balances
exploration and exploitation, reducing uncertainty only when system performance
would be increased. The VA-MPPI framework is evaluated in simulation against
deterministic and prescient controllers across multiple scenarios, including a
cluttered urban alleyway and an occluded off-road environment. The results
demonstrate that VA-MPPI significantly improves safety by reducing collision
with unseen obstacles while maintaining competitive performance. For example,
in the off-road scenario with 400 control samples, the VA-MPPI controller
achieved a success rate of 84%, compared to only 8% for the deterministic
controller, with all VA-MPPI failures arising from unmet stopping criteria
rather than collisions. Furthermore, the controller implicitly avoids
unobserved space, improving safety without explicit directives. The proposed
framework highlights the potential for robust, visibility-aware navigation in
unstructured and occluded environments, paving the way for future advancements
in autonomous ground vehicle systems.","Benjamin Johnson, Qilun Zhu, Robert Prucka, Morgan Barron, Miriam Figueroa-Santos, Matthew Castanier",2025-07-06T12:32:44Z,2025-07-06T12:32:44Z,http://arxiv.org/abs/2507.04371v1,http://arxiv.org/pdf/2507.04371v1.pdf,all:field AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Optimizing Indoor Farm Monitoring Efficiency Using UAV: Yield Estimation
  in a GNSS-Denied Cherry Tomato Greenhouse","As the agricultural workforce declines and labor costs rise, robotic yield
estimation has become increasingly important. While unmanned ground vehicles
(UGVs) are commonly used for indoor farm monitoring, their deployment in
greenhouses is often constrained by infrastructure limitations, sensor
placement challenges, and operational inefficiencies. To address these issues,
we develop a lightweight unmanned aerial vehicle (UAV) equipped with an RGB-D
camera, a 3D LiDAR, and an IMU sensor. The UAV employs a LiDAR-inertial
odometry algorithm for precise navigation in GNSS-denied environments and
utilizes a 3D multi-object tracking algorithm to estimate the count and weight
of cherry tomatoes. We evaluate the system using two dataset: one from a
harvesting row and another from a growing row. In the harvesting-row dataset,
the proposed system achieves 94.4\% counting accuracy and 87.5\% weight
estimation accuracy within a 13.2-meter flight completed in 10.5 seconds. For
the growing-row dataset, which consists of occluded unripened fruits, we
qualitatively analyze tracking performance and highlight future research
directions for improving perception in greenhouse with strong occlusions. Our
findings demonstrate the potential of UAVs for efficient robotic yield
estimation in commercial greenhouses.","Taewook Park, Jinwoo Lee, Hyondong Oh, Won-Jae Yun, Kyu-Wha Lee",2025-05-02T04:41:57Z,2025-05-02T04:41:57Z,http://arxiv.org/abs/2505.00995v1,http://arxiv.org/pdf/2505.00995v1.pdf,all:field AND all:UGV AND submittedDate:[202309062228 TO 202509052228],2025
"Real-time adaptive tracking of fluctuating relaxation rates in
  superconducting qubits","The fidelity of operations on a solid-state quantum processor is ultimately
bounded by decoherence effects induced by a fluctuating environment.
Characterizing environmental fluctuations is challenging because the
acquisition time of experimental protocols limits the precision with which the
environment can be measured and may obscure the detailed structure of these
fluctuations. Here we present a real-time Bayesian method for estimating the
relaxation rate of a qubit, leveraging a classical controller with an
integrated field-programmable gate array (FPGA). Using our FPGA-powered
Bayesian method, we adaptively and continuously track the relaxation-time
fluctuations of two fixed-frequency superconducting transmon qubits, which
exhibit average relaxation times of approximately 0.17 ms and occasionally
exceed 0.5 ms. Our technique allows for the estimation of these relaxation
times in a few milliseconds, more than two orders of magnitude faster than
previous nonadaptive methods, and allows us to observe fluctuations up to 5
times the qubit's average relaxation rates on significantly shorter timescales
than previously reported. Our statistical analysis reveals that these
fluctuations occur on much faster timescales than previously understood, with
two-level-system switching rates reaching up to 10 Hz. Our work offers an
appealing solution for rapid relaxation-rate characterization in device
screening and for improved understanding of fast relaxation dynamics.","Fabrizio Berritta, Jacob Benestad, Jan A. Krzywda, Oswin Krause, Malthe A. Marciniak, Svend Krøjer, Christopher W. Warren, Emil Hogedal, Andreas Nylander, Irshad Ahmad, Amr Osman, Janka Biznárová, Marcus Rommel, Anita Fadavi Roudsari, Jonas Bylander, Giovanna Tancredi, Jeroen Danon, Jacob Hastrup, Ferdinand Kuemmeth, Morten Kjaergaard",2025-06-11T10:14:23Z,2025-06-11T10:14:23Z,http://arxiv.org/abs/2506.09576v1,http://arxiv.org/pdf/2506.09576v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2025
"Efficient Frame Extraction: A Novel Approach Through Frame Similarity
  and Surgical Tool Tracking for Video Segmentation","The interest in leveraging Artificial Intelligence (AI) for surgical
procedures to automate analysis has witnessed a significant surge in recent
years. One of the primary tools for recording surgical procedures and
conducting subsequent analyses, such as performance assessment, is through
videos. However, these operative videos tend to be notably lengthy compared to
other fields, spanning from thirty minutes to several hours, which poses a
challenge for AI models to effectively learn from them. Despite this challenge,
the foreseeable increase in the volume of such videos in the near future
necessitates the development and implementation of innovative techniques to
tackle this issue effectively. In this article, we propose a novel technique
called Kinematics Adaptive Frame Recognition (KAFR) that can efficiently
eliminate redundant frames to reduce dataset size and computation time while
retaining useful frames to improve accuracy. Specifically, we compute the
similarity between consecutive frames by tracking the movement of surgical
tools. Our approach follows these steps: $i)$ Tracking phase: a YOLOv8 model is
utilized to detect tools presented in the scene, $ii)$ Similarity phase:
Similarities between consecutive frames are computed by estimating variation in
the spatial positions and velocities of the tools, $iii$) Classification phase:
An X3D CNN is trained to classify segmentation. We evaluate the effectiveness
of our approach by analyzing datasets obtained through retrospective reviews of
cases at two referral centers. The newly annotated Gastrojejunostomy (GJ)
dataset covers procedures performed between 2017 and 2021, while the previously
annotated Pancreaticojejunostomy (PJ) dataset spans from 2011 to 2022 at the
same centers.","Huu Phong Nguyen, Shekhar Madhav Khairnar, Sofia Garces Palacios, Amr Al-Abbas, Melissa E. Hogg, Amer H. Zureikat, Patricio M. Polanco, Herbert Zeh III, Ganesh Sankaranarayanan",2025-01-19T19:36:09Z,2025-04-28T19:02:47Z,http://arxiv.org/abs/2501.11153v3,http://arxiv.org/pdf/2501.11153v3.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2025
A CNN Approach to Polygenic Risk Prediction of Kidney Stone Formation,"Kidney stones are a common and debilitating health issue, and genetic factors
play a crucial role in determining susceptibility. While Genome-Wide
Association Studies (GWAS) have identified numerous single nucleotide
polymorphisms (SNPs) linked to kidney stone risk, translating these findings
into effective clinical tools remains a challenge. In this study, we explore
the potential of deep learning techniques, particularly Convolutional Neural
Networks (CNNs), to enhance Polygenic Risk Score (PRS) models for predicting
kidney stone susceptibility. Using a curated dataset of kidney stone-associated
SNPs from a recent GWAS, we apply CNNs to model non-linear genetic interactions
and improve prediction accuracy. Our approach includes SNP selection, genotype
filtering, and model training using a dataset of 560 individuals, divided into
training and testing subsets. We compare our CNN-based model with traditional
machine learning models, including logistic regression, random forest, and
support vector machines, demonstrating that the CNN outperforms these models in
terms of classification accuracy and ROC-AUC. The proposed model achieved a
validation accuracy of 62%, with an ROC-AUC of 0.68, suggesting its potential
for improving genetic-based risk prediction for kidney stones. This study
contributes to the growing field of genomics-driven precision medicine and
highlights the promise of deep learning in enhancing PRS models for complex
diseases.","Amr Salem, Anirban Mondal",2024-12-23T13:28:05Z,2024-12-23T13:28:05Z,http://arxiv.org/abs/2412.17559v1,http://arxiv.org/pdf/2412.17559v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"Stochastic Analysis of Entanglement-assisted Quantum Communication
  Channels","In this paper, we present a queueing model for quantum communication
networks, a rapidly growing field of research inspired by its technological
promise and recent experimental successes. The model consists of a primary
queue and a service queue where Bell pairs are formed and stored. The Bell
pairs are by nature extremely short-lived rendering the service queue (the
quantum queue) much faster than the primary queue. We study the asymptotic
behaviour of this multi-scale queueing system utilizing the theory of
stochastic averaging principle. We prove a Functional Law of Large Numbers
(FLLN) and a Functional Central Limit Theorem (FCLT) for the standard queue
averaging the dynamics of the fast service queue. Our proofs are probablistic
and rely on the stochastic analysis of Stochastic Differential Equations (SDEs)
driven by Poisson Random Measures.","Karim S. Elsayed, Olga Izyumtseva, Wasiur R. KhudaBukhsh, Amr Rizk",2024-12-20T18:59:58Z,2024-12-20T18:59:58Z,http://arxiv.org/abs/2412.16157v1,http://arxiv.org/pdf/2412.16157v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"AdaptoML-UX: An Adaptive User-centered GUI-based AutoML Toolkit for
  Non-AI Experts and HCI Researchers","The increasing integration of machine learning across various domains has
underscored the necessity for accessible systems that non-experts can utilize
effectively. To address this need, the field of automated machine learning
(AutoML) has developed tools to simplify the construction and optimization of
ML pipelines. However, existing AutoML solutions often lack efficiency in
creating online pipelines and ease of use for Human-Computer Interaction (HCI)
applications. Therefore, in this paper, we introduce AdaptoML-UX, an adaptive
framework that incorporates automated feature engineering, machine learning,
and incremental learning to assist non-AI experts in developing robust,
user-centered ML models. Our toolkit demonstrates the capability to adapt
efficiently to diverse problem domains and datasets, particularly in HCI,
thereby reducing the necessity for manual experimentation and conserving time
and resources. Furthermore, it supports model personalization through
incremental learning, customizing models to individual user behaviors. HCI
researchers can employ AdaptoML-UX
(\url{https://github.com/MichaelSargious/AdaptoML_UX}) without requiring
specialized expertise, as it automates the selection of algorithms, feature
engineering, and hyperparameter tuning based on the unique characteristics of
the data.","Amr Gomaa, Michael Sargious, Antonio Krüger",2024-10-22T22:52:14Z,2024-10-22T22:52:14Z,http://arxiv.org/abs/2410.17469v1,http://arxiv.org/pdf/2410.17469v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
AthenaK: A Performance-Portable Version of the Athena++ AMR Framework,"We describe AthenaK: a new implementation of the Athena++ block-based
adaptive mesh refinement (AMR) framework using the Kokkos programming model.
Finite volume methods for Newtonian, special relativistic (SR), and general
relativistic (GR) hydrodynamics and magnetohydrodynamics (MHD), and
GR-radiation hydrodynamics and MHD, as well as a module for evolving Lagrangian
tracer or charged test particles (e.g., cosmic rays) are implemented using the
framework. In two companion papers we describe (1) a new solver for the
Einstein equations based on the Z4c formalism and (2) a GRMHD solver in
dynamical spacetimes also implemented using the framework, enabling new
applications in numerical relativity. By adopting Kokkos, the code can be run
on virtually any hardware, including CPUs, GPUs from multiple vendors, and
emerging ARM processors. AthenaK shows excellent performance and weak scaling,
achieving over one billion cell updates per second for hydrodynamics in
three-dimensions on a single NVIDIA Grace Hopper processor and with a typical
parallel efficiency of 80% on 65536 AMD GPUs on the OLCF Frontier system. Such
performance portability enables AthenaK to leverage modern exascale computing
systems for challenging applications in astrophysical fluid dynamics, numerical
relativity, and multimessenger astrophysics.","James M. Stone, Patrick D. Mullen, Drummond Fielding, Philipp Grete, Minghao Guo, Philipp Kempski, Elias R. Most, Christopher J. White, George N. Wong",2024-09-24T12:57:19Z,2024-09-24T12:57:19Z,http://arxiv.org/abs/2409.16053v1,http://arxiv.org/pdf/2409.16053v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"Quality-Aware Task Offloading for Cooperative Perception in Vehicular
  Edge Computing","Task offloading in Vehicular Edge Computing (VEC) can advance cooperative
perception (CP) to improve traffic awareness in Autonomous Vehicles. In this
paper, we propose the Quality-aware Cooperative Perception Task Offloading
(QCPTO) scheme. Q-CPTO is the first task offloading scheme that enhances
traffic awareness by prioritizing the quality rather than the quantity of
cooperative perception. Q-CPTO improves the quality of CP by curtailing
perception redundancy and increasing the Value of Information (VOI) procured by
each user. We use Kalman filters (KFs) for VOI assessment, predicting the next
movement of each vehicle to estimate its region of interest. The estimated VOI
is then integrated into the task offloading problem. We formulate the task
offloading problem as an Integer Linear Program (ILP) that maximizes the VOI of
users and reduces perception redundancy by leveraging the spatially diverse
fields of view (FOVs) of vehicles, while adhering to strict latency
requirements. We also propose the Q-CPTO-Heuristic (Q-CPTOH) scheme to solve
the task offloading problem in a time-efficient manner. Extensive evaluations
show that Q-CPTO significantly outperforms prominent task offloading schemes by
up to 14% and 20% in terms of response delay and traffic awareness,
respectively. Furthermore, Q-CPTO-H closely approaches the optimal solution,
with marginal gaps of up to 1.4% and 2.1% in terms of traffic awareness and the
number of collaborating users, respectively, while reducing the runtime by up
to 84%.","Amr M. Zaki, Sara A. Elsayed, Khalid Elgazzar, Hossam S. Hassanein",2024-05-31T02:56:24Z,2024-05-31T02:56:24Z,http://arxiv.org/abs/2405.20587v1,http://arxiv.org/pdf/2405.20587v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"KI-GAN: Knowledge-Informed Generative Adversarial Networks for Enhanced
  Multi-Vehicle Trajectory Forecasting at Signalized Intersections","Reliable prediction of vehicle trajectories at signalized intersections is
crucial to urban traffic management and autonomous driving systems. However, it
presents unique challenges, due to the complex roadway layout at intersections,
involvement of traffic signal controls, and interactions among different types
of road users. To address these issues, we present in this paper a novel model
called Knowledge-Informed Generative Adversarial Network (KI-GAN), which
integrates both traffic signal information and multi-vehicle interactions to
predict vehicle trajectories accurately. Additionally, we propose a specialized
attention pooling method that accounts for vehicle orientation and proximity at
intersections. Based on the SinD dataset, our KI-GAN model is able to achieve
an Average Displacement Error (ADE) of 0.05 and a Final Displacement Error
(FDE) of 0.12 for a 6-second observation and 6-second prediction cycle. When
the prediction window is extended to 9 seconds, the ADE and FDE values are
further reduced to 0.11 and 0.26, respectively. These results demonstrate the
effectiveness of the proposed KI-GAN model in vehicle trajectory prediction
under complex scenarios at signalized intersections, which represents a
significant advancement in the target field.","Chuheng Wei, Guoyuan Wu, Matthew J. Barth, Amr Abdelraouf, Rohit Gupta, Kyungtae Han",2024-04-17T08:53:59Z,2024-04-19T14:28:00Z,http://arxiv.org/abs/2404.11181v2,http://arxiv.org/pdf/2404.11181v2.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2024
"Collaborative Optimization of the Age of Information under Partial
  Observability","The significance of the freshness of sensor and control data at the receiver
side, often referred to as Age of Information (AoI), is fundamentally
constrained by contention for limited network resources. Evidently, network
congestion is detrimental for AoI, where this congestion is partly self-induced
by the sensor transmission process in addition to the contention from other
transmitting sensors. In this work, we devise a decentralized AoI-minimizing
transmission policy for a number of sensor agents sharing capacity-limited,
non-FIFO duplex channels that introduce random delays in communication with a
common receiver. By implementing the same policy, however with no explicit
inter-agent communication, the agents minimize the expected AoI in this
partially observable system. We cater to the partial observability due to
random channel delays by designing a bootstrap particle filter that
independently maintains a belief over the AoI of each agent. We also leverage
mean-field control approximations and reinforcement learning to derive scalable
and optimal solutions for minimizing the expected AoI collaboratively.","Anam Tahir, Kai Cui, Bastian Alt, Amr Rizk, Heinz Koeppl",2023-12-20T12:34:54Z,2023-12-20T12:34:54Z,http://arxiv.org/abs/2312.12977v1,http://arxiv.org/pdf/2312.12977v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2023
A simple polynomial for a transposition over finite fields,"Let $q>2$, and let $a$ and $b$ be two elements of the finite field
$\mathbb{F}_q$ with $a\ne 0$. Carlitz represented the transposition $(0a)$ by a
polynomial of degree $(q-2)^3$. In this note, we represent the transposition
$(ab)$ by a polynomial of degree $q-2$. Also, we use this polynomial to
construct polynomials that represent permutations of finite local rings with
residue field $\mathbb{F}_q$.",Amr Ali Abdulkader Al-Maktry,2023-12-14T13:28:00Z,2023-12-14T13:28:00Z,http://arxiv.org/abs/2312.08921v1,http://arxiv.org/pdf/2312.08921v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2023
"It's all about you: Personalized in-Vehicle Gesture Recognition with a
  Time-of-Flight Camera","Despite significant advances in gesture recognition technology, recognizing
gestures in a driving environment remains challenging due to limited and costly
data and its dynamic, ever-changing nature. In this work, we propose a
model-adaptation approach to personalize the training of a CNNLSTM model and
improve recognition accuracy while reducing data requirements. Our approach
contributes to the field of dynamic hand gesture recognition while driving by
providing a more efficient and accurate method that can be customized for
individual users, ultimately enhancing the safety and convenience of in-vehicle
interactions, as well as driver's experience and system trust. We incorporate
hardware enhancement using a time-of-flight camera and algorithmic enhancement
through data augmentation, personalized adaptation, and incremental learning
techniques. We evaluate the performance of our approach in terms of recognition
accuracy, achieving up to 90\%, and show the effectiveness of personalized
adaptation and incremental learning for a user-centered design.","Amr Gomaa, Guillermo Reyes, Michael Feld",2023-10-02T21:48:19Z,2023-10-02T21:48:19Z,http://arxiv.org/abs/2310.01659v1,http://arxiv.org/pdf/2310.01659v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2023
"Integration of Quantum Accelerators with High Performance Computing -- A
  Review of Quantum Programming Tools","Quantum computing (QC) introduces a novel mode of computation with the
possibility of greater computational power that remains to be exploited -
presenting exciting opportunities for high performance computing (HPC)
applications. However, recent advancements in the field have made clear that QC
does not supplant conventional HPC, but can rather be incorporated into current
heterogeneous HPC infrastructures as an additional accelerator, thereby
enabling the optimal utilization of both paradigms. The desire for such
integration significantly affects the development of software for quantum
computers, which in turn influences the necessary software infrastructure. To
date, previous review papers have investigated various quantum programming
tools (QPTs) (such as languages, libraries, frameworks) in their ability to
program, compile, and execute quantum circuits. However, the integration effort
with classical HPC frameworks or systems has not been addressed. This study
aims to characterize existing QPTs from an HPC perspective, investigating if
existing QPTs have the potential to be efficiently integrated with classical
computing models and determining where work is still required. This work
structures a set of criteria into an analysis blueprint that enables HPC
scientists to assess whether a QPT is suitable for the quantum-accelerated
classical application at hand.","Amr Elsharkawy, Xiao-Ting Michelle To, Philipp Seitz, Yanbin Chen, Yannick Stade, Manuel Geiger, Qunsheng Huang, Xiaorang Guo, Muhammad Arslan Ansari, Christian B. Mendl, Dieter Kranzlmüller, Martin Schulz",2023-09-12T12:24:12Z,2023-09-18T08:02:54Z,http://arxiv.org/abs/2309.06167v2,http://arxiv.org/pdf/2309.06167v2.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2023
SoccerNet 2023 Challenges Results,"The SoccerNet 2023 challenges were the third annual video understanding
challenges organized by the SoccerNet team. For this third edition, the
challenges were composed of seven vision-based tasks split into three main
themes. The first theme, broadcast video understanding, is composed of three
high-level tasks related to describing events occurring in the video
broadcasts: (1) action spotting, focusing on retrieving all timestamps related
to global actions in soccer, (2) ball action spotting, focusing on retrieving
all timestamps related to the soccer ball change of state, and (3) dense video
captioning, focusing on describing the broadcast with natural language and
anchored timestamps. The second theme, field understanding, relates to the
single task of (4) camera calibration, focusing on retrieving the intrinsic and
extrinsic camera parameters from images. The third and last theme, player
understanding, is composed of three low-level tasks related to extracting
information about the players: (5) re-identification, focusing on retrieving
the same players across multiple views, (6) multiple object tracking, focusing
on tracking players and the ball through unedited video streams, and (7) jersey
number recognition, focusing on recognizing the jersey number of players from
tracklets. Compared to the previous editions of the SoccerNet challenges, tasks
(2-3-7) are novel, including new annotations and data, task (4) was enhanced
with more data and annotations, and task (6) now focuses on end-to-end
approaches. More information on the tasks, challenges, and leaderboards are
available on https://www.soccer-net.org. Baselines and development kits can be
found on https://github.com/SoccerNet.","Anthony Cioppa, Silvio Giancola, Vladimir Somers, Floriane Magera, Xin Zhou, Hassan Mkhallati, Adrien Deliège, Jan Held, Carlos Hinojosa, Amir M. Mansourian, Pierre Miralles, Olivier Barnich, Christophe De Vleeschouwer, Alexandre Alahi, Bernard Ghanem, Marc Van Droogenbroeck, Abdullah Kamal, Adrien Maglo, Albert Clapés, Amr Abdelaziz, Artur Xarles, Astrid Orcesi, Atom Scott, Bin Liu, Byoungkwon Lim, Chen Chen, Fabian Deuser, Feng Yan, Fufu Yu, Gal Shitrit, Guanshuo Wang, Gyusik Choi, Hankyul Kim, Hao Guo, Hasby Fahrudin, Hidenari Koguchi, Håkan Ardö, Ibrahim Salah, Ido Yerushalmy, Iftikar Muhammad, Ikuma Uchida, Ishay Be'ery, Jaonary Rabarisoa, Jeongae Lee, Jiajun Fu, Jianqin Yin, Jinghang Xu, Jongho Nang, Julien Denize, Junjie Li, Junpei Zhang, Juntae Kim, Kamil Synowiec, Kenji Kobayashi, Kexin Zhang, Konrad Habel, Kota Nakajima, Licheng Jiao, Lin Ma, Lizhi Wang, Luping Wang, Menglong Li, Mengying Zhou, Mohamed Nasr, Mohamed Abdelwahed, Mykola Liashuha, Nikolay Falaleev, Norbert Oswald, Qiong Jia, Quoc-Cuong Pham, Ran Song, Romain Hérault, Rui Peng, Ruilong Chen, Ruixuan Liu, Ruslan Baikulov, Ryuto Fukushima, Sergio Escalera, Seungcheon Lee, Shimin Chen, Shouhong Ding, Taiga Someya, Thomas B. Moeslund, Tianjiao Li, Wei Shen, Wei Zhang, Wei Li, Wei Dai, Weixin Luo, Wending Zhao, Wenjie Zhang, Xinquan Yang, Yanbiao Ma, Yeeun Joo, Yingsen Zeng, Yiyang Gan, Yongqiang Zhu, Yujie Zhong, Zheng Ruan, Zhiheng Li, Zhijian Huang, Ziyu Meng",2023-09-12T07:03:30Z,2023-09-12T07:03:30Z,http://arxiv.org/abs/2309.06006v1,http://arxiv.org/pdf/2309.06006v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2023
"Adaptive User-centered Neuro-symbolic Learning for Multimodal
  Interaction with Autonomous Systems","Recent advances in machine learning, particularly deep learning, have enabled
autonomous systems to perceive and comprehend objects and their environments in
a perceptual subsymbolic manner. These systems can now perform object
detection, sensor data fusion, and language understanding tasks. However, there
is a growing need to enhance these systems to understand objects and their
environments more conceptually and symbolically. It is essential to consider
both the explicit teaching provided by humans (e.g., describing a situation or
explaining how to act) and the implicit teaching obtained by observing human
behavior (e.g., through the system's sensors) to achieve this level of powerful
artificial intelligence. Thus, the system must be designed with multimodal
input and output capabilities to support implicit and explicit interaction
models. In this position paper, we argue for considering both types of inputs,
as well as human-in-the-loop and incremental learning techniques, for advancing
the field of artificial intelligence and enabling autonomous systems to learn
like humans. We propose several hypotheses and design guidelines and highlight
a use case from related work to achieve this goal.","Amr Gomaa, Michael Feld",2023-09-11T19:35:12Z,2023-09-11T19:35:12Z,http://arxiv.org/abs/2309.05787v1,http://arxiv.org/pdf/2309.05787v1.pdf,all:field AND all:AMR AND submittedDate:[202309062228 TO 202509052228],2023
Worldline Formulations of Covariant Fracton Theories,"We develop worldline formulations of covariant fracton gauge theories. These
are a one-parameter family of gauge theories of a rank-two symmetric tensor
field, invariant under a scalar gauge transformation involving a double
derivative. These theories, which can be interpreted as linearized gravity
theories invariant under longitudinal diffeomorphisms, provide a covariant
framework for studying Lorentz-breaking fracton quasiparticles, which are
excitations with restricted mobility due to dipole-moment conservation. We
construct three worldline models. The first two are obtained by deducing their
constraint structure directly from the spacetime gauge transformations. By
applying BRST quantization, we show that these models reproduce the BV spectrum
and the associated BRST transformations of two specific fracton theories. The
third model is defined as a deformation of the second one: although free, it is
analyzed by drawing inspiration from the standard treatment of interacting
worldline systems, and is shown to capture almost the entire family of
covariant fracton theories. Finally, we discuss the gauge-fixing, comparing the
BV-BRST spacetime perspective with the worldline analogue of the ``Siegel
gauge"" employed in string field theory.","Filippo Fecit, Davide Rovere",2025-08-20T10:15:31Z,2025-08-20T10:15:31Z,http://arxiv.org/abs/2508.14591v1,http://arxiv.org/pdf/2508.14591v1.pdf,all:field AND all:rover AND submittedDate:[202309062228 TO 202509052228],2025
In silico clinical trials in drug development: a systematic review,"In the context of clinical research, computational models have received
increasing attention over the past decades. In this systematic review, we aimed
to provide an overview of the role of so-called in silico clinical trials
(ISCTs) in medical applications. Exemplary for the broad field of clinical
medicine, we focused on in silico (IS) methods applied in drug development,
sometimes also referred to as model informed drug development (MIDD). We
searched PubMed and ClinicalTrials.gov for published articles and registered
clinical trials related to ISCTs. We identified 202 articles and 48 trials, and
of these, 76 articles and 19 trials were directly linked to drug development.
We extracted information from all 202 articles and 48 clinical trials and
conducted a more detailed review of the methods used in the 76 articles that
are connected to drug development. Regarding application, most articles and
trials focused on cancer and imaging-related research while rare and pediatric
diseases were only addressed in 14 articles and 5 trials, respectively. While
some models were informed combining mechanistic knowledge with clinical or
preclinical (in-vivo or in-vitro) data, the majority of models were fully
data-driven, illustrating that clinical data is a crucial part in the process
of generating synthetic data in ISCTs. Regarding reproducibility, a more
detailed analysis revealed that only 24% (18 out of 76) of the articles
provided an open-source implementation of the applied models, and in only 20%
of the articles the generated synthetic data were publicly available. Despite
the widely raised interest, we also found that it is still uncommon for ISCTs
to be part of a registered clinical trial and their application is restricted
to specific diseases leaving potential benefits of ISCTs not fully exploited.","Bohua Chen, Lucia Chantal Schneider, Christian Röver, Emmanuelle Comets, Markus Christian Elze, Andrew Hooker, Joanna IntHout, Anne-Sophie Jannot, Daria Julkowska, Yanis Mimouni, Marina Savelieva, Nigel Stallard, Moreno Ursino, Marc Vandemeulebroecke, Sebastian Weber, Martin Posch, Sarah Zohar, Tim Friede",2025-03-11T10:10:20Z,2025-08-04T20:59:46Z,http://arxiv.org/abs/2503.08746v3,http://arxiv.org/pdf/2503.08746v3.pdf,all:field AND all:rover AND submittedDate:[202309062228 TO 202509052228],2025
Anomalies in Covariant Fracton Theories,"Covariant (Lorentz invariant) fracton matter, minimally coupled and charged
under a symmetric rank two gauge tensor, is considered. The gauge
transformations correspond to linearized longitudinal diffeomorphisms.
Consistent possible anomalies are computed using the BRST cohomology method.
They depend only on the gauge field, treated as a background field, and on the
gauge parameter, promoted to an anticommuting scalar ghost field. The problem
is phrased in terms of polyforms, whose total degree is the sum of the form
degree and of the ghost number. The most general anomaly in two dimensions and
in four dimensions is computed and an anomaly in arbitrary dimensions is
individuated. In conclusion, it is shown that a simple higher-derivative scalar
field theory is an example of covariant fracton matter.",Davide Rovere,2024-06-10T18:00:04Z,2024-10-16T14:44:11Z,http://arxiv.org/abs/2406.06686v2,http://arxiv.org/pdf/2406.06686v2.pdf,all:field AND all:rover AND submittedDate:[202309062228 TO 202509052228],2024
Superconformal anomalies from superconformal Chern-Simons polynomials,"We consider the 4-dimensional $\mathcal{N}=1$ Lie superconformal algebra and
search for completely ""symmetric"" (in the graded sense) 3-index invariant
tensors. The solution we find is unique and we show that the corresponding
invariant polynomial cubic in the generalized curvatures of superconformal
gravity vanishes. Consequently, the associated Chern-Simons polynomial is a
non-trivial anomaly cocycle. We explicitly compute this cocycle to all orders
in the independent fields of superconformal gravity and establish that it is
BRST equivalent to the so-called superconformal $a$-anomaly. We briefly discuss
the possibility that the superconformal $c$-anomaly also admits a similar
Chern-Simons formulation and the potential holographic, 5-dimensional,
interpretation of our results.","Camillo Imbimbo, Davide Rovere, Alison Warman",2023-11-09T19:00:03Z,2024-05-30T11:07:52Z,http://arxiv.org/abs/2311.05684v3,http://arxiv.org/pdf/2311.05684v3.pdf,all:field AND all:rover AND submittedDate:[202309062228 TO 202509052228],2023
"A Multi-Loss Strategy for Vehicle Trajectory Prediction: Combining
  Off-Road, Diversity, and Directional Consistency Losses","Trajectory prediction is essential for the safety and efficiency of planning
in autonomous vehicles. However, current models often fail to fully capture
complex traffic rules and the complete range of potential vehicle movements.
Addressing these limitations, this study introduces three novel loss functions:
Offroad Loss, Direction Consistency Error, and Diversity Loss. These functions
are designed to keep predicted paths within driving area boundaries, aligned
with traffic directions, and cover a wider variety of plausible driving
scenarios. As all prediction modes should adhere to road rules and conditions,
this work overcomes the shortcomings of traditional ""winner takes all"" training
methods by applying the loss functions to all prediction modes. These loss
functions not only improve model training but can also serve as metrics for
evaluating the realism and diversity of trajectory predictions. Extensive
validation on the nuScenes and Argoverse 2 datasets with leading baseline
models demonstrates that our approach not only maintains accuracy but
significantly improves safety and robustness, reducing offroad errors on
average by 47% on original and by 37% on attacked scenes. This work sets a new
benchmark for trajectory prediction in autonomous driving, offering substantial
improvements in navigating complex environments. Our code is available at
https://github.com/vita-epfl/stay-on-track .","Ahmad Rahimi, Alexandre Alahi",2024-11-29T14:47:08Z,2024-11-29T14:47:08Z,http://arxiv.org/abs/2411.19747v1,http://arxiv.org/pdf/2411.19747v1.pdf,all:offroad AND all:robot AND submittedDate:[202309062228 TO 202509052228],2024
"ROAMER: Robust Offroad Autonomy using Multimodal State Estimation with
  Radar Velocity Integration","Reliable offroad autonomy requires low-latency, high-accuracy state estimates
of pose as well as velocity, which remain viable throughout environments with
sub-optimal operating conditions for the utilized perception modalities. As
state estimation remains a single point of failure system in the majority of
aspiring autonomous systems, failing to address the environmental degradation
the perception sensors could potentially experience given the operating
conditions, can be a mission-critical shortcoming. In this work, a method for
integration of radar velocity information in a LiDAR-inertial odometry solution
is proposed, enabling consistent estimation performance even with degraded
LiDAR-inertial odometry. The proposed method utilizes the direct
velocity-measuring capabilities of an Frequency Modulated Continuous Wave
(FMCW) radar sensor to enhance the LiDAR-inertial smoother solution onboard the
vehicle through integration of the forward velocity measurement into the
graph-based smoother. This leads to increased robustness in the overall
estimation solution, even in the absence of LiDAR data. This method was
validated by hardware experiments conducted onboard an all-terrain vehicle
traveling at high speed, ~12 m/s, in demanding offroad environments.","Morten Nissov, Shehryar Khattak, Jeffrey A. Edlund, Curtis Padgett, Kostas Alexis, Patrick Spieler",2024-01-30T19:46:26Z,2024-01-30T19:46:26Z,http://arxiv.org/abs/2401.17404v1,http://arxiv.org/pdf/2401.17404v1.pdf,all:offroad AND all:robot AND submittedDate:[202309062228 TO 202509052228],2024
Manipulating Trajectory Prediction with Backdoors,"Autonomous vehicles ought to predict the surrounding agents' trajectories to
allow safe maneuvers in uncertain and complex traffic situations. As companies
increasingly apply trajectory prediction in the real world, security becomes a
relevant concern. In this paper, we focus on backdoors - a security threat
acknowledged in other fields but so far overlooked for trajectory prediction.
To this end, we describe and investigate four triggers that could affect
trajectory prediction. We then show that these triggers (for example, a braking
vehicle), when correlated with a desired output (for example, a curve) during
training, cause the desired output of a state-of-the-art trajectory prediction
model. In other words, the model has good benign performance but is vulnerable
to backdoors. This is the case even if the trigger maneuver is performed by a
non-casual agent behind the target vehicle. As a side-effect, our analysis
reveals interesting limitations within trajectory prediction models. Finally,
we evaluate a range of defenses against backdoors. While some, like simple
offroad checks, do not enable detection for all triggers, clustering is a
promising candidate to support manual inspection to find backdoors.","Kaouther Messaoud, Kathrin Grosse, Mickael Chen, Matthieu Cord, Patrick Pérez, Alexandre Alahi",2023-12-21T14:01:51Z,2024-01-03T15:52:24Z,http://arxiv.org/abs/2312.13863v2,http://arxiv.org/pdf/2312.13863v2.pdf,all:offroad AND all:robot AND submittedDate:[202309062228 TO 202509052228],2023
"Uncertainty-aware hybrid paradigm of nonlinear MPC and model-based RL
  for offroad navigation: Exploration of transformers in the predictive model","In this paper, we investigate a hybrid scheme that combines nonlinear model
predictive control (MPC) and model-based reinforcement learning (RL) for
navigation planning of an autonomous model car across offroad, unstructured
terrains without relying on predefined maps. Our innovative approach takes
inspiration from BADGR, an LSTM-based network that primarily concentrates on
environment modeling, but distinguishes itself by substituting LSTM modules
with transformers to greatly elevate the performance our model. Addressing
uncertainty within the system, we train an ensemble of predictive models and
estimate the mutual information between model weights and outputs, facilitating
dynamic horizon planning through the introduction of variable speeds. Further
enhancing our methodology, we incorporate a nonlinear MPC controller that
accounts for the intricacies of the vehicle's model and states. The model-based
RL facet produces steering angles and quantifies inherent uncertainty. At the
same time, the nonlinear MPC suggests optimal throttle settings, striking a
balance between goal attainment speed and managing model uncertainty influenced
by velocity. In the conducted studies, our approach excels over the existing
baseline by consistently achieving higher metric values in predicting future
events and seamlessly integrating the vehicle's kinematic model for enhanced
decision-making. The code and the evaluation data are available at
https://github.com/FARAZLOTFI/offroad_autonomous_navigation/).","Faraz Lotfi, Khalil Virji, Farnoosh Faraji, Lucas Berry, Andrew Holliday, David Meger, Gregory Dudek",2023-10-01T18:47:02Z,2023-10-01T18:47:02Z,http://arxiv.org/abs/2310.00760v1,http://arxiv.org/pdf/2310.00760v1.pdf,all:offroad AND all:robot AND submittedDate:[202309062228 TO 202509052228],2023
"DUViN: Diffusion-Based Underwater Visual Navigation via
  Knowledge-Transferred Depth Features","Autonomous underwater navigation remains a challenging problem due to limited
sensing capabilities and the difficulty of constructing accurate maps in
underwater environments. In this paper, we propose a Diffusion-based Underwater
Visual Navigation policy via knowledge-transferred depth features, named DUViN,
which enables vision-based end-to-end 4-DoF motion control for underwater
vehicles in unknown environments. DUViN guides the vehicle to avoid obstacles
and maintain a safe and perception awareness altitude relative to the terrain
without relying on pre-built maps. To address the difficulty of collecting
large-scale underwater navigation datasets, we propose a method that ensures
robust generalization under domain shifts from in-air to underwater
environments by leveraging depth features and introducing a novel model
transfer strategy. Specifically, our training framework consists of two phases:
we first train the diffusion-based visual navigation policy on in-air datasets
using a pre-trained depth feature extractor. Secondly, we retrain the extractor
on an underwater depth estimation task and integrate the adapted extractor into
the trained navigation policy from the first step. Experiments in both
simulated and real-world underwater environments demonstrate the effectiveness
and generalization of our approach. The experimental videos are available at
https://www.youtube.com/playlist?list=PLqt2s-RyCf1gfXJgFzKjmwIqYhrP4I-7Y.","Jinghe Yang, Minh-Quan Le, Mingming Gong, Ye Pu",2025-09-03T03:43:12Z,2025-09-03T03:43:12Z,http://arxiv.org/abs/2509.02983v1,http://arxiv.org/pdf/2509.02983v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"QuadKAN: KAN-Enhanced Quadruped Motion Control via End-to-End
  Reinforcement Learning","We address vision-guided quadruped motion control with reinforcement learning
(RL) and highlight the necessity of combining proprioception with vision for
robust control. We propose QuadKAN, a spline-parameterized cross-modal policy
instantiated with Kolmogorov-Arnold Networks (KANs). The framework incorporates
a spline encoder for proprioception and a spline fusion head for
proprioception-vision inputs. This structured function class aligns the
state-to-action mapping with the piecewise-smooth nature of gait, improving
sample efficiency, reducing action jitter and energy consumption, and providing
interpretable posture-action sensitivities. We adopt Multi-Modal Delay
Randomization (MMDR) and perform end-to-end training with Proximal Policy
Optimization (PPO). Evaluations across diverse terrains, including both even
and uneven surfaces and scenarios with static or dynamic obstacles, demonstrate
that QuadKAN achieves consistently higher returns, greater distances, and fewer
collisions than state-of-the-art (SOTA) baselines. These results show that
spline-parameterized policies offer a simple, effective, and interpretable
alternative for robust vision-guided locomotion. A repository will be made
available upon acceptance.","Allen Wang, Gavin Tao",2025-08-26T16:05:32Z,2025-08-26T16:05:32Z,http://arxiv.org/abs/2508.19153v1,http://arxiv.org/pdf/2508.19153v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"On Kinodynamic Global Planning in a Simplicial Complex Environment: A
  Mixed Integer Approach","This work casts the kinodynamic planning problem for car-like vehicles as an
optimization task to compute a minimum-time trajectory and its associated
velocity profile, subject to boundary conditions on velocity, acceleration, and
steering. The approach simultaneously optimizes both the spatial path and the
sequence of acceleration and steering controls, ensuring continuous motion from
a specified initial position and velocity to a target end position and
velocity.The method analyzes the admissible control space and terrain to avoid
local minima. The proposed method operates efficiently in simplicial complex
environments, a preferred terrain representation for capturing intricate 3D
landscapes. The problem is initially posed as a mixed-integer fractional
program with quadratic constraints, which is then reformulated into a
mixed-integer bilinear objective through a variable transformation and
subsequently relaxed to a mixed-integer linear program using McCormick
envelopes. Comparative simulations against planners such as MPPI and log-MPPI
demonstrate that the proposed approach generates solutions 104 times faster
while strictly adhering to the specified constraints","Otobong Jerome, Alexandr Klimchik, Alexander Maloletov, Geesara Kulathunga",2025-08-22T16:35:01Z,2025-08-22T16:35:01Z,http://arxiv.org/abs/2508.16511v1,http://arxiv.org/pdf/2508.16511v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Validating Terrain Models in Digital Twins for Trustworthy sUAS
  Operations","With the increasing deployment of small Unmanned Aircraft Systems (sUAS) in
unfamiliar and complex environments, Environmental Digital Twins (EDT) that
comprise weather, airspace, and terrain data are critical for safe flight
planning and for maintaining appropriate altitudes during search and
surveillance operations. With the expansion of sUAS capabilities through edge
and cloud computing, accurate EDT are also vital for advanced sUAS
capabilities, like geolocation. However, real-world sUAS deployment introduces
significant sources of uncertainty, necessitating a robust validation process
for EDT components. This paper focuses on the validation of terrain models, one
of the key components of an EDT, for real-world sUAS tasks. These models are
constructed by fusing U.S. Geological Survey (USGS) datasets and satellite
imagery, incorporating high-resolution environmental data to support mission
tasks. Validating both the terrain models and their operational use by sUAS
under real-world conditions presents significant challenges, including limited
data granularity, terrain discontinuities, GPS and sensor inaccuracies, visual
detection uncertainties, as well as onboard resources and timing constraints.
We propose a 3-Dimensions validation process grounded in software engineering
principles, following a workflow across granularity of tests, simulation to
real world, and the analysis of simple to edge conditions. We demonstrate our
approach using a multi-sUAS platform equipped with a Terrain-Aware Digital
Shadow.","Arturo Miguel Russell Bernal, Maureen Petterson, Pedro Antonio Alarcon Granadeno, Michael Murphy, James Mason, Jane Cleland-Huang",2025-08-22T05:42:55Z,2025-08-22T05:42:55Z,http://arxiv.org/abs/2508.16104v1,http://arxiv.org/pdf/2508.16104v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"No More Blind Spots: Learning Vision-Based Omnidirectional Bipedal
  Locomotion for Challenging Terrain","Effective bipedal locomotion in dynamic environments, such as cluttered
indoor spaces or uneven terrain, requires agile and adaptive movement in all
directions. This necessitates omnidirectional terrain sensing and a controller
capable of processing such input. We present a learning framework for
vision-based omnidirectional bipedal locomotion, enabling seamless movement
using depth images. A key challenge is the high computational cost of rendering
omnidirectional depth images in simulation, making traditional sim-to-real
reinforcement learning (RL) impractical. Our method combines a robust blind
controller with a teacher policy that supervises a vision-based student policy,
trained on noise-augmented terrain data to avoid rendering costs during RL and
ensure robustness. We also introduce a data augmentation technique for
supervised student training, accelerating training by up to 10 times compared
to conventional methods. Our framework is validated through simulation and
real-world tests, demonstrating effective omnidirectional locomotion with
minimal reliance on expensive rendering. This is, to the best of our knowledge,
the first demonstration of vision-based omnidirectional bipedal locomotion,
showcasing its adaptability to diverse terrains.","Mohitvishnu S. Gadde, Pranay Dugar, Ashish Malik, Alan Fern",2025-08-16T06:20:46Z,2025-08-16T06:20:46Z,http://arxiv.org/abs/2508.11929v1,http://arxiv.org/pdf/2508.11929v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"LocoMamba: Vision-Driven Locomotion via End-to-End Deep Reinforcement
  Learning with Mamba","We introduce LocoMamba, a vision-driven cross-modal DRL framework built on
selective state-space models, specifically leveraging Mamba, that achieves
near-linear-time sequence modeling, effectively captures long-range
dependencies, and enables efficient training with longer sequences. First, we
embed proprioceptive states with a multilayer perceptron and patchify depth
images with a lightweight convolutional neural network, producing compact
tokens that improve state representation. Second, stacked Mamba layers fuse
these tokens via near-linear-time selective scanning, reducing latency and
memory footprint, remaining robust to token length and image resolution, and
providing an inductive bias that mitigates overfitting. Third, we train the
policy end-to-end with Proximal Policy Optimization under terrain and
appearance randomization and an obstacle-density curriculum, using a compact
state-centric reward that balances progress, smoothness, and safety. We
evaluate our method in challenging simulated environments with static and
moving obstacles as well as uneven terrain. Compared with state-of-the-art
baselines, our method achieves higher returns and success rates with fewer
collisions, exhibits stronger generalization to unseen terrains and obstacle
densities, and improves training efficiency by converging in fewer updates
under the same compute budget.","Yinuo Wang, Gavin Tao",2025-08-16T00:13:24Z,2025-08-28T21:09:41Z,http://arxiv.org/abs/2508.11849v2,http://arxiv.org/pdf/2508.11849v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Dynamical Trajectory Planning of Disturbance Consciousness for Air-Land
  Bimodal Unmanned Aerial Vehicles","Air-land bimodal vehicles provide a promising solution for navigating complex
environments by combining the flexibility of aerial locomotion with the energy
efficiency of ground mobility. To enhance the robustness of trajectory planning
under environmental disturbances, this paper presents a disturbance-aware
planning framework that incorporates real-time disturbance estimation into both
path searching and trajectory optimization. A key component of the framework is
a disturbance-adaptive safety boundary adjustment mechanism, which dynamically
modifies the vehicle's feasible dynamic boundaries based on estimated
disturbances to ensure trajectory feasibility. Leveraging the dynamics model of
the bimodal vehicle, the proposed approach achieves adaptive and reliable
motion planning across different terrains and operating conditions. A series of
real-world experiments and benchmark comparisons on a custom-built platform
validate the effectiveness and robustness of the method, demonstrating
improvements in tracking accuracy, task efficiency, and energy performance
under both ground and aerial disturbances.","Shaoting Liu, Zhou Liu",2025-08-08T03:09:53Z,2025-08-08T03:09:53Z,http://arxiv.org/abs/2508.05972v1,http://arxiv.org/pdf/2508.05972v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Uncertainty-aware Accurate Elevation Modeling for Off-road Navigation
  via Neural Processes","Terrain elevation modeling for off-road navigation aims to accurately
estimate changes in terrain geometry in real-time and quantify the
corresponding uncertainties. Having precise estimations and uncertainties plays
a crucial role in planning and control algorithms to explore safe and reliable
maneuver strategies. However, existing approaches, such as Gaussian Processes
(GPs) and neural network-based methods, often fail to meet these needs. They
are either unable to perform in real-time due to high computational demands,
underestimating sharp geometry changes, or harming elevation accuracy when
learned with uncertainties. Recently, Neural Processes (NPs) have emerged as a
promising approach that integrates the Bayesian uncertainty estimation of GPs
with the efficiency and flexibility of neural networks. Inspired by NPs, we
propose an effective NP-based method that precisely estimates sharp elevation
changes and quantifies the corresponding predictive uncertainty without losing
elevation accuracy. Our method leverages semantic features from LiDAR and
camera sensors to improve interpolation and extrapolation accuracy in
unobserved regions. Also, we introduce a local ball-query attention mechanism
to effectively reduce the computational complexity of global attention by 17\%
while preserving crucial local and spatial information. We evaluate our method
on off-road datasets having interesting geometric features, collected from
trails, deserts, and hills. Our results demonstrate superior performance over
baselines and showcase the potential of neural processes for effective and
expressive terrain modeling in complex off-road environments.","Sanghun Jung, Daehoon Gwak, Byron Boots, James Hays",2025-08-05T20:19:02Z,2025-08-07T19:56:22Z,http://arxiv.org/abs/2508.03890v2,http://arxiv.org/pdf/2508.03890v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Model-agnostic Meta-learning for Adaptive Gait Phase and Terrain
  Geometry Estimation with Wearable Soft Sensors","This letter presents a model-agnostic meta-learning (MAML) based framework
for simultaneous and accurate estimation of human gait phase and terrain
geometry using a small set of fabric-based wearable soft sensors, with
efficient adaptation to unseen subjects and strong generalization across
different subjects and terrains. Compared to rigid alternatives such as
inertial measurement units, fabric-based soft sensors improve comfort but
introduce nonlinearities due to hysteresis, placement error, and fabric
deformation. Moreover, inter-subject and inter-terrain variability, coupled
with limited calibration data in real-world deployments, further complicate
accurate estimation. To address these challenges, the proposed framework
integrates MAML into a deep learning architecture to learn a generalizable
model initialization that captures subject- and terrain-invariant structure.
This initialization enables efficient adaptation (i.e., adaptation with only a
small amount of calibration data and a few fine-tuning steps) to new users,
while maintaining strong generalization (i.e., high estimation accuracy across
subjects and terrains). Experiments on nine participants walking at various
speeds over five terrain conditions demonstrate that the proposed framework
outperforms baseline approaches in estimating gait phase, locomotion mode, and
incline angle, with superior accuracy, adaptation efficiency, and
generalization.","Zenan Zhu, Wenxi Chen, Pei-Chun Kao, Janelle Clark, Lily Behnke, Rebecca Kramer-Bottiglio, Holly Yanco, Yan Gu",2025-08-04T22:06:05Z,2025-08-04T22:06:05Z,http://arxiv.org/abs/2508.02930v1,http://arxiv.org/pdf/2508.02930v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"In-Situ Soil-Property Estimation and Bayesian Mapping with a Simulated
  Compact Track Loader","Existing earthmoving autonomy is largely confined to highly controlled and
well-characterized environments due to the complexity of vehicle-terrain
interaction dynamics and the partial observability of the terrain resulting
from unknown and spatially varying soil conditions. In this chapter, a a
soil-property mapping system is proposed to extend the environmental state, in
order to overcome these restrictions and facilitate development of more robust
autonomous earthmoving. A GPU accelerated elevation mapping system is extended
to incorporate a blind mapping component which traces the movement of the blade
through the terrain to displace and erode intersected soil, enabling separately
tracking undisturbed and disturbed soil. Each interaction is approximated as a
flat blade moving through a locally homogeneous soil, enabling modeling of
cutting forces using the fundamental equation of earthmoving (FEE). Building
upon our prior work on in situ soil-property estimation, a method is devised to
extract approximate geometric parameters of the model given the uneven terrain,
and an improved physics infused neural network (PINN) model is developed to
predict soil properties and uncertainties of these estimates. A simulation of a
compact track loader (CTL) with a blade attachment is used to collect data to
train the PINN model. Post-training, the model is leveraged online by the
mapping system to track soil property estimates spatially as separate layers in
the map, with updates being performed in a Bayesian manner. Initial experiments
show that the system accurately highlights regions requiring higher relative
interaction forces, indicating the promise of this approach in enabling
soil-aware planning for autonomous terrain shaping.","W. Jacob Wagner, Ahmet Soylemezoglu, Katherine Driggs-Campbell",2025-07-30T03:41:44Z,2025-07-30T03:41:44Z,http://arxiv.org/abs/2507.22356v1,http://arxiv.org/pdf/2507.22356v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Hierarchical Reinforcement Learning Framework for Adaptive Walking
  Control Using General Value Functions of Lower-Limb Sensor Signals","Rehabilitation technology is a natural setting to study the shared learning
and decision-making of human and machine agents. In this work, we explore the
use of Hierarchical Reinforcement Learning (HRL) to develop adaptive control
strategies for lower-limb exoskeletons, aiming to enhance mobility and autonomy
for individuals with motor impairments. Inspired by prominent models of
biological sensorimotor processing, our investigated HRL approach breaks down
the complex task of exoskeleton control adaptation into a higher-level
framework for terrain strategy adaptation and a lower-level framework for
providing predictive information; this latter element is implemented via the
continual learning of general value functions (GVFs). GVFs generated temporal
abstractions of future signal values from multiple wearable lower-limb sensors,
including electromyography, pressure insoles, and goniometers. We investigated
two methods for incorporating actual and predicted sensor signals into a policy
network with the intent to improve the decision-making capacity of the control
system of a lower-limb exoskeleton during ambulation across varied terrains. As
a key result, we found that the addition of predictions made from GVFs
increased overall network accuracy. Terrain-specific performance increases were
seen while walking on even ground, uneven ground, up and down ramps, and turns,
terrains that are often misclassified without predictive information. This
suggests that predictive information can aid decision-making during
uncertainty, e.g., on terrains that have a high chance of being misclassified.
This work, therefore, contributes new insights into the nuances of HRL and the
future development of exoskeletons to facilitate safe transitioning and
traversing across different walking environments.","Sonny T. Jones, Grange M. Simpson, Patrick M. Pilarski, Ashley N. Dalrymple",2025-07-22T19:47:04Z,2025-07-22T19:47:04Z,http://arxiv.org/abs/2507.16983v1,http://arxiv.org/pdf/2507.16983v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Domain Adaptation and Multi-view Attention for Learnable Landmark
  Tracking with Sparse Data","The detection and tracking of celestial surface terrain features are crucial
for autonomous spaceflight applications, including Terrain Relative Navigation
(TRN), Entry, Descent, and Landing (EDL), hazard analysis, and scientific data
collection. Traditional photoclinometry-based pipelines often rely on extensive
a priori imaging and offline processing, constrained by the computational
limitations of radiation-hardened systems. While historically effective, these
approaches typically increase mission costs and duration, operate at low
processing rates, and have limited generalization. Recently, learning-based
computer vision has gained popularity to enhance spacecraft autonomy and
overcome these limitations. While promising, emerging techniques frequently
impose computational demands exceeding the capabilities of typical spacecraft
hardware for real-time operation and are further challenged by the scarcity of
labeled training data for diverse extraterrestrial environments. In this work,
we present novel formulations for in-situ landmark tracking via detection and
description. We utilize lightweight, computationally efficient neural network
architectures designed for real-time execution on current-generation spacecraft
flight processors. For landmark detection, we propose improved domain
adaptation methods that enable the identification of celestial terrain features
with distinct, cheaply acquired training data. Concurrently, for landmark
description, we introduce a novel attention alignment formulation that learns
robust feature representations that maintain correspondence despite significant
landmark viewpoint variations. Together, these contributions form a unified
system for landmark tracking that demonstrates superior performance compared to
existing state-of-the-art techniques.","Timothy Chase Jr, Karthik Dantu",2025-07-12T23:00:52Z,2025-07-12T23:00:52Z,http://arxiv.org/abs/2507.09420v1,http://arxiv.org/pdf/2507.09420v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Comparison of Path Planning Algorithms for Autonomous Vehicle Navigation
  Using Satellite and Airborne LiDAR Data","Autonomous vehicle navigation in unstructured environments, such as forests
and mountainous regions, presents significant challenges due to irregular
terrain and complex road conditions. This work provides a comparative
evaluation of mainstream and well-established path planning algorithms applied
to weighted pixel-level road networks derived from high-resolution satellite
imagery and airborne LiDAR data. For 2D road-map navigation, where the weights
reflect road conditions and terrain difficulty, A*, Dijkstra, RRT*, and a Novel
Improved Ant Colony Optimization Algorithm (NIACO) are tested on the DeepGlobe
satellite dataset. For 3D road-map path planning, 3D A*, 3D Dijkstra,
RRT-Connect, and NIACO are evaluated using the Hamilton airborne LiDAR dataset,
which provides detailed elevation information. All algorithms are assessed
under identical start and end point conditions, focusing on path cost,
computation time, and memory consumption. Results demonstrate that Dijkstra
consistently offers the most stable and efficient performance in both 2D and 3D
scenarios, particularly when operating on dense, pixel-level geospatial
road-maps. These findings highlight the reliability of Dijkstra-based planning
for static terrain navigation and establish a foundation for future research on
dynamic path planning under complex environmental constraints.","Chang Liu, Zhexiong Xue, Tamas Sziranyi",2025-07-08T11:15:21Z,2025-07-08T11:15:21Z,http://arxiv.org/abs/2507.05884v1,http://arxiv.org/pdf/2507.05884v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Environment-Aware and Human-Cooperative Swing Control for Lower-Limb
  Prostheses in Diverse Obstacle Scenarios","Current control strategies for powered lower limb prostheses often lack
awareness of the environment and the user's intended interactions with it. This
limitation becomes particularly apparent in complex terrains. Obstacle
negotiation, a critical scenario exemplifying such challenges, requires both
real-time perception of obstacle geometry and responsiveness to user intention
about when and where to step over or onto, to dynamically adjust swing
trajectories. We propose a novel control strategy that fuses environmental
awareness and human cooperativeness: an on-board depth camera detects obstacles
ahead of swing phase, prompting an elevated early-swing trajectory to ensure
clearance, while late-swing control defers to natural biomechanical cues from
the user. This approach enables intuitive stepping strategies without requiring
unnatural movement patterns. Experiments with three non-amputee participants
demonstrated 100 percent success across more than 150 step-overs and 30
step-ons with randomly placed obstacles of varying heights (4-16 cm) and
distances (15-70 cm). By effectively addressing obstacle navigation -- a
gateway challenge for complex terrain mobility -- our system demonstrates
adaptability to both environmental constraints and user intentions, with
promising applications across diverse locomotion scenarios.","Haosen Xing, Haoran Ma, Sijin Zhang, Hartmut Geyer",2025-07-01T18:15:50Z,2025-07-01T18:15:50Z,http://arxiv.org/abs/2507.01111v1,http://arxiv.org/pdf/2507.01111v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Real-time Terrain Analysis for Off-road Autonomous Vehicles,"This research addresses critical autonomous vehicle control challenges
arising from road roughness variation, which induces course deviations and
potential loss of road contact during steering operations. We present a novel
real-time road roughness estimation system employing Bayesian calibration
methodology that processes axle accelerations to predict terrain roughness with
quantifiable confidence measures. The technical framework integrates a Gaussian
process surrogate model with a simulated half-vehicle model, systematically
processing vehicle velocity and road surface roughness parameters to generate
corresponding axle acceleration responses. The Bayesian calibration routine
performs inverse estimation of road roughness from observed accelerations and
velocities, yielding posterior distributions that quantify prediction
uncertainty for adaptive risk management. Training data generation utilizes
Latin Hypercube sampling across comprehensive velocity and roughness parameter
spaces, while the calibrated model integrates seamlessly with a Simplex
controller architecture to dynamically adjust velocity limits based on
real-time roughness predictions. Experimental validation on stochastically
generated surfaces featuring varying roughness regions demonstrates robust
real-time characterization capabilities, with the integrated Simplex control
strategy effectively enhancing autonomous vehicle operational safety through
proactive surface condition response. This innovative Bayesian framework
establishes a comprehensive foundation for mitigating roughness-related
operational risks while simultaneously improving efficiency and safety margins
in autonomous vehicle systems.","Edwina Lewis, Aditya Parameshwaran, Laura Redmond, Yue Wang",2025-06-26T14:59:50Z,2025-06-26T14:59:50Z,http://arxiv.org/abs/2506.21347v1,http://arxiv.org/pdf/2506.21347v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Critical Anatomy-Preserving & Terrain-Augmenting Navigation (CAPTAiN):
  Application to Laminectomy Surgical Education","Surgical training remains a crucial milestone in modern medicine, with
procedures such as laminectomy exemplifying the high risks involved.
Laminectomy drilling requires precise manual control to mill bony tissue while
preserving spinal segment integrity and avoiding breaches in the dura: the
protective membrane surrounding the spinal cord. Despite unintended tears
occurring in up to 11.3% of cases, no assistive tools are currently utilized to
reduce this risk. Variability in patient anatomy further complicates learning
for novice surgeons. This study introduces CAPTAiN, a critical
anatomy-preserving and terrain-augmenting navigation system that provides
layered, color-coded voxel guidance to enhance anatomical awareness during
spinal drilling. CAPTAiN was evaluated against a standard non-navigated
approach through 110 virtual laminectomies performed by 11 orthopedic residents
and medical students. CAPTAiN significantly improved surgical completion rates
of target anatomy (87.99% vs. 74.42%) and reduced cognitive load across
multiple NASA-TLX domains. It also minimized performance gaps across experience
levels, enabling novices to perform on par with advanced trainees. These
findings highlight CAPTAiN's potential to optimize surgical execution and
support skill development across experience levels. Beyond laminectomy, it
demonstrates potential for broader applications across various surgical and
drilling procedures, including those in neurosurgery, otolaryngology, and other
medical fields.","Jonathan Wang, Hisashi Ishida, David Usevitch, Kesavan Venkatesh, Yi Wang, Mehran Armand, Rachel Bronheim, Amit Jain, Adnan Munawar",2025-06-25T14:43:58Z,2025-07-27T14:01:00Z,http://arxiv.org/abs/2506.20496v2,http://arxiv.org/pdf/2506.20496v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Hierarchical Reinforcement Learning and Value Optimization for
  Challenging Quadruped Locomotion","We propose a novel hierarchical reinforcement learning framework for
quadruped locomotion over challenging terrain. Our approach incorporates a
two-layer hierarchy in which a high-level policy (HLP) selects optimal goals
for a low-level policy (LLP). The LLP is trained using an on-policy
actor-critic RL algorithm and is given footstep placements as goals. We propose
an HLP that does not require any additional training or environment samples and
instead operates via an online optimization process over the learned value
function of the LLP. We demonstrate the benefits of this framework by comparing
it with an end-to-end reinforcement learning (RL) approach. We observe
improvements in its ability to achieve higher rewards with fewer collisions
across an array of different terrains, including terrains more difficult than
any encountered during training.","Jeremiah Coholich, Muhammad Ali Murtaza, Seth Hutchinson, Zsolt Kira",2025-06-24T22:19:15Z,2025-06-24T22:19:15Z,http://arxiv.org/abs/2506.20036v1,http://arxiv.org/pdf/2506.20036v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"StereoTacTip: Vision-based Tactile Sensing with Biomimetic Skin-Marker
  Arrangements","Vision-Based Tactile Sensors (VBTSs) stand out for their superior performance
due to their high-information content output. Recently, marker-based VBTSs have
been shown to give accurate geometry reconstruction when using stereo cameras.
\uhl{However, many marker-based VBTSs use complex biomimetic skin-marker
arrangements, which presents issues for the geometric reconstruction of the
skin surface from the markers}. Here we investigate how the marker-based skin
morphology affects stereo vision-based tactile sensing, using a novel VBTS
called the StereoTacTip. To achieve accurate geometry reconstruction, we
introduce: (i) stereo marker matching and tracking using a novel
Delaunay-Triangulation-Ring-Coding algorithm; (ii) a refractive depth
correction model that corrects the depth distortion caused by refraction in the
internal media; (iii) a skin surface correction model from the marker
positions, relying on an inverse calculation of normals to the skin surface;
and (iv)~methods for geometry reconstruction over multiple contacts. To
demonstrate these findings, we reconstruct topographic terrains on a large 3D
map. Even though contributions (i) and (ii) were developed for biomimetic
markers, they should improve the performance of all marker-based VBTSs.
Overall, this work illustrates that a thorough understanding and evaluation of
the morphologically-complex skin and marker-based tactile sensor principles are
crucial for obtaining accurate geometric information.","Chenghua Lu, Kailuan Tang, Xueming Hui, Haoran Li, Saekwang Nam, Nathan F. Lepora",2025-06-22T13:49:16Z,2025-06-22T13:49:16Z,http://arxiv.org/abs/2506.18040v1,http://arxiv.org/pdf/2506.18040v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"UAV Object Detection and Positioning in a Mining Industrial Metaverse
  with Custom Geo-Referenced Data","The mining sector increasingly adopts digital tools to improve operational
efficiency, safety, and data-driven decision-making. One of the key challenges
remains the reliable acquisition of high-resolution, geo-referenced spatial
information to support core activities such as extraction planning and on-site
monitoring. This work presents an integrated system architecture that combines
UAV-based sensing, LiDAR terrain modeling, and deep learning-based object
detection to generate spatially accurate information for open-pit mining
environments. The proposed pipeline includes geo-referencing, 3D
reconstruction, and object localization, enabling structured spatial outputs to
be integrated into an industrial digital twin platform. Unlike traditional
static surveying methods, the system offers higher coverage and automation
potential, with modular components suitable for deployment in real-world
industrial contexts. While the current implementation operates in post-flight
batch mode, it lays the foundation for real-time extensions. The system
contributes to the development of AI-enhanced remote sensing in mining by
demonstrating a scalable and field-validated geospatial data workflow that
supports situational awareness and infrastructure safety.","Vasiliki Balaska, Ioannis Tsampikos Papapetros, Katerina Maria Oikonomou, Loukas Bampis, Antonios Gasteratos",2025-06-16T13:59:56Z,2025-06-16T13:59:56Z,http://arxiv.org/abs/2506.13505v1,http://arxiv.org/pdf/2506.13505v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Learning Smooth State-Dependent Traversability from Dense Point Clouds,"A key open challenge in off-road autonomy is that the traversability of
terrain often depends on the vehicle's state. In particular, some obstacles are
only traversable from some orientations. However, learning this interaction by
encoding the angle of approach as a model input demands a large and diverse
training dataset and is computationally inefficient during planning due to
repeated model inference. To address these challenges, we present SPARTA, a
method for estimating approach angle conditioned traversability from point
clouds. Specifically, we impose geometric structure into our network by
outputting a smooth analytical function over the 1-Sphere that predicts risk
distribution for any angle of approach with minimal overhead and can be reused
for subsequent queries. The function is composed of Fourier basis functions,
which has important advantages for generalization due to their periodic nature
and smoothness. We demonstrate SPARTA both in a high-fidelity simulation
platform, where our model achieves a 91\% success rate crossing a 40m boulder
field (compared to 73\% for the baseline), and on hardware, illustrating the
generalization ability of the model to real-world settings.","Zihao Dong, Alan Papalia, Leonard Jung, Alenna Spiro, Philip R. Osteen, Christa S. Robison, Michael Everett",2025-06-04T18:21:54Z,2025-06-04T18:21:54Z,http://arxiv.org/abs/2506.04362v1,http://arxiv.org/pdf/2506.04362v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Hold My Beer: Learning Gentle Humanoid Locomotion and End-Effector
  Stabilization Control","Can your humanoid walk up and hand you a full cup of beer, without spilling a
drop? While humanoids are increasingly featured in flashy demos like dancing,
delivering packages, traversing rough terrain, fine-grained control during
locomotion remains a significant challenge. In particular, stabilizing a filled
end-effector (EE) while walking is far from solved, due to a fundamental
mismatch in task dynamics: locomotion demands slow-timescale, robust control,
whereas EE stabilization requires rapid, high-precision corrections. To address
this, we propose SoFTA, a Slow-Fast Two-Agent framework that decouples
upper-body and lower-body control into separate agents operating at different
frequencies and with distinct rewards. This temporal and objective separation
mitigates policy interference and enables coordinated whole-body behavior.
SoFTA executes upper-body actions at 100 Hz for precise EE control and
lower-body actions at 50 Hz for robust gait. It reduces EE acceleration by 2-5x
relative to baselines and performs much closer to human-level stability,
enabling delicate tasks such as carrying nearly full cups, capturing steady
video during locomotion, and disturbance rejection with EE stability.","Yitang Li, Yuanhang Zhang, Wenli Xiao, Chaoyi Pan, Haoyang Weng, Guanqi He, Tairan He, Guanya Shi",2025-05-30T04:18:09Z,2025-06-03T22:45:46Z,http://arxiv.org/abs/2505.24198v2,http://arxiv.org/pdf/2505.24198v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
A simulation framework for autonomous lunar construction work,"We present a simulation framework for lunar construction work involving
multiple autonomous machines. The framework supports modelling of construction
scenarios and autonomy solutions, execution of the scenarios in simulation, and
analysis of work time and energy consumption throughout the construction
project. The simulations are based on physics-based models for contacting
multibody dynamics and deformable terrain, including vehicle-soil interaction
forces and soil flow in real time. A behaviour tree manages the operational
logic and error handling, which enables the representation of complex
behaviours through a discrete set of simpler tasks in a modular hierarchical
structure. High-level decision-making is separated from lower-level control
algorithms, with the two connected via ROS2. Excavation movements are
controlled through inverse kinematics and tracking controllers. The framework
is tested and demonstrated on two different lunar construction scenarios that
involve an excavator and dump truck with actively controlled articulated
crawlers.","Mattias Linde, Daniel Lindmark, Sandra Ålstig, Martin Servin",2025-05-28T08:16:05Z,2025-08-12T07:41:09Z,http://arxiv.org/abs/2505.22091v2,http://arxiv.org/pdf/2505.22091v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Convergent Functions, Divergent Forms","We introduce LOKI, a compute-efficient framework for co-designing
morphologies and control policies that generalize across unseen tasks. Inspired
by biological adaptation -- where animals quickly adjust to morphological
changes -- our method overcomes the inefficiencies of traditional evolutionary
and quality-diversity algorithms. We propose learning convergent functions:
shared control policies trained across clusters of morphologically similar
designs in a learned latent space, drastically reducing the training cost per
design. Simultaneously, we promote divergent forms by replacing mutation with
dynamic local search, enabling broader exploration and preventing premature
convergence. The policy reuse allows us to explore 780$\times$ more designs
using 78% fewer simulation steps and 40% less compute per design. Local
competition paired with a broader search results in a diverse set of
high-performing final morphologies. Using the UNIMAL design space and a
flat-terrain locomotion task, LOKI discovers a rich variety of designs --
ranging from quadrupeds to crabs, bipedals, and spinners -- far more diverse
than those produced by prior work. These morphologies also transfer better to
unseen downstream tasks in agility, stability, and manipulation domains (e.g.,
2$\times$ higher reward on bump and push box incline tasks). Overall, our
approach produces designs that are both diverse and adaptable, with
substantially greater sample efficiency than existing co-design methods.
(Project website: https://loki-codesign.github.io/)","Hyeonseong Jeon, Ainaz Eftekhar, Aaron Walsman, Kuo-Hao Zeng, Ali Farhadi, Ranjay Krishna",2025-05-27T18:45:37Z,2025-05-27T18:45:37Z,http://arxiv.org/abs/2505.21665v1,http://arxiv.org/pdf/2505.21665v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Deriving The Fundamental Equation of Earthmoving and Configuring Vortex
  Studio Earthmoving Simulation for Soil Property Estimation Experimentation","This document serves as supplementary material for two International Society
for Terrain-Vehicle Systems conference publications regarding in situ soil
property estimation by Wagner et al. in 2023 and 2025. It covers the derivation
of the fundamental equation of earthmoving for a flat blade moving through
sloped soil and provides some information regarding the advanced configuration
of Vortex Studio's soil-tool interaction simulation.",W. Jacob Wagner,2025-05-25T21:35:14Z,2025-05-25T21:35:14Z,http://arxiv.org/abs/2505.19330v1,http://arxiv.org/pdf/2505.19330v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"One Policy but Many Worlds: A Scalable Unified Policy for Versatile
  Humanoid Locomotion","Humanoid locomotion faces a critical scalability challenge: traditional
reinforcement learning (RL) methods require task-specific rewards and struggle
to leverage growing datasets, even as more training terrains are introduced. We
propose DreamPolicy, a unified framework that enables a single policy to master
diverse terrains and generalize zero-shot to unseen scenarios by systematically
integrating offline data and diffusion-driven motion synthesis. At its core,
DreamPolicy introduces Humanoid Motion Imagery (HMI) - future state predictions
synthesized through an autoregressive terrain-aware diffusion planner curated
by aggregating rollouts from specialized policies across various distinct
terrains. Unlike human motion datasets requiring laborious retargeting, our
data directly captures humanoid kinematics, enabling the diffusion planner to
synthesize ""dreamed"" trajectories that encode terrain-specific physical
constraints. These trajectories act as dynamic objectives for our
HMI-conditioned policy, bypassing manual reward engineering and enabling
cross-terrain generalization. DreamPolicy addresses the scalability limitations
of prior methods: while traditional RL fails to exploit growing datasets, our
framework scales seamlessly with more offline data. As the dataset expands, the
diffusion prior learns richer locomotion skills, which the policy leverages to
master new terrains without retraining. Experiments demonstrate that
DreamPolicy achieves average 90% success rates in training environments and an
average of 20% higher success on unseen terrains than the prevalent method. It
also generalizes to perturbed and composite scenarios where prior approaches
collapse. By unifying offline data, diffusion-based trajectory synthesis, and
policy optimization, DreamPolicy overcomes the ""one task, one policy""
bottleneck, establishing a paradigm for scalable, data-driven humanoid control.","Yahao Fan, Tianxiang Gui, Kaiyang Ji, Shutong Ding, Chixuan Zhang, Jiayuan Gu, Jingyi Yu, Jingya Wang, Ye Shi",2025-05-24T16:33:44Z,2025-06-03T03:10:46Z,http://arxiv.org/abs/2505.18780v2,http://arxiv.org/pdf/2505.18780v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Parameter-Efficient Fine-Tuning of Vision Foundation Model for Forest
  Floor Segmentation from UAV Imagery","Unmanned Aerial Vehicles (UAVs) are increasingly used for reforestation and
forest monitoring, including seed dispersal in hard-to-reach terrains. However,
a detailed understanding of the forest floor remains a challenge due to high
natural variability, quickly changing environmental parameters, and ambiguous
annotations due to unclear definitions. To address this issue, we adapt the
Segment Anything Model (SAM), a vision foundation model with strong
generalization capabilities, to segment forest floor objects such as tree
stumps, vegetation, and woody debris. To this end, we employ
parameter-efficient fine-tuning (PEFT) to fine-tune a small subset of
additional model parameters while keeping the original weights fixed. We adjust
SAM's mask decoder to generate masks corresponding to our dataset categories,
allowing for automatic segmentation without manual prompting. Our results show
that the adapter-based PEFT method achieves the highest mean intersection over
union (mIoU), while Low-rank Adaptation (LoRA), with fewer parameters, offers a
lightweight alternative for resource-constrained UAV platforms.","Mohammad Wasil, Ahmad Drak, Brennan Penfold, Ludovico Scarton, Maximilian Johenneken, Alexander Asteroth, Sebastian Houben",2025-05-13T19:59:29Z,2025-05-13T19:59:29Z,http://arxiv.org/abs/2505.08932v1,http://arxiv.org/pdf/2505.08932v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Land-Coverage Aware Path-Planning for Multi-UAV Swarms in Search and
  Rescue Scenarios","Unmanned Aerial Vehicles (UAVs) have become vital in search-and-rescue (SAR)
missions, with autonomous mission planning improving response times and
coverage efficiency. Early approaches primarily used path planning techniques
such as A*, potential-fields, or Dijkstra's algorithm, while recent approaches
have incorporated meta-heuristic frameworks like genetic algorithms and
particle swarm optimization to balance competing objectives such as network
connectivity, energy efficiency, and strategic placement of charging stations.
However, terrain-aware path planning remains under-explored, despite its
critical role in optimizing UAV SAR deployments. To address this gap, we
present a computer-vision based terrain-aware mission planner that autonomously
extracts and analyzes terrain topology to enhance SAR pre-flight planning. Our
framework uses a deep segmentation network fine-tuned on our own collection of
landcover datasets to transform satellite imagery into a structured, grid-based
representation of the operational area. This classification enables
terrain-specific UAV-task allocation, improving deployment strategies in
complex environments. We address the challenge of irregular terrain partitions,
by introducing a two-stage partitioning scheme that first evaluates terrain
monotonicity along coordinate axes before applying a cost-based recursive
partitioning process, minimizing unnecessary splits and optimizing path
efficiency. Empirical validation in a high-fidelity simulation environment
demonstrates that our approach improves search and dispatch time over multiple
meta-heuristic techniques and against a competing state-of-the-art method.
These results highlight its potential for large-scale SAR operations, where
rapid response and efficient UAV coordination are critical.","Pedro Antonio Alarcon Granadeno, Jane Cleland-Huang",2025-05-12T20:56:52Z,2025-05-12T20:56:52Z,http://arxiv.org/abs/2505.08060v1,http://arxiv.org/pdf/2505.08060v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Terrain-aware Low Altitude Path Planning,"In this paper, we study the problem of generating low-altitude path plans for
nap-of-the-earth (NOE) flight in real time with only RGB images from onboard
cameras and the vehicle pose. We propose a novel training method that combines
behavior cloning and self-supervised learning, where the self-supervision
component allows the learned policy to refine the paths generated by the expert
planner. Simulation studies show 24.7% reduction in average path elevation
compared to the standard behavior cloning approach.","Yixuan Jia, Andrea Tagliabue, Annika Thomas, Navid Dadkhah Tehrani, Jonathan P. How",2025-05-11T22:53:45Z,2025-06-23T20:18:37Z,http://arxiv.org/abs/2505.07141v2,http://arxiv.org/pdf/2505.07141v2.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"PARC: Physics-based Augmentation with Reinforcement Learning for
  Character Controllers","Humans excel in navigating diverse, complex environments with agile motor
skills, exemplified by parkour practitioners performing dynamic maneuvers, such
as climbing up walls and jumping across gaps. Reproducing these agile movements
with simulated characters remains challenging, in part due to the scarcity of
motion capture data for agile terrain traversal behaviors and the high cost of
acquiring such data. In this work, we introduce PARC (Physics-based
Augmentation with Reinforcement Learning for Character Controllers), a
framework that leverages machine learning and physics-based simulation to
iteratively augment motion datasets and expand the capabilities of terrain
traversal controllers. PARC begins by training a motion generator on a small
dataset consisting of core terrain traversal skills. The motion generator is
then used to produce synthetic data for traversing new terrains. However, these
generated motions often exhibit artifacts, such as incorrect contacts or
discontinuities. To correct these artifacts, we train a physics-based tracking
controller to imitate the motions in simulation. The corrected motions are then
added to the dataset, which is used to continue training the motion generator
in the next iteration. PARC's iterative process jointly expands the
capabilities of the motion generator and tracker, creating agile and versatile
models for interacting with complex environments. PARC provides an effective
approach to develop controllers for agile terrain traversal, which bridges the
gap between the scarcity of motion data and the need for versatile character
controllers.","Michael Xu, Yi Shi, KangKang Yin, Xue Bin Peng",2025-05-06T22:29:07Z,2025-05-06T22:29:07Z,http://arxiv.org/abs/2505.04002v1,http://arxiv.org/pdf/2505.04002v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Path Planning on Multi-level Point Cloud with a Weighted Traversability
  Graph","This article proposes a new path planning method for addressing multi-level
terrain situations. The proposed method includes innovations in three aspects:
1) the pre-processing of point cloud maps with a multi-level skip-list
structure and data-slimming algorithm for well-organized and simplified map
formalization and management, 2) the direct acquisition of local traversability
indexes through vehicle and point cloud interaction analysis, which saves work
in surface fitting, and 3) the assignment of traversability indexes on a
multi-level connectivity graph to generate a weighted traversability graph for
generally search-based path planning. The A* algorithm is modified to utilize
the traversability graph to generate a short and safe path. The effectiveness
and reliability of the proposed method are verified through indoor and outdoor
experiments conducted in various environments, including multi-floor buildings,
woodland, and rugged mountainous regions. The results demonstrate that the
proposed method can properly address 3D path planning problems for ground
vehicles in a wide range of situations.","Yujie Tang, Quan Li, Hao Geng, Yangmin Xie, Hang Shi, Yusheng Yang",2025-04-30T13:24:53Z,2025-04-30T13:24:53Z,http://arxiv.org/abs/2504.21622v1,http://arxiv.org/pdf/2504.21622v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"NavEX: A Multi-Agent Coverage in Non-Convex and Uneven Environments via
  Exemplar-Clustering","This paper addresses multi-agent deployment in non-convex and uneven
environments. To overcome the limitations of traditional approaches, we
introduce Navigable Exemplar-Based Dispatch Coverage (NavEX), a novel dispatch
coverage framework that combines exemplar-clustering with obstacle-aware and
traversability-aware shortest distances, offering a deployment framework based
on submodular optimization. NavEX provides a unified approach to solve two
critical coverage tasks: (a) fair-access deployment, aiming to provide
equitable service by minimizing agent-target distances, and (b) hotspot
deployment, prioritizing high-density target regions. A key feature of NavEX is
the use of exemplar-clustering for the coverage utility measure, which provides
the flexibility to employ non-Euclidean distance metrics that do not
necessarily conform to the triangle inequality. This allows NavEX to
incorporate visibility graphs for shortest-path computation in environments
with planar obstacles, and traversability-aware RRT* for complex, rugged
terrains. By leveraging submodular optimization, the NavEX framework enables
efficient, near-optimal solutions with provable performance guarantees for
multi-agent deployment in realistic and complex settings, as demonstrated by
our simulations.","Donipolo Ghimire, Carlos Nieto-Granda, Solmaz S. Kia",2025-04-29T18:50:49Z,2025-04-29T18:50:49Z,http://arxiv.org/abs/2504.21113v1,http://arxiv.org/pdf/2504.21113v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Meta-Learning Online Dynamics Model Adaptation in Off-Road Autonomous
  Driving","High-speed off-road autonomous driving presents unique challenges due to
complex, evolving terrain characteristics and the difficulty of accurately
modeling terrain-vehicle interactions. While dynamics models used in
model-based control can be learned from real-world data, they often struggle to
generalize to unseen terrain, making real-time adaptation essential. We propose
a novel framework that combines a Kalman filter-based online adaptation scheme
with meta-learned parameters to address these challenges. Offline meta-learning
optimizes the basis functions along which adaptation occurs, as well as the
adaptation parameters, while online adaptation dynamically adjusts the onboard
dynamics model in real time for model-based control. We validate our approach
through extensive experiments, including real-world testing on a full-scale
autonomous off-road vehicle, demonstrating that our method outperforms baseline
approaches in prediction accuracy, performance, and safety metrics,
particularly in safety-critical scenarios. Our results underscore the
effectiveness of meta-learned dynamics model adaptation, advancing the
development of reliable autonomous systems capable of navigating diverse and
unseen environments. Video is available at: https://youtu.be/cCKHHrDRQEA","Jacob Levy, Jason Gibson, Bogdan Vlahov, Erica Tevere, Evangelos Theodorou, David Fridovich-Keil, Patrick Spieler",2025-04-23T17:51:36Z,2025-04-23T17:51:36Z,http://arxiv.org/abs/2504.16923v1,http://arxiv.org/pdf/2504.16923v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"A Genetic Approach to Gradient-Free Kinodynamic Planning in Uneven
  Terrains","This paper proposes a genetic algorithm-based kinodynamic planning algorithm
(GAKD) for car-like vehicles navigating uneven terrains modeled as triangular
meshes. The algorithm's distinct feature is trajectory optimization over a
fixed-length receding horizon using a genetic algorithm with heuristic-based
mutation, ensuring the vehicle's controls remain within its valid operational
range. By addressing challenges posed by uneven terrain meshes, such as
changing face normals, GAKD offers a practical solution for path planning in
complex environments. Comparative evaluations against Model Predictive Path
Integral (MPPI) and log-MPPI methods show that GAKD achieves up to 20 percent
improvement in traversability cost while maintaining comparable path length.
These results demonstrate GAKD's potential in improving vehicle navigation on
challenging terrains.","Otobong Jerome, Alexandr Klimchik, Alexander Maloletov, Geesara Kulathunga",2025-04-17T06:11:31Z,2025-04-17T06:11:31Z,http://arxiv.org/abs/2504.12678v1,http://arxiv.org/pdf/2504.12678v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Self-Supervised Traversability Learning with Online Prototype Adaptation
  for Off-Road Autonomous Driving","Achieving reliable and safe autonomous driving in off-road environments
requires accurate and efficient terrain traversability analysis. However, this
task faces several challenges, including the scarcity of large-scale datasets
tailored for off-road scenarios, the high cost and potential errors of manual
annotation, the stringent real-time requirements of motion planning, and the
limited computational power of onboard units. To address these challenges, this
paper proposes a novel traversability learning method that leverages
self-supervised learning, eliminating the need for manual annotation. For the
first time, a Birds-Eye View (BEV) representation is used as input, reducing
computational burden and improving adaptability to downstream motion planning.
During vehicle operation, the proposed method conducts online analysis of
traversed regions and dynamically updates prototypes to adaptively assess the
traversability of the current environment, effectively handling dynamic scene
changes. We evaluate our approach against state-of-the-art benchmarks on both
public datasets and our own dataset, covering diverse seasons and geographical
locations. Experimental results demonstrate that our method significantly
outperforms recent approaches. Additionally, real-world vehicle experiments
show that our method operates at 10 Hz, meeting real-time requirements, while a
5.5 km autonomous driving experiment further validates the generated
traversability cost maps compatibility with downstream motion planning.","Yafeng Bu, Zhenping Sun, Xiaohui Li, Jun Zeng, Xin Zhang, Hui Shen",2025-04-16T14:17:31Z,2025-04-16T14:17:31Z,http://arxiv.org/abs/2504.12109v1,http://arxiv.org/pdf/2504.12109v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
Shape Your Ground: Refining Road Surfaces Beyond Planar Representations,"Road surface reconstruction from aerial images is fundamental for autonomous
driving, urban planning, and virtual simulation, where smoothness, compactness,
and accuracy are critical quality factors. Existing reconstruction methods
often produce artifacts and inconsistencies that limit usability, while
downstream tasks have a tendency to represent roads as planes for simplicity
but at the cost of accuracy. We introduce FlexRoad, the first framework to
directly address road surface smoothing by fitting Non-Uniform Rational
B-Splines (NURBS) surfaces to 3D road points obtained from photogrammetric
reconstructions or geodata providers. Our method at its core utilizes the
Elevation-Constrained Spatial Road Clustering (ECSRC) algorithm for robust
anomaly correction, significantly reducing surface roughness and fitting
errors. To facilitate quantitative comparison between road surface
reconstruction methods, we present GeoRoad Dataset (GeRoD), a diverse
collection of road surface and terrain profiles derived from openly accessible
geodata. Experiments on GeRoD and the photogrammetry-based DeepScenario Open 3D
Dataset (DSC3D) demonstrate that FlexRoad considerably surpasses commonly used
road surface representations across various metrics while being insensitive to
various input sources, terrains, and noise types. By performing ablation
studies, we identify the key role of each component towards high-quality
reconstruction performance, making FlexRoad a generic method for realistic road
surface modeling.","Oussema Dhaouadi, Johannes Meier, Jacques Kaiser, Daniel Cremers",2025-04-15T21:20:44Z,2025-04-15T21:20:44Z,http://arxiv.org/abs/2504.16103v1,http://arxiv.org/pdf/2504.16103v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Bipedal Robust Walking on Uneven Footholds: Piecewise Slope LIPM with
  Discrete Model Predictive Control","This study presents an enhanced theoretical formulation for bipedal
hierarchical control frameworks under uneven terrain conditions. Specifically,
owing to the inherent limitations of the Linear Inverted Pendulum Model (LIPM)
in handling terrain elevation variations, we develop a Piecewise Slope LIPM
(PS-LIPM). This innovative model enables dynamic adjustment of the Center of
Mass (CoM) height to align with topographical undulations during single-step
cycles. Another contribution is proposed a generalized Angular Momentum-based
LIPM (G-ALIP) for CoM velocity compensation using Centroidal Angular Momentum
(CAM) regulation. Building upon these advancements, we derive the DCM
step-to-step dynamics for Model Predictive Control MPC formulation, enabling
simultaneous optimization of step position and step duration. A hierarchical
control framework integrating MPC with a Whole-Body Controller (WBC) is
implemented for bipedal locomotion across uneven stepping stones. The results
validate the efficacy of the proposed hierarchical control framework and the
theoretical formulation.","Yapeng Shi, Sishu Li, Yongqiang Wu, Junjie Liu, Xiaokun Leng, Xizhe Zang, Songhao Piao",2025-04-03T03:54:10Z,2025-04-03T03:54:10Z,http://arxiv.org/abs/2504.02255v1,http://arxiv.org/pdf/2504.02255v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Dom, cars don't fly! -- Or do they? In-Air Vehicle Maneuver for
  High-Speed Off-Road Navigation","When pushing the speed limit for aggressive off-road navigation on uneven
terrain, it is inevitable that vehicles may become airborne from time to time.
During time-sensitive tasks, being able to fly over challenging terrain can
also save time, instead of cautiously circumventing or slowly negotiating
through. However, most off-road autonomy systems operate under the assumption
that the vehicles are always on the ground and therefore limit operational
speed. In this paper, we present a novel approach for in-air vehicle maneuver
during high-speed off-road navigation. Based on a hybrid forward kinodynamic
model using both physics principles and machine learning, our fixed-horizon,
sampling-based motion planner ensures accurate vehicle landing poses and their
derivatives within a short airborne time window using vehicle throttle and
steering commands. We test our approach in extensive in-air experiments both
indoors and outdoors, compare it against an error-driven control method, and
demonstrate that precise and timely in-air vehicle maneuver is possible through
existing ground vehicle controls.","Anuj Pokhrel, Aniket Datar, Xuesu Xiao",2025-03-24T20:51:22Z,2025-03-24T20:51:22Z,http://arxiv.org/abs/2503.19140v1,http://arxiv.org/pdf/2503.19140v1.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
"Human locomotor control timescales depend on the environmental context
  and sensory input modality","Everyday locomotion is a complex sensorimotor process that can unfold over
multiple timescales, from long-term path planning to rapid, reactive
adjustments. However, we lack an understanding of how factors such as
environmental demands, or the available sensory information simultaneously
influence these control timescales. To address this, we present a unified
data-driven framework to quantify the control timescales by identifying how
early we can predict future actions from past inputs. We apply this framework
across tasks including walking and running, environmental contexts including
treadmill, overground, and varied terrains, and sensory input modalities
including gaze fixations and body states. We find that deep neural network
architectures that effectively handle long-range dependencies, specifically
Gated Recurrent Units and Transformers, outperform other architectures and
widely used linear models when predicting future actions. Our framework reveals
the factors that influence locomotor foot placement control timescales. Across
environmental contexts, we discover that humans rely more on fast timescale
control in more complex terrain. Across input modalities, we find a hierarchy
of control timescales where gaze predicts foot placement before full-body
states, which predict before center-of-mass states. Our model also identifies
mid-swing as a critical phase when the swing foot's state predicts its future
placement, with this timescale adapting across environments. Overall, this work
offers data-driven insights into locomotor control in everyday settings,
offering models that can be integrated with rehabilitation technologies and
movement simulations to improve their applicability in everyday settings.","Wei-Chen Wang, Antoine De Comite, Alexandra Voloshina, Monica Daley, Nidhi Seethapathi",2025-03-20T16:57:15Z,2025-08-27T03:07:09Z,http://arxiv.org/abs/2503.16340v5,http://arxiv.org/pdf/2503.16340v5.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
ADAPT: An Autonomous Forklift for Construction Site Operation,"Efficient material logistics play a critical role in controlling costs and
schedules in the construction industry. However, manual material handling
remains prone to inefficiencies, delays, and safety risks. Autonomous forklifts
offer a promising solution to streamline on-site logistics, reducing reliance
on human operators and mitigating labor shortages. This paper presents the
development and evaluation of ADAPT (Autonomous Dynamic All-terrain Pallet
Transporter), a fully autonomous off-road forklift designed for construction
environments. Unlike structured warehouse settings, construction sites pose
significant challenges, including dynamic obstacles, unstructured terrain, and
varying weather conditions. To address these challenges, our system integrates
AI-driven perception techniques with traditional approaches for decision
making, planning, and control, enabling reliable operation in complex
environments. We validate the system through extensive real-world testing,
comparing its continuous performance against an experienced human operator
across various weather conditions. Our findings demonstrate that autonomous
outdoor forklifts can operate near human-level performance, offering a viable
path toward safer and more efficient construction logistics.","Johannes Huemer, Markus Murschitz, Matthias Schörghuber, Lukas Reisinger, Thomas Kadiofsky, Christoph Weidinger, Mario Niedermeyer, Benedikt Widy, Marcel Zeilinger, Csaba Beleznai, Tobias Glück, Andreas Kugi, Patrik Zips",2025-03-18T15:03:28Z,2025-05-02T09:17:40Z,http://arxiv.org/abs/2503.14331v3,http://arxiv.org/pdf/2503.14331v3.pdf,all:terrain AND all:robot AND submittedDate:[202309062228 TO 202509052228],2025
