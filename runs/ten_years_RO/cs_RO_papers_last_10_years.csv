title,summary,authors,published,url,pdf_url,categories
Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments,"Generating collision-free motion in dynamic, partially observable
environments is a fundamental challenge for robotic manipulators. Classical
motion planners can compute globally optimal trajectories but require full
environment knowledge and are typically too slow for dynamic scenes. Neural
motion policies offer a promising alternative by operating in closed-loop
directly on raw sensory inputs but often struggle to generalize in complex or
dynamic settings. We propose Deep Reactive Policy (DRP), a visuo-motor neural
motion policy designed for reactive motion generation in diverse dynamic
environments, operating directly on point cloud sensory input. At its core is
IMPACT, a transformer-based neural motion policy pretrained on 10 million
generated expert trajectories across diverse simulation scenarios. We further
improve IMPACT's static obstacle avoidance through iterative student-teacher
finetuning. We additionally enhance the policy's dynamic obstacle avoidance at
inference time using DCP-RMP, a locally reactive goal-proposal module. We
evaluate DRP on challenging tasks featuring cluttered scenes, dynamic moving
obstacles, and goal obstructions. DRP achieves strong generalization,
outperforming prior classical and neural methods in success rate across both
simulated and real-world settings. Video results and code available at
https://deep-reactive-policy.com","Jiahui Yang, Jason Jingzhou Liu, Yulong Li, Youssef Khaky, Kenneth Shaw, Deepak Pathak",2025-09-08 17:59:35+00:00,http://arxiv.org/abs/2509.06953v1,http://arxiv.org/pdf/2509.06953v1,"cs.RO, cs.AI, cs.CV, cs.LG, cs.SY, eess.SY"
F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions,"Executing language-conditioned tasks in dynamic visual environments remains a
central challenge in embodied AI. Existing Vision-Language-Action (VLA) models
predominantly adopt reactive state-to-action mappings, often leading to
short-sighted behaviors and poor robustness in dynamic scenes. In this paper,
we introduce F1, a pretrained VLA framework which integrates the visual
foresight generation into decision-making pipeline. F1 adopts a
Mixture-of-Transformer architecture with dedicated modules for perception,
foresight generation, and control, thereby bridging understanding, generation,
and actions. At its core, F1 employs a next-scale prediction mechanism to
synthesize goal-conditioned visual foresight as explicit planning targets. By
forecasting plausible future visual states, F1 reformulates action generation
as a foresight-guided inverse dynamics problem, enabling actions that
implicitly achieve visual goals. To endow F1 with robust and generalizable
capabilities, we propose a three-stage training recipe on an extensive dataset
comprising over 330k trajectories across 136 diverse tasks. This training
scheme enhances modular reasoning and equips the model with transferable visual
foresight, which is critical for complex and dynamic environments. Extensive
evaluations on real-world tasks and simulation benchmarks demonstrate F1
consistently outperforms existing approaches, achieving substantial gains in
both task success rate and generalization ability.","Qi Lv, Weijie Kong, Hao Li, Jia Zeng, Zherui Qiu, Delin Qu, Haoming Song, Qizhi Chen, Xiang Deng, Jiangmiao Pang",2025-09-08 17:58:30+00:00,http://arxiv.org/abs/2509.06951v2,http://arxiv.org/pdf/2509.06951v2,"cs.RO, cs.CV"
"""It was Tragic"": Exploring the Impact of a Robot's Shutdown","It is well established that people perceive robots as social entities, even
when they are not designed for social interaction. We evaluated whether the
social interpretation of robotic gestures should also be considered when
turning off a robot. In the experiment, participants engaged in a brief
preliminary neutral interaction while a robotic arm showed interest in their
actions. At the end of the task, participants were asked to turn off the
robotic arm under two conditions: (1) a Non-designed condition, where all of
the robot's engines were immediately and simultaneously turned off, as robots
typically shut down; (2) a Designed condition, where the robot's engines
gradually folded inward in a motion resembling ""falling asleep."" Our findings
revealed that all participants anthropomorphized the robot's movement when it
was turned off. In the Non-designed condition, most participants interpreted
the robot's turn-off movement negatively, as if the robot had ""died."" In the
Designed condition, most participants interpreted it more neutrally, stating
that the robot ""went to sleep."" The robot's turn-off movement also impacted its
perception, leading to higher likeability, perceived intelligence, and animacy
in the Designed condition. We conclude that the impact of common edge
interactions, such as turning off a robot, should be carefully designed while
considering people's automatic tendency to perceive robots as social entities.","Agam Oberlender, Hadas Erel",2025-09-08 17:46:00+00:00,http://arxiv.org/abs/2509.06934v1,http://arxiv.org/pdf/2509.06934v1,"cs.HC, cs.RO"
LLaDA-VLA: Vision Language Diffusion Action Models,"The rapid progress of auto-regressive vision-language models (VLMs) has
inspired growing interest in vision-language-action models (VLA) for robotic
manipulation. Recently, masked diffusion models, a paradigm distinct from
autoregressive models, have begun to demonstrate competitive performance in
text generation and multimodal applications, leading to the development of a
series of diffusion-based VLMs (d-VLMs). However, leveraging such models for
robot policy learning remains largely unexplored. In this work, we present
LLaDA-VLA, the first Vision-Language-Diffusion-Action model built upon
pretrained d-VLMs for robotic manipulation. To effectively adapt d-VLMs to
robotic domain, we introduce two key designs: (1) a localized special-token
classification strategy that replaces full-vocabulary classification with
special action token classification, reducing adaptation difficulty; (2) a
hierarchical action-structured decoding strategy that decodes action sequences
hierarchically considering the dependencies within and across actions.
Extensive experiments demonstrate that LLaDA-VLA significantly outperforms
state-of-the-art VLAs on both simulation and real-world robots.","Yuqing Wen, Hebei Li, Kefan Gu, Yucheng Zhao, Tiancai Wang, Xiaoyan Sun",2025-09-08 17:45:40+00:00,http://arxiv.org/abs/2509.06932v1,http://arxiv.org/pdf/2509.06932v1,"cs.RO, cs.CV"
Nanobot Algorithms for Treatment of Diffuse Cancer,"Motile nanosized particles, or ""nanobots"", promise more effective and less
toxic targeted drug delivery because of their unique scale and precision. We
consider the case in which the cancer is ""diffuse"", dispersed such that there
are multiple distinct cancer sites. We investigate the problem of a swarm of
nanobots locating these sites and treating them by dropping drug payloads at
the sites. To improve the success of the treatment, the drug payloads must be
allocated between sites according to their ""demands""; this requires extra
nanobot coordination. We present a mathematical model of the behavior of the
nanobot agents and of their colloidal environment. This includes a movement
model for agents based upon experimental findings from actual nanoparticles in
which bots noisily ascend and descend chemical gradients. We present three
algorithms: The first algorithm, called KM, is the most representative of
reality, with agents simply following naturally existing chemical signals that
surround each cancer site. The second algorithm, KMA, includes an additional
chemical payload which amplifies the existing natural signals. The third
algorithm, KMAR, includes another additional chemical payload which counteracts
the other signals, instead inducing negative chemotaxis in agents such that
they are repelled from sites that are already sufficiently treated. We present
simulation results for all algorithms across different types of cancer
arrangements. For KM, we show that the treatment is generally successful unless
the natural chemical signals are weak, in which case the treatment progresses
too slowly. For KMA, we demonstrate a significant improvement in treatment
speed but a drop in eventual success, except for concentrated cancer patterns.
For KMAR, our results show great performance across all types of cancer
patterns, demonstrating robustness and adaptability.","Noble Harasha, Nancy Lynch",2025-09-08 17:11:59+00:00,http://arxiv.org/abs/2509.06893v1,http://arxiv.org/pdf/2509.06893v1,"cs.MA, cs.RO, q-bio.QM"
Dynamic Modeling and Efficient Data-Driven Optimal Control for Micro Autonomous Surface Vehicles,"Micro Autonomous Surface Vehicles (MicroASVs) offer significant potential for
operations in confined or shallow waters and swarm robotics applications.
However, achieving precise and robust control at such small scales remains
highly challenging, mainly due to the complexity of modeling nonlinear
hydrodynamic forces and the increased sensitivity to self-motion effects and
environmental disturbances, including waves and boundary effects in confined
spaces. This paper presents a physics-driven dynamics model for an
over-actuated MicroASV and introduces a data-driven optimal control framework
that leverages a weak formulation-based online model learning method. Our
approach continuously refines the physics-driven model in real time, enabling
adaptive control that adjusts to changing system parameters. Simulation results
demonstrate that the proposed method substantially enhances trajectory tracking
accuracy and robustness, even under unknown payloads and external disturbances.
These findings highlight the potential of data-driven online learning-based
optimal control to improve MicroASV performance, paving the way for more
reliable and precise autonomous surface vehicle operations.","Zhiheng Chen, Wei Wang",2025-09-08 17:01:04+00:00,http://arxiv.org/abs/2509.06882v1,http://arxiv.org/pdf/2509.06882v1,cs.RO
CRISP -- Compliant ROS2 Controllers for Learning-Based Manipulation Policies and Teleoperation,"Learning-based controllers, such as diffusion policies and vision-language
action models, often generate low-frequency or discontinuous robot state
changes. Achieving smooth reference tracking requires a low-level controller
that converts high-level targets commands into joint torques, enabling
compliant behavior during contact interactions. We present CRISP, a lightweight
C++ implementation of compliant Cartesian and joint-space controllers for the
ROS2 control standard, designed for seamless integration with high-level
learning-based policies as well as teleoperation. The controllers are
compatible with any manipulator that exposes a joint-torque interface. Through
our Python and Gymnasium interfaces, CRISP provides a unified pipeline for
recording data from hardware and simulation and deploying high-level
learning-based policies seamlessly, facilitating rapid experimentation. The
system has been validated on hardware with the Franka Robotics FR3 and in
simulation with the Kuka IIWA14 and Kinova Gen3. Designed for rapid
integration, flexible deployment, and real-time performance, our implementation
provides a unified pipeline for data collection and policy execution, lowering
the barrier to applying learning-based methods on ROS2-compatible manipulators.
Detailed documentation is available at the project website -
https://utiasDSL.github.io/crisp_controllers.","Daniel San José Pro, Oliver Hausdörfer, Ralf Römer, Maximilian Dösch, Martin Schuck, Angela P. Schöllig",2025-09-08 15:55:50+00:00,http://arxiv.org/abs/2509.06819v1,http://arxiv.org/pdf/2509.06819v1,cs.RO
Embodied Hazard Mitigation using Vision-Language Models for Autonomous Mobile Robots,"Autonomous robots operating in dynamic environments should identify and
report anomalies. Embodying proactive mitigation improves safety and
operational continuity. This paper presents a multimodal anomaly detection and
mitigation system that integrates vision-language models and large language
models to identify and report hazardous situations and conflicts in real-time.
The proposed system enables robots to perceive, interpret, report, and if
possible respond to urban and environmental anomalies through proactive
detection mechanisms and automated mitigation actions. A key contribution in
this paper is the integration of Hazardous and Conflict states into the robot's
decision-making framework, where each anomaly type can trigger specific
mitigation strategies. User studies (n = 30) demonstrated the effectiveness of
the system in anomaly detection with 91.2% prediction accuracy and relatively
low latency response times using edge-ai architecture.","Oluwadamilola Sotomi, Devika Kodi, Kiruthiga Chandra Shekar, Aliasghar Arab",2025-09-08 14:53:19+00:00,http://arxiv.org/abs/2509.06768v1,http://arxiv.org/pdf/2509.06768v1,cs.RO
Event Spectroscopy: Event-based Multispectral and Depth Sensing using Structured Light,"Uncrewed aerial vehicles (UAVs) are increasingly deployed in forest
environments for tasks such as environmental monitoring and search and rescue,
which require safe navigation through dense foliage and precise data
collection. Traditional sensing approaches, including passive multispectral and
RGB imaging, suffer from latency, poor depth resolution, and strong dependence
on ambient light - especially under forest canopies. In this work, we present a
novel event spectroscopy system that simultaneously enables high-resolution,
low-latency depth reconstruction and multispectral imaging using a single
sensor. Depth is reconstructed using structured light, and by modulating the
wavelength of the projected structured light, our system captures spectral
information in controlled bands between 650 nm and 850 nm. We demonstrate up to
$60\%$ improvement in RMSE over commercial depth sensors and validate the
spectral accuracy against a reference spectrometer and commercial multispectral
cameras, demonstrating comparable performance. A portable version limited to
RGB (3 wavelengths) is used to collect real-world depth and spectral data from
a Masoala Rainforest. We demonstrate the use of this prototype for color image
reconstruction and material differentiation between leaves and branches using
spectral and depth data. Our results show that adding depth (available at no
extra effort with our setup) to material differentiation improves the accuracy
by over $30\%$ compared to color-only method. Our system, tested in both lab
and real-world rainforest environments, shows strong performance in depth
estimation, RGB reconstruction, and material differentiation - paving the way
for lightweight, integrated, and robust UAV perception and data collection in
complex natural environments.","Christian Geckeler, Niklas Neugebauer, Manasi Muglikar, Davide Scaramuzza, Stefano Mintchev",2025-09-08 14:34:55+00:00,http://arxiv.org/abs/2509.06741v1,http://arxiv.org/pdf/2509.06741v1,"cs.CV, cs.RO"
VehicleWorld: A Highly Integrated Multi-Device Environment for Intelligent Vehicle Interaction,"Intelligent vehicle cockpits present unique challenges for API Agents,
requiring coordination across tightly-coupled subsystems that exceed typical
task environments' complexity. Traditional Function Calling (FC) approaches
operate statelessly, requiring multiple exploratory calls to build
environmental awareness before execution, leading to inefficiency and limited
error recovery. We introduce VehicleWorld, the first comprehensive environment
for the automotive domain, featuring 30 modules, 250 APIs, and 680 properties
with fully executable implementations that provide real-time state information
during agent execution. This environment enables precise evaluation of vehicle
agent behaviors across diverse, challenging scenarios. Through systematic
analysis, we discovered that direct state prediction outperforms function
calling for environmental control. Building on this insight, we propose
State-based Function Call (SFC), a novel approach that maintains explicit
system state awareness and implements direct state transitions to achieve
target conditions. Experimental results demonstrate that SFC significantly
outperforms traditional FC approaches, achieving superior execution accuracy
and reduced latency. We have made all implementation code publicly available on
Github https://github.com/OpenMOSS/VehicleWorld.","Jie Yang, Jiajun Chen, Zhangyue Yin, Shuo Chen, Yuxin Wang, Yiran Guo, Yuan Li, Yining Zheng, Xuanjing Huang, Xipeng Qiu",2025-09-08 14:28:25+00:00,http://arxiv.org/abs/2509.06736v1,http://arxiv.org/pdf/2509.06736v1,"cs.AI, cs.CL, cs.RO"
Safe Robust Predictive Control-based Motion Planning of Automated Surface Vessels in Inland Waterways,"Deploying self-navigating surface vessels in inland waterways offers a
sustainable alternative to reduce road traffic congestion and emissions.
However, navigating confined waterways presents unique challenges, including
narrow channels, higher traffic density, and hydrodynamic disturbances.
Existing methods for autonomous vessel navigation often lack the robustness or
precision required for such environments. This paper presents a new motion
planning approach for Automated Surface Vessels (ASVs) using Robust Model
Predictive Control (RMPC) combined with Control Barrier Functions (CBFs). By
incorporating channel borders and obstacles as safety constraints within the
control design framework, the proposed method ensures both collision avoidance
and robust navigation on complex waterways. Simulation results demonstrate the
efficacy of the proposed method in safely guiding ASVs under realistic
conditions, highlighting its improved safety and adaptability compared to the
state-of-the-art.","Sajad Ahmadi, Hossein Nejatbakhsh Esfahani, Javad Mohammadpour Velni",2025-09-08 13:43:09+00:00,http://arxiv.org/abs/2509.06687v1,http://arxiv.org/pdf/2509.06687v1,"cs.RO, cs.SY, eess.SY"
An Adaptive Coverage Control Approach for Multiple Autonomous Off-road Vehicles in Dynamic Agricultural Fields,"This paper presents an adaptive coverage control method for a fleet of
off-road and Unmanned Ground Vehicles (UGVs) operating in dynamic
(time-varying) agricultural environments. Traditional coverage control
approaches often assume static conditions, making them unsuitable for
real-world farming scenarios where obstacles, such as moving machinery and
uneven terrains, create continuous challenges. To address this, we propose a
real-time path planning framework that integrates Unmanned Aerial Vehicles
(UAVs) for obstacle detection and terrain assessment, allowing UGVs to
dynamically adjust their coverage paths. The environment is modeled as a
weighted directed graph, where the edge weights are continuously updated based
on the UAV observations to reflect obstacle motion and terrain variations. The
proposed approach incorporates Voronoi-based partitioning, adaptive edge weight
assignment, and cost-based path optimization to enhance navigation efficiency.
Simulation results demonstrate the effectiveness of the proposed method in
improving path planning, reducing traversal costs, and maintaining robust
coverage in the presence of dynamic obstacles and muddy terrains.","Sajad Ahmadi, Mohammadreza Davoodi, Javad Mohammadpour Velni",2025-09-08 13:38:39+00:00,http://arxiv.org/abs/2509.06682v1,http://arxiv.org/pdf/2509.06682v1,"cs.RO, cs.SY, eess.SY"
Online Clustering of Seafloor Imagery for Interpretation during Long-Term AUV Operations,"As long-endurance and seafloor-resident AUVs become more capable, there is an
increasing need for extended, real-time interpretation of seafloor imagery to
enable adaptive missions and optimise communication efficiency. Although
offline image analysis methods are well established, they rely on access to
complete datasets and human-labelled examples to manage the strong influence of
environmental and operational conditions on seafloor image
appearance-requirements that cannot be met in real-time settings. To address
this, we introduce an online clustering framework (OCF) capable of interpreting
seafloor imagery without supervision, which is designed to operate in real-time
on continuous data streams in a scalable, adaptive, and self-consistent manner.
The method enables the efficient review and consolidation of common patterns
across the entire data history in constant time by identifying and maintaining
a set of representative samples that capture the evolving feature distribution,
supporting dynamic cluster merging and splitting without reprocessing the full
image history. We evaluate the framework on three diverse seafloor image
datasets, analysing the impact of different representative sampling strategies
on both clustering accuracy and computational cost. The OCF achieves the
highest average F1 score of 0.68 across the three datasets among all
comparative online clustering approaches, with a standard deviation of 3%
across three distinct survey trajectories, demonstrating its superior
clustering capability and robustness to trajectory variation. In addition, it
maintains consistently lower and bounded computational time as the data volume
increases. These properties are beneficial for generating survey data summaries
and supporting informative path planning in long-term, persistent autonomous
marine exploration.","Cailei Liang, Adrian Bodenmann, Sam Fenton, Blair Thornton",2025-09-08 13:36:27+00:00,http://arxiv.org/abs/2509.06678v1,http://arxiv.org/pdf/2509.06678v1,"cs.CV, cs.RO"
Investigating Location-Regularised Self-Supervised Feature Learning for Seafloor Visual Imagery,"High-throughput interpretation of robotically gathered seafloor visual
imagery can increase the efficiency of marine monitoring and exploration.
Although recent research has suggested that location metadata can enhance
self-supervised feature learning (SSL), its benefits across different SSL
strategies, models and seafloor image datasets are underexplored. This study
evaluates the impact of location-based regularisation on six state-of-the-art
SSL frameworks, which include Convolutional Neural Network (CNN) and Vision
Transformer (ViT) models with varying latent-space dimensionality. Evaluation
across three diverse seafloor image datasets finds that location-regularisation
consistently improves downstream classification performance over standard SSL,
with average F1-score gains of $4.9 \pm 4.0%$ for CNNs and $6.3 \pm 8.9%$ for
ViTs, respectively. While CNNs pretrained on generic datasets benefit from
high-dimensional latent representations, dataset-optimised SSL achieves similar
performance across the high (512) and low (128) dimensional latent
representations. Location-regularised SSL improves CNN performance over
pre-trained models by $2.7 \pm 2.7%$ and $10.1 \pm 9.4%$ for high and
low-dimensional latent representations, respectively. For ViTs,
high-dimensionality benefits both pre-trained and dataset-optimised SSL.
Although location-regularisation improves SSL performance compared to standard
SSL methods, pre-trained ViTs show strong generalisation, matching the
best-performing location-regularised SSL with F1-scores of $0.795 \pm 0.075$
and $0.795 \pm 0.077$, respectively. The findings highlight the value of
location metadata for SSL regularisation, particularly when using
low-dimensional latent representations, and demonstrate strong generalisation
of high-dimensional ViTs for seafloor image analysis.","Cailei Liang, Adrian Bodenmann, Emma J Curtis, Samuel Simmons, Kazunori Nagano, Stan Brown, Adam Riese, Blair Thornton",2025-09-08 13:19:04+00:00,http://arxiv.org/abs/2509.06660v1,http://arxiv.org/pdf/2509.06660v1,"cs.CV, cs.RO"
T-araVLN: Translator for Agricultural Robotic Agents on Vision-and-Language Navigation,"Agricultural robotic agents have been becoming powerful helpers in a wide
range of agricultural tasks, nevertheless, still heavily rely on manual
operation or untransportable railway for movement. The AgriVLN method and the
A2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the
agricultural domain, enabling agents navigate to the target position following
the natural language instructions. AgriVLN effectively understands the simple
instructions, however, often misunderstands the complicated instructions. To
bridge this gap, we propose the method of Translator for Agricultural Robotic
Agents on Vision-and-Language Navigation (T-araVLN), in which the Instruction
Translator module translates the original instruction to be both refined and
precise. Being evaluated on the A2A benchmark, our T-araVLN effectively
improves SR from 0.47 to 0.63 and reduces NE from 2.91m to 2.28m, demonstrating
the state-of-the-art performance in the agricultural domain. Code:
https://github.com/AlexTraveling/T-araVLN.","Xiaobei Zhao, Xingqi Lyu, Xiang Li",2025-09-08 12:59:36+00:00,http://arxiv.org/abs/2509.06644v2,http://arxiv.org/pdf/2509.06644v2,cs.RO
LiHRA: A LiDAR-Based HRI Dataset for Automated Risk Monitoring Methods,"We present LiHRA, a novel dataset designed to facilitate the development of
automated, learning-based, or classical risk monitoring (RM) methods for
Human-Robot Interaction (HRI) scenarios. The growing prevalence of
collaborative robots in industrial environments has increased the need for
reliable safety systems. However, the lack of high-quality datasets that
capture realistic human-robot interactions, including potentially dangerous
events, slows development. LiHRA addresses this challenge by providing a
comprehensive, multi-modal dataset combining 3D LiDAR point clouds, human body
keypoints, and robot joint states, capturing the complete spatial and dynamic
context of human-robot collaboration. This combination of modalities allows for
precise tracking of human movement, robot actions, and environmental
conditions, enabling accurate RM during collaborative tasks. The LiHRA dataset
covers six representative HRI scenarios involving collaborative and coexistent
tasks, object handovers, and surface polishing, with safe and hazardous
versions of each scenario. In total, the data set includes 4,431 labeled point
clouds recorded at 10 Hz, providing a rich resource for training and
benchmarking classical and AI-driven RM algorithms. Finally, to demonstrate
LiHRA's utility, we introduce an RM method that quantifies the risk level in
each scenario over time. This method leverages contextual information,
including robot states and the dynamic model of the robot. With its combination
of high-resolution LiDAR data, precise human tracking, robot state data, and
realistic collision events, LiHRA offers an essential foundation for future
research into real-time RM and adaptive safety strategies in human-robot
workspaces.","Frederik Plahl, Georgios Katranis, Ilshat Mamaev, Andrey Morozov",2025-09-08 12:07:08+00:00,http://arxiv.org/abs/2509.06597v1,http://arxiv.org/pdf/2509.06597v1,cs.RO
A Robust Approach for LiDAR-Inertial Odometry Without Sensor-Specific Modeling,"Accurate odometry is a critical component in a robotic navigation stack, and
subsequent modules such as planning and control often rely on an estimate of
the robot's motion. Sensor-based odometry approaches should be robust across
sensor types and deployable in different target domains, from solid-state
LiDARs mounted on cars in urban-driving scenarios to spinning LiDARs on
handheld packages used in unstructured natural environments. In this paper, we
propose a robust LiDAR-inertial odometry system that does not rely on
sensor-specific modeling. Sensor fusion techniques for LiDAR and inertial
measurement unit (IMU) data typically integrate IMU data iteratively in a
Kalman filter or use pre-integration in a factor graph framework, combined with
LiDAR scan matching often exploiting some form of feature extraction. We
propose an alternative strategy that only requires a simplified motion model
for IMU integration and directly registers LiDAR scans in a scan-to-map
approach. Our approach allows us to impose a novel regularization on the LiDAR
registration, improving the overall odometry performance. We detail extensive
experiments on a number of datasets covering a wide array of commonly used
robotic sensors and platforms. We show that our approach works with the exact
same configuration in all these scenarios, demonstrating its robustness. We
have open-sourced our implementation so that the community can build further on
our work and use it in their navigation stacks.","Meher V. R. Malladi, Tiziano Guadagnino, Luca Lobefaro, Cyrill Stachniss",2025-09-08 12:04:12+00:00,http://arxiv.org/abs/2509.06593v1,http://arxiv.org/pdf/2509.06593v1,cs.RO
Co-Located VR with Hybrid SLAM-based HMD Tracking and Motion Capture Synchronization,"We introduce a multi-user VR co-location framework that synchronizes users
within a shared virtual environment aligned to physical space. Our approach
combines a motion capture system with SLAM-based inside-out tracking to deliver
smooth, high-framerate, low-latency performance. Previous methods either rely
on continuous external tracking, which introduces latency and jitter, or on
one-time calibration, which cannot correct drift over time. In contrast, our
approach combines the responsiveness of local HMD SLAM tracking with the
flexibility to realign to an external source when needed. It also supports
real-time pose sharing across devices, ensuring consistent spatial alignment
and engagement between users. Our evaluation demonstrates that our framework
achieves the spatial accuracy required for natural multi-user interaction while
offering improved comfort, scalability, and robustness over existing co-located
VR solutions.","Carlos A. Pinheiro de Sousa, Niklas Gröne, Mathias Günther, Oliver Deussen",2025-09-08 11:54:56+00:00,http://arxiv.org/abs/2509.06582v1,http://arxiv.org/pdf/2509.06582v1,"cs.RO, cs.HC"
Event Driven CBBA with Reduced Communication,"In various scenarios such as multi-drone surveillance and search-and-rescue
operations, deploying multiple robots is essential to accomplish multiple tasks
at once. Due to the limited communication range of these vehicles, a
decentralised task allocation algorithm is crucial for effective task
distribution among robots. The consensus-based bundle algorithm (CBBA) has been
promising for multi-robot operation, offering theoretical guarantees. However,
CBBA demands continuous communication, leading to potential congestion and
packet loss that can hinder performance. In this study, we introduce an
event-driven communication mechanism designed to address these communication
challenges while maintaining the convergence and performance bounds of CBBA. We
demonstrate theoretically that the solution quality matches that of CBBA and
validate the approach with Monte-Carlo simulations across varying targets,
agents, and bundles. Results indicate that the proposed algorithm (ED-CBBA) can
reduce message transmissions by up to 52%.","Vinita Sao, Tu Dac Ho, Sujoy Bhore, P. B. Sujit",2025-09-08 09:45:25+00:00,http://arxiv.org/abs/2509.06481v1,http://arxiv.org/pdf/2509.06481v1,cs.RO
Interactive Shaping of Granular Media Using Reinforcement Learning,"Autonomous manipulation of granular media, such as sand, is crucial for
applications in construction, excavation, and additive manufacturing. However,
shaping granular materials presents unique challenges due to their
high-dimensional configuration space and complex dynamics, where traditional
rule-based approaches struggle without extensive engineering efforts.
Reinforcement learning (RL) offers a promising alternative by enabling agents
to learn adaptive manipulation strategies through trial and error. In this
work, we present an RL framework that enables a robotic arm with a cubic
end-effector and a stereo camera to shape granular media into desired target
structures. We show the importance of compact observations and concise reward
formulations for the large configuration space, validating our design choices
with an ablation study. Our results demonstrate the effectiveness of the
proposed approach for the training of visual policies that manipulate granular
media including their real-world deployment, significantly outperforming two
baseline approaches in terms of target shape accuracy.","Benedikt Kreis, Malte Mosbach, Anny Ripke, Muhammad Ehsan Ullah, Sven Behnke, Maren Bennewitz",2025-09-08 09:30:22+00:00,http://arxiv.org/abs/2509.06469v2,http://arxiv.org/pdf/2509.06469v2,cs.RO
Real-time Photorealistic Mapping for Situational Awareness in Robot Teleoperation,"Achieving efficient remote teleoperation is particularly challenging in
unknown environments, as the teleoperator must rapidly build an understanding
of the site's layout. Online 3D mapping is a proven strategy to tackle this
challenge, as it enables the teleoperator to progressively explore the site
from multiple perspectives. However, traditional online map-based teleoperation
systems struggle to generate visually accurate 3D maps in real-time due to the
high computational cost involved, leading to poor teleoperation performances.
In this work, we propose a solution to improve teleoperation efficiency in
unknown environments. Our approach proposes a novel, modular and efficient
GPU-based integration between recent advancement in gaussian splatting SLAM and
existing online map-based teleoperation systems. We compare the proposed
solution against state-of-the-art teleoperation systems and validate its
performances through real-world experiments using an aerial vehicle. The
results show significant improvements in decision-making speed and more
accurate interaction with the environment, leading to greater teleoperation
efficiency. In doing so, our system enhances remote teleoperation by seamlessly
integrating photorealistic mapping generation with real-time performances,
enabling effective teleoperation in unfamiliar environments.","Ian Page, Pierre Susbielle, Olivier Aycard, Pierre-Brice Wieber",2025-09-08 08:28:30+00:00,http://arxiv.org/abs/2509.06433v1,http://arxiv.org/pdf/2509.06433v1,cs.RO
Musculoskeletal simulation of limb movement biomechanics in Drosophila melanogaster,"Computational models are critical to advance our understanding of how neural,
biomechanical, and physical systems interact to orchestrate animal behaviors.
Despite the availability of near-complete reconstructions of the Drosophila
melanogaster central nervous system, musculature, and exoskeleton, anatomically
and physically grounded models of fly leg muscles are still missing. These
models provide an indispensable bridge between motor neuron activity and joint
movements. Here, we introduce the first 3D, data-driven musculoskeletal model
of Drosophila legs, implemented in both OpenSim and MuJoCo simulation
environments. Our model incorporates a Hill-type muscle representation based on
high-resolution X-ray scans from multiple fixed specimens. We present a
pipeline for constructing muscle models using morphological imaging data and
for optimizing unknown muscle parameters specific to the fly. We then combine
our musculoskeletal models with detailed 3D pose estimation data from behaving
flies to achieve muscle-actuated behavioral replay in OpenSim. Simulations of
muscle activity across diverse walking and grooming behaviors predict
coordinated muscle synergies that can be tested experimentally. Furthermore, by
training imitation learning policies in MuJoCo, we test the effect of different
passive joint properties on learning speed and find that damping and stiffness
facilitate learning. Overall, our model enables the investigation of motor
control in an experimentally tractable model organism, providing insights into
how biomechanics contribute to generation of complex limb movements. Moreover,
our model can be used to control embodied artificial agents to generate
naturalistic and compliant locomotion in simulated environments.","Pembe Gizem Özdil, Chuanfang Ning, Jasper S. Phelps, Sibo Wang-Chen, Guy Elisha, Alexander Blanke, Auke Ijspeert, Pavan Ramdya",2025-09-08 08:21:14+00:00,http://arxiv.org/abs/2509.06426v1,http://arxiv.org/pdf/2509.06426v1,"q-bio.NC, cs.AI, cs.LG, cs.RO"
Safety Meets Speed: Accelerated Neural MPC with Safety Guarantees and No Retraining,"While Model Predictive Control (MPC) enforces safety via constraints, its
real-time execution can exceed embedded compute budgets. We propose a
Barrier-integrated Adaptive Neural Model Predictive Control (BAN-MPC) framework
that synergizes neural networks' fast computation with MPC's
constraint-handling capability. To ensure strict safety, we replace traditional
Euclidean distance with Control Barrier Functions (CBFs) for collision
avoidance. We integrate an offline-learned neural value function into the
optimization objective of a Short-horizon MPC, substantially reducing online
computational complexity. Additionally, we use a second neural network to learn
the sensitivity of the value function to system parameters, and adaptively
adjust the neural value function based on this neural sensitivity when model
parameters change, eliminating the need for retraining and reducing offline
computation costs. The hardware in-the-loop (HIL) experiments on Jetson Nano
show that BAN-MPC solves 200 times faster than traditional MPC, enabling
collision-free navigation with control error below 5\% under model parameter
variations within 15\%, making it an effective embedded MPC alternative.","Kaikai Wang, Tianxun Li, Liang Xu, Qinglei Hu, Keyou You",2025-09-08 07:51:50+00:00,http://arxiv.org/abs/2509.06404v1,http://arxiv.org/pdf/2509.06404v1,cs.RO
Adaptive Evolution Factor Risk Ellipse Framework for Reliable and Safe Autonomous Driving,"In recent years, ensuring safety, efficiency, and comfort in interactive
autonomous driving has become a critical challenge. Traditional model-based
techniques, such as game-theoretic methods and robust control, are often overly
conservative or computationally intensive. Conversely, learning-based
approaches typically require extensive training data and frequently exhibit
limited interpretability and generalizability. Simpler strategies, such as Risk
Potential Fields (RPF), provide lightweight alternatives with minimal data
demands but are inherently static and struggle to adapt effectively to dynamic
traffic conditions. To overcome these limitations, we propose the Evolutionary
Risk Potential Field (ERPF), a novel approach that dynamically updates risk
assessments in dynamical scenarios based on historical obstacle proximity data.
We introduce a Risk-Ellipse construct that combines longitudinal reach and
lateral uncertainty into a unified spatial temporal collision envelope.
Additionally, we define an adaptive Evolution Factor metric, computed through
sigmoid normalization of Time to Collision (TTC) and Time-Window-of-Hazard
(TWH), which dynamically adjusts the dimensions of the ellipse axes in real
time. This adaptive risk metric is integrated seamlessly into a Model
Predictive Control (MPC) framework, enabling autonomous vehicles to proactively
address complex interactive driving scenarios in terms of uncertain driving of
surrounding vehicles. Comprehensive comparative experiments demonstrate that
our ERPF-MPC approach consistently achieves smoother trajectories, higher
average speeds, and collision-free navigation, offering a robust and adaptive
solution suitable for complex interactive driving environments.","Fujiang Yuan, Zhen Tian, Yangfan He, Guojian Zou, Chunhong Yuan, Yanhong Peng, Zhihao Lin",2025-09-08 07:00:48+00:00,http://arxiv.org/abs/2509.06375v1,http://arxiv.org/pdf/2509.06375v1,cs.RO
MAPF-HD: Multi-Agent Path Finding in High-Density Environments,"Multi-agent path finding (MAPF) involves planning efficient paths for
multiple agents to move simultaneously while avoiding collisions. In typical
warehouse environments, agents are often sparsely distributed along aisles.
However, increasing the agent density can improve space efficiency. When the
agent density is high, we must optimize the paths not only for goal-assigned
agents but also for those obstructing them. This study proposes a novel MAPF
framework for high-density environments (MAPF-HD). Several studies have
explored MAPF in similar settings using integer linear programming (ILP).
However, ILP-based methods require substantial computation time to optimize all
agent paths simultaneously. Even in small grid-based environments with fewer
than $100$ cells, these computations can incur tens to hundreds of seconds.
These high computational costs render these methods impractical for large-scale
applications such as automated warehouses and valet parking. To address these
limitations, we introduce the phased null-agent swapping (PHANS) method. PHANS
employs a heuristic approach to incrementally swap positions between agents and
empty vertices. This method solves the MAPF-HD problem within seconds to tens
of seconds, even in large environments containing more than $700$ cells. The
proposed method can potentially improve efficiency in various real-world
applications such as warehouse logistics, traffic management, or crowd control.
Code is available at https://github.com/ToyotaCRDL/MAPF-in-High-Density-Envs.","Hiroya Makino, Seigo Ito",2025-09-08 06:59:46+00:00,http://arxiv.org/abs/2509.06374v1,http://arxiv.org/pdf/2509.06374v1,"cs.MA, cs.RO"
Towards bridging the gap: Systematic sim-to-real transfer for diverse legged robots,"Legged robots must achieve both robust locomotion and energy efficiency to be
practical in real-world environments. Yet controllers trained in simulation
often fail to transfer reliably, and most existing approaches neglect
actuator-specific energy losses or depend on complex, hand-tuned reward
formulations. We propose a framework that integrates sim-to-real reinforcement
learning with a physics-grounded energy model for permanent magnet synchronous
motors. The framework requires a minimal parameter set to capture the
simulation-to-reality gap and employs a compact four-term reward with a
first-principle-based energetic loss formulation that balances electrical and
mechanical dissipation. We evaluate and validate the approach through a
bottom-up dynamic parameter identification study, spanning actuators,
full-robot in-air trajectories and on-ground locomotion. The framework is
tested on three primary platforms and deployed on ten additional robots,
demonstrating reliable policy transfer without randomization of dynamic
parameters. Our method improves energetic efficiency over state-of-the-art
methods, achieving a 32 percent reduction in the full Cost of Transport of
ANYmal (value 1.27). All code, models, and datasets will be released.","Filip Bjelonic, Fabian Tischhauser, Marco Hutter",2025-09-08 05:32:28+00:00,http://arxiv.org/abs/2509.06342v1,http://arxiv.org/pdf/2509.06342v1,cs.RO
Multi-Modal Camera-Based Detection of Vulnerable Road Users,"Vulnerable road users (VRUs) such as pedestrians, cyclists, and motorcyclists
represent more than half of global traffic deaths, yet their detection remains
challenging in poor lighting, adverse weather, and unbalanced data sets. This
paper presents a multimodal detection framework that integrates RGB and thermal
infrared imaging with a fine-tuned YOLOv8 model. Training leveraged KITTI,
BDD100K, and Teledyne FLIR datasets, with class re-weighting and light
augmentations to improve minority-class performance and robustness, experiments
show that 640-pixel resolution and partial backbone freezing optimise accuracy
and efficiency, while class-weighted losses enhance recall for rare VRUs.
Results highlight that thermal models achieve the highest precision, and
RGB-to-thermal augmentation boosts recall, demonstrating the potential of
multimodal detection to improve VRU safety at intersections.","Penelope Brown, Julie Stephany Berrio Perez, Mao Shan, Stewart Worrall",2025-09-08 04:39:07+00:00,http://arxiv.org/abs/2509.06333v1,http://arxiv.org/pdf/2509.06333v1,"cs.CV, cs.RO"
Learning to Walk with Less: a Dyna-Style Approach to Quadrupedal Locomotion,"Traditional RL-based locomotion controllers often suffer from low data
efficiency, requiring extensive interaction to achieve robust performance. We
present a model-based reinforcement learning (MBRL) framework that improves
sample efficiency for quadrupedal locomotion by appending synthetic data to the
end of standard rollouts in PPO-based controllers, following the Dyna-Style
paradigm. A predictive model, trained alongside the policy, generates
short-horizon synthetic transitions that are gradually integrated using a
scheduling strategy based on the policy update iterations. Through an ablation
study, we identified a strong correlation between sample efficiency and rollout
length, which guided the design of our experiments. We validated our approach
in simulation on the Unitree Go1 robot and showed that replacing part of the
simulated steps with synthetic ones not only mimics extended rollouts but also
improves policy return and reduces variance. Finally, we demonstrate that this
improvement transfers to the ability to track a wide range of locomotion
commands using fewer simulated steps.","Francisco Affonso, Felipe Andrade G. Tommaselli, Juliano Negri, Vivian S. Medeiros, Mateus V. Gasparino, Girish Chowdhary, Marcelo Becker",2025-09-08 02:48:23+00:00,http://arxiv.org/abs/2509.06296v1,http://arxiv.org/pdf/2509.06296v1,"cs.RO, cs.AI"
DCReg: Decoupled Characterization for Efficient Degenerate LiDAR Registration,"LiDAR point cloud registration is fundamental to robotic perception and
navigation. However, in geometrically degenerate or narrow environments,
registration problems become ill-conditioned, leading to unstable solutions and
degraded accuracy. While existing approaches attempt to handle these issues,
they fail to address the core challenge: accurately detection, interpret, and
resolve this ill-conditioning, leading to missed detections or corrupted
solutions. In this study, we introduce DCReg, a principled framework that
systematically addresses the ill-conditioned registration problems through
three integrated innovations. First, DCReg achieves reliable ill-conditioning
detection by employing a Schur complement decomposition to the hessian matrix.
This technique decouples the registration problem into clean rotational and
translational subspaces, eliminating coupling effects that mask degeneracy
patterns in conventional analyses. Second, within these cleanly subspaces, we
develop quantitative characterization techniques that establish explicit
mappings between mathematical eigenspaces and physical motion directions,
providing actionable insights about which specific motions lack constraints.
Finally, leveraging this clean subspace, we design a targeted mitigation
strategy: a novel preconditioner that selectively stabilizes only the
identified ill-conditioned directions while preserving all well-constrained
information in observable space. This enables efficient and robust optimization
via the Preconditioned Conjugate Gradient method with a single physical
interpretable parameter. Extensive experiments demonstrate DCReg achieves at
least 20% - 50% improvement in localization accuracy and 5-100 times speedup
over state-of-the-art methods across diverse environments. Our implementation
will be available at https://github.com/JokerJohn/DCReg.","Xiangcheng Hu, Xieyuanli Chen, Mingkai Jia, Jin Wu, Ping Tan, Steven L. Waslander",2025-09-08 02:12:54+00:00,http://arxiv.org/abs/2509.06285v1,http://arxiv.org/pdf/2509.06285v1,cs.RO
O$^3$Afford: One-Shot 3D Object-to-Object Affordance Grounding for Generalizable Robotic Manipulation,"Grounding object affordance is fundamental to robotic manipulation as it
establishes the critical link between perception and action among interacting
objects. However, prior works predominantly focus on predicting single-object
affordance, overlooking the fact that most real-world interactions involve
relationships between pairs of objects. In this work, we address the challenge
of object-to-object affordance grounding under limited data contraints.
Inspired by recent advances in few-shot learning with 2D vision foundation
models, we propose a novel one-shot 3D object-to-object affordance learning
approach for robotic manipulation. Semantic features from vision foundation
models combined with point cloud representation for geometric understanding
enable our one-shot learning pipeline to generalize effectively to novel
objects and categories. We further integrate our 3D affordance representation
with large language models (LLMs) for robotics manipulation, significantly
enhancing LLMs' capability to comprehend and reason about object interactions
when generating task-specific constraint functions. Our experiments on 3D
object-to-object affordance grounding and robotic manipulation demonstrate that
our O$^3$Afford significantly outperforms existing baselines in terms of both
accuracy and generalization capability.","Tongxuan Tian, Xuhui Kang, Yen-Ling Kuo",2025-09-07 22:45:06+00:00,http://arxiv.org/abs/2509.06233v1,http://arxiv.org/pdf/2509.06233v1,"cs.RO, cs.CV"
Grasp-MPC: Closed-Loop Visual Grasping via Value-Guided Model Predictive Control,"Grasping of diverse objects in unstructured environments remains a
significant challenge. Open-loop grasping methods, effective in controlled
settings, struggle in cluttered environments. Grasp prediction errors and
object pose changes during grasping are the main causes of failure. In
contrast, closed-loop methods address these challenges in simplified settings
(e.g., single object on a table) on a limited set of objects, with no path to
generalization. We propose Grasp-MPC, a closed-loop 6-DoF vision-based grasping
policy designed for robust and reactive grasping of novel objects in cluttered
environments. Grasp-MPC incorporates a value function, trained on visual
observations from a large-scale synthetic dataset of 2 million grasp
trajectories that include successful and failed attempts. We deploy this
learned value function in an MPC framework in combination with other cost terms
that encourage collision avoidance and smooth execution. We evaluate Grasp-MPC
on FetchBench and real-world settings across diverse environments. Grasp-MPC
improves grasp success rates by up to 32.6% in simulation and 33.3% in
real-world noisy conditions, outperforming open-loop, diffusion policy,
transformer policy, and IQL approaches. Videos and more at
http://grasp-mpc.github.io.","Jun Yamada, Adithyavairavan Murali, Ajay Mandlekar, Clemens Eppner, Ingmar Posner, Balakumar Sundaralingam",2025-09-07 20:28:21+00:00,http://arxiv.org/abs/2509.06201v1,http://arxiv.org/pdf/2509.06201v1,"cs.RO, cs.AI, cs.LG"
Learning in ImaginationLand: Omnidirectional Policies through 3D Generative Models (OP-Gen),"Recent 3D generative models, which are capable of generating full object
shapes from just a few images, now open up new opportunities in robotics. In
this work, we show that 3D generative models can be used to augment a dataset
from a single real-world demonstration, after which an omnidirectional policy
can be learned within this imagined dataset. We found that this enables a robot
to perform a task when initialised from states very far from those observed
during the demonstration, including starting from the opposite side of the
object relative to the real-world demonstration, significantly reducing the
number of demonstrations required for policy learning. Through several
real-world experiments across tasks such as grasping objects, opening a drawer,
and placing trash into a bin, we study these omnidirectional policies by
investigating the effect of various design choices on policy behaviour, and we
show superior performance to recent baselines which use alternative methods for
data augmentation.","Yifei Ren, Edward Johns",2025-09-07 20:00:59+00:00,http://arxiv.org/abs/2509.06191v1,http://arxiv.org/pdf/2509.06191v1,"cs.RO, cs.CV, cs.LG"
A Hybrid TDMA/CSMA Protocol for Time-Sensitive Traffic in Robot Applications,"Recent progress in robotics has underscored the demand for real-time control
in applications such as manufacturing, healthcare, and autonomous systems,
where the timely delivery of mission-critical commands under heterogeneous
robotic traffic is paramount for operational efficacy and safety. In these
scenarios, mission-critical traffic follows a strict deadline-constrained
communication pattern: commands must arrive within defined QoS deadlines,
otherwise late arrivals can degrade performance or destabilize control loops.In
this work, we demonstrate on a real-time SDR platform that CSMA, widely adopted
in robotic communications,suffers severe degradation under high robot traffic
loads, with contention-induced collisions and delays disrupting the on-time
arrival of mission-critical packets. To address this problem, we propose an
IEEE 802.11-compatible hybrid TDMA/CSMA protocol that combines TDMA's
deterministic slot scheduling with CSMA's adaptability for heterogeneous robot
traffic.The protocol achieves collision-free, low-latency mission-critical
command delivery and IEEE 802.11 compatibility through the synergistic
integration of sub-microsecond PTP-based slot synchronization-essential for
establishing precise timing for TDMA, a three-session superframe with dynamic
TDMA allocation for structured and adaptable traffic management,and beacon-NAV
protection to preemptively secure these critical communication sessions from
interference. Emulation experiments on real-time SDR testbed and Robot
Operating System (ROS) simulation show that the proposed protocol reduces
missed-deadline errors by 93% compared to the CSMA baseline. In high-speed
robot path-tracking ROS simulations, the protocol lowers Root Mean Square (RMS)
trajectory error by up to 90% compared with a CSMA baseline, all while
maintaining throughput for non-critical traffic within +-2%.","Shiqi Xu, Lihao Zhang, Yuyang Du, Qun Yang, Soung Chang Liew",2025-09-07 16:06:08+00:00,http://arxiv.org/abs/2509.06119v1,http://arxiv.org/pdf/2509.06119v1,"cs.RO, cs.SY, eess.SY"
Hybrid A* Path Planning with Multi-Modal Motion Extension for Four-Wheel Steering Mobile Robots,"Four-wheel independent steering (4WIS) systems provide mobile robots with a
rich set of motion modes, such as Ackermann steering, lateral steering, and
parallel movement, offering superior maneuverability in constrained
environments. However, existing path planning methods generally assume a single
kinematic model and thus fail to fully exploit the multi-modal capabilities of
4WIS platforms. To address this limitation, we propose an extended Hybrid A*
framework that operates in a four-dimensional state space incorporating both
spatial states and motion modes. Within this framework, we design multi-modal
Reeds-Shepp curves tailored to the distinct kinematic constraints of each
motion mode, develop an enhanced heuristic function that accounts for
mode-switching costs, and introduce a terminal connection strategy with
intelligent mode selection to ensure smooth transitions between different
steering patterns. The proposed planner enables seamless integration of
multiple motion modalities within a single path, significantly improving
flexibility and adaptability in complex environments. Results demonstrate
significantly improved planning performance for 4WIS robots in complex
environments.","Runjiao Bao, Lin Zhang, Tianwei Niu, Haoyu Yuan, Shoukun Wang",2025-09-07 15:55:12+00:00,http://arxiv.org/abs/2509.06115v1,http://arxiv.org/pdf/2509.06115v1,"cs.RO, cs.SY, eess.SY"
"Advancing Resource Extraction Systems in Martian Volcanic Terrain: Rover Design, Power Consumption and Hazard Analysis","This study proposes a schematic plan for in-situ resource utilization (ISRU)
in Martian volcanic terrains. The work investigated the complexity of volcanic
terrains and Martian environmental hazards and suggested comprehensive
engineering strategies to overcome the odds and establish a successful mining
program in Martian volcanic regions. Slope stabilization methods - such as
terracing and anchored drilling rigs - with terrain-adaptive rovers capable of
autonomous operations on steep unstable slopes has been suggested as feasible
solutions to navigate the complex geological terrains of Martian volcanoes. The
mid range rover design with a mass of approximately 2.1 t, proposed here for
mining operations, incorporates a six-wheel rocker-bogie suspension,
anchoring-enabled drilling arm, dust-mitigation solar arrays, and advanced
sensing systems for hazard detection and navigation. A comparative analysis
regarding choice of roads and rails for building transport infrastructure has
also been performed. We have also looked into the energy requirement of the
rover to work under extreme environmental conditions of Mars and suggested a
combination of solar and nuclear power to account for the huge energy
requirements of sustained operations on Mars. The results demonstrate that
mission success in these environments depends on integrating mechanical
resilience, environmental adaptability, and operational autonomy, enabling
sustainable access to resources in one of Mars' most geologically challenging
settings.","Divij Gupta, Arkajit Aich",2025-09-07 15:34:19+00:00,http://arxiv.org/abs/2509.06103v1,http://arxiv.org/pdf/2509.06103v1,"astro-ph.IM, astro-ph.EP, cs.RO, physics.space-ph"
Energy-Efficient Path Planning with Multi-Location Object Pickup for Mobile Robots on Uneven Terrain,"Autonomous Mobile Robots (AMRs) operate on battery power, making energy
efficiency a critical consideration, particularly in outdoor environments where
terrain variations affect energy consumption. While prior research has
primarily focused on computing energy-efficient paths from a source to a
destination, these approaches often overlook practical scenarios where a robot
needs to pick up an object en route - an action that can significantly impact
energy consumption due to changes in payload. This paper introduces the
Object-Pickup Minimum Energy Path Problem (OMEPP), which addresses
energy-efficient route planning for AMRs required to pick up an object from one
of many possible locations and deliver it to a destination. To address OMEPP,
we first introduce a baseline algorithm that employs the Z star algorithm, a
variant of A star tailored for energy-efficient routing, to iteratively visit
each pickup point. While this approach guarantees optimality, it suffers from
high computational cost due to repeated searches at each pickup location. To
mitigate this inefficiency, we propose a concurrent PCPD search that manages
multiple Z star searches simultaneously across all pickup points. Central to
our solution is the Payload-Constrained Path Database (PCPD), an extension of
the Compressed Path Database (CPD) that incorporates payload constraints. We
demonstrate that PCPD significantly reduces branching factors during search,
improving overall performance. Although the concurrent PCPD search may produce
slightly suboptimal solutions, extensive experiments on real-world datasets
show it achieves near-optimal performance while being one to two orders of
magnitude faster than the baseline algorithm.","Faiza Babakano, Ahmed Fahmin, Bojie Shen, Muhammad Aamir Cheema, Isma Farah Siddiqui",2025-09-07 13:57:43+00:00,http://arxiv.org/abs/2509.06061v1,http://arxiv.org/pdf/2509.06061v1,"cs.RO, cs.DB"
"Robotic Manipulation Framework Based on Semantic Keypoints for Packing Shoes of Different Sizes, Shapes, and Softness","With the rapid development of the warehousing and logistics industries, the
packing of goods has gradually attracted the attention of academia and
industry. The packing of footwear products is a typical representative
paired-item packing task involving irregular shapes and deformable objects.
Although studies on shoe packing have been conducted, different initial states
due to the irregular shapes of shoes and standard packing placement poses have
not been considered. This study proposes a robotic manipulation framework,
including a perception module, reorientation planners, and a packing planner,
that can complete the packing of pairs of shoes in any initial state. First, to
adapt to the large intraclass variations due to the state, shape, and
deformation of the shoe, we propose a vision module based on semantic
keypoints, which can also infer more information such as size, state, pose, and
manipulation points by combining geometric features. Subsequently, we not only
proposed primitive-based reorientation methods for different states of a single
deformable shoe but also proposed a fast reorientation method for the top state
using box edge contact and gravity, which further improved the efficiency of
reorientation. Finally, based on the perception module and reorientation
methods, we propose a task planner for shoe pair packing in any initial state
to provide an optimal packing strategy. Real-world experiments were conducted
to verify the robustness of the reorientation methods and the effectiveness of
the packing strategy for various types of shoes. In this study, we highlight
the potential of semantic keypoint representation methods, introduce new
perspectives on the reorientation of 3D deformable objects and multi-object
manipulation, and provide a reference for paired object packing.","Yi Dong, Yangjun Liu, Jinjun Duan, Yang Li, Zhendong Dai",2025-09-07 13:15:24+00:00,http://arxiv.org/abs/2509.06048v1,http://arxiv.org/pdf/2509.06048v1,cs.RO
"ZLATTE: A Geometry-Aware, Learning-Free Framework for Language-Driven Trajectory Reshaping in Human-Robot Interaction","We present ZLATTE, a geometry-aware, learning-free framework for
language-driven trajectory reshaping in human-robot interaction. Unlike prior
learning-based methods, ZLATTE leverages Vision-Language Models to register
objects as geometric primitives and employs a Large Language Model to translate
natural language instructions into explicit geometric and kinematic
constraints. These constraints are integrated into a potential field
optimization to adapt initial trajectories while preserving feasibility and
safety. A multi-agent strategy further enhances robustness under complex or
conflicting commands. Simulation and real-world experiments demonstrate that
ZLATTE achieves smoother, safer, and more interpretable trajectory
modifications compared to state-of-the-art baselines.","Junhui Huang, Yuhe Gong, Changsheng Li, Xingguang Duan, Luis Figueredo",2025-09-07 12:13:51+00:00,http://arxiv.org/abs/2509.06031v1,http://arxiv.org/pdf/2509.06031v1,cs.RO
eKalibr-Inertial: Continuous-Time Spatiotemporal Calibration for Event-Based Visual-Inertial Systems,"The bioinspired event camera, distinguished by its exceptional temporal
resolution, high dynamic range, and low power consumption, has been extensively
studied in recent years for motion estimation, robotic perception, and object
detection. In ego-motion estimation, the visual-inertial setup is commonly
adopted due to complementary characteristics between sensors (e.g., scale
perception and low drift). For optimal event-based visual-inertial fusion,
accurate spatiotemporal (extrinsic and temporal) calibration is required. In
this work, we present eKalibr-Inertial, an accurate spatiotemporal calibrator
for event-based visual-inertial systems, utilizing the widely used circle grid
board. Building upon the grid pattern recognition and tracking methods in
eKalibr and eKalibr-Stereo, the proposed method starts with a rigorous and
efficient initialization, where all parameters in the estimator would be
accurately recovered. Subsequently, a continuous-time-based batch optimization
is conducted to refine the initialized parameters toward better states. The
results of extensive real-world experiments show that eKalibr-Inertial can
achieve accurate event-based visual-inertial spatiotemporal calibration. The
implementation of eKalibr-Inertial is open-sourced at
(https://github.com/Unsigned-Long/eKalibr) to benefit the research community.","Shuolong Chen, Xingxing Li, Liu Yuan",2025-09-07 04:44:56+00:00,http://arxiv.org/abs/2509.05923v1,http://arxiv.org/pdf/2509.05923v1,"cs.RO, cs.CV"
Programming tension in 3D printed networks inspired by spiderwebs,"Each element in tensioned structural networks -- such as tensegrity,
architectural fabrics, or medical braces/meshes -- requires a specific tension
level to achieve and maintain the desired shape, stability, and compliance.
These structures are challenging to manufacture, 3D print, or assemble because
flattening the network during fabrication introduces multiplicative
inaccuracies in the network's final tension gradients. This study overcomes
this challenge by offering a fabrication algorithm for direct 3D printing of
such networks with programmed tension gradients, an approach analogous to the
spinning of spiderwebs. The algorithm: (i) defines the desired network and
prescribes its tension gradients using the force density method; (ii) converts
the network into an unstretched counterpart by numerically optimizing vertex
locations toward target element lengths and converting straight elements into
arcs to resolve any remaining error; and (iii) decomposes the network into
printable toolpaths; Optional additional steps are: (iv) flattening curved 2D
networks or 3D networks to ensure 3D printing compatibility; and (v)
automatically resolving any unwanted crossings introduced by the flattening
process. The proposed method is experimentally validated using 2D unit cells of
viscoelastic filaments, where accurate tension gradients are achieved with an
average element strain error of less than 1.0\%. The method remains effective
for networks with element minimum length and maximum stress of 5.8 mm and 7.3
MPa, respectively. The method is used to demonstrate the fabrication of three
complex cases: a flat spiderweb, a curved mesh, and a tensegrity system. The
programmable tension gradient algorithm can be utilized to produce compact,
integrated cable networks, enabling novel applications such as moment-exerting
structures in medical braces and splints.","Thijs Masmeijer, Caleb Swain, Jeff Hill, Ed Habtour",2025-09-06 22:42:11+00:00,http://arxiv.org/abs/2509.05855v1,http://arxiv.org/pdf/2509.05855v1,"cs.GR, cs.RO"
Scenario-based Decision-making Using Game Theory for Interactive Autonomous Driving: A Survey,"Game-based interactive driving simulations have emerged as versatile
platforms for advancing decision-making algorithms in road transport mobility.
While these environments offer safe, scalable, and engaging settings for
testing driving strategies, ensuring both realism and robust performance amid
dynamic and diverse scenarios remains a significant challenge. Recently, the
integration of game-based techniques with advanced learning frameworks has
enabled the development of adaptive decision-making models that effectively
manage the complexities inherent in varied driving conditions. These models
outperform traditional simulation methods, especially when addressing
scenario-specific challenges, ranging from obstacle avoidance on highways and
precise maneuvering during on-ramp merging to navigation in roundabouts,
unsignalized intersections, and even the high-speed demands of autonomous
racing. Despite numerous innovations in game-based interactive driving, a
systematic review comparing these approaches across different scenarios is
still missing. This survey provides a comprehensive evaluation of game-based
interactive driving methods by summarizing recent advancements and inherent
roadway features in each scenario. Furthermore, the reviewed algorithms are
critically assessed based on their adaptation of the standard game model and an
analysis of their specific mechanisms to understand their impact on
decision-making performance. Finally, the survey discusses the limitations of
current approaches and outlines promising directions for future research.","Zhihao Lin, Zhen Tian",2025-09-06 17:16:46+00:00,http://arxiv.org/abs/2509.05777v1,http://arxiv.org/pdf/2509.05777v1,cs.RO
"InterAct: A Large-Scale Dataset of Dynamic, Expressive and Interactive Activities between Two People in Daily Scenarios","We address the problem of accurate capture of interactive behaviors between
two people in daily scenarios. Most previous works either only consider one
person or solely focus on conversational gestures of two people, assuming the
body orientation and/or position of each actor are constant or barely change
over each interaction. In contrast, we propose to simultaneously model two
people's activities, and target objective-driven, dynamic, and semantically
consistent interactions which often span longer duration and cover bigger
space. To this end, we capture a new multi-modal dataset dubbed InterAct, which
is composed of 241 motion sequences where two people perform a realistic and
coherent scenario for one minute or longer over a complete interaction. For
each sequence, two actors are assigned different roles and emotion labels, and
collaborate to finish one task or conduct a common interaction activity. The
audios, body motions, and facial expressions of both persons are captured.
InterAct contains diverse and complex motions of individuals and interesting
and relatively long-term interaction patterns barely seen before. We also
demonstrate a simple yet effective diffusion-based method that estimates
interactive face expressions and body motions of two people from speech inputs.
Our method regresses the body motions in a hierarchical manner, and we also
propose a novel fine-tuning mechanism to improve the lip accuracy of facial
expressions. To facilitate further research, the data and code is made
available at https://hku-cg.github.io/interact/ .","Leo Ho, Yinghao Huang, Dafei Qin, Mingyi Shi, Wangpok Tse, Wei Liu, Junichi Yamagishi, Taku Komura",2025-09-06 15:36:47+00:00,http://arxiv.org/abs/2509.05747v1,http://arxiv.org/pdf/2509.05747v1,"cs.CV, cs.AI, cs.LG, cs.MA, cs.RO, I.5.4"
LiDAR-BIND-T: Improving SLAM with Temporally Consistent Cross-Modal LiDAR Reconstruction,"This paper extends LiDAR-BIND, a modular multi-modal fusion framework that
binds heterogeneous sensors (radar, sonar) to a LiDAR-defined latent space,
with mechanisms that explicitly enforce temporal consistency. We introduce
three contributions: (i) temporal embedding similarity that aligns consecutive
latents, (ii) a motion-aligned transformation loss that matches displacement
between predictions and ground truth LiDAR, and (iii) windows temporal fusion
using a specialised temporal module. We further update the model architecture
to better preserve spatial structure. Evaluations on radar/sonar-to-LiDAR
translation demonstrate improved temporal and spatial coherence, yielding lower
absolute trajectory error and better occupancy map accuracy in
Cartographer-based SLAM (Simultaneous Localisation and Mapping). We propose
different metrics based on the Fr\'echet Video Motion Distance (FVMD) and a
correlation-peak distance metric providing practical temporal quality
indicators to evaluate SLAM performance. The proposed temporal LiDAR-BIND, or
LiDAR-BIND-T, maintains plug-and-play modality fusion while substantially
enhancing temporal stability, resulting in improved robustness and performance
for downstream SLAM.","Niels Balemans, Ali Anwar, Jan Steckel, Siegfried Mercelis",2025-09-06 14:21:27+00:00,http://arxiv.org/abs/2509.05728v1,http://arxiv.org/pdf/2509.05728v1,"cs.CV, cs.AI, cs.LG, cs.RO"
Super-LIO: A Robust and Efficient LiDAR-Inertial Odometry System with a Compact Mapping Strategy,"LiDAR-Inertial Odometry (LIO) is a foundational technique for autonomous
systems, yet its deployment on resource-constrained platforms remains
challenging due to computational and memory limitations. We propose Super-LIO,
a robust LIO system that demands both high performance and accuracy, ideal for
applications such as aerial robots and mobile autonomous systems. At the core
of Super-LIO is a compact octo-voxel-based map structure, termed OctVox, that
limits each voxel to eight fused subvoxels, enabling strict point density
control and incremental denoising during map updates. This design enables a
simple yet efficient and accurate map structure, which can be easily integrated
into existing LIO frameworks. Additionally, Super-LIO designs a
heuristic-guided KNN strategy (HKNN) that accelerates the correspondence search
by leveraging spatial locality, further reducing runtime overhead. We evaluated
the proposed system using four publicly available datasets and several
self-collected datasets, totaling more than 30 sequences. Extensive testing on
both X86 and ARM platforms confirms that Super-LIO offers superior efficiency
and robustness, while maintaining competitive accuracy. Super-LIO processes
each frame approximately 73% faster than SOTA, while consuming less CPU
resources. The system is fully open-source and plug-and-play compatible with a
wide range of LiDAR sensors and platforms. The implementation is available at:
https://github.com/Liansheng-Wang/Super-LIO.git","Liansheng Wang, Xinke Zhang, Chenhui Li, Dongjiao He, Yihan Pan, Jianjun Yi",2025-09-06 14:07:35+00:00,http://arxiv.org/abs/2509.05723v1,http://arxiv.org/pdf/2509.05723v1,cs.RO
A*-PRM: A Dynamic Weight-Based Probabilistic Roadmap Algorithm,"Robot path planning is a fundamental challenge in enhancing the environmental
adaptability of autonomous navigation systems. This paper presents a hybrid
path planning algorithm, A-star PRM, which incorporates dynamic weights. By
embedding the Manhattan distance heuristic of the A-star algorithm into the
random sampling process of PRM, the algorithm achieves a balanced optimization
of path quality and computational efficiency. The approach uses a hierarchical
sampling strategy and a dynamic connection mechanism, greatly improving
adaptability to complex obstacle distributions. Experiments show that under a
baseline configuration with one thousand sampled vertices, the path length of
A-star PRM is 1073.23 plus or minus 14.8 meters and is 42.3 percent shorter
than that of PRM with p value less than 0.01. With high-density sampling using
three thousand vertices, the path length is reduced by 0.94 percent, 1036.61
meters compared with 1046.42 meters, while the increase in computational time
is cut to about one tenth of the PRM increase, 71 percent compared with 785
percent. These results confirm the comprehensive advantages of A-star PRM in
path quality, stability, and computational efficiency. Compared with existing
hybrid algorithms, the proposed method shows clear benefits, especially in
narrow channels and scenarios with dynamic obstacles.","Siyuan Wang, Shuyi Zhang, Zhen Tian, Yuheng Yao, Gongsen Wang, Yu Zhao",2025-09-06 12:33:10+00:00,http://arxiv.org/abs/2509.05701v1,http://arxiv.org/pdf/2509.05701v1,cs.RO
Sharing but Not Caring: Similar Outcomes for Shared Control and Switching Control in Telepresence-Robot Navigation,"Telepresence robots enable users to interact with remote environments, but
efficient and intuitive navigation remains a challenge. In this work, we
developed and evaluated a shared control method, in which the robot navigates
autonomously while allowing users to affect the path generation to better suit
their needs. We compared this with control switching, where users toggle
between direct and automated control. We hypothesized that shared control would
maintain efficiency comparable to control switching while potentially reducing
user workload. The results of two consecutive user studies (each with final
sample of n=20) showed that shared control does not degrade navigation
efficiency, but did not show a significant reduction in task load compared to
control switching. Further research is needed to explore the underlying factors
that influence user preference and performance in these control systems.","Juho Kalliokoski, Evan G. Center, Steven M. LaValle, Timo Ojala, Basak Sakcak",2025-09-06 10:27:05+00:00,http://arxiv.org/abs/2509.05672v1,http://arxiv.org/pdf/2509.05672v1,cs.RO
Stereovision Image Processing for Planetary Navigation Maps with Semi-Global Matching and Superpixel Segmentation,"Mars exploration requires precise and reliable terrain models to ensure safe
rover navigation across its unpredictable and often hazardous landscapes.
Stereoscopic vision serves a critical role in the rover's perception, allowing
scene reconstruction by generating precise depth maps through stereo matching.
State-of-the-art Martian planetary exploration uses traditional local
block-matching, aggregates cost over square windows, and refines disparities
via smoothness constraints. However, this method often struggles with
low-texture images, occlusion, and repetitive patterns because it considers
only limited neighbouring pixels and lacks a wider understanding of scene
context. This paper uses Semi-Global Matching (SGM) with superpixel-based
refinement to mitigate the inherent block artefacts and recover lost details.
The approach balances the efficiency and accuracy of SGM and adds context-aware
segmentation to support more coherent depth inference. The proposed method has
been evaluated in three datasets with successful results: In a Mars analogue,
the terrain maps obtained show improved structural consistency, particularly in
sloped or occlusion-prone regions. Large gaps behind rocks, which are common in
raw disparity outputs, are reduced, and surface details like small rocks and
edges are captured more accurately. Another two datasets, evaluated to test the
method's general robustness and adaptability, show more precise disparity maps
and more consistent terrain models, better suited for the demands of autonomous
navigation on Mars, and competitive accuracy across both non-occluded and
full-image error metrics. This paper outlines the entire terrain modelling
process, from finding corresponding features to generating the final 2D
navigation maps, offering a complete pipeline suitable for integration in
future planetary exploration missions.","Yan-Shan Lu, Miguel Arana-Catania, Saurabh Upadhyay, Leonard Felicetti",2025-09-06 08:53:10+00:00,http://arxiv.org/abs/2509.05645v1,http://arxiv.org/pdf/2509.05645v1,"astro-ph.IM, astro-ph.EP, cs.CV, cs.RO"
SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning,"Pruning accelerates compute-bound models by reducing computation. Recently
applied to Vision-Language-Action (VLA) models, existing methods prune tokens
using only local info from current action, ignoring global context from prior
actions, causing >20% success rate drop and limited speedup. We observe high
similarity across consecutive actions and propose leveraging both local
(current) and global (past) info for smarter token selection. We introduce
SpecPrune-VLA, a training-free method with two-level pruning and heuristic
control: (1) Static pruning at action level: uses global history and local
context to reduce visual tokens per action; (2) Dynamic pruning at layer level:
prunes tokens per layer based on layer-specific importance; (3) Lightweight
action-aware controller: classifies actions as coarse/fine-grained (by speed),
adjusting pruning aggressiveness since fine-grained actions are
pruning-sensitive. Experiments on LIBERO show SpecPrune-VLA achieves 1.46 times
speedup on NVIDIA A800 and 1.57 times on NVIDIA GeForce RTX 3090 vs.
OpenVLA-OFT, with negligible success rate loss.","Hanzhen Wang, Jiaming Xu, Jiayi Pan, Yongkang Zhou, Guohao Dai",2025-09-06 06:22:19+00:00,http://arxiv.org/abs/2509.05614v1,http://arxiv.org/pdf/2509.05614v1,"cs.CV, cs.AI, cs.RO"
MonoGlass3D: Monocular 3D Glass Detection with Plane Regression and Adaptive Feature Fusion,"Detecting and localizing glass in 3D environments poses significant
challenges for visual perception systems, as the optical properties of glass
often hinder conventional sensors from accurately distinguishing glass
surfaces. The lack of real-world datasets focused on glass objects further
impedes progress in this field. To address this issue, we introduce a new
dataset featuring a wide range of glass configurations with precise 3D
annotations, collected from distinct real-world scenarios. On the basis of this
dataset, we propose MonoGlass3D, a novel approach tailored for monocular 3D
glass detection across diverse environments. To overcome the challenges posed
by the ambiguous appearance and context diversity of glass, we propose an
adaptive feature fusion module that empowers the network to effectively capture
contextual information in varying conditions. Additionally, to exploit the
distinct planar geometry of glass surfaces, we present a plane regression
pipeline, which enables seamless integration of geometric properties within our
framework. Extensive experiments demonstrate that our method outperforms
state-of-the-art approaches in both glass segmentation and monocular glass
depth estimation. Our results highlight the advantages of combining geometric
and contextual cues for transparent surface understanding.","Kai Zhang, Guoyang Zhao, Jianxing Shi, Bonan Liu, Weiqing Qi, Jun Ma",2025-09-06 05:15:42+00:00,http://arxiv.org/abs/2509.05599v1,http://arxiv.org/pdf/2509.05599v1,cs.RO
Learning to Walk in Costume: Adversarial Motion Priors for Aesthetically Constrained Humanoids,"We present a Reinforcement Learning (RL)-based locomotion system for Cosmo, a
custom-built humanoid robot designed for entertainment applications. Unlike
traditional humanoids, entertainment robots present unique challenges due to
aesthetic-driven design choices. Cosmo embodies these with a disproportionately
large head (16% of total mass), limited sensing, and protective shells that
considerably restrict movement. To address these challenges, we apply
Adversarial Motion Priors (AMP) to enable the robot to learn natural-looking
movements while maintaining physical stability. We develop tailored domain
randomization techniques and specialized reward structures to ensure safe
sim-to-real, protecting valuable hardware components during deployment. Our
experiments demonstrate that AMP generates stable standing and walking
behaviors despite Cosmo's extreme mass distribution and movement constraints.
These results establish a promising direction for robots that balance aesthetic
appeal with functional performance, suggesting that learning-based methods can
effectively adapt to aesthetic-driven design constraints.","Arturo Flores Alvarez, Fatemeh Zargarbashi, Havel Liu, Shiqi Wang, Liam Edwards, Jessica Anz, Alex Xu, Fan Shi, Stelian Coros, Dennis W. Hong",2025-09-06 03:52:10+00:00,http://arxiv.org/abs/2509.05581v1,http://arxiv.org/pdf/2509.05581v1,"cs.RO, cs.AI, cs.SY, eess.SY, 68T40, I.2.9; I.2.6"
OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision,"Multimodal large language models (MLLMs) have shown strong vision-language
reasoning abilities but still lack robust 3D spatial understanding, which is
critical for autonomous driving. This limitation stems from two key challenges:
(1) the difficulty of constructing accessible yet effective 3D representations
without expensive manual annotations, and (2) the loss of fine-grained spatial
details in VLMs due to the absence of large-scale 3D vision-language
pretraining. To address these challenges, we propose OccVLA, a novel framework
that integrates 3D occupancy representations into a unified multimodal
reasoning process. Unlike prior approaches that rely on explicit 3D inputs,
OccVLA treats dense 3D occupancy as both a predictive output and a supervisory
signal, enabling the model to learn fine-grained spatial structures directly
from 2D visual inputs. The occupancy predictions are regarded as implicit
reasoning processes and can be skipped during inference without performance
degradation, thereby adding no extra computational overhead. OccVLA achieves
state-of-the-art results on the nuScenes benchmark for trajectory planning and
demonstrates superior performance on 3D visual question-answering tasks,
offering a scalable, interpretable, and fully vision-based solution for
autonomous driving.","Ruixun Liu, Lingyu Kong, Derun Li, Hang Zhao",2025-09-06 03:47:21+00:00,http://arxiv.org/abs/2509.05578v1,http://arxiv.org/pdf/2509.05578v1,"cs.AI, cs.RO"
TeleopLab: Accessible and Intuitive Teleoperation of a Robotic Manipulator for Remote Labs,"Teleoperation offers a promising solution for enabling hands-on learning in
remote education, particularly in environments requiring interaction with
real-world equipment. However, such remote experiences can be costly or
non-intuitive. To address these challenges, we present TeleopLab, a mobile
device teleoperation system that allows students to control a robotic arm and
operate lab equipment. TeleopLab comprises a robotic arm, an adaptive gripper,
cameras, lab equipment for a diverse range of applications, a user interface
accessible through smartphones, and video call software. We conducted a user
study, focusing on task performance, students' perspectives toward the system,
usability, and workload assessment. Our results demonstrate a 46.1% reduction
in task completion time as users gained familiarity with the system.
Quantitative feedback highlighted improvements in students' perspectives after
using the system, while NASA TLX and SUS assessments indicated a manageable
workload of 38.2 and a positive usability of 73.8. TeleopLab successfully
bridges the gap between physical labs and remote education, offering a scalable
and effective platform for remote STEM learning.","Ziling Chen, Yeo Jung Yoon, Rolando Bautista-Montesano, Zhen Zhao, Ajay Mandlekar, John Liu",2025-09-06 00:32:45+00:00,http://arxiv.org/abs/2509.05547v1,http://arxiv.org/pdf/2509.05547v1,"cs.RO, cs.HC"
OpenEgo: A Large-Scale Multimodal Egocentric Dataset for Dexterous Manipulation,"Egocentric human videos provide scalable demonstrations for imitation
learning, but existing corpora often lack either fine-grained, temporally
localized action descriptions or dexterous hand annotations. We introduce
OpenEgo, a multimodal egocentric manipulation dataset with standardized
hand-pose annotations and intention-aligned action primitives. OpenEgo totals
1107 hours across six public datasets, covering 290 manipulation tasks in 600+
environments. We unify hand-pose layouts and provide descriptive, timestamped
action primitives. To validate its utility, we train language-conditioned
imitation-learning policies to predict dexterous hand trajectories. OpenEgo is
designed to lower the barrier to learning dexterous manipulation from
egocentric video and to support reproducible research in vision-language-action
learning. All resources and instructions will be released at
www.openegocentric.com.","Ahad Jawaid, Yu Xiang",2025-09-05 21:47:55+00:00,http://arxiv.org/abs/2509.05513v1,http://arxiv.org/pdf/2509.05513v1,"cs.CV, cs.AI, cs.RO"
Quaternion Approximation Networks for Enhanced Image Classification and Oriented Object Detection,"This paper introduces Quaternion Approximate Networks (QUAN), a novel deep
learning framework that leverages quaternion algebra for rotation equivariant
image classification and object detection. Unlike conventional quaternion
neural networks attempting to operate entirely in the quaternion domain, QUAN
approximates quaternion convolution through Hamilton product decomposition
using real-valued operations. This approach preserves geometric properties
while enabling efficient implementation with custom CUDA kernels. We introduce
Independent Quaternion Batch Normalization (IQBN) for training stability and
extend quaternion operations to spatial attention mechanisms. QUAN is evaluated
on image classification (CIFAR-10/100, ImageNet), object detection (COCO,
DOTA), and robotic perception tasks. In classification tasks, QUAN achieves
higher accuracy with fewer parameters and faster convergence compared to
existing convolution and quaternion-based models. For objection detection, QUAN
demonstrates improved parameter efficiency and rotation handling over standard
Convolutional Neural Networks (CNNs) while establishing the SOTA for quaternion
CNNs in this downstream task. These results highlight its potential for
deployment in resource-constrained robotic systems requiring rotation-aware
perception and application in other domains.","Bryce Grant, Peng Wang",2025-09-05 21:41:40+00:00,http://arxiv.org/abs/2509.05512v1,http://arxiv.org/pdf/2509.05512v1,"cs.CV, cs.RO"
Microrobot Vascular Parkour: Analytic Geometry-based Path Planning with Real-time Dynamic Obstacle Avoidance,"Autonomous microrobots in blood vessels could enable minimally invasive
therapies, but navigation is challenged by dense, moving obstacles. We propose
a real-time path planning framework that couples an analytic geometry global
planner (AGP) with two reactive local escape controllers, one based on rules
and one based on reinforcement learning, to handle sudden moving obstacles.
Using real-time imaging, the system estimates the positions of the microrobot,
obstacles, and targets and computes collision-free motions. In simulation, AGP
yields shorter paths and faster planning than weighted A* (WA*), particle swarm
optimization (PSO), and rapidly exploring random trees (RRT), while maintaining
feasibility and determinism. We extend AGP from 2D to 3D without loss of speed.
In both simulations and experiments, the combined global planner and local
controllers reliably avoid moving obstacles and reach targets. The average
planning time is 40 ms per frame, compatible with 25 fps image acquisition and
real-time closed-loop control. These results advance autonomous microrobot
navigation and targeted drug delivery in vascular environments.","Yanda Yang, Max Sokolich, Fatma Ceren Kirmizitas, Sambeeta Das, Andreas A. Malikopoulos",2025-09-05 21:22:37+00:00,http://arxiv.org/abs/2509.05500v1,http://arxiv.org/pdf/2509.05500v1,"cs.RO, cs.AI"
Learning Tool-Aware Adaptive Compliant Control for Autonomous Regolith Excavation,"Autonomous regolith excavation is a cornerstone of in-situ resource
utilization for a sustained human presence beyond Earth. However, this task is
fundamentally hindered by the complex interaction dynamics of granular media
and the operational need for robots to use diverse tools. To address these
challenges, this work introduces a framework where a model-based reinforcement
learning agent learns within a parallelized simulation. This environment
leverages high-fidelity particle physics and procedural generation to create a
vast distribution of both lunar terrains and excavation tool geometries. To
master this diversity, the agent learns an adaptive interaction strategy by
dynamically modulating its own stiffness and damping at each control step
through operational space control. Our experiments demonstrate that training
with a procedural distribution of tools is critical for generalization and
enables the development of sophisticated tool-aware behavior. Furthermore, we
show that augmenting the agent with visual feedback significantly improves task
success. These results represent a validated methodology for developing the
robust and versatile autonomous systems required for the foundational tasks of
future space missions.","Andrej Orsula, Matthieu Geist, Miguel Olivares-Mendez, Carol Martinez",2025-09-05 20:09:28+00:00,http://arxiv.org/abs/2509.05475v1,http://arxiv.org/pdf/2509.05475v1,"cs.RO, cs.AI, cs.LG"
HapMorph: A Pneumatic Framework for Multi-Dimensional Haptic Property Rendering,"Haptic interfaces that can simultaneously modulate multiple physical
properties remain a fundamental challenge in human-robot interaction. Existing
systems typically allow the rendering of either geometric features or
mechanical properties, but rarely both, within wearable form factors. Here, we
introduce HapMorph, a pneumatic framework that enables continuous, simultaneous
modulation of object size and stiffness through antagonistic fabric-based
pneumatic actuators (AFPAs). We implemented a HapMorph protoytpe designed for
hands interaction achieving size variation from 50 to 104 mm, stiffness
modulation up to 4.7 N/mm and mass of the wearable parts of just 21 g. Through
systematic characterization, we demonstrate decoupled control of size and
stiffness properties via dual-chamber pressure regulation. Human perception
studies with 10 participants reveal that users can distinguish nine discrete
states across three size categories and three stiffness levels with 89.4%
accuracy and 6.7 s average response time. We further demonstrate extended
architectures that combine AFPAs with complementary pneumatic structures to
enable shape or geometry morphing with concurrent stiffness control. Our
results establish antagonistic pneumatic principle as a pathway toward
next-generation haptic interfaces, capable of multi-dimensiona rendering
properties within practical wearable constraints.","Rui Chen, Domenico Chiaradia, Antonio Frisoli, Daniele Leonardis",2025-09-05 18:26:58+00:00,http://arxiv.org/abs/2509.05433v1,http://arxiv.org/pdf/2509.05433v1,cs.RO
Robust Model Predictive Control Design for Autonomous Vehicles with Perception-based Observers,"This paper presents a robust model predictive control (MPC) framework that
explicitly addresses the non-Gaussian noise inherent in deep learning-based
perception modules used for state estimation. Recognizing that accurate
uncertainty quantification of the perception module is essential for safe
feedback control, our approach departs from the conventional assumption of
zero-mean noise quantification of the perception error. Instead, it employs
set-based state estimation with constrained zonotopes to capture biased,
heavy-tailed uncertainties while maintaining bounded estimation errors. To
improve computational efficiency, the robust MPC is reformulated as a linear
program (LP), using a Minkowski-Lyapunov-based cost function with an added
slack variable to prevent degenerate solutions. Closed-loop stability is
ensured through Minkowski-Lyapunov inequalities and contractive zonotopic
invariant sets. The largest stabilizing terminal set and its corresponding
feedback gain are then derived via an ellipsoidal approximation of the
zonotopes. The proposed framework is validated through both simulations and
hardware experiments on an omnidirectional mobile robot along with a camera and
a convolutional neural network-based perception module implemented within a
ROS2 framework. The results demonstrate that the perception-aware MPC provides
stable and accurate control performance under heavy-tailed noise conditions,
significantly outperforming traditional Gaussian-noise-based designs in terms
of both state estimation error bounding and overall control performance.","Nariman Niknejad, Gokul S. Sankar, Bahare Kiumarsi, Hamidreza Modares",2025-09-05 16:03:57+00:00,http://arxiv.org/abs/2509.05201v1,http://arxiv.org/pdf/2509.05201v1,"cs.RO, cs.CV, cs.LG, cs.SY, eess.SY"
Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet,"The classification of 3D point clouds is crucial for applications such as
autonomous driving, robotics, and augmented reality. However, the commonly used
ModelNet40 dataset suffers from limitations such as inconsistent labeling, 2D
data, size mismatches, and inadequate class differentiation, which hinder model
performance. This paper introduces ModelNet-R, a meticulously refined version
of ModelNet40 designed to address these issues and serve as a more reliable
benchmark. Additionally, this paper proposes Point-SkipNet, a lightweight
graph-based neural network that leverages efficient sampling, neighborhood
grouping, and skip connections to achieve high classification accuracy with
reduced computational overhead. Extensive experiments demonstrate that models
trained in ModelNet-R exhibit significant performance improvements. Notably,
Point-SkipNet achieves state-of-the-art accuracy on ModelNet-R with a
substantially lower parameter count compared to contemporary models. This
research highlights the crucial role of dataset quality in optimizing model
efficiency for 3D point cloud classification. For more details, see the code
at: https://github.com/m-saeid/ModeNetR_PointSkipNet.","Mohammad Saeid, Amir Salarpour, Pedram MohajerAnsari",2025-09-05 15:57:36+00:00,http://arxiv.org/abs/2509.05198v1,http://arxiv.org/pdf/2509.05198v1,"cs.CV, cs.AI, cs.LG, cs.RO"
RoboBallet: Planning for Multi-Robot Reaching with Graph Neural Networks and Reinforcement Learning,"Modern robotic manufacturing requires collision-free coordination of multiple
robots to complete numerous tasks in shared, obstacle-rich workspaces. Although
individual tasks may be simple in isolation, automated joint task allocation,
scheduling, and motion planning under spatio-temporal constraints remain
computationally intractable for classical methods at real-world scales.
Existing multi-arm systems deployed in the industry rely on human intuition and
experience to design feasible trajectories manually in a labor-intensive
process. To address this challenge, we propose a reinforcement learning (RL)
framework to achieve automated task and motion planning, tested in an
obstacle-rich environment with eight robots performing 40 reaching tasks in a
shared workspace, where any robot can perform any task in any order. Our
approach builds on a graph neural network (GNN) policy trained via RL on
procedurally-generated environments with diverse obstacle layouts, robot
configurations, and task distributions. It employs a graph representation of
scenes and a graph policy neural network trained through reinforcement learning
to generate trajectories of multiple robots, jointly solving the sub-problems
of task allocation, scheduling, and motion planning. Trained on large randomly
generated task sets in simulation, our policy generalizes zero-shot to unseen
settings with varying robot placements, obstacle geometries, and task poses. We
further demonstrate that the high-speed capability of our solution enables its
use in workcell layout optimization, improving solution times. The speed and
scalability of our planner also open the door to new capabilities such as
fault-tolerant planning and online perception-based re-planning, where rapid
adaptation to dynamic task sets is required.","Matthew Lai, Keegan Go, Zhibin Li, Torsten Kroger, Stefan Schaal, Kelsey Allen, Jonathan Scholz",2025-09-05 14:28:34+00:00,http://arxiv.org/abs/2509.05397v1,http://arxiv.org/pdf/2509.05397v1,"cs.RO, cs.LG"
Analyzing Gait Adaptation with Hemiplegia Simulation Suits and Digital Twins,"To advance the development of assistive and rehabilitation robots, it is
essential to conduct experiments early in the design cycle. However, testing
early prototypes directly with users can pose safety risks. To address this, we
explore the use of condition-specific simulation suits worn by healthy
participants in controlled environments as a means to study gait changes
associated with various impairments and support rapid prototyping. This paper
presents a study analyzing the impact of a hemiplegia simulation suit on gait.
We collected biomechanical data using a Vicon motion capture system and Delsys
Trigno EMG and IMU sensors under four walking conditions: with and without a
rollator, and with and without the simulation suit. The gait data was
integrated into a digital twin model, enabling machine learning analyses to
detect the use of the simulation suit and rollator, identify turning behavior,
and evaluate how the suit affects gait over time. Our findings show that the
simulation suit significantly alters movement and muscle activation patterns,
prompting users to compensate with more abrupt motions. We also identify key
features and sensor modalities that are most informative for accurately
capturing gait dynamics and modeling human-rollator interaction within the
digital twin framework.","Jialin Chen, Jeremie Clos, Dominic Price, Praminda Caleb-Solly",2025-09-05 13:58:21+00:00,http://arxiv.org/abs/2509.05116v1,http://arxiv.org/pdf/2509.05116v1,"cs.ET, cs.RO"
Shared Autonomy through LLMs and Reinforcement Learning for Applications to Ship Hull Inspections,"Shared autonomy is a promising paradigm in robotic systems, particularly
within the maritime domain, where complex, high-risk, and uncertain
environments necessitate effective human-robot collaboration. This paper
investigates the interaction of three complementary approaches to advance
shared autonomy in heterogeneous marine robotic fleets: (i) the integration of
Large Language Models (LLMs) to facilitate intuitive high-level task
specification and support hull inspection missions, (ii) the implementation of
human-in-the-loop interaction frameworks in multi-agent settings to enable
adaptive and intent-aware coordination, and (iii) the development of a modular
Mission Manager based on Behavior Trees to provide interpretable and flexible
mission control. Preliminary results from simulation and real-world lake-like
environments demonstrate the potential of this multi-layered architecture to
reduce operator cognitive load, enhance transparency, and improve adaptive
behaviour alignment with human intent. Ongoing work focuses on fully
integrating these components, refining coordination mechanisms, and validating
the system in operational port scenarios. This study contributes to
establishing a modular and scalable foundation for trustworthy,
human-collaborative autonomy in safety-critical maritime robotics applications.","Cristiano Caissutti, Estelle Gerbier, Ehsan Khorrambakht, Paolo Marinelli, Andrea Munafo', Andrea Caiti",2025-09-05 12:06:06+00:00,http://arxiv.org/abs/2509.05042v1,http://arxiv.org/pdf/2509.05042v1,cs.RO
Pointing-Guided Target Estimation via Transformer-Based Attention,"Deictic gestures, like pointing, are a fundamental form of non-verbal
communication, enabling humans to direct attention to specific objects or
locations. This capability is essential in Human-Robot Interaction (HRI), where
robots should be able to predict human intent and anticipate appropriate
responses. In this work, we propose the Multi-Modality Inter-TransFormer
(MM-ITF), a modular architecture to predict objects in a controlled tabletop
scenario with the NICOL robot, where humans indicate targets through natural
pointing gestures. Leveraging inter-modality attention, MM-ITF maps 2D pointing
gestures to object locations, assigns a likelihood score to each, and
identifies the most likely target. Our results demonstrate that the method can
accurately predict the intended object using monocular RGB data, thus enabling
intuitive and accessible human-robot collaboration. To evaluate the
performance, we introduce a patch confusion matrix, providing insights into the
model's predictions across candidate object locations. Code available at:
https://github.com/lucamuellercode/MMITF.","Luca Müller, Hassan Ali, Philipp Allgeuer, Lukáš Gajdošech, Stefan Wermter",2025-09-05 11:42:03+00:00,http://arxiv.org/abs/2509.05031v1,http://arxiv.org/pdf/2509.05031v1,"cs.RO, cs.AI, cs.CV, I.2.9; I.2.10; I.2.6"
FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies,"Developing efficient Vision-Language-Action (VLA) policies is crucial for
practical robotics deployment, yet current approaches face prohibitive
computational costs and resource requirements. Existing diffusion-based VLA
policies require multi-billion-parameter models and massive datasets to achieve
strong performance. We tackle this efficiency challenge with two contributions:
intermediate-modality fusion, which reallocates capacity to the diffusion head
by pruning up to $50\%$ of LLM layers, and action-specific Global-AdaLN
conditioning, which cuts parameters by $20\%$ through modular adaptation. We
integrate these advances into a novel 950 M-parameter VLA called FLOWER.
Pretrained in just 200 H100 GPU hours, FLOWER delivers competitive performance
with bigger VLAs across $190$ tasks spanning ten simulation and real-world
benchmarks and demonstrates robustness across diverse robotic embodiments. In
addition, FLOWER achieves a new SoTA of 4.53 on the CALVIN ABC benchmark.
Demos, code and pretrained weights are available at
https://intuitive-robots.github.io/flower_vla/.","Moritz Reuss, Hongyi Zhou, Marcel Rühle, Ömer Erdinç Yağmurlu, Fabian Otto, Rudolf Lioutikov",2025-09-05 10:43:12+00:00,http://arxiv.org/abs/2509.04996v1,http://arxiv.org/pdf/2509.04996v1,cs.RO
Lyapunov-Based Deep Learning Control for Robots with Unknown Jacobian,"Deep learning, with its exceptional learning capabilities and flexibility,
has been widely applied in various applications. However, its black-box nature
poses a significant challenge in real-time robotic applications, particularly
in robot control, where trustworthiness and robustness are critical in ensuring
safety. In robot motion control, it is essential to analyze and ensure system
stability, necessitating the establishment of methodologies that address this
need. This paper aims to develop a theoretical framework for end-to-end deep
learning control that can be integrated into existing robot control theories.
The proposed control algorithm leverages a modular learning approach to update
the weights of all layers in real time, ensuring system stability based on
Lyapunov-like analysis. Experimental results on industrial robots are presented
to illustrate the performance of the proposed deep learning controller. The
proposed method offers an effective solution to the black-box problem in deep
learning, demonstrating the possibility of deploying real-time deep learning
strategies for robot kinematic control in a stable manner. This achievement
provides a critical foundation for future advancements in deep learning based
real-time robotic applications.","Koji Matsuno, Chien Chern Cheah",2025-09-05 10:22:46+00:00,http://arxiv.org/abs/2509.04984v1,http://arxiv.org/pdf/2509.04984v1,cs.RO
Evaluating Magic Leap 2 Tool Tracking for AR Sensor Guidance in Industrial Inspections,"Rigorous evaluation of commercial Augmented Reality (AR) hardware is crucial,
yet public benchmarks for tool tracking on modern Head-Mounted Displays (HMDs)
are limited. This paper addresses this gap by systematically assessing the
Magic Leap 2 (ML2) controllers tracking performance. Using a robotic arm for
repeatable motion (EN ISO 9283) and an optical tracking system as ground truth,
our protocol evaluates static and dynamic performance under various conditions,
including realistic paths from a hydrogen leak inspection use case. The results
provide a quantitative baseline of the ML2 controller's accuracy and
repeatability and present a robust, transferable evaluation methodology. The
findings provide a basis to assess the controllers suitability for the
inspection use case and similar industrial sensor-based AR guidance tasks.","Christian Masuhr, Julian Koch, Thorsten Schüppstuhl",2025-09-05 10:13:28+00:00,http://arxiv.org/abs/2509.05391v1,http://arxiv.org/pdf/2509.05391v1,"cs.RO, cs.HC, cs.MM"
DeGuV: Depth-Guided Visual Reinforcement Learning for Generalization and Interpretability in Manipulation,"Reinforcement learning (RL) agents can learn to solve complex tasks from
visual inputs, but generalizing these learned skills to new environments
remains a major challenge in RL application, especially robotics. While data
augmentation can improve generalization, it often compromises sample efficiency
and training stability. This paper introduces DeGuV, an RL framework that
enhances both generalization and sample efficiency. In specific, we leverage a
learnable masker network that produces a mask from the depth input, preserving
only critical visual information while discarding irrelevant pixels. Through
this, we ensure that our RL agents focus on essential features, improving
robustness under data augmentation. In addition, we incorporate contrastive
learning and stabilize Q-value estimation under augmentation to further enhance
sample efficiency and training stability. We evaluate our proposed method on
the RL-ViGen benchmark using the Franka Emika robot and demonstrate its
effectiveness in zero-shot sim-to-real transfer. Our results show that DeGuV
outperforms state-of-the-art methods in both generalization and sample
efficiency while also improving interpretability by highlighting the most
relevant regions in the visual input","Tien Pham, Xinyun Chi, Khang Nguyen, Manfred Huber, Angelo Cangelosi",2025-09-05 09:52:08+00:00,http://arxiv.org/abs/2509.04970v1,http://arxiv.org/pdf/2509.04970v1,"cs.RO, cs.AI"
Ground-Aware Octree-A* Hybrid Path Planning for Memory-Efficient 3D Navigation of Ground Vehicles,"In this paper, we propose a 3D path planning method that integrates the A*
algorithm with the octree structure. Unmanned Ground Vehicles (UGVs) and legged
robots have been extensively studied, enabling locomotion across a variety of
terrains. Advances in mobility have enabled obstacles to be regarded not only
as hindrances to be avoided, but also as navigational aids when beneficial. A
modified 3D A* algorithm generates an optimal path by leveraging obstacles
during the planning process. By incorporating a height-based penalty into the
cost function, the algorithm enables the use of traversable obstacles to aid
locomotion while avoiding those that are impassable, resulting in more
efficient and realistic path generation. The octree-based 3D grid map achieves
compression by merging high-resolution nodes into larger blocks, especially in
obstacle-free or sparsely populated areas. This reduces the number of nodes
explored by the A* algorithm, thereby improving computational efficiency and
memory usage, and supporting real-time path planning in practical environments.
Benchmark results demonstrate that the use of octree structure ensures an
optimal path while significantly reducing memory usage and computation time.","Byeong-Il Ham, Hyun-Bin Kim, Kyung-Soo Kim",2025-09-05 09:15:20+00:00,http://arxiv.org/abs/2509.04950v1,http://arxiv.org/pdf/2509.04950v1,cs.RO
Towards an Accurate and Effective Robot Vision (The Problem of Topological Localization for Mobile Robots),"Topological localization is a fundamental problem in mobile robotics, since
robots must be able to determine their position in order to accomplish tasks.
Visual localization and place recognition are challenging due to perceptual
ambiguity, sensor noise, and illumination variations. This work addresses
topological localization in an office environment using only images acquired
with a perspective color camera mounted on a robot platform, without relying on
temporal continuity of image sequences. We evaluate state-of-the-art visual
descriptors, including Color Histograms, SIFT, ASIFT, RGB-SIFT, and
Bag-of-Visual-Words approaches inspired by text retrieval. Our contributions
include a systematic, quantitative comparison of these features, distance
measures, and classifiers. Performance was analyzed using standard evaluation
metrics and visualizations, extending previous experiments. Results demonstrate
the advantages of proper configurations of appearance descriptors, similarity
measures, and classifiers. The quality of these configurations was further
validated in the Robot Vision task of the ImageCLEF evaluation campaign, where
the system identified the most likely location of novel image sequences. Future
work will explore hierarchical models, ranking methods, and feature
combinations to build more robust localization systems, reducing training and
runtime while avoiding the curse of dimensionality. Ultimately, this aims
toward integrated, real-time localization across varied illumination and longer
routes.",Emanuela Boros,2025-09-05 09:14:59+00:00,http://arxiv.org/abs/2509.04948v1,http://arxiv.org/pdf/2509.04948v1,"cs.RO, cs.CV"
A Knowledge-Driven Diffusion Policy for End-to-End Autonomous Driving Based on Expert Routing,"End-to-end autonomous driving remains constrained by the need to generate
multi-modal actions, maintain temporal stability, and generalize across diverse
scenarios. Existing methods often collapse multi-modality, struggle with
long-horizon consistency, or lack modular adaptability. This paper presents
KDP, a knowledge-driven diffusion policy that integrates generative diffusion
modeling with a sparse mixture-of-experts routing mechanism. The diffusion
component generates temporally coherent and multi-modal action sequences, while
the expert routing mechanism activates specialized and reusable experts
according to context, enabling modular knowledge composition. Extensive
experiments across representative driving scenarios demonstrate that KDP
achieves consistently higher success rates, reduced collision risk, and
smoother control compared to prevailing paradigms. Ablation studies highlight
the effectiveness of sparse expert activation and the Transformer backbone, and
activation analyses reveal structured specialization and cross-scenario reuse
of experts. These results establish diffusion with expert routing as a scalable
and interpretable paradigm for knowledge-driven end-to-end autonomous driving.","Chengkai Xu, Jiaqi Liu, Yicheng Guo, Peng Hang, Jian Sun",2025-09-05 07:07:18+00:00,http://arxiv.org/abs/2509.04853v1,http://arxiv.org/pdf/2509.04853v1,"cs.RO, cs.AI"
COMMET: A System for Human-Induced Conflicts in Mobile Manipulation of Everyday Tasks,"Continuous advancements in robotics and AI are driving the integration of
robots from industry into everyday environments. However, dynamic and
unpredictable human activities in daily lives would directly or indirectly
conflict with robot actions. Besides, due to the social attributes of such
human-induced conflicts, solutions are not always unique and depend highly on
the user's personal preferences. To address these challenges and facilitate the
development of household robots, we propose COMMET, a system for human-induced
COnflicts in Mobile Manipulation of Everyday Tasks. COMMET employs a hybrid
detection approach, which begins with multi-modal retrieval and escalates to
fine-tuned model inference for low-confidence cases. Based on collected user
preferred options and settings, GPT-4o will be used to summarize user
preferences from relevant cases. In preliminary studies, our detection module
shows better accuracy and latency compared with GPT models. To facilitate
future research, we also design a user-friendly interface for user data
collection and demonstrate an effective workflow for real-world deployments.","Dongping Li, Shaoting Peng, John Pohovey, Katherine Rose Driggs-Campbell",2025-09-05 06:37:33+00:00,http://arxiv.org/abs/2509.04836v1,http://arxiv.org/pdf/2509.04836v1,cs.RO
Imitation Learning Based on Disentangled Representation Learning of Behavioral Characteristics,"In the field of robot learning, coordinating robot actions through language
instructions is becoming increasingly feasible. However, adapting actions to
human instructions remains challenging, as such instructions are often
qualitative and require exploring behaviors that satisfy varying conditions.
This paper proposes a motion generation model that adapts robot actions in
response to modifier directives human instructions imposing behavioral
conditions during task execution. The proposed method learns a mapping from
modifier directives to actions by segmenting demonstrations into short
sequences, assigning weakly supervised labels corresponding to specific
modifier types. We evaluated our method in wiping and pick and place tasks.
Results show that it can adjust motions online in response to modifier
directives, unlike conventional batch-based methods that cannot adapt during
execution.","Ryoga Oishi, Sho Sakaino, Toshiaki Tsuji",2025-09-05 01:31:07+00:00,http://arxiv.org/abs/2509.04737v1,http://arxiv.org/pdf/2509.04737v1,cs.RO
Language-Driven Hierarchical Task Structures as Explicit World Models for Multi-Agent Learning,"The convergence of Language models, Agent models, and World models represents
a critical frontier for artificial intelligence. While recent progress has
focused on scaling Language and Agent models, the development of sophisticated,
explicit World Models remains a key bottleneck, particularly for complex,
long-horizon multi-agent tasks. In domains such as robotic soccer, agents
trained via standard reinforcement learning in high-fidelity but
structurally-flat simulators often fail due to intractable exploration spaces
and sparse rewards. This position paper argues that the next frontier in
developing capable agents lies in creating environments that possess an
explicit, hierarchical World Model. We contend that this is best achieved
through hierarchical scaffolding, where complex goals are decomposed into
structured, manageable subgoals. Drawing evidence from a systematic review of
2024 research in multi-agent soccer, we identify a clear and decisive trend
towards integrating symbolic and hierarchical methods with multi-agent
reinforcement learning (MARL). These approaches implicitly or explicitly
construct a task-based world model to guide agent learning. We then propose a
paradigm shift: leveraging Large Language Models to dynamically generate this
hierarchical scaffold, effectively using language to structure the World Model
on the fly. This language-driven world model provides an intrinsic curriculum,
dense and meaningful learning signals, and a framework for compositional
learning, enabling Agent Models to acquire sophisticated, strategic behaviors
with far greater sample efficiency. By building environments with explicit,
language-configurable task layers, we can bridge the gap between low-level
reactive behaviors and high-level strategic team play, creating a powerful and
generalizable framework for training the next generation of intelligent agents.",Brennen Hill,2025-09-05 01:03:51+00:00,http://arxiv.org/abs/2509.04731v1,http://arxiv.org/pdf/2509.04731v1,"cs.AI, cs.CL, cs.LG, cs.MA, cs.RO, 68T05, 90C40, 91A26, 68T42, 93E35, I.2.11; I.2.6; I.2.8; I.2.9; I.2.7"
Hierarchical Reduced-Order Model Predictive Control for Robust Locomotion on Humanoid Robots,"As humanoid robots enter real-world environments, ensuring robust locomotion
across diverse environments is crucial. This paper presents a computationally
efficient hierarchical control framework for humanoid robot locomotion based on
reduced-order models -- enabling versatile step planning and incorporating arm
and torso dynamics to better stabilize the walking. At the high level, we use
the step-to-step dynamics of the ALIP model to simultaneously optimize over
step periods, step lengths, and ankle torques via nonlinear MPC. The ALIP
trajectories are used as references to a linear MPC framework that extends the
standard SRB-MPC to also include simplified arm and torso dynamics. We validate
the performance of our approach through simulation and hardware experiments on
the Unitree G1 humanoid robot. In the proposed framework the high-level step
planner runs at 40 Hz and the mid-level MPC at 500 Hz using the onboard
mini-PC. Adaptive step timing increased the push recovery success rate by 36%,
and the upper body control improved the yaw disturbance rejection. We also
demonstrate robust locomotion across diverse indoor and outdoor terrains,
including grass, stone pavement, and uneven gym mats.","Adrian B. Ghansah, Sergio A. Esteban, Aaron D. Ames",2025-09-05 00:31:32+00:00,http://arxiv.org/abs/2509.04722v1,http://arxiv.org/pdf/2509.04722v1,cs.RO
Bootstrapping Reinforcement Learning with Sub-optimal Policies for Autonomous Driving,"Automated vehicle control using reinforcement learning (RL) has attracted
significant attention due to its potential to learn driving policies through
environment interaction. However, RL agents often face training challenges in
sample efficiency and effective exploration, making it difficult to discover an
optimal driving strategy. To address these issues, we propose guiding the RL
driving agent with a demonstration policy that need not be a highly optimized
or expert-level controller. Specifically, we integrate a rule-based lane change
controller with the Soft Actor Critic (SAC) algorithm to enhance exploration
and learning efficiency. Our approach demonstrates improved driving performance
and can be extended to other driving scenarios that can similarly benefit from
demonstration-based guidance.","Zhihao Zhang, Chengyang Peng, Ekim Yurtsever, Keith A. Redmill",2025-09-04 23:56:26+00:00,http://arxiv.org/abs/2509.04712v1,http://arxiv.org/pdf/2509.04712v1,"cs.RO, cs.AI, cs.LG, cs.SY, eess.SY"
Domain Adaptation for Different Sensor Configurations in 3D Object Detection,"Recent advances in autonomous driving have underscored the importance of
accurate 3D object detection, with LiDAR playing a central role due to its
robustness under diverse visibility conditions. However, different vehicle
platforms often deploy distinct sensor configurations, causing performance
degradation when models trained on one configuration are applied to another
because of shifts in the point cloud distribution. Prior work on multi-dataset
training and domain adaptation for 3D object detection has largely addressed
environmental domain gaps and density variation within a single LiDAR; in
contrast, the domain gap for different sensor configurations remains largely
unexplored. In this work, we address domain adaptation across different sensor
configurations in 3D object detection. We propose two techniques: Downstream
Fine-tuning (dataset-specific fine-tuning after multi-dataset training) and
Partial Layer Fine-tuning (updating only a subset of layers to improve
cross-configuration generalization). Using paired datasets collected in the
same geographic region with multiple sensor configurations, we show that joint
training with Downstream Fine-tuning and Partial Layer Fine-tuning consistently
outperforms naive joint training for each configuration. Our findings provide a
practical and scalable solution for adapting 3D object detection models to the
diverse vehicle platforms.","Satoshi Tanaka, Kok Seang Tan, Isamu Yamashita",2025-09-04 23:54:25+00:00,http://arxiv.org/abs/2509.04711v1,http://arxiv.org/pdf/2509.04711v1,"cs.CV, cs.RO"
Surformer v2: A Multimodal Classifier for Surface Understanding from Touch and Vision,"Multimodal surface material classification plays a critical role in advancing
tactile perception for robotic manipulation and interaction. In this paper, we
present Surformer v2, an enhanced multi-modal classification architecture
designed to integrate visual and tactile sensory streams through a
late(decision level) fusion mechanism. Building on our earlier Surformer v1
framework [1], which employed handcrafted feature extraction followed by
mid-level fusion architecture with multi-head cross-attention layers, Surformer
v2 integrates the feature extraction process within the model itself and shifts
to late fusion. The vision branch leverages a CNN-based classifier(Efficient
V-Net), while the tactile branch employs an encoder-only transformer model,
allowing each modality to extract modality-specific features optimized for
classification. Rather than merging feature maps, the model performs
decision-level fusion by combining the output logits using a learnable weighted
sum, enabling adaptive emphasis on each modality depending on data context and
training dynamics. We evaluate Surformer v2 on the Touch and Go dataset [2], a
multi-modal benchmark comprising surface images and corresponding tactile
sensor readings. Our results demonstrate that Surformer v2 performs well,
maintaining competitive inference speed, suitable for real-time robotic
applications. These findings underscore the effectiveness of decision-level
fusion and transformer-based tactile modeling for enhancing surface
understanding in multi-modal robotic perception.","Manish Kansana, Sindhuja Penchala, Shahram Rahimi, Noorbakhsh Amiri Golilarz",2025-09-04 21:05:33+00:00,http://arxiv.org/abs/2509.04658v1,http://arxiv.org/pdf/2509.04658v1,cs.RO
Cumplimiento del Reglamento (UE) 2024/1689 en robótica y sistemas autónomos: una revisión sistemática de la literatura,"This systematic literature review analyzes the current state of compliance
with Regulation (EU) 2024/1689 in autonomous robotic systems, focusing on
cybersecurity frameworks and methodologies. Using the PRISMA protocol, 22
studies were selected from 243 initial records across IEEE Xplore, ACM DL,
Scopus, and Web of Science. Findings reveal partial regulatory alignment: while
progress has been made in risk management and encrypted communications,
significant gaps persist in explainability modules, real-time human oversight,
and knowledge base traceability. Only 40% of reviewed solutions explicitly
address transparency requirements, and 30% implement failure intervention
mechanisms. The study concludes that modular approaches integrating risk,
supervision, and continuous auditing are essential to meet the AI Act mandates
in autonomous robotics.",Yoana Pita Lorenzo,2025-09-04 20:56:33+00:00,http://arxiv.org/abs/2509.05380v1,http://arxiv.org/pdf/2509.05380v1,"cs.CY, cs.AI, cs.CR, cs.RO"
Planning from Point Clouds over Continuous Actions for Multi-object Rearrangement,"Long-horizon planning for robot manipulation is a challenging problem that
requires reasoning about the effects of a sequence of actions on a physical 3D
scene. While traditional task planning methods are shown to be effective for
long-horizon manipulation, they require discretizing the continuous state and
action space into symbolic descriptions of objects, object relationships, and
actions. Instead, we propose a hybrid learning-and-planning approach that
leverages learned models as domain-specific priors to guide search in
high-dimensional continuous action spaces. We introduce SPOT: Search over Point
cloud Object Transformations, which plans by searching for a sequence of
transformations from an initial scene point cloud to a goal-satisfying point
cloud. SPOT samples candidate actions from learned suggesters that operate on
partially observed point clouds, eliminating the need to discretize actions or
object relationships. We evaluate SPOT on multi-object rearrangement tasks,
reporting task planning success and task execution success in both simulation
and real-world environments. Our experiments show that SPOT generates
successful plans and outperforms a policy-learning approach. We also perform
ablations that highlight the importance of search-based planning.","Kallol Saha, Amber Li, Angela Rodriguez-Izquierdo, Lifan Yu, Ben Eisner, Maxim Likhachev, David Held",2025-09-04 20:07:15+00:00,http://arxiv.org/abs/2509.04645v1,http://arxiv.org/pdf/2509.04645v1,cs.RO
Action Chunking with Transformers for Image-Based Spacecraft Guidance and Control,"We present an imitation learning approach for spacecraft guidance,
navigation, and control(GNC) that achieves high performance from limited data.
Using only 100 expert demonstrations, equivalent to 6,300 environment
interactions, our method, which implements Action Chunking with Transformers
(ACT), learns a control policy that maps visual and state observations to
thrust and torque commands. ACT generates smoother, more consistent
trajectories than a meta-reinforcement learning (meta-RL) baseline trained with
40 million interactions. We evaluate ACT on a rendezvous task: in-orbit docking
with the International Space Station (ISS). We show that our approach achieves
greater accuracy, smoother control, and greater sample efficiency.","Alejandro Posadas-Nava, Andrea Scorsoglio, Luca Ghilardi, Roberto Furfaro, Richard Linares",2025-09-04 19:29:02+00:00,http://arxiv.org/abs/2509.04628v1,http://arxiv.org/pdf/2509.04628v1,"cs.RO, cs.AI"
"UAV-Based Intelligent Traffic Surveillance System: Real-Time Vehicle Detection, Classification, Tracking, and Behavioral Analysis","Traffic congestion and violations pose significant challenges for urban
mobility and road safety. Traditional traffic monitoring systems, such as fixed
cameras and sensor-based methods, are often constrained by limited coverage,
low adaptability, and poor scalability. To address these challenges, this paper
introduces an advanced unmanned aerial vehicle (UAV)-based traffic surveillance
system capable of accurate vehicle detection, classification, tracking, and
behavioral analysis in real-world, unconstrained urban environments. The system
leverages multi-scale and multi-angle template matching, Kalman filtering, and
homography-based calibration to process aerial video data collected from
altitudes of approximately 200 meters. A case study in urban area demonstrates
robust performance, achieving a detection precision of 91.8%, an F1-score of
90.5%, and tracking metrics (MOTA/MOTP) of 92.1% and 93.7%, respectively.
Beyond precise detection, the system classifies five vehicle types and
automatically detects critical traffic violations, including unsafe lane
changes, illegal double parking, and crosswalk obstructions, through the fusion
of geofencing, motion filtering, and trajectory deviation analysis. The
integrated analytics module supports origin-destination tracking, vehicle count
visualization, inter-class correlation analysis, and heatmap-based congestion
modeling. Additionally, the system enables entry-exit trajectory profiling,
vehicle density estimation across road segments, and movement direction
logging, supporting comprehensive multi-scale urban mobility analytics.
Experimental results confirms the system's scalability, accuracy, and practical
relevance, highlighting its potential as an enforcement-aware,
infrastructure-independent traffic monitoring solution for next-generation
smart cities.","Ali Khanpour, Tianyi Wang, Afra Vahidi-Shams, Wim Ectors, Farzam Nakhaie, Amirhossein Taheri, Christian Claudel",2025-09-04 19:19:25+00:00,http://arxiv.org/abs/2509.04624v1,http://arxiv.org/pdf/2509.04624v1,"cs.CV, cs.ET, cs.RO, cs.SY, eess.IV, eess.SY"
EMMA: Scaling Mobile Manipulation via Egocentric Human Data,"Scaling mobile manipulation imitation learning is bottlenecked by expensive
mobile robot teleoperation. We present Egocentric Mobile MAnipulation (EMMA),
an end-to-end framework training mobile manipulation policies from human mobile
manipulation data with static robot data, sidestepping mobile teleoperation. To
accomplish this, we co-train human full-body motion data with static robot
data. In our experiments across three real-world tasks, EMMA demonstrates
comparable performance to baselines trained on teleoperated mobile robot data
(Mobile ALOHA), achieving higher or equivalent task performance in full task
success. We find that EMMA is able to generalize to new spatial configurations
and scenes, and we observe positive performance scaling as we increase the
hours of human data, opening new avenues for scalable robotic learning in
real-world environments. Details of this project can be found at
https://ego-moma.github.io/.","Lawrence Y. Zhu, Pranav Kuppili, Ryan Punamiya, Patcharapong Aphiwetsa, Dhruv Patel, Simar Kareer, Sehoon Ha, Danfei Xu",2025-09-04 17:59:10+00:00,http://arxiv.org/abs/2509.04443v1,http://arxiv.org/pdf/2509.04443v1,cs.RO
DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation,"We introduce perioperation, a paradigm for robotic data collection that
sensorizes and records human manipulation while maximizing the transferability
of the data to real robots. We implement this paradigm in DEXOP, a passive hand
exoskeleton designed to maximize human ability to collect rich sensory (vision
+ tactile) data for diverse dexterous manipulation tasks in natural
environments. DEXOP mechanically connects human fingers to robot fingers,
providing users with direct contact feedback (via proprioception) and mirrors
the human hand pose to the passive robot hand to maximize the transfer of
demonstrated skills to the robot. The force feedback and pose mirroring make
task demonstrations more natural for humans compared to teleoperation,
increasing both speed and accuracy. We evaluate DEXOP across a range of
dexterous, contact-rich tasks, demonstrating its ability to collect
high-quality demonstration data at scale. Policies learned with DEXOP data
significantly improve task performance per unit time of data collection
compared to teleoperation, making DEXOP a powerful tool for advancing robot
dexterity. Our project page is at https://dex-op.github.io.","Hao-Shu Fang, Branden Romero, Yichen Xie, Arthur Hu, Bo-Ruei Huang, Juan Alvarez, Matthew Kim, Gabriel Margolis, Kavya Anbarasu, Masayoshi Tomizuka, Edward Adelson, Pulkit Agrawal",2025-09-04 17:57:13+00:00,http://arxiv.org/abs/2509.04441v2,http://arxiv.org/pdf/2509.04441v2,"cs.RO, cs.AI, cs.CV, cs.HC"
SAFE--MA--RRT: Multi-Agent Motion Planning with Data-Driven Safety Certificates,"This paper proposes a fully data-driven motion-planning framework for
homogeneous linear multi-agent systems that operate in shared, obstacle-filled
workspaces without access to explicit system models. Each agent independently
learns its closed-loop behavior from experimental data by solving convex
semidefinite programs that generate locally invariant ellipsoids and
corresponding state-feedback gains. These ellipsoids, centered along grid-based
waypoints, certify the dynamic feasibility of short-range transitions and
define safe regions of operation. A sampling-based planner constructs a tree of
such waypoints, where transitions are allowed only when adjacent ellipsoids
overlap, ensuring invariant-to-invariant transitions and continuous safety. All
agents expand their trees simultaneously and are coordinated through a
space-time reservation table that guarantees inter-agent safety by preventing
simultaneous occupancy and head-on collisions. Each successful edge in the tree
is equipped with its own local controller, enabling execution without
re-solving optimization problems at runtime. The resulting trajectories are not
only dynamically feasible but also provably safe with respect to both
environmental constraints and inter-agent collisions. Simulation results
demonstrate the effectiveness of the approach in synthesizing synchronized,
safe trajectories for multiple agents under shared dynamics and constraints,
using only data and convex optimization tools.","Babak Esmaeili, Hamidreza Modares",2025-09-04 17:34:59+00:00,http://arxiv.org/abs/2509.04413v1,http://arxiv.org/pdf/2509.04413v1,"eess.SY, cs.LG, cs.MA, cs.RO, cs.SY, math.OC"
Leveraging Equivariances and Symmetries in the Control Barrier Function Synthesis,"The synthesis of Control Barrier Functions (CBFs) often involves demanding
computations or a meticulous construction. However, structural properties of
the system dynamics and constraints have the potential to mitigate these
challenges. In this paper, we explore how equivariances in the dynamics,
loosely speaking a form of symmetry, can be leveraged in the CBF synthesis.
Although CBFs are generally not inherently symmetric, we show how equivariances
in the dynamics and symmetries in the constraints induce symmetries in CBFs
derived through reachability analysis. This insight allows us to infer their
CBF values across the entire domain from their values on a subset, leading to
significant computational savings. Interestingly, equivariances can be even
leveraged to the CBF synthesis for non-symmetric constraints. Specifically, we
show how a partially known CBF can be leveraged together with equivariances to
construct a CBF for various new constraints. Throughout the paper, we provide
examples illustrating the theoretical findings. Furthermore, a numerical study
investigates the computational gains from invoking equivariances into the CBF
synthesis.","Adrian Wiltz, Dimos V. Dimarogonas",2025-09-04 17:10:15+00:00,http://arxiv.org/abs/2509.04399v1,http://arxiv.org/pdf/2509.04399v1,"eess.SY, cs.RO, cs.SY"
"Privacy Perceptions in Robot-Assisted Well-Being Coaching: Examining the Roles of Information Transparency, User Control, and Proactivity","Social robots are increasingly recognized as valuable supporters in the field
of well-being coaching. They can function as independent coaches or provide
support alongside human coaches, and healthcare professionals. In coaching
interactions, these robots often handle sensitive information shared by users,
making privacy a relevant issue. Despite this, little is known about the
factors that shape users' privacy perceptions. This research aims to examine
three key factors systematically: (1) the transparency about information usage,
(2) the level of specific user control over how the robot uses their
information, and (3) the robot's behavioral approach - whether it acts
proactively or only responds on demand. Our results from an online study (N =
200) show that even when users grant the robot general access to personal data,
they additionally expect the ability to explicitly control how that information
is interpreted and shared during sessions. Experimental conditions that
provided such control received significantly higher ratings for perceived
privacy appropriateness and trust. Compared to user control, the effects of
transparency and proactivity on privacy appropriateness perception were low,
and we found no significant impact. The results suggest that merely informing
users or proactive sharing is insufficient without accompanying user control.
These insights underscore the need for further research on mechanisms that
allow users to manage robots' information processing and sharing, especially
when social robots take on more proactive roles alongside humans.","Atikkhan Faridkhan Nilgar, Manuel Dietrich, Kristof Van Laerhoven",2025-09-04 16:19:24+00:00,http://arxiv.org/abs/2509.04358v1,http://arxiv.org/pdf/2509.04358v1,"cs.HC, cs.RO"
SRWToolkit: An Open Source Wizard of Oz Toolkit to Create Social Robotic Avatars,"We present SRWToolkit, an open-source Wizard of Oz toolkit designed to
facilitate the rapid prototyping of social robotic avatars powered by local
large language models (LLMs). Our web-based toolkit enables multimodal
interaction through text input, button-activated speech, and wake-word command.
The toolkit offers real-time configuration of avatar appearance, behavior,
language, and voice via an intuitive control panel. In contrast to prior works
that rely on cloud-based LLM services, SRWToolkit emphasizes modularity and
ensures on-device functionality through local LLM inference. In our small-scale
user study ($n=11$), participants created and interacted with diverse robotic
roles (hospital receptionist, mathematics teacher, and driving assistant),
which demonstrated positive outcomes in the toolkit's usability, trust, and
user experience. The toolkit enables rapid and efficient development of robot
characters customized to researchers' needs, supporting scalable research in
human-robot interaction.","Atikkhan Faridkhan Nilgar, Kristof Van Laerhoven, Ayub Kinoti",2025-09-04 16:18:04+00:00,http://arxiv.org/abs/2509.04356v1,http://arxiv.org/pdf/2509.04356v1,"cs.HC, cs.RO"
OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent Detection,"Grasping assistance is essential for restoring autonomy in individuals with
motor impairments, particularly in unstructured environments where object
categories and user intentions are diverse and unpredictable. We present
OVGrasp, a hierarchical control framework for soft exoskeleton-based grasp
assistance that integrates RGB-D vision, open-vocabulary prompts, and voice
commands to enable robust multimodal interaction. To enhance generalization in
open environments, OVGrasp incorporates a vision-language foundation model with
an open-vocabulary mechanism, allowing zero-shot detection of previously unseen
objects without retraining. A multimodal decision-maker further fuses spatial
and linguistic cues to infer user intent, such as grasp or release, in
multi-object scenarios. We deploy the complete framework on a custom
egocentric-view wearable exoskeleton and conduct systematic evaluations on 15
objects across three grasp types. Experimental results with ten participants
demonstrate that OVGrasp achieves a grasping ability score (GAS) of 87.00%,
outperforming state-of-the-art baselines and achieving improved kinematic
alignment with natural hand motion.","Chen Hu, Shan Luo, Letizia Gionfrida",2025-09-04 15:42:36+00:00,http://arxiv.org/abs/2509.04324v1,http://arxiv.org/pdf/2509.04324v1,"cs.RO, cs.CV"
Compatibility of Multiple Control Barrier Functions for Constrained Nonlinear Systems,"Control barrier functions (CBFs) are a powerful tool for the constrained
control of nonlinear systems; however, the majority of results in the
literature focus on systems subject to a single CBF constraint, making it
challenging to synthesize provably safe controllers that handle multiple state
constraints. This paper presents a framework for constrained control of
nonlinear systems subject to box constraints on the systems' vector-valued
outputs using multiple CBFs. Our results illustrate that when the output has a
vector relative degree, the CBF constraints encoding these box constraints are
compatible, and the resulting optimization-based controller is locally
Lipschitz continuous and admits a closed-form expression. Additional results
are presented to characterize the degradation of nominal tracking objectives in
the presence of safety constraints. Simulations of a planar quadrotor are
presented to demonstrate the efficacy of the proposed framework.","Max H. Cohen, Eugene Lavretsky, Aaron D. Ames",2025-09-04 13:52:11+00:00,http://arxiv.org/abs/2509.04220v1,http://arxiv.org/pdf/2509.04220v1,"eess.SY, cs.RO, cs.SY, math.OC"
YOLO Ensemble for UAV-based Multispectral Defect Detection in Wind Turbine Components,"Unmanned aerial vehicles (UAVs) equipped with advanced sensors have opened up
new opportunities for monitoring wind power plants, including blades, towers,
and other critical components. However, reliable defect detection requires
high-resolution data and efficient methods to process multispectral imagery. In
this research, we aim to enhance defect detection accuracy through the
development of an ensemble of YOLO-based deep learning models that integrate
both visible and thermal channels. We propose an ensemble approach that
integrates a general-purpose YOLOv8 model with a specialized thermal model,
using a sophisticated bounding box fusion algorithm to combine their
predictions. Our experiments show this approach achieves a mean Average
Precision (mAP@.5) of 0.93 and an F1-score of 0.90, outperforming a standalone
YOLOv8 model, which scored an mAP@.5 of 0.91. These findings demonstrate that
combining multiple YOLO architectures with fused multispectral data provides a
more reliable solution, improving the detection of both visual and thermal
defects.","Serhii Svystun, Pavlo Radiuk, Oleksandr Melnychenko, Oleg Savenko, Anatoliy Sachenko",2025-09-04 12:32:04+00:00,http://arxiv.org/abs/2509.04156v1,http://arxiv.org/pdf/2509.04156v1,"cs.CV, cs.AI, cs.RO, 68T07, 68T45, 68U10, 68T40, I.2.10; I.4.8; I.5.4; I.2.9"
Lightweight Kinematic and Static Modeling of Cable-Driven Continuum Robots via Actuation-Space Energy Formulation,"Continuum robots, inspired by octopus arms and elephant trunks, combine
dexterity with intrinsic compliance, making them well suited for unstructured
and confined environments. Yet their continuously deformable morphology poses
challenges for motion planning and control, calling for accurate but
lightweight models. We propose the Lightweight Actuation Space Energy Modeling
(LASEM) framework for cable driven continuum robots, which formulates actuation
potential energy directly in actuation space. LASEM yields an analytical
forward model derived from geometrically nonlinear beam and rod theories via
Hamilton's principle, while avoiding explicit modeling of cable backbone
contact. It accepts both force and displacement inputs, thereby unifying
kinematic and static formulations. Assuming the friction is neglected, the
framework generalizes to nonuniform geometries, arbitrary cable routings,
distributed loading and axial extensibility, while remaining computationally
efficient for real-time use. Numerical simulations validate its accuracy, and a
semi-analytical iterative scheme is developed for inverse kinematics. To
address discretization in practical robots, LASEM further reformulates the
functional minimization as a numerical optimization, which also naturally
incorporates cable potential energy without explicit contact modeling.","Ke Wu, Yuhao Wang, Kevin Henry, Cesare Stefanini, Gang Zheng",2025-09-04 11:33:53+00:00,http://arxiv.org/abs/2509.04119v1,http://arxiv.org/pdf/2509.04119v1,cs.RO
Cloud-Assisted Remote Control for Aerial Robots: From Theory to Proof-of-Concept Implementation,"Cloud robotics has emerged as a promising technology for robotics
applications due to its advantages of offloading computationally intensive
tasks, facilitating data sharing, and enhancing robot coordination. However,
integrating cloud computing with robotics remains a complex challenge due to
network latency, security concerns, and the need for efficient resource
management. In this work, we present a scalable and intuitive framework for
testing cloud and edge robotic systems. The framework consists of two main
components enabled by containerized technology: (a) a containerized cloud
cluster and (b) the containerized robot simulation environment. The system
incorporates two endpoints of a User Datagram Protocol (UDP) tunnel, enabling
bidirectional communication between the cloud cluster container and the robot
simulation environment, while simulating realistic network conditions. To
achieve this, we consider the use case of cloud-assisted remote control for
aerial robots, while utilizing Linux-based traffic control to introduce
artificial delay and jitter, replicating variable network conditions
encountered in practical cloud-robot deployments.","Achilleas Santi Seisa, Viswa Narayanan Sankaranarayanan, Gerasimos Damigos, Sumeet Gajanan Satpute, George Nikolakopoulos",2025-09-04 10:53:27+00:00,http://arxiv.org/abs/2509.04095v1,http://arxiv.org/pdf/2509.04095v1,"cs.RO, cs.DC"
Object-Reconstruction-Aware Whole-body Control of Mobile Manipulators,"Object reconstruction and inspection tasks play a crucial role in various
robotics applications. Identifying paths that reveal the most unknown areas of
the object becomes paramount in this context, as it directly affects
efficiency, and this problem is known as the view path planning problem.
Current methods often use sampling-based path planning techniques, evaluating
potential views along the path to enhance reconstruction performance. However,
these methods are computationally expensive as they require evaluating several
candidate views on the path. To this end, we propose a computationally
efficient solution that relies on calculating a focus point in the most
informative (unknown) region and having the robot maintain this point in the
camera field of view along the path. We incorporated this strategy into the
whole-body control of a mobile manipulator employing a visibility constraint
without the need for an additional path planner. We conducted comprehensive and
realistic simulations using a large dataset of 114 diverse objects of varying
sizes from 57 categories to compare our method with a sampling-based planning
strategy using Bayesian data analysis. Furthermore, we performed real-world
experiments with an 8-DoF mobile manipulator to demonstrate the proposed
method's performance in practice. Our results suggest that there is no
significant difference in object coverage and entropy. In contrast, our method
is approximately nine times faster than the baseline sampling-based method in
terms of the average time the robot spends between views.","Fatih Dursun, Bruno Vilhena Adorno, Simon Watson, Wei Pan",2025-09-04 10:52:27+00:00,http://arxiv.org/abs/2509.04094v1,http://arxiv.org/pdf/2509.04094v1,cs.RO
Keypoint-based Diffusion for Robotic Motion Planning on the NICOL Robot,"We propose a novel diffusion-based action model for robotic motion planning.
Commonly, established numerical planning approaches are used to solve general
motion planning problems, but have significant runtime requirements. By
leveraging the power of deep learning, we are able to achieve good results in a
much smaller runtime by learning from a dataset generated by these planners.
While our initial model uses point cloud embeddings in the input to predict
keypoint-based joint sequences in its output, we observed in our ablation study
that it remained challenging to condition the network on the point cloud
embeddings. We identified some biases in our dataset and refined it, which
improved the model's performance. Our model, even without the use of the point
cloud encodings, outperforms numerical models by an order of magnitude
regarding the runtime, while reaching a success rate of up to 90% of collision
free solutions on the test set.","Lennart Clasmeier, Jan-Gerrit Habekost, Connor Gäde, Philipp Allgeuer, Stefan Wermter",2025-09-04 10:11:51+00:00,http://arxiv.org/abs/2509.04076v1,http://arxiv.org/pdf/2509.04076v1,"cs.RO, cs.AI"
Solving Robotics Tasks with Prior Demonstration via Exploration-Efficient Deep Reinforcement Learning,"This paper proposes an exploration-efficient Deep Reinforcement Learning with
Reference policy (DRLR) framework for learning robotics tasks that incorporates
demonstrations. The DRLR framework is developed based on an algorithm called
Imitation Bootstrapped Reinforcement Learning (IBRL). We propose to improve
IBRL by modifying the action selection module. The proposed action selection
module provides a calibrated Q-value, which mitigates the bootstrapping error
that otherwise leads to inefficient exploration. Furthermore, to prevent the RL
policy from converging to a sub-optimal policy, SAC is used as the RL policy
instead of TD3. The effectiveness of our method in mitigating bootstrapping
error and preventing overfitting is empirically validated by learning two
robotics tasks: bucket loading and open drawer, which require extensive
interactions with the environment. Simulation results also demonstrate the
robustness of the DRLR framework across tasks with both low and high
state-action dimensions, and varying demonstration qualities. To evaluate the
developed framework on a real-world industrial robotics task, the bucket
loading task is deployed on a real wheel loader. The sim2real results validate
the successful deployment of the DRLR framework.","Chengyandan Shen, Christoffer Sloth",2025-09-04 10:02:32+00:00,http://arxiv.org/abs/2509.04069v1,http://arxiv.org/pdf/2509.04069v1,"cs.RO, cs.LG"
Balancing Signal and Variance: Adaptive Offline RL Post-Training for VLA Flow Models,"Vision-Language-Action (VLA) models based on flow matching have shown
excellent performance in general-purpose robotic manipulation tasks. However,
the action accuracy of these models on complex downstream tasks is
unsatisfactory. One important reason is that these models rely solely on the
post-training paradigm of imitation learning, which makes it difficult to have
a deeper understanding of the distribution properties of data quality, which is
exactly what Reinforcement Learning (RL) excels at. In this paper, we
theoretically propose an offline RL post-training objective for VLA flow models
and induce an efficient and feasible offline RL fine-tuning algorithm --
Adaptive Reinforced Flow Matching (ARFM). By introducing an adaptively adjusted
scaling factor in the VLA flow model loss, we construct a principled
bias-variance trade-off objective function to optimally control the impact of
RL signal on flow loss. ARFM adaptively balances RL advantage preservation and
flow loss gradient variance control, resulting in a more stable and efficient
fine-tuning process. Extensive simulation and real-world experimental results
show that ARFM exhibits excellent generalization, robustness, few-shot
learning, and continuous learning performance.","Hongyin Zhang, Shiyuan Zhang, Junxi Jin, Qixin Zeng, Yifan Qiao, Hongchao Lu, Donglin Wang",2025-09-04 09:48:43+00:00,http://arxiv.org/abs/2509.04063v1,http://arxiv.org/pdf/2509.04063v1,"cs.RO, cs.LG"
Integrated Wheel Sensor Communication using ESP32 -- A Contribution towards a Digital Twin of the Road System,"While current onboard state estimation methods are adequate for most driving
and safety-related applications, they do not provide insights into the
interaction between tires and road surfaces. This paper explores a novel
communication concept for efficiently transmitting integrated wheel sensor data
from an ESP32 microcontroller. Our proposed approach utilizes a
publish-subscribe system, surpassing comparable solutions in the literature
regarding data transmission volume. We tested this approach on a drum tire test
rig with our prototype sensors system utilizing a diverse selection of sample
frequencies between 1 Hz and 32 000 Hz to demonstrate the efficacy of our
communication concept. The implemented prototype sensor showcases minimal data
loss, approximately 0.1 % of the sampled data, validating the reliability of
our developed communication system. This work contributes to advancing
real-time data acquisition, providing insights into optimizing integrated wheel
sensor communication.","Ventseslav Yordanov, Simon Schäfer, Alexander Mann, Stefan Kowalewski, Bassam Alrifaee, Lutz Eckstein",2025-09-04 09:47:10+00:00,http://arxiv.org/abs/2509.04061v1,http://arxiv.org/pdf/2509.04061v1,cs.RO
FPC-VLA: A Vision-Language-Action Framework with a Supervisor for Failure Prediction and Correction,"Robotic manipulation is a fundamental component of automation. However,
traditional perception-planning pipelines often fall short in open-ended tasks
due to limited flexibility, while the architecture of a single end-to-end
Vision-Language-Action (VLA) offers promising capabilities but lacks crucial
mechanisms for anticipating and recovering from failure. To address these
challenges, we propose FPC-VLA, a dual-model framework that integrates VLA with
a supervisor for failure prediction and correction. The supervisor evaluates
action viability through vision-language queries and generates corrective
strategies when risks arise, trained efficiently without manual labeling. A
similarity-guided fusion module further refines actions by leveraging past
predictions. Evaluation results on multiple simulation platforms (SIMPLER and
LIBERO) and robot embodiments (WidowX, Google Robot, Franka) show that FPC-VLA
outperforms state-of-the-art models in both zero-shot and fine-tuned settings.
By activating the supervisor only at keyframes, our approach significantly
increases task success rates with minimal impact on execution time. Successful
real-world deployments on diverse, long-horizon tasks confirm FPC-VLA's strong
generalization and practical utility for building more reliable autonomous
systems.","Yifan Yang, Zhixiang Duan, Tianshi Xie, Fuyu Cao, Pinxi Shen, Peili Song, Piaopiao Jin, Guokang Sun, Shaoqing Xu, Yangwei You, Jingtai Liu",2025-09-04 08:47:26+00:00,http://arxiv.org/abs/2509.04018v1,http://arxiv.org/pdf/2509.04018v1,cs.RO
Odometry Calibration and Pose Estimation of a 4WIS4WID Mobile Wall Climbing Robot,"This paper presents the design of a pose estimator for a four wheel
independent steer four wheel independent drive (4WIS4WID) wall climbing mobile
robot, based on the fusion of multimodal measurements, including wheel
odometry, visual odometry, and an inertial measurement unit (IMU) data using
Extended Kalman Filter (EKF) and Unscented Kalman Filter (UKF). The pose
estimator is a critical component of wall climbing mobile robots, as their
operational environment involves carrying precise measurement equipment and
maintenance tools in construction, requiring information about pose on the
building at the time of measurement. Due to the complex geometry and material
properties of building facades, the use of traditional localization sensors
such as laser, ultrasonic, or radar is often infeasible for wall-climbing
robots. Moreover, GPS-based localization is generally unreliable in these
environments because of signal degradation caused by reinforced concrete and
electromagnetic interference. Consequently, robot odometry remains the primary
source of velocity and position information, despite being susceptible to drift
caused by both systematic and non-systematic errors. The calibrations of the
robot's systematic parameters were conducted using nonlinear optimization and
Levenberg-Marquardt methods as Newton-Gauss and gradient-based model fitting
methods, while Genetic algorithm and Particle swarm were used as
stochastic-based methods for kinematic parameter calibration. Performance and
results of the calibration methods and pose estimators were validated in detail
with experiments on the experimental mobile wall climbing robot.","Branimir Ćaran, Vladimir Milić, Marko Švaco, Bojan Jerbić",2025-09-04 08:44:36+00:00,http://arxiv.org/abs/2509.04016v1,http://arxiv.org/pdf/2509.04016v1,cs.RO
In-Context Policy Adaptation via Cross-Domain Skill Diffusion,"In this work, we present an in-context policy adaptation (ICPAD) framework
designed for long-horizon multi-task environments, exploring diffusion-based
skill learning techniques in cross-domain settings. The framework enables rapid
adaptation of skill-based reinforcement learning policies to diverse target
domains, especially under stringent constraints on no model updates and only
limited target domain data. Specifically, the framework employs a cross-domain
skill diffusion scheme, where domain-agnostic prototype skills and a
domain-grounded skill adapter are learned jointly and effectively from an
offline dataset through cross-domain consistent diffusion processes. The
prototype skills act as primitives for common behavior representations of
long-horizon policies, serving as a lingua franca to bridge different domains.
Furthermore, to enhance the in-context adaptation performance, we develop a
dynamic domain prompting scheme that guides the diffusion-based skill adapter
toward better alignment with the target domain. Through experiments with
robotic manipulation in Metaworld and autonomous driving in CARLA, we show that
our $\oursol$ framework achieves superior policy adaptation performance under
limited target domain data conditions for various cross-domain configurations
including differences in environment dynamics, agent embodiment, and task
horizon.","Minjong Yoo, Woo Kyung Kim, Honguk Woo",2025-09-04 06:55:38+00:00,http://arxiv.org/abs/2509.04535v1,http://arxiv.org/pdf/2509.04535v1,"cs.RO, cs.AI, cs.LG"
Long-Horizon Visual Imitation Learning via Plan and Code Reflection,"Learning from long-horizon demonstrations with complex action sequences
presents significant challenges for visual imitation learning, particularly in
understanding temporal relationships of actions and spatial relationships
between objects. In this paper, we propose a new agent framework that
incorporates two dedicated reflection modules to enhance both plan and code
generation. The plan generation module produces an initial action sequence,
which is then verified by the plan reflection module to ensure temporal
coherence and spatial alignment with the demonstration video. The code
generation module translates the plan into executable code, while the code
reflection module verifies and refines the generated code to ensure correctness
and consistency with the generated plan. These two reflection modules jointly
enable the agent to detect and correct errors in both the plan generation and
code generation, improving performance in tasks with intricate temporal and
spatial dependencies. To support systematic evaluation, we introduce
LongVILBench, a benchmark comprising 300 human demonstrations with action
sequences of up to 18 steps. LongVILBench emphasizes temporal and spatial
complexity across multiple task types. Experimental results demonstrate that
existing methods perform poorly on this benchmark, whereas our new framework
establishes a strong baseline for long-horizon visual imitation learning.","Quan Chen, Chenrui Shi, Qi Chen, Yuwei Wu, Zhi Gao, Xintong Zhang, Rui Gao, Kun Wu, Yunde Jia",2025-09-04 06:10:32+00:00,http://arxiv.org/abs/2509.05368v1,http://arxiv.org/pdf/2509.05368v1,"cs.RO, cs.AI, cs.LG, I.2.9; I.2.10"
Reactive In-Air Clothing Manipulation with Confidence-Aware Dense Correspondence and Visuotactile Affordance,"Manipulating clothing is challenging due to complex configurations, variable
material dynamics, and frequent self-occlusion. Prior systems often flatten
garments or assume visibility of key features. We present a dual-arm
visuotactile framework that combines confidence-aware dense visual
correspondence and tactile-supervised grasp affordance to operate directly on
crumpled and suspended garments. The correspondence model is trained on a
custom, high-fidelity simulated dataset using a distributional loss that
captures cloth symmetries and generates correspondence confidence estimates.
These estimates guide a reactive state machine that adapts folding strategies
based on perceptual uncertainty. In parallel, a visuotactile grasp affordance
network, self-supervised using high-resolution tactile feedback, determines
which regions are physically graspable. The same tactile classifier is used
during execution for real-time grasp validation. By deferring action in
low-confidence states, the system handles highly occluded table-top and in-air
configurations. We demonstrate our task-agnostic grasp selection module in
folding and hanging tasks. Moreover, our dense descriptors provide a reusable
intermediate representation for other planning modalities, such as extracting
grasp targets from human video demonstrations, paving the way for more
generalizable and scalable garment manipulation.","Neha Sunil, Megha Tippur, Arnau Saumell, Edward Adelson, Alberto Rodriguez",2025-09-04 05:16:56+00:00,http://arxiv.org/abs/2509.03889v1,http://arxiv.org/pdf/2509.03889v1,"cs.RO, cs.AI, cs.LG"
Learning Multi-Stage Pick-and-Place with a Legged Mobile Manipulator,"Quadruped-based mobile manipulation presents significant challenges in
robotics due to the diversity of required skills, the extended task horizon,
and partial observability. After presenting a multi-stage pick-and-place task
as a succinct yet sufficiently rich setup that captures key desiderata for
quadruped-based mobile manipulation, we propose an approach that can train a
visuo-motor policy entirely in simulation, and achieve nearly 80\% success in
the real world. The policy efficiently performs search, approach, grasp,
transport, and drop into actions, with emerged behaviors such as re-grasping
and task chaining. We conduct an extensive set of real-world experiments with
ablation studies highlighting key techniques for efficient training and
effective sim-to-real transfer. Additional experiments demonstrate deployment
across a variety of indoor and outdoor environments. Demo videos and additional
resources are available on the project page:
https://horizonrobotics.github.io/gail/SLIM.","Haichao Zhang, Haonan Yu, Le Zhao, Andrew Choi, Qinxun Bai, Yiqing Yang, Wei Xu",2025-09-04 03:36:07+00:00,http://arxiv.org/abs/2509.03859v3,http://arxiv.org/pdf/2509.03859v3,cs.RO
INGRID: Intelligent Generative Robotic Design Using Large Language Models,"The integration of large language models (LLMs) into robotic systems has
accelerated progress in embodied artificial intelligence, yet current
approaches remain constrained by existing robotic architectures, particularly
serial mechanisms. This hardware dependency fundamentally limits the scope of
robotic intelligence. Here, we present INGRID (Intelligent Generative Robotic
Design), a framework that enables the automated design of parallel robotic
mechanisms through deep integration with reciprocal screw theory and kinematic
synthesis methods. We decompose the design challenge into four progressive
tasks: constraint analysis, kinematic joint generation, chain construction, and
complete mechanism design. INGRID demonstrates the ability to generate novel
parallel mechanisms with both fixed and variable mobility, discovering
kinematic configurations not previously documented in the literature. We
validate our approach through three case studies demonstrating how INGRID
assists users in designing task-specific parallel robots based on desired
mobility requirements. By bridging the gap between mechanism theory and machine
learning, INGRID enables researchers without specialized robotics training to
create custom parallel mechanisms, thereby decoupling advances in robotic
intelligence from hardware constraints. This work establishes a foundation for
mechanism intelligence, where AI systems actively design robotic hardware,
potentially transforming the development of embodied AI systems.","Guanglu Jia, Ceng Zhang, Gregory S. Chirikjian",2025-09-04 03:08:01+00:00,http://arxiv.org/abs/2509.03842v1,http://arxiv.org/pdf/2509.03842v1,"cs.RO, cs.AI"
Real-Time Buoyancy Estimation for AUV Simulations Using Convex Hull-Based Submerged Volume Calculation,"Accurate real-time buoyancy modeling is essential for high-fidelity
Autonomous Underwater Vehicle (AUV) simulations, yet NVIDIA Isaac Sim lacks a
native buoyancy system, requiring external solutions for precise underwater
physics. This paper presents a novel convex hull-based approach to dynamically
compute the submerged volume of an AUV in real time. By extracting mesh
geometry from the simulation environment and calculating the hull portion
intersecting the water level along the z-axis, our method enhances accuracy
over traditional geometric approximations. A cross-sectional area extension
reduces computational overhead, enabling efficient buoyant force updates that
adapt to orientation, depth, and sinusoidal wave fluctuations (+-0.3 m). Tested
on a custom AUV design for SAUVC 2025, this approach delivers real-time
performance and scalability, improving simulation fidelity for underwater
robotics research without precomputed hydrodynamic models.","Ad-Deen Mahbub, Md Ragib Shaharear",2025-09-04 01:42:11+00:00,http://arxiv.org/abs/2509.03804v1,http://arxiv.org/pdf/2509.03804v1,"cs.RO, cs.SY, eess.SY"
Memory Optimization for Convex Hull Support Point Queries,"This paper evaluates several improvements to the memory layout of convex
hulls to improve computation times for support point queries. The support point
query is a fundamental part of common collision algorithms, and the work
presented achieves a significant speedup depending on the number of vertices of
the convex hull.",Michael Greer,2025-09-03 22:45:50+00:00,http://arxiv.org/abs/2509.03753v1,http://arxiv.org/pdf/2509.03753v1,"cs.GR, cs.CG, cs.RO, 68U05, I.3.5"
Avoidance of an unexpected obstacle without reinforcement learning: Why not using advanced control-theoretic tools?,"This communication on collision avoidance with unexpected obstacles is
motivated by some critical appraisals on reinforcement learning (RL) which
""requires ridiculously large numbers of trials to learn any new task"" (Yann
LeCun). We use the classic Dubins' car in order to replace RL with
flatness-based control, combined with the HEOL feedback setting, and the latest
model-free predictive control approach. The two approaches lead to convincing
computer experiments where the results with the model-based one are only
slightly better. They exhibit a satisfactory robustness with respect to
randomly generated mismatches/disturbances, which become excellent in the
model-free case. Those properties would have been perhaps difficult to obtain
with today's popular machine learning techniques in AI. Finally, we should
emphasize that our two methods require a low computational burden.","Cédric Join, Michel Fliess",2025-09-03 21:03:43+00:00,http://arxiv.org/abs/2509.03721v1,http://arxiv.org/pdf/2509.03721v1,"eess.SY, cs.RO, cs.SY, math.OC"
Low-Cost Open-Source Ambidextrous Robotic Hand with 23 Direct-Drive servos for American Sign Language Alphabet,"Accessible communication through sign language is vital for deaf communities,
1 yet robotic solutions are often costly and limited. This study presents
VulcanV3, a low- 2 cost, open-source, 3D-printed ambidextrous robotic hand
capable of reproducing the full 3 American Sign Language (ASL) alphabet (52
signs for right- and left-hand configurations). 4 The system employs 23
direct-drive servo actuators for precise finger and wrist movements, 5
controlled by an Arduino Mega with dual PCA9685 modules. Unlike most humanoid
upper- 6 limb systems, which rarely employ direct-drive actuation, VulcanV3
achieves complete ASL 7 coverage with a reversible design. All CAD files and
code are released under permissive 8 open-source licenses to enable
replication. Empirical tests confirmed accurate reproduction 9 of all 52 ASL
handshapes, while a participant study (n = 33) achieved 96.97% recognition 10
accuracy, improving to 98.78% after video demonstration. VulcanV3 advances
assistive 11 robotics by combining affordability, full ASL coverage, and
ambidexterity in an openly 12 shared platform, contributing to accessible
communication technologies and inclusive 13 innovation.",Kelvin Daniel Gonzalez Amador,2025-09-03 20:13:19+00:00,http://arxiv.org/abs/2509.03690v1,http://arxiv.org/pdf/2509.03690v1,"cs.RO, I.2.9"
Efficient Virtuoso: A Latent Diffusion Transformer Model for Goal-Conditioned Trajectory Planning,"The ability to generate a diverse and plausible distribution of future
trajectories is a critical capability for autonomous vehicle planning systems.
While recent generative models have shown promise, achieving high fidelity,
computational efficiency, and precise control remains a significant challenge.
In this paper, we present the Efficient Virtuoso, a conditional latent
diffusion model for goal-conditioned trajectory planning. Our approach
introduces a novel two-stage normalization pipeline that first scales
trajectories to preserve their geometric aspect ratio and then normalizes the
resulting PCA latent space to ensure a stable training target. The denoising
process is performed efficiently in this low-dimensional latent space by a
simple MLP denoiser, which is conditioned on a rich scene context fused by a
powerful Transformer-based StateEncoder. We demonstrate that our method
achieves state-of-the-art performance on the Waymo Open Motion Dataset,
achieving a minimum Average Displacement Error (minADE) of 0.25. Furthermore,
through a rigorous ablation study on goal representation, we provide a key
insight: while a single endpoint goal can resolve strategic ambiguity, a
richer, multi-step sparse route is essential for enabling the precise,
high-fidelity tactical execution that mirrors nuanced human driving behavior.",Antonio Guillen-Perez,2025-09-03 19:18:02+00:00,http://arxiv.org/abs/2509.03658v2,http://arxiv.org/pdf/2509.03658v2,"cs.RO, cs.AI, cs.LG"
Cooperative Grasping for Collective Object Transport in Constrained Environments,"We propose a novel framework for decision-making in cooperative grasping for
two-robot object transport in constrained environments. The core of the
framework is a Conditional Embedding (CE) model consisting of two neural
networks that map grasp configuration information into an embedding space. The
resulting embedding vectors are then used to identify feasible grasp
configurations that allow two robots to collaboratively transport an object. To
ensure generalizability across diverse environments and object geometries, the
neural networks are trained on a dataset comprising a range of environment maps
and object shapes. We employ a supervised learning approach with negative
sampling to ensure that the learned embeddings effectively distinguish between
feasible and infeasible grasp configurations. Evaluation results across a wide
range of environments and objects in simulations demonstrate the model's
ability to reliably identify feasible grasp configurations. We further validate
the framework through experiments on a physical robotic platform, confirming
its practical applicability.","David Alvear, George Turkiyyah, Shinkyu Park",2025-09-03 18:44:42+00:00,http://arxiv.org/abs/2509.03638v1,http://arxiv.org/pdf/2509.03638v1,"cs.RO, cs.MA"
Can the Waymo Open Motion Dataset Support Realistic Behavioral Modeling? A Validation Study with Naturalistic Trajectories,"The Waymo Open Motion Dataset (WOMD) has become a popular resource for
data-driven modeling of autonomous vehicles (AVs) behavior. However, its
validity for behavioral analysis remains uncertain due to proprietary
post-processing, the absence of error quantification, and the segmentation of
trajectories into 20-second clips. This study examines whether WOMD accurately
captures the dynamics and interactions observed in real-world AV operations.
Leveraging an independently collected naturalistic dataset from Level 4 AV
operations in Phoenix, Arizona (PHX), we perform comparative analyses across
three representative urban driving scenarios: discharging at signalized
intersections, car-following, and lane-changing behaviors. For the discharging
analysis, headways are manually extracted from aerial video to ensure
negligible measurement error. For the car-following and lane-changing cases, we
apply the Simulation-Extrapolation (SIMEX) method to account for empirically
estimated error in the PHX data and use Dynamic Time Warping (DTW) distances to
quantify behavioral differences. Results across all scenarios consistently show
that behavior in PHX falls outside the behavioral envelope of WOMD. Notably,
WOMD underrepresents short headways and abrupt decelerations. These findings
suggest that behavioral models calibrated solely on WOMD may systematically
underestimate the variability, risk, and complexity of naturalistic driving.
Caution is therefore warranted when using WOMD for behavior modeling without
proper validation against independently collected data.","Yanlin Zhang, Sungyong Chung, Nachuan Li, Dana Monzer, Hani S. Mahmassani, Samer H. Hamdar, Alireza Talebpour",2025-09-03 17:56:46+00:00,http://arxiv.org/abs/2509.03515v1,http://arxiv.org/pdf/2509.03515v1,"cs.RO, cs.AI, cs.LG, cs.SY, eess.SY, stat.AP"
Real-Time Instrument Planning and Perception for Novel Measurements of Dynamic Phenomena,"Advancements in onboard computing mean remote sensing agents can employ
state-of-the-art computer vision and machine learning at the edge. These
capabilities can be leveraged to unlock new rare, transient, and pinpoint
measurements of dynamic science phenomena. In this paper, we present an
automated workflow that synthesizes the detection of these dynamic events in
look-ahead satellite imagery with autonomous trajectory planning for a
follow-up high-resolution sensor to obtain pinpoint measurements. We apply this
workflow to the use case of observing volcanic plumes. We analyze
classification approaches including traditional machine learning algorithms and
convolutional neural networks. We present several trajectory planning
algorithms that track the morphological features of a plume and integrate these
algorithms with the classifiers. We show through simulation an order of
magnitude increase in the utility return of the high-resolution instrument
compared to baselines while maintaining efficient runtimes.","Itai Zilberstein, Alberto Candela, Steve Chien",2025-09-03 17:32:15+00:00,http://arxiv.org/abs/2509.03500v1,http://arxiv.org/pdf/2509.03500v1,"cs.RO, cs.AI"
sam-llm: interpretable lane change trajectoryprediction via parametric finetuning,"This work introduces SAM-LLM, a novel hybrid architecture that bridges the
gap between the contextual reasoning of Large Language Models (LLMs) and the
physical precision of kinematic lane change models for autonomous driving. The
system is designed for interpretable lane change trajectory prediction by
finetuning an LLM to output the core physical parameters of a trajectory model
instead of raw coordinates. For lane-keeping scenarios, the model predicts
discrete coordinates, but for lane change maneuvers, it generates the
parameters for an enhanced Sinusoidal Acceleration Model (SAM), including
lateral displacement, maneuver duration, initial lateral velocity, and
longitudinal velocity change. This parametric approach yields a complete,
continuous, and physically plausible trajectory model that is inherently
interpretable and computationally efficient, achieving an 80% reduction in
output size compared to coordinate-based methods. The SAM-LLM achieves a
state-of-the-art overall intention prediction accuracy of 98.73%, demonstrating
performance equivalent to traditional LLM predictors while offering significant
advantages in explainability and resource efficiency.","Zhuo Cao, Yunxiao Shi, Min Xu",2025-09-03 16:37:49+00:00,http://arxiv.org/abs/2509.03462v1,http://arxiv.org/pdf/2509.03462v1,"cs.AI, cs.CV, cs.RO"
SmartPoser: Arm Pose Estimation with a Smartphone and Smartwatch Using UWB and IMU Data,"The ability to track a user's arm pose could be valuable in a wide range of
applications, including fitness, rehabilitation, augmented reality input, life
logging, and context-aware assistants. Unfortunately, this capability is not
readily available to consumers. Systems either require cameras, which carry
privacy issues, or utilize multiple worn IMUs or markers. In this work, we
describe how an off-the-shelf smartphone and smartwatch can work together to
accurately estimate arm pose. Moving beyond prior work, we take advantage of
more recent ultra-wideband (UWB) functionality on these devices to capture
absolute distance between the two devices. This measurement is the perfect
complement to inertial data, which is relative and suffers from drift. We
quantify the performance of our software-only approach using off-the-shelf
devices, showing it can estimate the wrist and elbow joints with a \hl{median
positional error of 11.0~cm}, without the user having to provide training data.","Nathan DeVrio, Vimal Mollyn, Chris Harrison",2025-09-03 16:16:55+00:00,http://arxiv.org/abs/2509.03451v1,http://arxiv.org/pdf/2509.03451v1,"cs.HC, cs.CV, cs.GR, cs.RO"
Cost-Optimized Systems Engineering for IoT-Enabled Robot Nurse in Infectious Pandemic Management,"The utilization of robotic technology has gained traction in healthcare
facilities due to progress in the field that enables time and cost savings,
minimizes waste, and improves patient care. Digital healthcare technologies
that leverage automation, such as robotics and artificial intelligence, have
the potential to enhance the sustainability and profitability of healthcare
systems in the long run. However, the recent COVID-19 pandemic has amplified
the need for cyber-physical robots to automate check-ups and medication
administration. A robot nurse is controlled by the Internet of Things (IoT) and
can serve as an automated medical assistant while also allowing supervisory
control based on custom commands. This system helps reduce infection risk and
improves outcomes in pandemic settings. This research presents a test case with
a nurse robot that can assess a patient's health status and take action
accordingly. We also evaluate the system's performance in medication
administration, health-status monitoring, and life-cycle considerations.","Md Mhamud Hussen Sifat, Md Maruf, Md Rokunuzzaman",2025-09-03 16:06:39+00:00,http://arxiv.org/abs/2509.03436v1,http://arxiv.org/pdf/2509.03436v1,"cs.RO, cs.HC, cs.SY, eess.SY, I.2.9; C.3; J.3"
EclipseTouch: Touch Segmentation on Ad Hoc Surfaces using Worn Infrared Shadow Casting,"The ability to detect touch events on uninstrumented, everyday surfaces has
been a long-standing goal for mixed reality systems. Prior work has shown that
virtual interfaces bound to physical surfaces offer performance and ergonomic
benefits over tapping at interfaces floating in the air. A wide variety of
approaches have been previously developed, to which we contribute a new
headset-integrated technique called \systemname. We use a combination of a
computer-triggered camera and one or more infrared emitters to create
structured shadows, from which we can accurately estimate hover distance (mean
error of 6.9~mm) and touch contact (98.0\% accuracy). We discuss how our
technique works across a range of conditions, including surface material,
interaction orientation, and environmental lighting.","Vimal Mollyn, Nathan DeVrio, Chris Harrison",2025-09-03 15:59:28+00:00,http://arxiv.org/abs/2509.03430v1,http://arxiv.org/pdf/2509.03430v1,"cs.HC, cs.CV, cs.GR, cs.RO"
Self-Organizing Aerial Swarm Robotics for Resilient Load Transportation : A Table-Mechanics-Inspired Approach,"In comparison with existing approaches, which struggle with scalability,
communication dependency, and robustness against dynamic failures, cooperative
aerial transportation via robot swarms holds transformative potential for
logistics and disaster response. Here, we present a physics-inspired
cooperative transportation approach for flying robot swarms that imitates the
dissipative mechanics of table-leg load distribution. By developing a
decentralized dissipative force model, our approach enables autonomous
formation stabilization and adaptive load allocation without the requirement of
explicit communication. Based on local neighbor robots and the suspended
payload, each robot dynamically adjusts its position. This is similar to
energy-dissipating table leg reactions. The stability of the resultant control
system is rigorously proved. Simulations demonstrate that the tracking errors
of the proposed approach are 20%, 68%, 55.5%, and 21.9% of existing approaches
under the cases of capability variation, cable uncertainty, limited vision, and
payload variation, respectively. In real-world experiments with six flying
robots, the cooperative aerial transportation system achieved a 94% success
rate under single-robot failure, disconnection events, 25% payload variation,
and 40% cable length uncertainty, demonstrating strong robustness under outdoor
winds up to Beaufort scale 4. Overall, this physics-inspired approach bridges
swarm intelligence and mechanical stability principles, offering a scalable
framework for heterogeneous aerial systems to collectively handle complex
transportation tasks in communication-constrained environments.","Quan Quan, Jiwen Xu, Runxiao Liu, Yi Ding, Jiaxing Che, Kai-Yuan Cai",2025-09-03 15:11:41+00:00,http://arxiv.org/abs/2509.03563v1,http://arxiv.org/pdf/2509.03563v1,cs.RO
ANNIE: Be Careful of Your Robots,"The integration of vision-language-action (VLA) models into embodied AI (EAI)
robots is rapidly advancing their ability to perform complex, long-horizon
tasks in humancentric environments. However, EAI systems introduce critical
security risks: a compromised VLA model can directly translate adversarial
perturbations on sensory input into unsafe physical actions. Traditional safety
definitions and methodologies from the machine learning community are no longer
sufficient. EAI systems raise new questions, such as what constitutes safety,
how to measure it, and how to design effective attack and defense mechanisms in
physically grounded, interactive settings. In this work, we present the first
systematic study of adversarial safety attacks on embodied AI systems, grounded
in ISO standards for human-robot interactions. We (1) formalize a principled
taxonomy of safety violations (critical, dangerous, risky) based on physical
constraints such as separation distance, velocity, and collision boundaries;
(2) introduce ANNIEBench, a benchmark of nine safety-critical scenarios with
2,400 video-action sequences for evaluating embodied safety; and (3)
ANNIE-Attack, a task-aware adversarial framework with an attack leader model
that decomposes long-horizon goals into frame-level perturbations. Our
evaluation across representative EAI models shows attack success rates
exceeding 50% across all safety categories. We further demonstrate sparse and
adaptive attack strategies and validate the real-world impact through physical
robot experiments. These results expose a previously underexplored but highly
consequential attack surface in embodied AI systems, highlighting the urgent
need for security-driven defenses in the physical AI era. Code is available at
https://github.com/RLCLab/Annie.","Yiyang Huang, Zixuan Wang, Zishen Wan, Yapeng Tian, Haobo Xu, Yinhe Han, Yiming Gan",2025-09-03 15:00:28+00:00,http://arxiv.org/abs/2509.03383v1,http://arxiv.org/pdf/2509.03383v1,"cs.AI, cs.RO"
Dependency Chain Analysis of ROS 2 DDS QoS Policies: From Lifecycle Tutorial to Static Verification,"Robot Operating System 2 (ROS 2) relies on the Data Distribution Service
(DDS), which offers more than 20 Quality of Service (QoS) policies governing
availability, reliability, and resource usage. Yet ROS 2 users lack clear
guidance on safe policy combinations and validation processes prior to
deployment, which often leads to trial-and-error tuning and unexpected runtime
failures. To address these challenges, we analyze DDS Publisher-Subscriber
communication over a life cycle divided into Discovery, Data Exchange, and
Disassociation, and provide a user oriented tutorial explaining how 16 QoS
policies operate in each phase. Building on this analysis, we derive a QoS
dependency chain that formalizes inter-policy relationships and classifies 41
dependency violation rules, capturing constraints that commonly cause
communication failures in practice. Finally, we introduce QoS Guard, a ROS 2
package that statically validates DDS XML profiles offline, flags conflicts,
and enables safe, predeployment tuning without establishing a live ROS 2
session. Together, these contributions give ROS 2 users both conceptual insight
and a concrete tool that enables early detection of misconfigurations,
improving the reliability and resource efficiency of ROS 2 based robotic
systems.","Sanghoon Lee, Junha Kang, Kyung-Joon Park",2025-09-03 14:57:21+00:00,http://arxiv.org/abs/2509.03381v1,http://arxiv.org/pdf/2509.03381v1,"cs.NI, cs.RO"
AI Safety Assurance in Electric Vehicles: A Case Study on AI-Driven SOC Estimation,"Integrating Artificial Intelligence (AI) technology in electric vehicles (EV)
introduces unique challenges for safety assurance, particularly within the
framework of ISO 26262, which governs functional safety in the automotive
domain. Traditional assessment methodologies are not geared toward evaluating
AI-based functions and require evolving standards and practices. This paper
explores how an independent assessment of an AI component in an EV can be
achieved when combining ISO 26262 with the recently released ISO/PAS 8800,
whose scope is AI safety for road vehicles. The AI-driven State of Charge (SOC)
battery estimation exemplifies the process. Key features relevant to the
independent assessment of this extended evaluation approach are identified. As
part of the evaluation, robustness testing of the AI component is conducted
using fault injection experiments, wherein perturbed sensor inputs are
systematically introduced to assess the component's resilience to input
variance.","Martin Skoglund, Fredrik Warg, Aria Mirzai, Anders Thorsen, Karl Lundgren, Peter Folkesson, Bastian Havers-zulka",2025-09-03 12:41:25+00:00,http://arxiv.org/abs/2509.03270v1,http://arxiv.org/pdf/2509.03270v1,"cs.SE, cs.RO"
Parallel-Constraint Model Predictive Control: Exploiting Parallel Computation for Improving Safety,"Ensuring constraint satisfaction is a key requirement for safety-critical
systems, which include most robotic platforms. For example, constraints can be
used for modeling joint position/velocity/torque limits and collision
avoidance. Constrained systems are often controlled using Model Predictive
Control, because of its ability to naturally handle constraints, relying on
numerical optimization. However, ensuring constraint satisfaction is
challenging for nonlinear systems/constraints. A well-known tool to make
controllers safe is the so-called control-invariant set (a.k.a. safe set). In
our previous work, we have shown that safety can be improved by letting the
safe-set constraint recede along the MPC horizon. In this paper, we push that
idea further by exploiting parallel computation to improve safety. We solve
several MPC problems at the same time, where each problem instantiates the
safe-set constraint at a different time step along the horizon. Finally, the
controller can select the best solution according to some user-defined
criteria. We validated this idea through extensive simulations with a 3-joint
robotic arm, showing that significant improvements can be achieved in terms of
safety and performance, even using as little as 4 computational cores.","Elias Fontanari, Gianni Lunardi, Matteo Saveriano, Andrea Del Prete",2025-09-03 12:23:46+00:00,http://arxiv.org/abs/2509.03261v1,http://arxiv.org/pdf/2509.03261v1,"cs.RO, cs.SY, eess.SY"
Spiking Neural Networks for Continuous Control via End-to-End Model-Based Learning,"Despite recent progress in training spiking neural networks (SNNs) for
classification, their application to continuous motor control remains limited.
Here, we demonstrate that fully spiking architectures can be trained end-to-end
to control robotic arms with multiple degrees of freedom in continuous
environments. Our predictive-control framework combines Leaky
Integrate-and-Fire dynamics with surrogate gradients, jointly optimizing a
forward model for dynamics prediction and a policy network for goal-directed
action. We evaluate this approach on both a planar 2D reaching task and a
simulated 6-DOF Franka Emika Panda robot. Results show that SNNs can achieve
stable training and accurate torque control, establishing their viability for
high-dimensional motor tasks. An extensive ablation study highlights the role
of initialization, learnable time constants, and regularization in shaping
training dynamics. We conclude that while stable and effective control can be
achieved, recurrent spiking networks remain highly sensitive to hyperparameter
settings, underscoring the importance of principled design choices.","Justus Huebotter, Pablo Lanillos, Marcel van Gerven, Serge Thill",2025-09-03 12:05:58+00:00,http://arxiv.org/abs/2509.05356v1,http://arxiv.org/pdf/2509.05356v1,"cs.RO, cs.AI, cs.LG"
Vibration Damping in Underactuated Cable-suspended Artwork -- Flying Belt Motion Control,"This paper presents a comprehensive refurbishment of the interactive robotic
art installation Standards and Double Standards by Rafael Lozano-Hemmer. The
installation features an array of belts suspended from the ceiling, each
actuated by stepper motors and dynamically oriented by a vision-based tracking
system that follows the movements of exhibition visitors. The original system
was limited by oscillatory dynamics, resulting in torsional and pendulum-like
vibrations that constrained rotational speed and reduced interactive
responsiveness. To address these challenges, the refurbishment involved
significant upgrades to both hardware and motion control algorithms. A detailed
mathematical model of the flying belt system was developed to accurately
capture its dynamic behavior, providing a foundation for advanced control
design. An input shaping method, formulated as a convex optimization problem,
was implemented to effectively suppress vibrations, enabling smoother and
faster belt movements. Experimental results demonstrate substantial
improvements in system performance and audience interaction. This work
exemplifies the integration of robotics, control engineering, and interactive
art, offering new solutions to technical challenges in real-time motion control
and vibration damping for large-scale kinetic installations.","Martin Goubej, Lauria Clarke, Martin Hrabačka, David Tolar",2025-09-03 11:52:06+00:00,http://arxiv.org/abs/2509.03238v1,http://arxiv.org/pdf/2509.03238v1,"cs.RO, cs.SY, eess.SY"
Human-LLM Synergy in Context-Aware Adaptive Architecture for Scalable Drone Swarm Operation,"The deployment of autonomous drone swarms in disaster response missions
necessitates the development of flexible, scalable, and robust coordination
systems. Traditional fixed architectures struggle to cope with dynamic and
unpredictable environments, leading to inefficiencies in energy consumption and
connectivity. This paper addresses this gap by proposing an adaptive
architecture for drone swarms, leveraging a Large Language Model to dynamically
select the optimal architecture as centralized, hierarchical, or holonic based
on real time mission parameters such as task complexity, swarm size, and
communication stability. Our system addresses the challenges of scalability,
adaptability, and robustness,ensuring efficient energy consumption and
maintaining connectivity under varying conditions. Extensive simulations
demonstrate that our adaptive architecture outperforms traditional static
models in terms of scalability, energy efficiency, and connectivity. These
results highlight the potential of our approach to provide a scalable,
adaptable, and resilient solution for real world disaster response scenarios.","Ahmed R. Sadik, Muhammad Ashfaq, Niko Mäkitalo, Tommi Mikkonen",2025-09-03 11:46:19+00:00,http://arxiv.org/abs/2509.05355v1,http://arxiv.org/pdf/2509.05355v1,"cs.RO, cs.MA"
Exploring persuasive interactions with generative social robots: An experimental framework,"Integrating generative AI such as Large Language Models into social robots
has improved their ability to engage in natural, human-like communication. This
study presents a method to examine their persuasive capabilities. We designed
an experimental framework focused on decision making and tested it in a pilot
that varied robot appearance and self-knowledge. Using qualitative analysis, we
evaluated interaction quality, persuasion effectiveness, and the robot's
communicative strategies. Participants generally experienced the interaction
positively, describing the robot as competent, friendly, and supportive, while
noting practical limits such as delayed responses and occasional
speech-recognition errors. Persuasiveness was highly context dependent and
shaped by robot behavior: Participants responded well to polite, reasoned
suggestions and expressive gestures, but emphasized the need for more
personalized, context-aware arguments and clearer social roles. These findings
suggest that generative social robots can influence user decisions, but their
effectiveness depends on communicative nuance and contextual relevance. We
propose refinements to the framework to further study persuasive dynamics
between robots and human users.","Stephan Vonschallen, Larissa Julia Corina Finsler, Theresa Schmiedel, Friederike Eyssel",2025-09-03 11:42:24+00:00,http://arxiv.org/abs/2509.03231v2,http://arxiv.org/pdf/2509.03231v2,cs.RO
The Role of Embodiment in Intuitive Whole-Body Teleoperation for Mobile Manipulation,"Intuitive Teleoperation interfaces are essential for mobile manipulation
robots to ensure high quality data collection while reducing operator workload.
A strong sense of embodiment combined with minimal physical and cognitive
demands not only enhances the user experience during large-scale data
collection, but also helps maintain data quality over extended periods. This
becomes especially crucial for challenging long-horizon mobile manipulation
tasks that require whole-body coordination. We compare two distinct robot
control paradigms: a coupled embodiment integrating arm manipulation and base
navigation functions, and a decoupled embodiment treating these systems as
separate control entities. Additionally, we evaluate two visual feedback
mechanisms: immersive virtual reality and conventional screen-based
visualization of the robot's field of view. These configurations were
systematically assessed across a complex, multi-stage task sequence requiring
integrated planning and execution. Our results show that the use of VR as a
feedback modality increases task completion time, cognitive workload, and
perceived effort of the teleoperator. Coupling manipulation and navigation
leads to a comparable workload on the user as decoupling the embodiments, while
preliminary experiments suggest that data acquired by coupled teleoperation
leads to better imitation learning performance. Our holistic view on intuitive
teleoperation interfaces provides valuable insight into collecting
high-quality, high-dimensional mobile manipulation data at scale with the human
operator in mind. Project
website:https://sophiamoyen.github.io/role-embodiment-wbc-moma-teleop/","Sophia Bianchi Moyen, Rickmer Krohn, Sophie Lueth, Kay Pompetzki, Jan Peters, Vignesh Prasad, Georgia Chalvatzaki",2025-09-03 11:25:36+00:00,http://arxiv.org/abs/2509.03222v1,http://arxiv.org/pdf/2509.03222v1,"cs.RO, cs.HC, cs.LG"
Efficient Active Training for Deep LiDAR Odometry,"Robust and efficient deep LiDAR odometry models are crucial for accurate
localization and 3D reconstruction, but typically require extensive and diverse
training data to adapt to diverse environments, leading to inefficiencies. To
tackle this, we introduce an active training framework designed to selectively
extract training data from diverse environments, thereby reducing the training
load and enhancing model generalization. Our framework is based on two key
strategies: Initial Training Set Selection (ITSS) and Active Incremental
Selection (AIS). ITSS begins by breaking down motion sequences from general
weather into nodes and edges for detailed trajectory analysis, prioritizing
diverse sequences to form a rich initial training dataset for training the base
model. For complex sequences that are difficult to analyze, especially under
challenging snowy weather conditions, AIS uses scene reconstruction and
prediction inconsistency to iteratively select training samples, refining the
model to handle a wide range of real-world scenarios. Experiments across
datasets and weather conditions validate our approach's effectiveness. Notably,
our method matches the performance of full-dataset training with just 52\% of
the sequence volume, demonstrating the training efficiency and robustness of
our active training paradigm. By optimizing the training process, our approach
sets the stage for more agile and reliable LiDAR odometry systems, capable of
navigating diverse environmental conditions with greater precision.","Beibei Zhou, Zhiyuan Zhang, Zhenbo Song, Jianhui Guo, Hui Kong",2025-09-03 11:00:17+00:00,http://arxiv.org/abs/2509.03211v1,http://arxiv.org/pdf/2509.03211v1,"cs.RO, cs.CV"
Decentralised self-organisation of pivoting cube ensembles using geometric deep learning,"We present a decentralized model for autonomous reconfiguration of
homogeneous pivoting cube modular robots in two dimensions. Each cube in the
ensemble is controlled by a neural network that only gains information from
other cubes in its local neighborhood, trained using reinforcement learning.
Furthermore, using geometric deep learning, we include the grid symmetries of
the cube ensemble in the neural network architecture. We find that even the
most localized versions succeed in reconfiguring to the target shape, although
reconfiguration happens faster the more information about the whole ensemble is
available to individual cubes. Near-optimal reconfiguration is achieved with
only nearest neighbor interactions by using multiple information passing
between cubes, allowing them to accumulate more global information about the
ensemble. Compared to standard neural network architectures, using geometric
deep learning approaches provided only minor benefits. Overall, we successfully
demonstrate mostly local control of a modular self-assembling system, which is
transferable to other space-relevant systems with different action spaces, such
as sliding cube modular robots and CubeSat swarms.","Nadezhda Dobreva, Emmanuel Blazquez, Jai Grover, Dario Izzo, Yuzhen Qin, Dominik Dold",2025-09-03 08:50:41+00:00,http://arxiv.org/abs/2509.03140v1,http://arxiv.org/pdf/2509.03140v1,"cs.NE, cs.AI, cs.RO"
Forbal: Force Balanced 2-5 Degree of Freedom Robot Manipulator Built from a Five Bar Linkage,"A force balanced manipulator design based on the closed chain planar five bar
linkage is developed and experimentally validated. We present 2 variants as a
modular design: Forbal-2, a planar 2-DOF manipulator, and its extension to
5-DOF spatial motion called Forbal-5. The design considerations in terms of
geometric, kinematic, and dynamic design that fulfill the force balance
conditions while maximizing workspace are discussed. Then, the inverse
kinematics of both variants are derived from geometric principles. We validate
the improvements from force balancing the manipulator through comparative
experiments with counter mass balanced and unbalanced configurations. The
results show how the balanced configuration yields a reduction in the average
reaction moments of up to 66%, a reduction of average joint torques of up to
79%, as well as a noticeable reduction in position error for Forbal-2. For
Forbal-5, which has a higher end effector payload mass, the joint torques are
reduced up to 84% for the balanced configuration. Experimental results validate
that the balanced manipulator design is suitable for applications where the
reduction of joint torques and reaction forces/moments helps achieve millimeter
level precision.","Yash Vyas, Matteo Bottin",2025-09-03 08:20:55+00:00,http://arxiv.org/abs/2509.03119v2,http://arxiv.org/pdf/2509.03119v2,"cs.RO, cs.SY, eess.SY"
Population-aware Online Mirror Descent for Mean-Field Games with Common Noise by Deep Reinforcement Learning,"Mean Field Games (MFGs) offer a powerful framework for studying large-scale
multi-agent systems. Yet, learning Nash equilibria in MFGs remains a
challenging problem, particularly when the initial distribution is unknown or
when the population is subject to common noise. In this paper, we introduce an
efficient deep reinforcement learning (DRL) algorithm designed to achieve
population-dependent Nash equilibria without relying on averaging or historical
sampling, inspired by Munchausen RL and Online Mirror Descent. The resulting
policy is adaptable to various initial distributions and sources of common
noise. Through numerical experiments on seven canonical examples, we
demonstrate that our algorithm exhibits superior convergence properties
compared to state-of-the-art algorithms, particularly a DRL version of
Fictitious Play for population-dependent policies. The performance in the
presence of common noise underscores the robustness and adaptability of our
approach.","Zida Wu, Mathieu Lauriere, Matthieu Geist, Olivier Pietquin, Ankur Mehta",2025-09-03 05:33:46+00:00,http://arxiv.org/abs/2509.03030v1,http://arxiv.org/pdf/2509.03030v1,"cs.LG, cs.MA, cs.RO, cs.SY, eess.SY"
Uncertainty-aware Test-Time Training (UT$^3$) for Efficient On-the-fly Domain Adaptive Dense Regression,"Deep neural networks (DNNs) are increasingly being used in autonomous
systems. However, DNNs do not generalize well to domain shift. Adapting to a
continuously evolving environment is a safety-critical challenge inevitably
faced by all autonomous systems deployed to the real world. Recent work on
test-time training proposes methods that adapt to a new test distribution on
the fly by optimizing the DNN model for each test input using self-supervision.
However, these techniques result in a sharp increase in inference time as
multiple forward and backward passes are required for a single test sample (for
test-time training) before finally making the prediction based on the
fine-tuned features. This is undesirable for real-world robotics applications
where these models may be deployed to resource constraint hardware with strong
latency requirements. In this work, we propose a new framework (called UT$^3$)
that leverages test-time training for improved performance in the presence of
continuous domain shift while also decreasing the inference time, making it
suitable for real-world applications. Our method proposes an uncertainty-aware
self-supervision task for efficient test-time training that leverages the
quantified uncertainty to selectively apply the training leading to sharp
improvements in the inference time while performing comparably to standard
test-time training protocol. Our proposed protocol offers a continuous setting
to identify the selected keyframes, allowing the end-user to control how often
to apply test-time training. We demonstrate the efficacy of our method on a
dense regression task - monocular depth estimation.",Uddeshya Upadhyay,2025-09-03 04:41:43+00:00,http://arxiv.org/abs/2509.03012v1,http://arxiv.org/pdf/2509.03012v1,"cs.RO, cs.CV"
CTBC: Contact-Triggered Blind Climbing for Wheeled Bipedal Robots with Instruction Learning and Reinforcement Learning,"In recent years, wheeled bipedal robots have gained increasing attention due
to their advantages in mobility, such as high-speed locomotion on flat terrain.
However, their performance on complex environments (e.g., staircases) remains
inferior to that of traditional legged robots. To overcome this limitation, we
propose a general contact-triggered blind climbing (CTBC) framework for wheeled
bipedal robots. Upon detecting wheel-obstacle contact, the robot triggers a
leg-lifting motion to overcome the obstacle. By leveraging a strongly-guided
feedforward trajectory, our method enables the robot to rapidly acquire agile
leg-lifting skills, significantly enhancing its capability to traverse
unstructured terrains. The approach has been experimentally validated and
successfully deployed on LimX Dynamics' wheeled bipedal robot, Tron1.
Real-world tests demonstrate that Tron1 can reliably climb obstacles well
beyond its wheel radius using only proprioceptive feedback.","Rankun Li, Hao Wang, Qi Li, Zhuo Han, Yifei Chu, Linqi Ye, Wende Xie, Wenlong Liao",2025-09-03 03:46:43+00:00,http://arxiv.org/abs/2509.02986v1,http://arxiv.org/pdf/2509.02986v1,cs.RO
DUViN: Diffusion-Based Underwater Visual Navigation via Knowledge-Transferred Depth Features,"Autonomous underwater navigation remains a challenging problem due to limited
sensing capabilities and the difficulty of constructing accurate maps in
underwater environments. In this paper, we propose a Diffusion-based Underwater
Visual Navigation policy via knowledge-transferred depth features, named DUViN,
which enables vision-based end-to-end 4-DoF motion control for underwater
vehicles in unknown environments. DUViN guides the vehicle to avoid obstacles
and maintain a safe and perception awareness altitude relative to the terrain
without relying on pre-built maps. To address the difficulty of collecting
large-scale underwater navigation datasets, we propose a method that ensures
robust generalization under domain shifts from in-air to underwater
environments by leveraging depth features and introducing a novel model
transfer strategy. Specifically, our training framework consists of two phases:
we first train the diffusion-based visual navigation policy on in-air datasets
using a pre-trained depth feature extractor. Secondly, we retrain the extractor
on an underwater depth estimation task and integrate the adapted extractor into
the trained navigation policy from the first step. Experiments in both
simulated and real-world underwater environments demonstrate the effectiveness
and generalization of our approach. The experimental videos are available at
https://www.youtube.com/playlist?list=PLqt2s-RyCf1gfXJgFzKjmwIqYhrP4I-7Y.","Jinghe Yang, Minh-Quan Le, Mingming Gong, Ye Pu",2025-09-03 03:43:12+00:00,http://arxiv.org/abs/2509.02983v1,http://arxiv.org/pdf/2509.02983v1,"cs.RO, cs.CV"
IL-SLAM: Intelligent Line-assisted SLAM Based on Feature Awareness for Dynamic Environments,"Visual Simultaneous Localization and Mapping (SLAM) plays a crucial role in
autonomous systems. Traditional SLAM methods, based on static environment
assumptions, struggle to handle complex dynamic environments. Recent dynamic
SLAM systems employ geometric constraints and deep learning to remove dynamic
features, yet this creates a new challenge: insufficient remaining point
features for subsequent SLAM processes. Existing solutions address this by
continuously introducing additional line and plane features to supplement point
features, achieving robust tracking and pose estimation. However, current
methods continuously introduce additional features regardless of necessity,
causing two problems: unnecessary computational overhead and potential
performance degradation from accumulated low-quality additional features and
noise. To address these issues, this paper proposes a feature-aware mechanism
that evaluates whether current features are adequate to determine if line
feature support should be activated. This decision mechanism enables the system
to introduce line features only when necessary, significantly reducing
computational complexity of additional features while minimizing the
introduction of low-quality features and noise. In subsequent processing, the
introduced line features assist in obtaining better initial camera poses
through tracking, local mapping, and loop closure, but are excluded from global
optimization to avoid potential negative impacts from low-quality additional
features in long-term process. Extensive experiments on TUM datasets
demonstrate substantial improvements in both ATE and RPE metrics compared to
ORB-SLAM3 baseline and superior performance over other dynamic SLAM and
multi-feature methods.","Haolan Zhang, Thanh Nguyen Canh, Chenghao Li, Ruidong Yang, Yonghoon Ji, Nak Young Chong",2025-09-03 03:19:09+00:00,http://arxiv.org/abs/2509.02972v1,http://arxiv.org/pdf/2509.02972v1,cs.RO
VendiRL: A Framework for Self-Supervised Reinforcement Learning of Diversely Diverse Skills,"In self-supervised reinforcement learning (RL), one of the key challenges is
learning a diverse set of skills to prepare agents for unknown future tasks.
Despite impressive advances, scalability and evaluation remain prevalent
issues. Regarding scalability, the search for meaningful skills can be obscured
by high-dimensional feature spaces, where relevant features may vary across
downstream task domains. For evaluating skill diversity, defining what
constitutes ""diversity"" typically requires a hard commitment to a specific
notion of what it means for skills to be diverse, potentially leading to
inconsistencies in how skill diversity is understood, making results across
different approaches hard to compare, and leaving many forms of diversity
unexplored. To address these issues, we adopt a measure of sample diversity
that translates ideas from ecology to machine learning -- the Vendi Score --
allowing the user to specify and evaluate any desired form of diversity. We
demonstrate how this metric facilitates skill evaluation and introduce VendiRL,
a unified framework for learning diversely diverse sets of skills. Given
distinct similarity functions, VendiRL motivates distinct forms of diversity,
which could support skill-diversity pretraining in new and richly interactive
environments where optimising for various forms of diversity may be desirable.",Erik M. Lintunen,2025-09-03 01:53:29+00:00,http://arxiv.org/abs/2509.02930v1,http://arxiv.org/pdf/2509.02930v1,"cs.LG, cs.AI, cs.RO"
Approximate constrained stochastic optimal control via parameterized input inference,"Approximate methods to solve stochastic optimal control (SOC) problems have
received significant interest from researchers in the past decade.
Probabilistic inference approaches to SOC have been developed to solve
nonlinear quadratic Gaussian problems. In this work, we propose an
Expectation-Maximization (EM) based inference procedure to generate
state-feedback controls for constrained SOC problems. We consider the
inequality constraints for the state and controls and also the structural
constraints for the controls. We employ barrier functions to address state and
control constraints. We show that the expectation step leads to smoothing of
the state-control pair while the the maximization step on the non-zero subsets
of the control parameters allows inference of structured stochastic optimal
controllers. We demonstrate the effectiveness of the algorithm on unicycle
obstacle avoidance, four-unicycle formation control, and quadcopter navigation
in windy environment examples. In these examples, we perform an empirical study
on the parametric effect of barrier functions on the state constraint
satisfaction. We also present a comparative study of smoothing algorithms on
the performance of the proposed approach.","Shahbaz P Qadri Syed, He Bai",2025-09-03 01:19:40+00:00,http://arxiv.org/abs/2509.02922v1,http://arxiv.org/pdf/2509.02922v1,"eess.SY, cs.MA, cs.RO, cs.SY, math.OC"
"Generalizable Skill Learning for Construction Robots with Crowdsourced Natural Language Instructions, Composable Skills Standardization, and Large Language Model","The quasi-repetitive nature of construction work and the resulting lack of
generalizability in programming construction robots presents persistent
challenges to the broad adoption of robots in the construction industry. Robots
cannot achieve generalist capabilities as skills learnt from one domain cannot
readily transfer to another work domain or be directly used to perform a
different set of tasks. Human workers have to arduously reprogram their
scene-understanding, path-planning, and manipulation components to enable the
robots to perform alternate work tasks. The methods presented in this paper
resolve a significant proportion of such reprogramming workload by proposing a
generalizable learning architecture that directly teaches robots versatile
task-performance skills through crowdsourced online natural language
instructions. A Large Language Model (LLM), a standardized and modularized
hierarchical modeling approach, and Building Information Modeling-Robot sematic
data pipeline are developed to address the multi-task skill transfer problem.
The proposed skill standardization scheme and LLM-based hierarchical skill
learning framework were tested with a long-horizon drywall installation
experiment using a full-scale industrial robotic manipulator. The resulting
robot task learning scheme achieves multi-task reprogramming with minimal
effort and high quality.","Hongrui Yu, Vineet R. Kamat, Carol C. Menassa",2025-09-02 22:46:27+00:00,http://arxiv.org/abs/2509.02876v1,http://arxiv.org/pdf/2509.02876v1,cs.RO
Robotic 3D Flower Pose Estimation for Small-Scale Urban Farms,"The small scale of urban farms and the commercial availability of low-cost
robots (such as the FarmBot) that automate simple tending tasks enable an
accessible platform for plant phenotyping. We have used a FarmBot with a custom
camera end-effector to estimate strawberry plant flower pose (for robotic
pollination) from acquired 3D point cloud models. We describe a novel algorithm
that translates individual occupancy grids along orthogonal axes of a point
cloud to obtain 2D images corresponding to the six viewpoints. For each image,
2D object detection models for flowers are used to identify 2D bounding boxes
which can be converted into the 3D space to extract flower point clouds. Pose
estimation is performed by fitting three shapes (superellipsoids, paraboloids
and planes) to the flower point clouds and compared with manually labeled
ground truth. Our method successfully finds approximately 80% of flowers
scanned using our customized FarmBot platform and has a mean flower pose error
of 7.7 degrees, which is sufficient for robotic pollination and rivals previous
results. All code will be made available at
https://github.com/harshmuriki/flowerPose.git.","Harsh Muriki, Hong Ray Teo, Ved Sengupta, Ai-Ping Hu",2025-09-02 22:36:10+00:00,http://arxiv.org/abs/2509.02870v1,http://arxiv.org/pdf/2509.02870v1,cs.RO
Multi-Embodiment Locomotion at Scale with extreme Embodiment Randomization,"We present a single, general locomotion policy trained on a diverse
collection of 50 legged robots. By combining an improved embodiment-aware
architecture (URMAv2) with a performance-based curriculum for extreme
Embodiment Randomization, our policy learns to control millions of
morphological variations. Our policy achieves zero-shot transfer to unseen
real-world humanoid and quadruped robots.","Nico Bohlinger, Jan Peters",2025-09-02 20:32:02+00:00,http://arxiv.org/abs/2509.02815v1,http://arxiv.org/pdf/2509.02815v1,"cs.RO, cs.LG"
Improving the Resilience of Quadrotors in Underground Environments by Combining Learning-based and Safety Controllers,"Autonomously controlling quadrotors in large-scale subterranean environments
is applicable to many areas such as environmental surveying, mining operations,
and search and rescue. Learning-based controllers represent an appealing
approach to autonomy, but are known to not generalize well to
`out-of-distribution' environments not encountered during training. In this
work, we train a normalizing flow-based prior over the environment, which
provides a measure of how far out-of-distribution the quadrotor is at any given
time. We use this measure as a runtime monitor, allowing us to switch between a
learning-based controller and a safe controller when we are sufficiently
out-of-distribution. Our methods are benchmarked on a point-to-point navigation
task in a simulated 3D cave environment based on real-world point cloud data
from the DARPA Subterranean Challenge Final Event Dataset. Our experimental
results show that our combined controller simultaneously possesses the liveness
of the learning-based controller (completing the task quickly) and the safety
of the safety controller (avoiding collision).","Isaac Ronald Ward, Mark Paral, Kristopher Riordan, Mykel J. Kochenderfer",2025-09-02 20:22:54+00:00,http://arxiv.org/abs/2509.02808v1,http://arxiv.org/pdf/2509.02808v1,"cs.RO, cs.AI, cs.SY, eess.SY"
A Digital Twin for Robotic Post Mortem Tissue Sampling using Virtual Reality,"Studying tissue samples obtained during autopsies is the gold standard when
diagnosing the cause of death and for understanding disease pathophysiology.
Recently, the interest in post mortem minimally invasive biopsies has grown
which is a less destructive approach in comparison to an open autopsy and
reduces the risk of infection. While manual biopsies under ultrasound guidance
are more widely performed, robotic post mortem biopsies have been recently
proposed. This approach can further reduce the risk of infection for
physicians. However, planning of the procedure and control of the robot need to
be efficient and usable. We explore a virtual reality setup with a digital twin
to realize fully remote planning and control of robotic post mortem biopsies.
The setup is evaluated with forensic pathologists in a usability study for
three interaction methods. Furthermore, we evaluate clinical feasibility and
evaluate the system with three human cadavers. Overall, 132 needle insertions
were performed with an off-axis needle placement error of 5.30+-3.25 mm. Tissue
samples were successfully biopsied and histopathologically verified. Users
reported a very intuitive needle placement approach, indicating that the system
is a promising, precise, and low-risk alternative to conventional approaches.","Maximilian Neidhardt, Ludwig Bosse, Vidas Raudonis, Kristina Allgoewer, Axel Heinemann, Benjamin Ondruschka, Alexander Schlaefer",2025-09-02 19:06:25+00:00,http://arxiv.org/abs/2509.02760v2,http://arxiv.org/pdf/2509.02760v2,cs.RO
The Impact of Adaptive Emotional Alignment on Mental State Attribution and User Empathy in HRI,"The paper presents an experiment on the effects of adaptive emotional
alignment between agents, considered a prerequisite for empathic communication,
in Human-Robot Interaction (HRI). Using the NAO robot, we investigate the
impact of an emotionally aligned, empathic, dialogue on these aspects: (i) the
robot's persuasive effectiveness, (ii) the user's communication style, and
(iii) the attribution of mental states and empathy to the robot. In an
experiment with 42 participants, two conditions were compared: one with neutral
communication and another where the robot provided responses adapted to the
emotions expressed by the users. The results show that emotional alignment does
not influence users' communication styles or have a persuasive effect. However,
it significantly influences attribution of mental states to the robot and its
perceived empathy","Giorgia Buracchio, Ariele Callegari, Massimo Donini, Cristina Gena, Antonio Lieto, Alberto Lillo, Claudio Mattutino, Alessandro Mazzei, Linda Pigureddu, Manuel Striani, Fabiana Vernero",2025-09-02 18:56:44+00:00,http://arxiv.org/abs/2509.02749v1,http://arxiv.org/pdf/2509.02749v1,cs.RO
Acrobotics: A Generalist Approahc To Quadrupedal Robots' Parkour,"Climbing, crouching, bridging gaps, and walking up stairs are just a few of
the advantages that quadruped robots have over wheeled robots, making them more
suitable for navigating rough and unstructured terrain. However, executing such
manoeuvres requires precise temporal coordination and complex agent-environment
interactions. Moreover, legged locomotion is inherently more prone to slippage
and tripping, and the classical approach of modeling such cases to design a
robust controller thus quickly becomes impractical. In contrast, reinforcement
learning offers a compelling solution by enabling optimal control through trial
and error. We present a generalist reinforcement learning algorithm for
quadrupedal agents in dynamic motion scenarios. The learned policy rivals
state-of-the-art specialist policies trained using a mixture of experts
approach, while using only 25% as many agents during training. Our experiments
also highlight the key components of the generalist locomotion policy and the
primary factors contributing to its success.","Guillaume Gagné-Labelle, Vassil Atanassov, Ioannis Havoutis",2025-09-02 18:23:17+00:00,http://arxiv.org/abs/2509.02727v1,http://arxiv.org/pdf/2509.02727v1,cs.RO
2nd Place Solution for CVPR2024 E2E Challenge: End-to-End Autonomous Driving Using Vision Language Model,"End-to-end autonomous driving has drawn tremendous attention recently. Many
works focus on using modular deep neural networks to construct the end-to-end
archi-tecture. However, whether using powerful large language models (LLM),
especially multi-modality Vision Language Models (VLM) could benefit the
end-to-end driving tasks remain a question. In our work, we demonstrate that
combining end-to-end architectural design and knowledgeable VLMs yield
impressive performance on the driving tasks. It is worth noting that our method
only uses a single camera and is the best camera-only solution across the
leaderboard, demonstrating the effectiveness of vision-based driving approach
and the potential for end-to-end driving tasks.","Zilong Guo, Yi Luo, Long Sha, Dongxu Wang, Panqu Wang, Chenyang Xu, Yi Yang",2025-09-02 17:52:29+00:00,http://arxiv.org/abs/2509.02659v1,http://arxiv.org/pdf/2509.02659v1,"cs.CV, cs.RO"
Manipulation as in Simulation: Enabling Accurate Geometry Perception in Robots,"Modern robotic manipulation primarily relies on visual observations in a 2D
color space for skill learning but suffers from poor generalization. In
contrast, humans, living in a 3D world, depend more on physical properties-such
as distance, size, and shape-than on texture when interacting with objects.
Since such 3D geometric information can be acquired from widely available depth
cameras, it appears feasible to endow robots with similar perceptual
capabilities. Our pilot study found that using depth cameras for manipulation
is challenging, primarily due to their limited accuracy and susceptibility to
various types of noise. In this work, we propose Camera Depth Models (CDMs) as
a simple plugin on daily-use depth cameras, which take RGB images and raw depth
signals as input and output denoised, accurate metric depth. To achieve this,
we develop a neural data engine that generates high-quality paired data from
simulation by modeling a depth camera's noise pattern. Our results show that
CDMs achieve nearly simulation-level accuracy in depth prediction, effectively
bridging the sim-to-real gap for manipulation tasks. Notably, our experiments
demonstrate, for the first time, that a policy trained on raw simulated depth,
without the need for adding noise or real-world fine-tuning, generalizes
seamlessly to real-world robots on two challenging long-horizon tasks involving
articulated, reflective, and slender objects, with little to no performance
degradation. We hope our findings will inspire future research in utilizing
simulation data and 3D information in general robot policies.","Minghuan Liu, Zhengbang Zhu, Xiaoshen Han, Peng Hu, Haotong Lin, Xinyao Li, Jingxiao Chen, Jiafeng Xu, Yichu Yang, Yunfeng Lin, Xinghang Li, Yong Yu, Weinan Zhang, Tao Kong, Bingyi Kang",2025-09-02 17:29:38+00:00,http://arxiv.org/abs/2509.02530v1,http://arxiv.org/pdf/2509.02530v1,"cs.RO, cs.AI, cs.CV"
Fault-tolerant Model Predictive Control for Spacecraft,"Given the cost and critical functions of satellite constellations, ensuring
mission longevity and safe decommissioning is essential for space
sustainability. This article presents a Model Predictive Control for spacecraft
trajectory and setpoint stabilization under multiple actuation failures. The
proposed solution allows us to efficiently control the faulty spacecraft
enabling safe navigation towards servicing or collision-free trajectories. The
proposed scheme ensures closed-loop asymptotic stability and is shown to be
recursively feasible. We demonstrate its efficacy through open-source numerical
results and realistic experiments using the ATMOS platform.","Raphael Stöckner, Pedro Roque, Maria Charitidou, Dimos V. Dimarogonas",2025-09-02 17:28:38+00:00,http://arxiv.org/abs/2509.02527v1,http://arxiv.org/pdf/2509.02527v1,cs.RO
Classification of Vision-Based Tactile Sensors: A Review,"Vision-based tactile sensors (VBTS) have gained widespread application in
robotic hands, grippers and prosthetics due to their high spatial resolution,
low manufacturing costs, and ease of customization. While VBTSs have common
design features, such as a camera module, they can differ in a rich diversity
of sensing principles, material compositions, multimodal approaches, and data
interpretation methods. Here, we propose a novel classification of VBTS that
categorizes the technology into two primary sensing principles based on the
underlying transduction of contact into a tactile image: the Marker-Based
Transduction Principle and the Intensity-Based Transduction Principle.
Marker-Based Transduction interprets tactile information by detecting marker
displacement and changes in marker density. In contrast, Intensity-Based
Transduction maps external disturbances with variations in pixel values.
Depending on the design of the contact module, Marker-Based Transduction can be
further divided into two subtypes: Simple Marker-Based (SMB) and Morphological
Marker-Based (MMB) mechanisms. Similarly, the Intensity-Based Transduction
Principle encompasses the Reflective Layer-based (RLB) and Transparent
Layer-Based (TLB) mechanisms. This paper provides a comparative study of the
hardware characteristics of these four types of sensors including various
combination types, and discusses the commonly used methods for interpreting
tactile information. This~comparison reveals some current challenges faced by
VBTS technology and directions for future research.","Haoran Li, Yijiong Lin, Chenghua Lu, Max Yang, Efi Psomopoulou, Nathan F Lepora",2025-09-02 16:29:06+00:00,http://arxiv.org/abs/2509.02478v2,http://arxiv.org/pdf/2509.02478v2,cs.RO
Coral: A Unifying Abstraction Layer for Composable Robotics Software,"Despite the multitude of excellent software components and tools available in
the robotics and broader software engineering communities, successful
integration of software for robotic systems remains a time-consuming and
challenging task for users of all knowledge and skill levels. And with robotics
software often being built into tightly coupled, monolithic systems, even minor
alterations to improve performance, adjust to changing task requirements, or
deploy to new hardware can require significant engineering investment. To help
solve this problem, this paper presents Coral, an abstraction layer for
building, deploying, and coordinating independent software components that
maximizes composability to allow for rapid system integration without modifying
low-level code. Rather than replacing existing tools, Coral complements them by
introducing a higher-level abstraction that constrains the integration process
to semantically meaningful choices, reducing the configuration burden without
limiting adaptability to diverse domains, systems, and tasks. We describe Coral
in detail and demonstrate its utility in integrating software for scenarios of
increasing complexity, including LiDAR-based SLAM and multi-robot corrosion
mitigation tasks. By enabling practical composability in robotics software,
Coral offers a scalable solution to a broad range of robotics system
integration challenges, improving component reusability, system
reconfigurability, and accessibility to both expert and non-expert users. We
release Coral open source.","Steven Swanbeck, Mitch Pryor",2025-09-02 16:04:15+00:00,http://arxiv.org/abs/2509.02453v1,http://arxiv.org/pdf/2509.02453v1,cs.RO
U-ARM : Ultra low-cost general teleoperation interface for robot manipulation,"We propose U-Arm, a low-cost and rapidly adaptable leader-follower
teleoperation framework designed to interface with most of commercially
available robotic arms. Our system supports teleoperation through three
structurally distinct 3D-printed leader arms that share consistent control
logic, enabling seamless compatibility with diverse commercial robot
configurations. Compared with previous open-source leader-follower interfaces,
we further optimized both the mechanical design and servo selection, achieving
a bill of materials (BOM) cost of only \$50.5 for the 6-DoF leader arm and
\$56.8 for the 7-DoF version. To enhance usability, we mitigate the common
challenge in controlling redundant degrees of freedom by %engineering methods
mechanical and control optimizations. Experimental results demonstrate that
U-Arm achieves 39\% higher data collection efficiency and comparable task
success rates across multiple manipulation scenarios compared with Joycon,
another low-cost teleoperation interface. We have open-sourced all CAD models
of three configs and also provided simulation support for validating
teleoperation workflows. We also open-sourced real-world manipulation data
collected with U-Arm. The project website is
https://github.com/MINT-SJTU/LeRobot-Anything-U-Arm.","Yanwen Zou, Zhaoye Zhou, Chenyang Shi, Zewei Ye, Junda Huang, Yan Ding, Bo Zhao",2025-09-02 15:39:38+00:00,http://arxiv.org/abs/2509.02437v1,http://arxiv.org/pdf/2509.02437v1,cs.RO
OpenGuide: Assistive Object Retrieval in Indoor Spaces for Individuals with Visual Impairments,"Indoor built environments like homes and offices often present complex and
cluttered layouts that pose significant challenges for individuals who are
blind or visually impaired, especially when performing tasks that involve
locating and gathering multiple objects. While many existing assistive
technologies focus on basic navigation or obstacle avoidance, few systems
provide scalable and efficient multi-object search capabilities in real-world,
partially observable settings. To address this gap, we introduce OpenGuide, an
assistive mobile robot system that combines natural language understanding with
vision-language foundation models (VLM), frontier-based exploration, and a
Partially Observable Markov Decision Process (POMDP) planner. OpenGuide
interprets open-vocabulary requests, reasons about object-scene relationships,
and adaptively navigates and localizes multiple target items in novel
environments. Our approach enables robust recovery from missed detections
through value decay and belief-space reasoning, resulting in more effective
exploration and object localization. We validate OpenGuide in simulated and
real-world experiments, demonstrating substantial improvements in task success
rate and search efficiency over prior methods. This work establishes a
foundation for scalable, human-centered robotic assistance in assisted living
environments.","Yifan Xu, Qianwei Wang, Vineet Kamat, Carol Menassa",2025-09-02 15:27:12+00:00,http://arxiv.org/abs/2509.02425v1,http://arxiv.org/pdf/2509.02425v1,"cs.RO, cs.HC"
Physics-Informed Machine Learning with Adaptive Grids for Optical Microrobot Depth Estimation,"Optical microrobots actuated by optical tweezers (OT) offer great potential
for biomedical applications such as cell manipulation and microscale assembly.
These tasks demand accurate three-dimensional perception to ensure precise
control in complex and dynamic biological environments. However, the
transparent nature of microrobots and low-contrast microscopic imaging
challenge conventional deep learning methods, which also require large
annotated datasets that are costly to obtain. To address these challenges, we
propose a physics-informed, data-efficient framework for depth estimation of
optical microrobots. Our method augments convolutional feature extraction with
physics-based focus metrics, such as entropy, Laplacian of Gaussian, and
gradient sharpness, calculated using an adaptive grid strategy. This approach
allocates finer grids over microrobot regions and coarser grids over background
areas, enhancing depth sensitivity while reducing computational complexity. We
evaluate our framework on multiple microrobot types and demonstrate significant
improvements over baseline models. Specifically, our approach reduces mean
squared error (MSE) by over 60% and improves the coefficient of determination
(R^2) across all test cases. Notably, even when trained on only 20% of the
available data, our model outperforms ResNet50 trained on the full dataset,
highlighting its robustness under limited data conditions. Our code is
available at: https://github.com/LannWei/CBS2025.","Lan Wei, Lou Genoud, Dandan Zhang",2025-09-02 14:11:26+00:00,http://arxiv.org/abs/2509.02343v1,http://arxiv.org/pdf/2509.02343v1,cs.RO
Language-Guided Long Horizon Manipulation with LLM-based Planning and Visual Perception,"Language-guided long-horizon manipulation of deformable objects presents
significant challenges due to high degrees of freedom, complex dynamics, and
the need for accurate vision-language grounding. In this work, we focus on
multi-step cloth folding, a representative deformable-object manipulation task
that requires both structured long-horizon planning and fine-grained visual
perception. To this end, we propose a unified framework that integrates a Large
Language Model (LLM)-based planner, a Vision-Language Model (VLM)-based
perception system, and a task execution module. Specifically, the LLM-based
planner decomposes high-level language instructions into low-level action
primitives, bridging the semantic-execution gap, aligning perception with
action, and enhancing generalization. The VLM-based perception module employs a
SigLIP2-driven architecture with a bidirectional cross-attention fusion
mechanism and weight-decomposed low-rank adaptation (DoRA) fine-tuning to
achieve language-conditioned fine-grained visual grounding. Experiments in both
simulation and real-world settings demonstrate the method's effectiveness. In
simulation, it outperforms state-of-the-art baselines by 2.23, 1.87, and 33.3
on seen instructions, unseen instructions, and unseen tasks, respectively. On a
real robot, it robustly executes multi-step folding sequences from language
instructions across diverse cloth materials and configurations, demonstrating
strong generalization in practical scenarios. Project page:
https://language-guided.netlify.app/","Changshi Zhou, Haichuan Xu, Ningquan Gu, Zhipeng Wang, Bin Cheng, Pengpeng Zhang, Yanchao Dong, Mitsuhiro Hayashibe, Yanmin Zhou, Bin He",2025-09-02 13:50:45+00:00,http://arxiv.org/abs/2509.02324v1,http://arxiv.org/pdf/2509.02324v1,cs.RO
Sem-RaDiff: Diffusion-Based 3D Radar Semantic Perception in Cluttered Agricultural Environments,"Accurate and robust environmental perception is crucial for robot autonomous
navigation. While current methods typically adopt optical sensors (e.g.,
camera, LiDAR) as primary sensing modalities, their susceptibility to visual
occlusion often leads to degraded performance or complete system failure. In
this paper, we focus on agricultural scenarios where robots are exposed to the
risk of onboard sensor contamination. Leveraging radar's strong penetration
capability, we introduce a radar-based 3D environmental perception framework as
a viable alternative. It comprises three core modules designed for dense and
accurate semantic perception: 1) Parallel frame accumulation to enhance
signal-to-noise ratio of radar raw data. 2) A diffusion model-based
hierarchical learning framework that first filters radar sidelobe artifacts
then generates fine-grained 3D semantic point clouds. 3) A specifically
designed sparse 3D network optimized for processing large-scale radar raw data.
We conducted extensive benchmark comparisons and experimental evaluations on a
self-built dataset collected in real-world agricultural field scenes. Results
demonstrate that our method achieves superior structural and semantic
prediction performance compared to existing methods, while simultaneously
reducing computational and memory costs by 51.3% and 27.5%, respectively.
Furthermore, our approach achieves complete reconstruction and accurate
classification of thin structures such as poles and wires-which existing
methods struggle to perceive-highlighting its potential for dense and accurate
3D radar perception.","Ruibin Zhang, Fei Gao",2025-09-02 13:07:02+00:00,http://arxiv.org/abs/2509.02283v2,http://arxiv.org/pdf/2509.02283v2,cs.RO
Human-Inspired Soft Anthropomorphic Hand System for Neuromorphic Object and Pose Recognition Using Multimodal Signals,"The human somatosensory system integrates multimodal sensory feedback,
including tactile, proprioceptive, and thermal signals, to enable comprehensive
perception and effective interaction with the environment. Inspired by the
biological mechanism, we present a sensorized soft anthropomorphic hand
equipped with diverse sensors designed to emulate the sensory modalities of the
human hand. This system incorporates biologically inspired encoding schemes
that convert multimodal sensory data into spike trains, enabling
highly-efficient processing through Spiking Neural Networks (SNNs). By
utilizing these neuromorphic signals, the proposed framework achieves 97.14%
accuracy in object recognition across varying poses, significantly
outperforming previous studies on soft hands. Additionally, we introduce a
novel differentiator neuron model to enhance material classification by
capturing dynamic thermal responses. Our results demonstrate the benefits of
multimodal sensory fusion and highlight the potential of neuromorphic
approaches for achieving efficient, robust, and human-like perception in
robotic systems.","Fengyi Wang, Xiangyu Fu, Nitish Thakor, Gordon Cheng",2025-09-02 12:52:53+00:00,http://arxiv.org/abs/2509.02275v1,http://arxiv.org/pdf/2509.02275v1,cs.RO
Adaptive Navigation Strategy for Low-Thrust Proximity Operations in Circular Relative Orbit,"This paper presents an adaptive observer-based navigation strategy for
spacecraft in Circular Relative Orbit (CRO) scenarios, addressing challenges in
proximity operations like formation flight and uncooperative target inspection.
The proposed method adjusts observer gains based on the estimated state to
achieve fast convergence and low noise sensitivity in state estimation. A
Lyapunov-based analysis ensures stability and accuracy, while simulations using
vision-based sensor data validate the approach under realistic conditions.
Compared to classical observers with time-invariant gains, the proposed method
enhances trajectory tracking precision and reduces control input switching,
making it a promising solution for autonomous spacecraft localization and
control.","Dario Ruggiero, Mauro Mancini, Elisa Capello",2025-09-02 11:18:16+00:00,http://arxiv.org/abs/2509.02204v1,http://arxiv.org/pdf/2509.02204v1,"cs.RO, cs.SY, eess.SY"
Enhancing Reliability in LLM-Integrated Robotic Systems: A Unified Approach to Security and Safety,"Integrating large language models (LLMs) into robotic systems has
revolutionised embodied artificial intelligence, enabling advanced
decision-making and adaptability. However, ensuring reliability, encompassing
both security against adversarial attacks and safety in complex environments,
remains a critical challenge. To address this, we propose a unified framework
that mitigates prompt injection attacks while enforcing operational safety
through robust validation mechanisms. Our approach combines prompt assembling,
state management, and safety validation, evaluated using both performance and
security metrics. Experiments show a 30.8% improvement under injection attacks
and up to a 325% improvement in complex environment settings under adversarial
conditions compared to baseline scenarios. This work bridges the gap between
safety and security in LLM-based robotic systems, offering actionable insights
for deploying reliable LLM-integrated mobile robots in real-world settings. The
framework is open-sourced with simulation and physical deployment demos at
https://llmeyesim.vercel.app/","Wenxiao Zhang, Xiangrui Kong, Conan Dewitt, Thomas Bräunl, Jin B. Hong",2025-09-02 10:14:28+00:00,http://arxiv.org/abs/2509.02163v1,http://arxiv.org/pdf/2509.02163v1,"cs.RO, cs.AI"
Systematic Evaluation of Trade-Offs in Motion Planning Algorithms for Optimal Industrial Robotic Work Cell Design,"The performance of industrial robotic work cells depends on optimizing
various hyperparameters referring to the cell layout, such as robot base
placement, tool placement, and kinematic design. Achieving this requires a
bilevel optimization approach, where the high-level optimization adjusts these
hyperparameters, and the low-level optimization computes robot motions.
However, computing the optimal robot motion is computationally infeasible,
introducing trade-offs in motion planning to make the problem tractable. These
trade-offs significantly impact the overall performance of the bilevel
optimization, but their effects still need to be systematically evaluated. In
this paper, we introduce metrics to assess these trade-offs regarding
optimality, time gain, robustness, and consistency. Through extensive
simulation studies, we investigate how simplifications in motion-level
optimization affect the high-level optimization outcomes, balancing
computational complexity with solution quality. The proposed algorithms are
applied to find the time-optimal kinematic design for a modular robot in two
palletization scenarios.","G. de Mathelin, C. Hartl-Nesic, A. Kugi",2025-09-02 09:49:21+00:00,http://arxiv.org/abs/2509.02146v1,http://arxiv.org/pdf/2509.02146v1,cs.RO
Learning Social Heuristics for Human-Aware Path Planning,"Social robotic navigation has been at the center of numerous studies in
recent years. Most of the research has focused on driving the robotic agent
along obstacle-free trajectories, respecting social distances from humans, and
predicting their movements to optimize navigation. However, in order to really
be socially accepted, the robots must be able to attain certain social norms
that cannot arise from conventional navigation, but require a dedicated
learning process. We propose Heuristic Planning with Learned Social Value
(HPLSV), a method to learn a value function encapsulating the cost of social
navigation, and use it as an additional heuristic in heuristic-search path
planning. In this preliminary work, we apply the methodology to the common
social scenario of joining a queue of people, with the intention of
generalizing to further human activities.","Andrea Eirale, Matteo Leonetti, Marcello Chiaberge",2025-09-02 09:36:11+00:00,http://arxiv.org/abs/2509.02134v1,http://arxiv.org/pdf/2509.02134v1,"cs.RO, cs.AI, cs.LG"
INF-3DP: Implicit Neural Fields for Collision-Free Multi-Axis 3D Printing,"We introduce a general, scalable computational framework for multi-axis 3D
printing based on implicit neural fields (INFs) that unifies all stages of
toolpath generation and global collision-free motion planning. In our pipeline,
input models are represented as signed distance fields, with fabrication
objectives such as support-free printing, surface finish quality, and extrusion
control being directly encoded in the optimization of an implicit guidance
field. This unified approach enables toolpath optimization across both surface
and interior domains, allowing shell and infill paths to be generated via
implicit field interpolation. The printing sequence and multi-axis motion are
then jointly optimized over a continuous quaternion field. Our continuous
formulation constructs the evolving printing object as a time-varying SDF,
supporting differentiable global collision handling throughout INF-based motion
planning. Compared to explicit-representation-based methods, INF-3DP achieves
up to two orders of magnitude speedup and significantly reduces
waypoint-to-surface error. We validate our framework on diverse, complex models
and demonstrate its efficiency with physical fabrication experiments using a
robot-assisted multi-axis system.","Jiasheng Qu, Zhuo Huang, Dezhao Guo, Hailin Sun, Aoran Lyu, Chengkai Dai, Yeung Yam, Guoxin Fang",2025-09-02 09:22:52+00:00,http://arxiv.org/abs/2509.05345v1,http://arxiv.org/pdf/2509.05345v1,"cs.RO, cs.CG"
A Geometric Method for Base Parameter Analysis in Robot Inertia Identification Based on Projective Geometric Algebra,"This paper proposes a novel geometric method for analytically determining the
base inertial parameters of robotic systems. The rigid body dynamics is
reformulated using projective geometric algebra, leading to a new
identification model named ``tetrahedral-point (TP)"" model. Based on the rigid
body TP model, coefficients in the regresoor matrix of the identification model
are derived in closed-form, exhibiting clear geometric interpretations.
Building directly from the dynamic model, three foundational principles for
base parameter analysis are proposed: the shared points principle, fixed points
principle, and planar rotations principle. With these principles, algorithms
are developed to automatically determine all the base parameters. The core
algorithm, referred to as Dynamics Regressor Nullspace Generator (DRNG),
achieves $O(1)$-complexity theoretically following an $O(N)$-complexity
preprocessing stage, where $N$ is the number of rigid bodies. The proposed
method and algorithms are validated across four robots: Puma560, Unitree Go2, a
2RRU-1RRS parallel kinematics mechanism (PKM), and a 2PRS-1PSR PKM. In all
cases, the algorithms successfully identify the complete set of base
parameters. Notably, the approach demonstrates high robustness and
computational efficiency, particularly in the cases of PKMs. Through the
comprehensive demonstrations, the method is shown to be general, robust, and
efficient.","Guangzhen Sun, Ye Ding, Xiangyang Zhu",2025-09-02 08:20:34+00:00,http://arxiv.org/abs/2509.02071v1,http://arxiv.org/pdf/2509.02071v1,cs.RO
Align-Then-stEer: Adapting the Vision-Language Action Models through Unified Latent Guidance,"Vision-Language-Action (VLA) models pre-trained on large, diverse datasets
show remarkable potential for general-purpose robotic manipulation. However, a
primary bottleneck remains in adapting these models to downstream tasks,
especially when the robot's embodiment or the task itself differs from the
pre-training data. This discrepancy leads to a significant mismatch in action
distributions, demanding extensive data and compute for effective fine-tuning.
To address this challenge, we introduce \textbf{Align-Then-stEer
(\texttt{ATE})}, a novel, data-efficient, and plug-and-play adaptation
framework. \texttt{ATE} first aligns disparate action spaces by constructing a
unified latent space, where a variational autoencoder constrained by reverse KL
divergence embeds adaptation actions into modes of the pre-training action
latent distribution. Subsequently, it steers the diffusion- or flow-based VLA's
generation process during fine-tuning via a guidance mechanism that pushes the
model's output distribution towards the target domain. We conduct extensive
experiments on cross-embodiment and cross-task manipulation in both simulation
and real world. Compared to direct fine-tuning of representative VLAs, our
method improves the average multi-task success rate by up to \textbf{9.8\%} in
simulation and achieves a striking \textbf{32\% success rate gain} in a
real-world cross-embodiment setting. Our work presents a general and
lightweight solution that greatly enhances the practicality of deploying VLA
models to new robotic platforms and tasks.","Yang Zhang, Chenwei Wang, Ouyang Lu, Yuan Zhao, Yunfei Ge, Zhenglong Sun, Xiu Li, Chi Zhang, Chenjia Bai, Xuelong Li",2025-09-02 07:51:59+00:00,http://arxiv.org/abs/2509.02055v2,http://arxiv.org/pdf/2509.02055v2,"cs.RO, cs.AI"
Generalizing Unsupervised Lidar Odometry Model from Normal to Snowy Weather Conditions,"Deep learning-based LiDAR odometry is crucial for autonomous driving and
robotic navigation, yet its performance under adverse weather, especially
snowfall, remains challenging. Existing models struggle to generalize across
conditions due to sensitivity to snow-induced noise, limiting real-world use.
In this work, we present an unsupervised LiDAR odometry model to close the gap
between clear and snowy weather conditions. Our approach focuses on effective
denoising to mitigate the impact of snowflake noise and outlier points on pose
estimation, while also maintaining computational efficiency for real-time
applications.
  To achieve this, we introduce a Patch Spatial Measure (PSM) module that
evaluates the dispersion of points within each patch, enabling effective
detection of sparse and discrete noise.
  We further propose a Patch Point Weight Predictor (PPWP) to assign adaptive
point-wise weights, enhancing their discriminative capacity within local
regions. To support real-time performance, we first apply an intensity
threshold mask to quickly suppress dense snowflake clusters near the LiDAR, and
then perform multi-modal feature fusion to refine the point-wise weight
prediction, improving overall robustness under adverse weather. Our model is
trained in clear weather conditions and rigorously tested across various
scenarios, including snowy and dynamic. Extensive experimental results confirm
the effectiveness of our method, demonstrating robust performance in both clear
and snowy weather. This advancement enhances the model's generalizability and
paves the way for more reliable autonomous systems capable of operating across
a wider range of environmental conditions.","Beibei Zhou, Zhiyuan Zhang, Zhenbo Song, Jianhui Guo, Hui Kong",2025-09-02 06:52:25+00:00,http://arxiv.org/abs/2509.02011v1,http://arxiv.org/pdf/2509.02011v1,cs.RO
MIRAGE: Multimodal Intention Recognition and Admittance-Guided Enhancement in VR-based Multi-object Teleoperation,"Effective human-robot interaction (HRI) in multi-object teleoperation tasks
faces significant challenges due to perceptual ambiguities in virtual reality
(VR) environments and the limitations of single-modality intention recognition.
This paper proposes a shared control framework that combines a virtual
admittance (VA) model with a Multimodal-CNN-based Human Intention Perception
Network (MMIPN) to enhance teleoperation performance and user experience. The
VA model employs artificial potential fields to guide operators toward target
objects by adjusting admittance force and optimizing motion trajectories. MMIPN
processes multimodal inputs, including gaze movement, robot motions, and
environmental context, to estimate human grasping intentions, helping to
overcome depth perception challenges in VR. Our user study evaluated four
conditions across two factors, and the results showed that MMIPN significantly
improved grasp success rates, while the VA model enhanced movement efficiency
by reducing path lengths. Gaze data emerged as the most crucial input modality.
These findings demonstrate the effectiveness of combining multimodal cues with
implicit guidance in VR-based teleoperation, providing a robust solution for
multi-object grasping tasks and enabling more natural interactions across
various applications in the future.","Chi Sun, Xian Wang, Abhishek Kumar, Chengbin Cui, Lik-Hang Lee",2025-09-02 06:20:28+00:00,http://arxiv.org/abs/2509.01996v1,http://arxiv.org/pdf/2509.01996v1,"cs.RO, cs.HC"
Geometric Control of Mechanical Systems with Symmetries Based on Sliding Modes,"In this paper, we propose a framework for designing sliding mode controllers
for a class of mechanical systems with symmetry, both unconstrained and
constrained, that evolve on principal fiber bundles. Control laws are developed
based on the reduced motion equations by exploring symmetries, leading to a
sliding mode control strategy where the reaching stage is executed on the base
space, and the sliding stage is performed on the structure group. Thus, design
complexity is reduced, and difficult choices for coordinate representations
when working with a particular Lie group are avoided. For this purpose, a
sliding subgroup is constructed on the structure group based on a kinematic
controller, and the sliding variable will converge to the identity of the state
manifold upon reaching the sliding subgroup. A reaching law based on a general
sliding vector field is then designed on the base space using the local form of
the mechanical connection to drive the sliding variable to the sliding
subgroup, and its time evolution is given according to the appropriate
covariant derivative. Almost global asymptotic stability and local exponential
stability are demonstrated using a Lyapunov analysis. We apply the results to a
fully actuated system (a rigid spacecraft actuated by reaction wheels) and a
subactuated nonholonomic system (unicycle mobile robot actuated by wheels),
which is also simulated for illustration.","Eduardo Espindola, Yu Tang",2025-09-02 06:06:21+00:00,http://arxiv.org/abs/2509.01985v1,http://arxiv.org/pdf/2509.01985v1,cs.RO
Hybrid Autonomy Framework for a Future Mars Science Helicopter,"Autonomous aerial vehicles, such as NASA's Ingenuity, enable rapid planetary
surface exploration beyond the reach of ground-based robots. Thus, NASA is
studying a Mars Science Helicopter (MSH), an advanced concept capable of
performing long-range science missions and autonomously navigating challenging
Martian terrain. Given significant Earth-Mars communication delays and mission
complexity, an advanced autonomy framework is required to ensure safe and
efficient operation by continuously adapting behavior based on mission
objectives and real-time conditions, without human intervention. This study
presents a deterministic high-level control framework for aerial exploration,
integrating a Finite State Machine (FSM) with Behavior Trees (BTs) to achieve a
scalable, robust, and computationally efficient autonomy solution for critical
scenarios like deep space exploration. In this paper we outline key
capabilities of a possible MSH and detail the FSM-BT hybrid autonomy framework
which orchestrates them to achieve the desired objectives. Monte Carlo
simulations and real field tests validate the framework, demonstrating its
robustness and adaptability to both discrete events and real-time system
feedback. These inputs trigger state transitions or dynamically adjust behavior
execution, enabling reactive and context-aware responses. The framework is
middleware-agnostic, supporting integration with systems like F-Prime and
extending beyond aerial robotics.","Luca Di Pierno, Robert Hewitt, Stephan Weiss, Roland Brockers",2025-09-02 05:47:14+00:00,http://arxiv.org/abs/2509.01980v1,http://arxiv.org/pdf/2509.01980v1,"cs.RO, cs.SY, eess.SY"
Ensemble-Based Event Camera Place Recognition Under Varying Illumination,"Compared to conventional cameras, event cameras provide a high dynamic range
and low latency, offering greater robustness to rapid motion and challenging
lighting conditions. Although the potential of event cameras for visual place
recognition (VPR) has been established, developing robust VPR frameworks under
severe illumination changes remains an open research problem. In this paper, we
introduce an ensemble-based approach to event camera place recognition that
combines sequence-matched results from multiple event-to-frame reconstructions,
VPR feature extractors, and temporal resolutions. Unlike previous event-based
ensemble methods, which only utilise temporal resolution, our broader fusion
strategy delivers significantly improved robustness under varied lighting
conditions (e.g., afternoon, sunset, night), achieving a 57% relative
improvement in Recall@1 across day-night transitions. We evaluate our approach
on two long-term driving datasets (with 8 km per traverse) without metric
subsampling, thereby preserving natural variations in speed and stop duration
that influence event density. We also conduct a comprehensive analysis of key
design choices, including binning strategies, polarity handling, reconstruction
methods, and feature extractors, to identify the most critical components for
robust performance. Additionally, we propose a modification to the standard
sequence matching framework that enhances performance at longer sequence
lengths. To facilitate future research, we will release our codebase and
benchmarking framework.","Therese Joseph, Tobias Fischer, Michael Milford",2025-09-02 05:17:07+00:00,http://arxiv.org/abs/2509.01968v1,http://arxiv.org/pdf/2509.01968v1,"cs.CV, cs.RO"
Robustness Enhancement for Multi-Quadrotor Centralized Transportation System via Online Tuning and Learning,"This paper introduces an adaptive-neuro geometric control for a centralized
multi-quadrotor cooperative transportation system, which enhances both
adaptivity and disturbance rejection. Our strategy is to coactively tune the
model parameters and learn the external disturbances in real-time. To realize
this, we augmented the existing geometric control with multiple neural networks
and adaptive laws, where the estimated model parameters and the weights of the
neural networks are simultaneously tuned and adjusted online. The
Lyapunov-based adaptation guarantees bounded estimation errors without
requiring either pre-training or the persistent excitation (PE) condition. The
proposed control system has been proven to be stable in the sense of Lyapunov
under certain preconditions, and its enhanced robustness under scenarios of
disturbed environment and model-unmatched plant was demonstrated by numerical
simulations.","Tianhua Gao, Kohji Tomita, Akiya Kamimura",2025-09-02 04:47:21+00:00,http://arxiv.org/abs/2509.01952v1,http://arxiv.org/pdf/2509.01952v1,"eess.SY, cs.RO, cs.SY, math.OC"
Online Identification using Adaptive Laws and Neural Networks for Multi-Quadrotor Centralized Transportation System,"This paper introduces an adaptive-neuro identification method that enhances
the robustness of a centralized multi-quadrotor transportation system. This
method leverages online tuning and learning on decomposed error subspaces,
enabling efficient real-time compensation to time-varying disturbances and
model uncertainties acting on the payload. The strategy is to decompose the
high-dimensional error space into a set of low-dimensional subspaces. In this
way, the identification problem for unseen features is naturally transformed
into submappings (``slices'') addressed by multiple adaptive laws and shallow
neural networks, which are updated online via Lyapunov-based adaptation without
requiring persistent excitation (PE) and offline training. Due to the
model-free nature of neural networks, this approach can be well adapted to
highly coupled and nonlinear centralized transportation systems. It serves as a
feedforward compensator for the payload controller without explicitly relying
on the dynamics coupled with the payload, such as cables and quadrotors. The
proposed control system has been proven to be stable in the sense of Lyapunov,
and its enhanced robustness under time-varying disturbances and model
uncertainties was demonstrated by numerical simulations.","Tianhua Gao, Kohji Tomita, Akiya Kamimura",2025-09-02 04:45:35+00:00,http://arxiv.org/abs/2509.01951v1,http://arxiv.org/pdf/2509.01951v1,"eess.SY, cs.RO, cs.SY, math.OC"
AutoDrive-R$^2$: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving,"Vision-Language-Action (VLA) models in autonomous driving systems have
recently demonstrated transformative potential by integrating multimodal
perception with decision-making capabilities. However, the interpretability and
coherence of the decision process and the plausibility of action sequences
remain largely underexplored. To address these issues, we propose
AutoDrive-R$^2$, a novel VLA framework that enhances both reasoning and
self-reflection capabilities of autonomous driving systems through
chain-of-thought (CoT) processing and reinforcement learning (RL).
Specifically, we first propose an innovative CoT dataset named nuScenesR$^2$-6K
for supervised fine-tuning, which effectively builds cognitive bridges between
input information and output trajectories through a four-step logical chain
with self-reflection for validation. Moreover, to maximize both reasoning and
self-reflection during the RL stage, we further employ the Group Relative
Policy Optimization (GRPO) algorithm within a physics-grounded reward framework
that incorporates spatial alignment, vehicle dynamic, and temporal smoothness
criteria to ensure reliable and realistic trajectory planning. Extensive
evaluation results across both nuScenes and Waymo datasets demonstrates the
state-of-the-art performance and robust generalization capacity of our proposed
method.","Zhenlong Yuan, Jing Tang, Jinguo Luo, Rui Chen, Chengxuan Qian, Lei Sun, Xiangxiang Chu, Yujun Cai, Dapeng Zhang, Shuo Li",2025-09-02 04:32:24+00:00,http://arxiv.org/abs/2509.01944v1,http://arxiv.org/pdf/2509.01944v1,"cs.RO, cs.CV"
AI-Driven Marine Robotics: Emerging Trends in Underwater Perception and Ecosystem Monitoring,"Marine ecosystems face increasing pressure due to climate change, driving the
need for scalable, AI-powered monitoring solutions. This paper examines the
rapid emergence of underwater AI as a major research frontier and analyzes the
factors that have transformed marine perception from a niche application into a
catalyst for AI innovation. We identify three convergent drivers: environmental
necessity for ecosystem-scale monitoring, democratization of underwater
datasets through citizen science platforms, and researcher migration from
saturated terrestrial computer vision domains. Our analysis reveals how unique
underwater challenges - turbidity, cryptic species detection, expert annotation
bottlenecks, and cross-ecosystem generalization - are driving fundamental
advances in weakly supervised learning, open-set recognition, and robust
perception under degraded conditions. We survey emerging trends in datasets,
scene understanding and 3D reconstruction, highlighting the paradigm shift from
passive observation toward AI-driven, targeted intervention capabilities. The
paper demonstrates how underwater constraints are pushing the boundaries of
foundation models, self-supervised learning, and perception, with
methodological innovations that extend far beyond marine applications to
benefit general computer vision, robotics, and environmental monitoring.","Scarlett Raine, Tobias Fischer",2025-09-02 01:51:31+00:00,http://arxiv.org/abs/2509.01878v1,http://arxiv.org/pdf/2509.01878v1,"cs.RO, cs.CV, cs.LG"
Multi-vessel Interaction-Aware Trajectory Prediction and Collision Risk Assessment,"Accurate vessel trajectory prediction is essential for enhancing situational
awareness and preventing collisions. Still, existing data-driven models are
constrained mainly to single-vessel forecasting, overlooking vessel
interactions, navigation rules, and explicit collision risk assessment. We
present a transformer-based framework for multi-vessel trajectory prediction
with integrated collision risk analysis. For a given target vessel, the
framework identifies nearby vessels. It jointly predicts their future
trajectories through parallel streams encoding kinematic and derived physical
features, causal convolutions for temporal locality, spatial transformations
for positional encoding, and hybrid positional embeddings that capture both
local motion patterns and long-range dependencies. Evaluated on large-scale
real-world AIS data using joint multi-vessel metrics, the model demonstrates
superior forecasting capabilities beyond traditional single-vessel displacement
errors. By simulating interactions among predicted trajectories, the framework
further quantifies potential collision risks, offering actionable insights to
strengthen maritime safety and decision support.","Md Mahbub Alam, Jose F. Rodrigues-Jr, Gabriel Spadon",2025-09-01 23:38:01+00:00,http://arxiv.org/abs/2509.01836v1,http://arxiv.org/pdf/2509.01836v1,"cs.RO, cs.AI, cs.LG"
Nonlinear Model Predictive Control-Based Reverse Path-Planning and Path-Tracking Control of a Vehicle with Trailer System,"Reverse parking maneuvers of a vehicle with trailer system is a challenging
task to complete for human drivers due to the unstable nature of the system and
unintuitive controls required to orientate the trailer properly. This paper
hence proposes an optimization-based automation routine to handle the
path-planning and path-tracking control process of such type of maneuvers. The
proposed approach utilizes nonlinear model predictive control (NMPC) to
robustly guide the vehicle-trailer system into the desired parking space, and
an optional forward repositioning maneuver can be added as an additional stage
of the parking process to obtain better system configurations, before backward
motion can be attempted again to get a good final pose. The novelty of the
proposed approach is the simplicity of its formulation, as the path-planning
and path-tracking operations are only conducted on the trailer being viewed as
a standalone vehicle, before the control inputs are propagated to the tractor
vehicle via inverse kinematic relationships also derived in this paper.
Simulation case studies and hardware-in-the-loop tests are performed, and the
results demonstrate the efficacy of the proposed approach.","Xincheng Cao, Haochong Chen, Bilin Aksun-Guvenc, Levent Guvenc, Brian Link, Peter J Richmond, Dokyung Yim, Shihong Fan, John Harber",2025-09-01 22:50:56+00:00,http://arxiv.org/abs/2509.01820v1,http://arxiv.org/pdf/2509.01820v1,"eess.SY, cs.RO, cs.SY"
ManiFlow: A General Robot Manipulation Policy via Consistency Flow Training,"This paper introduces ManiFlow, a visuomotor imitation learning policy for
general robot manipulation that generates precise, high-dimensional actions
conditioned on diverse visual, language and proprioceptive inputs. We leverage
flow matching with consistency training to enable high-quality dexterous action
generation in just 1-2 inference steps. To handle diverse input modalities
efficiently, we propose DiT-X, a diffusion transformer architecture with
adaptive cross-attention and AdaLN-Zero conditioning that enables fine-grained
feature interactions between action tokens and multi-modal observations.
ManiFlow demonstrates consistent improvements across diverse simulation
benchmarks and nearly doubles success rates on real-world tasks across
single-arm, bimanual, and humanoid robot setups with increasing dexterity. The
extensive evaluation further demonstrates the strong robustness and
generalizability of ManiFlow to novel objects and background changes, and
highlights its strong scaling capability with larger-scale datasets. Our
website: maniflow-policy.github.io.","Ge Yan, Jiyue Zhu, Yuquan Deng, Shiqi Yang, Ri-Zhao Qiu, Xuxin Cheng, Marius Memmel, Ranjay Krishna, Ankit Goyal, Xiaolong Wang, Dieter Fox",2025-09-01 22:50:55+00:00,http://arxiv.org/abs/2509.01819v1,http://arxiv.org/pdf/2509.01819v1,cs.RO
EgoTouch: On-Body Touch Input Using AR/VR Headset Cameras,"In augmented and virtual reality (AR/VR) experiences, a user's arms and hands
can provide a convenient and tactile surface for touch input. Prior work has
shown on-body input to have significant speed, accuracy, and ergonomic benefits
over in-air interfaces, which are common today. In this work, we demonstrate
high accuracy, bare hands (i.e., no special instrumentation of the user) skin
input using just an RGB camera, like those already integrated into all modern
XR headsets. Our results show this approach can be accurate, and robust across
diverse lighting conditions, skin tones, and body motion (e.g., input while
walking). Finally, our pipeline also provides rich input metadata including
touch force, finger identification, angle of attack, and rotation. We believe
these are the requisite technical ingredients to more fully unlock on-skin
interfaces that have been well motivated in the HCI literature but have lacked
robust and practical methods.","Vimal Mollyn, Chris Harrison",2025-09-01 21:32:30+00:00,http://arxiv.org/abs/2509.01786v1,http://arxiv.org/pdf/2509.01786v1,"cs.HC, cs.CV, cs.RO"
Non-conflicting Energy Minimization in Reinforcement Learning based Robot Control,"Efficient robot control often requires balancing task performance with energy
expenditure. A common approach in reinforcement learning (RL) is to penalize
energy use directly as part of the reward function. This requires carefully
tuning weight terms to avoid undesirable trade-offs where energy minimization
harms task success. In this work, we propose a hyperparameter-free gradient
optimization method to minimize energy expenditure without conflicting with
task performance. Inspired by recent works in multitask learning, our method
applies policy gradient projection between task and energy objectives to derive
policy updates that minimize energy expenditure in ways that do not impact task
performance. We evaluate this technique on standard locomotion benchmarks of
DM-Control and HumanoidBench and demonstrate a reduction of 64% energy usage
while maintaining comparable task performance. Further, we conduct experiments
on a Unitree GO2 quadruped showcasing Sim2Real transfer of energy efficient
policies. Our method is easy to implement in standard RL pipelines with minimal
code changes, is applicable to any policy gradient method, and offers a
principled alternative to reward shaping for energy efficient control policies.","Skand Peri, Akhil Perincherry, Bikram Pandit, Stefan Lee",2025-09-01 20:52:41+00:00,http://arxiv.org/abs/2509.01765v1,http://arxiv.org/pdf/2509.01765v1,cs.RO
Fail2Progress: Learning from Real-World Robot Failures with Stein Variational Inference,"Skill effect models for long-horizon manipulation tasks are prone to failures
in conditions not covered by training data distributions. Therefore, enabling
robots to reason about and learn from failures is necessary. We investigate the
problem of efficiently generating a dataset targeted to observed failures.
After fine-tuning a skill effect model on this dataset, we evaluate the extent
to which the model can recover from failures and minimize future failures. We
propose Fail2Progress, an approach that leverages Stein variational inference
to generate multiple simulation environments in parallel, enabling efficient
data sample generation similar to observed failures. Our method is capable of
handling several challenging mobile manipulation tasks, including transporting
multiple objects, organizing a constrained shelf, and tabletop organization.
Through large-scale simulation and real-world experiments, we demonstrate that
our approach excels at learning from failures across different numbers of
objects. Furthermore, we show that Fail2Progress outperforms several baselines.","Yixuan Huang, Novella Alvina, Mohanraj Devendran Shanthi, Tucker Hermans",2025-09-01 20:00:56+00:00,http://arxiv.org/abs/2509.01746v1,http://arxiv.org/pdf/2509.01746v1,cs.RO
Constrained Decoding for Robotics Foundation Models,"Recent advances in the development of robotic foundation models have led to
promising end-to-end and general-purpose capabilities in robotic systems. These
models are pretrained on vast datasets of robot trajectories to process
multi-modal inputs and directly output a sequence of action that the system
then executes in the real world. Although this approach is attractive from the
perspective of improved generalization across diverse tasks, these models are
still data-driven and, therefore, lack explicit notions of behavioral
correctness and safety constraints. We address these limitations by introducing
a constrained decoding framework for robotics foundation models that enforces
logical constraints on action trajectories in dynamical systems. Our method
ensures that generated actions provably satisfy signal temporal logic (STL)
specifications at runtime without retraining, while remaining agnostic of the
underlying foundation model. We perform comprehensive evaluation of our
approach across state-of-the-art navigation foundation models and we show that
our decoding-time interventions are useful not only for filtering unsafe
actions but also for conditional action-generation. Videos available on our
website: https://constrained-robot-fms.github.io","Parv Kapoor, Akila Ganlath, Changliu Liu, Sebastian Scherer, Eunsuk Kang",2025-09-01 19:17:40+00:00,http://arxiv.org/abs/2509.01728v1,http://arxiv.org/pdf/2509.01728v1,"cs.RO, cs.LG, cs.LO"
Articulated Object Estimation in the Wild,"Understanding the 3D motion of articulated objects is essential in robotic
scene understanding, mobile manipulation, and motion planning. Prior methods
for articulation estimation have primarily focused on controlled settings,
assuming either fixed camera viewpoints or direct observations of various
object states, which tend to fail in more realistic unconstrained environments.
In contrast, humans effortlessly infer articulation by watching others
manipulate objects. Inspired by this, we introduce ArtiPoint, a novel
estimation framework that can infer articulated object models under dynamic
camera motion and partial observability. By combining deep point tracking with
a factor graph optimization framework, ArtiPoint robustly estimates articulated
part trajectories and articulation axes directly from raw RGB-D videos. To
foster future research in this domain, we introduce Arti4D, the first
ego-centric in-the-wild dataset that captures articulated object interactions
at a scene level, accompanied by articulation labels and ground-truth camera
poses. We benchmark ArtiPoint against a range of classical and learning-based
baselines, demonstrating its superior performance on Arti4D. We make code and
Arti4D publicly available at https://artipoint.cs.uni-freiburg.de.","Abdelrhman Werby, Martin Büchner, Adrian Röfer, Chenguang Huang, Wolfram Burgard, Abhinav Valada",2025-09-01 18:34:17+00:00,http://arxiv.org/abs/2509.01708v1,http://arxiv.org/pdf/2509.01708v1,"cs.RO, cs.CV"
MoTo: A Zero-shot Plug-in Interaction-aware Navigation for General Mobile Manipulation,"Mobile manipulation stands as a core challenge in robotics, enabling robots
to assist humans across varied tasks and dynamic daily environments.
Conventional mobile manipulation approaches often struggle to generalize across
different tasks and environments due to the lack of large-scale training.
However, recent advances in manipulation foundation models demonstrate
impressive generalization capability on a wide range of fixed-base manipulation
tasks, which are still limited to a fixed setting. Therefore, we devise a
plug-in module named MoTo, which can be combined with any off-the-shelf
manipulation foundation model to empower them with mobile manipulation ability.
Specifically, we propose an interaction-aware navigation policy to generate
robot docking points for generalized mobile manipulation. To enable zero-shot
ability, we propose an interaction keypoints framework via vision-language
models (VLM) under multi-view consistency for both target object and robotic
arm following instructions, where fixed-base manipulation foundation models can
be employed. We further propose motion planning objectives for the mobile base
and robot arm, which minimize the distance between the two keypoints and
maintain the physical feasibility of trajectories. In this way, MoTo guides the
robot to move to the docking points where fixed-base manipulation can be
successfully performed, and leverages VLM generation and trajectory
optimization to achieve mobile manipulation in a zero-shot manner, without any
requirement on mobile manipulation expert data. Extensive experimental results
on OVMM and real-world demonstrate that MoTo achieves success rates of 2.68%
and 16.67% higher than the state-of-the-art mobile manipulation methods,
respectively, without requiring additional training data.","Zhenyu Wu, Angyuan Ma, Xiuwei Xu, Hang Yin, Yinan Liang, Ziwei Wang, Jiwen Lu, Haibin Yan",2025-09-01 17:59:03+00:00,http://arxiv.org/abs/2509.01658v1,http://arxiv.org/pdf/2509.01658v1,cs.RO
Data Retrieval with Importance Weights for Few-Shot Imitation Learning,"While large-scale robot datasets have propelled recent progress in imitation
learning, learning from smaller task specific datasets remains critical for
deployment in new environments and unseen tasks. One such approach to few-shot
imitation learning is retrieval-based imitation learning, which extracts
relevant samples from large, widely available prior datasets to augment a
limited demonstration dataset. To determine the relevant data from prior
datasets, retrieval-based approaches most commonly calculate a prior data
point's minimum distance to a point in the target dataset in latent space.
While retrieval-based methods have shown success using this metric for data
selection, we demonstrate its equivalence to the limit of a Gaussian kernel
density (KDE) estimate of the target data distribution. This reveals two
shortcomings of the retrieval rule used in prior work. First, it relies on
high-variance nearest neighbor estimates that are susceptible to noise. Second,
it does not account for the distribution of prior data when retrieving data. To
address these issues, we introduce Importance Weighted Retrieval (IWR), which
estimates importance weights, or the ratio between the target and prior data
distributions for retrieval, using Gaussian KDEs. By considering the
probability ratio, IWR seeks to mitigate the bias of previous selection rules,
and by using reasonable modeling parameters, IWR effectively smooths estimates
using all data points. Across both simulation environments and real-world
evaluations on the Bridge dataset we find that our method, IWR, consistently
improves performance of existing retrieval-based methods, despite only
requiring minor modifications.","Amber Xie, Rahul Chand, Dorsa Sadigh, Joey Hejna",2025-09-01 17:58:41+00:00,http://arxiv.org/abs/2509.01657v1,http://arxiv.org/pdf/2509.01657v1,"cs.RO, cs.AI"
Speculative Design of Equitable Robotics: Queer Fictions and Futures,"This paper examines the speculative topic of equitable robots through an
exploratory essay format. It focuses specifically on robots by and for LGBTQ+
populations. It aims to provoke thought and conversations in the field about
what aspirational queer robotics futures may look like, both in the arts and
sciences. First, it briefly reviews the state-of-the-art of queer robotics in
fiction and science, drawing together threads from each. Then, it discusses
queering robots through three speculative design proposals for queer robot
roles: 1) reflecting the queerness of their ''in-group'' queer users, building
and celebrating ''in-group'' identity, 2) a new kind of queer activism by
implementing queer robot identity performance to interact with ''out-group''
users, with a goal of reducing bigotry through familiarisation, and 3) a
network of queer-owned robots, through which the community could reach each
other, and distribute and access important resources. The paper then questions
whether robots should be queered, and what ethical implications this raises.
Finally, the paper makes suggestions for what aspirational queer robotics
futures may look like, and what would be required to get there.",Minja Axelsson,2025-09-01 17:36:43+00:00,http://arxiv.org/abs/2509.01643v1,http://arxiv.org/pdf/2509.01643v1,"cs.RO, cs.CY, cs.HC, I.2.9; J.5; K.4.2"
Learning to Coordinate: Distributed Meta-Trajectory Optimization Via Differentiable ADMM-DDP,"Distributed trajectory optimization via ADMM-DDP is a powerful approach for
coordinating multi-agent systems, but it requires extensive tuning of tightly
coupled hyperparameters that jointly govern local task performance and global
coordination. In this paper, we propose Learning to Coordinate (L2C), a general
framework that meta-learns these hyperparameters, modeled by lightweight
agent-wise neural networks, to adapt across diverse tasks and agent
configurations. L2C differentiates end-to-end through the ADMM-DDP pipeline in
a distributed manner. It also enables efficient meta-gradient computation by
reusing DDP components such as Riccati recursions and feedback gains. These
gradients correspond to the optimal solutions of distributed matrix-valued LQR
problems, coordinated across agents via an auxiliary ADMM framework that
becomes convex under mild assumptions. Training is further accelerated by
truncating iterations and meta-learning ADMM penalty parameters optimized for
rapid residual reduction, with provable Lipschitz-bounded gradient errors. On a
challenging cooperative aerial transport task, L2C generates dynamically
feasible trajectories in high-fidelity simulation using IsaacSIM, reconfigures
quadrotor formations for safe 6-DoF load manipulation in tight spaces, and
adapts robustly to varying team sizes and task conditions, while achieving up
to $88\%$ faster gradient computation than state-of-the-art methods.","Bingheng Wang, Yichao Gao, Tianchen Sun, Lin Zhao",2025-09-01 17:17:05+00:00,http://arxiv.org/abs/2509.01630v2,http://arxiv.org/pdf/2509.01630v2,"cs.LG, cs.MA, cs.RO, cs.SY, eess.SY"
A Hybrid Input based Deep Reinforcement Learning for Lane Change Decision-Making of Autonomous Vehicle,"Lane change decision-making for autonomous vehicles is a complex but
high-reward behavior. In this paper, we propose a hybrid input based deep
reinforcement learning (DRL) algorithm, which realizes abstract lane change
decisions and lane change actions for autonomous vehicles within traffic flow.
Firstly, a surrounding vehicles trajectory prediction method is proposed to
reduce the risk of future behavior of surrounding vehicles to ego vehicle, and
the prediction results are input into the reinforcement learning model as
additional information. Secondly, to comprehensively leverage environmental
information, the model extracts feature from high-dimensional images and
low-dimensional sensor data simultaneously. The fusion of surrounding vehicle
trajectory prediction and multi-modal information are used as state space of
reinforcement learning to improve the rationality of lane change decision.
Finally, we integrate reinforcement learning macro decisions with end-to-end
vehicle control to achieve a holistic lane change process. Experiments were
conducted within the CARLA simulator, and the results demonstrated that the
utilization of a hybrid state space significantly enhances the safety of
vehicle lane change decisions.","Ziteng Gao, Jiaqi Qu, Chaoyu Chen",2025-09-01 16:43:50+00:00,http://arxiv.org/abs/2509.01611v1,http://arxiv.org/pdf/2509.01611v1,cs.RO
TransForSeg: A Multitask Stereo ViT for Joint Stereo Segmentation and 3D Force Estimation in Catheterization,"Recently, the emergence of multitask deep learning models has enhanced
catheterization procedures by providing tactile and visual perception data
through an end-to-end architecture. This information is derived from a
segmentation and force estimation head, which localizes the catheter in X-ray
images and estimates the applied pressure based on its deflection within the
image. These stereo vision architectures incorporate a CNN-based
encoder-decoder that captures the dependencies between X-ray images from two
viewpoints, enabling simultaneous 3D force estimation and stereo segmentation
of the catheter. With these tasks in mind, this work approaches the problem
from a new perspective. We propose a novel encoder-decoder Vision Transformer
model that processes two input X-ray images as separate sequences. Given
sequences of X-ray patches from two perspectives, the transformer captures
long-range dependencies without the need to gradually expand the receptive
field for either image. The embeddings generated by both the encoder and
decoder are fed into two shared segmentation heads, while a regression head
employs the fused information from the decoder for 3D force estimation. The
proposed model is a stereo Vision Transformer capable of simultaneously
segmenting the catheter from two angles while estimating the generated forces
at its tip in 3D. This model has undergone extensive experiments on synthetic
X-ray images with various noise levels and has been compared against
state-of-the-art pure segmentation models, vision-based catheter force
estimation methods, and a multitask catheter segmentation and force estimation
approach. It outperforms existing models, setting a new state-of-the-art in
both catheter segmentation and force estimation.","Pedram Fekri, Mehrdad Zadeh, Javad Dargahi",2025-09-01 16:36:23+00:00,http://arxiv.org/abs/2509.01605v1,http://arxiv.org/pdf/2509.01605v1,"cs.CV, cs.AI, cs.LG, cs.RO"
Aleatoric Uncertainty from AI-based 6D Object Pose Predictors for Object-relative State Estimation,"Deep Learning (DL) has become essential in various robotics applications due
to excelling at processing raw sensory data to extract task specific
information from semantic objects. For example, vision-based object-relative
navigation relies on a DL-based 6D object pose predictor to provide the
relative pose between the object and the robot as measurements to the robot's
state estimator. Accurately knowing the uncertainty inherent in such Deep
Neural Network (DNN) based measurements is essential for probabilistic state
estimators subsequently guiding the robot's tasks. Thus, in this letter, we
show that we can extend any existing DL-based object-relative pose predictor
for aleatoric uncertainty inference simply by including two multi-layer
perceptrons detached from the translational and rotational part of the DL
predictor. This allows for efficient training while freezing the existing
pre-trained predictor. We then use the inferred 6D pose and its uncertainty as
a measurement and corresponding noise covariance matrix in an extended Kalman
filter (EKF). Our approach induces minimal computational overhead such that the
state estimator can be deployed on edge devices while benefiting from the
dynamically inferred measurement uncertainty. This increases the performance of
the object-relative state estimation task compared to a fix-covariance
approach. We conduct evaluations on synthetic data and real-world data to
underline the benefits of aleatoric uncertainty inference for the
object-relative state estimation task.","Thomas Jantos, Stephan Weiss, Jan Steinbrener",2025-09-01 16:12:10+00:00,http://arxiv.org/abs/2509.01583v1,http://arxiv.org/pdf/2509.01583v1,"cs.RO, cs.CV"
Quantum game models for interaction-aware decision-making in automated driving,"Decision-making in automated driving must consider interactions with
surrounding agents to be effective. However, traditional methods often neglect
or oversimplify these interactions because they are difficult to model and
solve, which can lead to overly conservative behavior of the ego vehicle. To
address this gap, we propose two quantum game models, QG-U1 (Quantum Game -
Unitary 1) and QG-G4 (Quantum Game - Gates 4), for interaction-aware
decision-making. These models extend classical game theory by incorporating
principles of quantum mechanics, such as superposition, interference, and
entanglement. Specifically, QG-U1 and QG-G4 are designed for two-player games
with two strategies per player and can be executed in real time on a standard
computer without requiring quantum hardware. We evaluate both models in merging
and roundabout scenarios and compare them with classical game-theoretic methods
and baseline approaches (IDM, MOBIL, and a utility-based technique). Results
show that QG-G4 achieves lower collision rates and higher success rates
compared to baseline methods, while both quantum models yield higher expected
payoffs than classical game approaches under certain parameter settings.","Karim Essalmi, Fernando Garrido, Fawzi Nashashibi",2025-09-01 16:12:02+00:00,http://arxiv.org/abs/2509.01582v1,http://arxiv.org/pdf/2509.01582v1,"cs.GT, cs.RO"
FGO-SLAM: Enhancing Gaussian SLAM with Globally Consistent Opacity Radiance Field,"Visual SLAM has regained attention due to its ability to provide perceptual
capabilities and simulation test data for Embodied AI. However, traditional
SLAM methods struggle to meet the demands of high-quality scene reconstruction,
and Gaussian SLAM systems, despite their rapid rendering and high-quality
mapping capabilities, lack effective pose optimization methods and face
challenges in geometric reconstruction. To address these issues, we introduce
FGO-SLAM, a Gaussian SLAM system that employs an opacity radiance field as the
scene representation to enhance geometric mapping performance. After initial
pose estimation, we apply global adjustment to optimize camera poses and sparse
point cloud, ensuring robust tracking of our approach. Additionally, we
maintain a globally consistent opacity radiance field based on 3D Gaussians and
introduce depth distortion and normal consistency terms to refine the scene
representation. Furthermore, after constructing tetrahedral grids, we identify
level sets to directly extract surfaces from 3D Gaussians. Results across
various real-world and large-scale synthetic datasets demonstrate that our
method achieves state-of-the-art tracking accuracy and mapping performance.","Fan Zhu, Yifan Zhao, Ziyu Chen, Biao Yu, Hui Zhu",2025-09-01 15:20:41+00:00,http://arxiv.org/abs/2509.01547v1,http://arxiv.org/pdf/2509.01547v1,cs.RO
Plantbot: Integrating Plant and Robot through LLM Modular Agent Networks,"We introduce Plantbot, a hybrid lifeform that connects a living plant with a
mobile robot through a network of large language model (LLM) modules. Each
module - responsible for sensing, vision, dialogue, or action - operates
asynchronously and communicates via natural language, enabling seamless
interaction across biological and artificial domains. This architecture
leverages the capacity of LLMs to serve as hybrid interfaces, where natural
language functions as a universal protocol, translating multimodal data (soil
moisture, temperature, visual context) into linguistic messages that coordinate
system behaviors. The integrated network transforms plant states into robotic
actions, installing normativity essential for agency within the sensor-motor
loop. By combining biological and robotic elements through LLM-mediated
communication, Plantbot behaves as an embodied, adaptive agent capable of
responding autonomously to environmental conditions. This approach suggests
possibilities for a new model of artificial life, where decentralized, LLM
modules coordination enable novel interactions between biological and
artificial systems.","Atsushi Masumori, Norihiro Maruyama, Itsuki Doi, johnsmith, Hiroki Sato, Takashi Ikegami",2025-09-01 14:12:28+00:00,http://arxiv.org/abs/2509.05338v1,http://arxiv.org/pdf/2509.05338v1,"cs.RO, cs.AI"
Who Owns The Robot?: Four Ethical and Socio-technical Questions about Wellbeing Robots in the Real World through Community Engagement,"Recent studies indicate that robotic coaches can play a crucial role in
promoting wellbeing. However, the real-world deployment of wellbeing robots
raises numerous ethical and socio-technical questions and concerns. To explore
these questions, we undertake a community-centered investigation to examine
three different communities' perspectives on using robotic wellbeing coaches in
real-world environments. We frame our work as an anticipatory ethical
investigation, which we undertake to better inform the development of robotic
technologies with communities' opinions, with the ultimate goal of aligning
robot development with public interest. We conducted workshops with three
communities who are under-represented in robotics development: 1) members of
the public at a science festival, 2) women computer scientists at a conference,
and 3) humanities researchers interested in history and philosophy of science.
In the workshops, we collected qualitative data using the Social Robot
Co-Design Canvas on Ethics. We analysed the collected qualitative data with
Thematic Analysis, informed by notes taken during workshops. Through our
analysis, we identify four themes regarding key ethical and socio-technical
questions about the real-world use of wellbeing robots. We group participants'
insights and discussions around these broad thematic questions, discuss them in
light of state-of-the-art literature, and highlight areas for future
investigation. Finally, we provide the four questions as a broad framework that
roboticists can and should use during robotic development and deployment, in
order to reflect on the ethics and socio-technical dimensions of their robotic
applications, and to engage in dialogue with communities of robot users. The
four questions are: 1) Is the robot safe and how can we know that?, 2) Who is
the robot built for and with?, 3) Who owns the robot and the data?, and 4) Why
a robot?.","Minja Axelsson, Jiaee Cheong, Rune Nyrup, Hatice Gunes",2025-09-01 13:38:50+00:00,http://arxiv.org/abs/2509.02624v1,http://arxiv.org/pdf/2509.02624v1,"cs.CY, cs.AI, cs.HC, cs.RO, I.2.9; K.4.2; K.4.1"
Analyzing Reluctance to Ask for Help When Cooperating With Robots: Insights to Integrate Artificial Agents in HRC,"As robot technology advances, collaboration between humans and robots will
become more prevalent in industrial tasks. When humans run into issues in such
scenarios, a likely future involves relying on artificial agents or robots for
aid. This study identifies key aspects for the design of future user-assisting
agents. We analyze quantitative and qualitative data from a user study
examining the impact of on-demand assistance received from a remote human in a
human-robot collaboration (HRC) assembly task. We study scenarios in which
users require help and we assess their experiences in requesting and receiving
assistance. Additionally, we investigate participants' perceptions of future
non-human assisting agents and whether assistance should be on-demand or
unsolicited. Through a user study, we analyze the impact that such design
decisions (human or artificial assistant, on-demand or unsolicited help) can
have on elicited emotional responses, productivity, and preferences of humans
engaged in HRC tasks.","Ane San Martin, Michael Hagenow, Julie Shah, Johan Kildal, Elena Lazkano",2025-09-01 13:08:09+00:00,http://arxiv.org/abs/2509.01450v1,http://arxiv.org/pdf/2509.01450v1,"cs.RO, cs.HC"
Anticipatory Fall Detection in Humans with Hybrid Directed Graph Neural Networks and Long Short-Term Memory,"Detecting and preventing falls in humans is a critical component of assistive
robotic systems. While significant progress has been made in detecting falls,
the prediction of falls before they happen, and analysis of the transient state
between stability and an impending fall remain unexplored. In this paper, we
propose a anticipatory fall detection method that utilizes a hybrid model
combining Dynamic Graph Neural Networks (DGNN) with Long Short-Term Memory
(LSTM) networks that decoupled the motion prediction and gait classification
tasks to anticipate falls with high accuracy. Our approach employs real-time
skeletal features extracted from video sequences as input for the proposed
model. The DGNN acts as a classifier, distinguishing between three gait states:
stable, transient, and fall. The LSTM-based network then predicts human
movement in subsequent time steps, enabling early detection of falls. The
proposed model was trained and validated using the OUMVLP-Pose and URFD
datasets, demonstrating superior performance in terms of prediction error and
recognition accuracy compared to models relying solely on DGNN and models from
literature. The results indicate that decoupling prediction and classification
improves performance compared to addressing the unified problem using only the
DGNN. Furthermore, our method allows for the monitoring of the transient state,
offering valuable insights that could enhance the functionality of advanced
assistance systems.","Younggeol Cho, Gokhan Solak, Olivia Nocentini, Marta Lorenzini, Andrea Fortuna, Arash Ajoudani",2025-09-01 12:56:31+00:00,http://arxiv.org/abs/2509.05337v1,http://arxiv.org/pdf/2509.05337v1,"cs.CV, cs.RO"
End-to-End Low-Level Neural Control of an Industrial-Grade 6D Magnetic Levitation System,"Magnetic levitation is poised to revolutionize industrial automation by
integrating flexible in-machine product transport and seamless manipulation. It
is expected to become the standard drive for automated manufacturing. However,
controlling such systems is inherently challenging due to their complex,
unstable dynamics. Traditional control approaches, which rely on hand-crafted
control engineering, typically yield robust but conservative solutions, with
their performance closely tied to the expertise of the engineering team. In
contrast, neural control learning presents a promising alternative. This paper
presents the first neural controller for 6D magnetic levitation. Trained
end-to-end on interaction data from a proprietary controller, it directly maps
raw sensor data and 6D reference poses to coil current commands. The neural
controller can effectively generalize to previously unseen situations while
maintaining accurate and robust control. These results underscore the practical
feasibility of learning-based neural control in complex physical systems and
suggest a future where such a paradigm could enhance or even substitute
traditional engineering approaches in demanding real-world applications. The
trained neural controller, source code, and demonstration videos are publicly
available at https://sites.google.com/view/neural-maglev.","Philipp Hartmann, Jannick Stranghöner, Klaus Neumann",2025-09-01 11:33:30+00:00,http://arxiv.org/abs/2509.01388v1,http://arxiv.org/pdf/2509.01388v1,"eess.SY, cs.AI, cs.RO, cs.SY, I.2.9; I.2.8; I.2.6; D.4.7; C.3; J.7"
TopoNav: Topological Graphs as a Key Enabler for Advanced Object Navigation,"Object Navigation (ObjectNav) has made great progress with large language
models (LLMs), but still faces challenges in memory management, especially in
long-horizon tasks and dynamic scenes. To address this, we propose TopoNav, a
new framework that leverages topological structures as spatial memory. By
building and updating a topological graph that captures scene connections,
adjacency, and semantic meaning, TopoNav helps agents accumulate spatial
knowledge over time, retrieve key information, and reason effectively toward
distant goals. Our experiments show that TopoNav achieves state-of-the-art
performance on benchmark ObjectNav datasets, with higher success rates and more
efficient paths. It particularly excels in diverse and complex environments, as
it connects temporary visual inputs with lasting spatial understanding.","Peiran Liu, Qiang Zhang, Daojie Peng, Lingfeng Zhang, Yihao Qin, Hang Zhou, Jun Ma, Renjing Xu, Yiding Ji",2025-09-01 11:05:38+00:00,http://arxiv.org/abs/2509.01364v1,http://arxiv.org/pdf/2509.01364v1,cs.RO
Disentangled Multi-Context Meta-Learning: Unlocking robust and Generalized Task Learning,"In meta-learning and its downstream tasks, many methods rely on implicit
adaptation to task variations, where multiple factors are mixed together in a
single entangled representation. This makes it difficult to interpret which
factors drive performance and can hinder generalization. In this work, we
introduce a disentangled multi-context meta-learning framework that explicitly
assigns each task factor to a distinct context vector. By decoupling these
variations, our approach improves robustness through deeper task understanding
and enhances generalization by enabling context vector sharing across tasks
with shared factors. We evaluate our approach in two domains. First, on a
sinusoidal regression task, our model outperforms baselines on
out-of-distribution tasks and generalizes to unseen sine functions by sharing
context vectors associated with shared amplitudes or phase shifts. Second, in a
quadruped robot locomotion task, we disentangle the robot-specific properties
and the characteristics of the terrain in the robot dynamics model. By
transferring disentangled context vectors acquired from the dynamics model into
reinforcement learning, the resulting policy achieves improved robustness under
out-of-distribution conditions, surpassing the baselines that rely on a single
unified context. Furthermore, by effectively sharing context, our model enables
successful sim-to-real policy transfer to challenging terrains with
out-of-distribution robot-specific properties, using just 20 seconds of real
data from flat terrain, a result not achievable with single-task adaptation.","Seonsoo Kim, Jun-Gill Kang, Taehong Kim, Seongil Hong",2025-09-01 09:33:52+00:00,http://arxiv.org/abs/2509.01297v1,http://arxiv.org/pdf/2509.01297v1,cs.RO
Metamorphic Testing of Multimodal Human Trajectory Prediction,"Context: Predicting human trajectories is crucial for the safety and
reliability of autonomous systems, such as automated vehicles and mobile
robots. However, rigorously testing the underlying multimodal Human Trajectory
Prediction (HTP) models, which typically use multiple input sources (e.g.,
trajectory history and environment maps) and produce stochastic outputs
(multiple possible future paths), presents significant challenges. The primary
difficulty lies in the absence of a definitive test oracle, as numerous future
trajectories might be plausible for any given scenario. Objectives: This
research presents the application of Metamorphic Testing (MT) as a systematic
methodology for testing multimodal HTP systems. We address the oracle problem
through metamorphic relations (MRs) adapted for the complexities and stochastic
nature of HTP. Methods: We present five MRs, targeting transformations of both
historical trajectory data and semantic segmentation maps used as an
environmental context. These MRs encompass: 1) label-preserving geometric
transformations (mirroring, rotation, rescaling) applied to both trajectory and
map inputs, where outputs are expected to transform correspondingly. 2)
Map-altering transformations (changing semantic class labels, introducing
obstacles) with predictable changes in trajectory distributions. We propose
probabilistic violation criteria based on distance metrics between probability
distributions, such as the Wasserstein or Hellinger distance. Conclusion: This
study introduces tool, a MT framework for the oracle-less testing of
multimodal, stochastic HTP systems. It allows for assessment of model
robustness against input transformations and contextual changes without
reliance on ground-truth trajectories.","Helge Spieker, Nadjib Lazaar, Arnaud Gotlieb, Nassim Belmecheri",2025-09-01 09:30:35+00:00,http://arxiv.org/abs/2509.01294v1,http://arxiv.org/pdf/2509.01294v1,"cs.SE, cs.RO"
Toward a Holistic Multi-Criteria Trajectory Evaluation Framework for Autonomous Driving in Mixed Traffic Environment,"This paper presents a unified framework for the evaluation and optimization
of autonomous vehicle trajectories, integrating formal safety, comfort, and
efficiency criteria. An innovative geometric indicator, based on the analysis
of safety zones using adaptive ellipses, is used to accurately quantify
collision risks. Our method applies the Shoelace formula to compute the
intersection area in the case of misaligned and time-varying configurations.
Comfort is modeled using indicators centered on longitudinal and lateral jerk,
while efficiency is assessed by overall travel time. These criteria are
aggregated into a comprehensive objective function solved using a PSO based
algorithm. The approach was successfully validated under real traffic
conditions via experiments conducted in an urban intersection involving an
autonomous vehicle interacting with a human-operated vehicle, and in simulation
using data recorded from human driving in real traffic.","Nouhed Naidja, Stéphane Font, Marc Revilloud, Guillaume Sandou",2025-09-01 09:22:07+00:00,http://arxiv.org/abs/2509.01291v1,http://arxiv.org/pdf/2509.01291v1,cs.RO
Towards Data-Driven Metrics for Social Robot Navigation Benchmarking,"This paper presents a joint effort towards the development of a data-driven
Social Robot Navigation metric to facilitate benchmarking and policy
optimization. We provide our motivations for our approach and describe our
proposal for storing rated social navigation trajectory datasets. Following
these guidelines, we compiled a dataset with 4427 trajectories -- 182 real and
4245 simulated -- and presented it to human raters, yielding a total of 4402
rated trajectories after data quality assurance. We also trained an RNN-based
baseline metric on the dataset and present quantitative and qualitative
results. All data, software, and model weights are publicly available.","Pilar Bachiller-Burgos, Ulysses Bernardet, Luis V. Calderita, Pranup Chhetri, Anthony Francis, Noriaki Hirose, Noé Pérez, Dhruv Shah, Phani T. Singamaneni, Xuesu Xiao, Luis J. Manso",2025-09-01 08:42:28+00:00,http://arxiv.org/abs/2509.01251v1,http://arxiv.org/pdf/2509.01251v1,cs.RO
An AI-Based Shopping Assistant System to Support the Visually Impaired,"Shopping plays a significant role in shaping consumer identity and social
integration. However, for individuals with visual impairments, navigating in
supermarkets and identifying products can be an overwhelming and challenging
experience. This paper presents an AI-based shopping assistant prototype
designed to enhance the autonomy and inclusivity of visually impaired
individuals in supermarket environments. The system integrates multiple
technologies, including computer vision, speech recognition, text-to-speech
synthesis, and indoor navigation, into a single, user-friendly platform. Using
cameras for ArUco marker detection and real-time environmental scanning, the
system helps users navigate the store, identify product locations, provide
real-time auditory guidance, and gain context about their surroundings. The
assistant interacts with the user through voice commands and multimodal
feedback, promoting a more dynamic and engaging shopping experience. The system
was evaluated through experiments, which demonstrated its ability to guide
users effectively and improve their shopping experience. This paper contributes
to the development of inclusive AI-driven assistive technologies aimed at
enhancing accessibility and user independence for the shopping experience.","Larissa R. de S. Shibata, Ankit A. Ravankar, Jose Victorio Salazar Luces, Yasuhisa Hirata",2025-09-01 08:38:54+00:00,http://arxiv.org/abs/2509.01246v1,http://arxiv.org/pdf/2509.01246v1,"cs.HC, cs.RO"
OpenMulti: Open-Vocabulary Instance-Level Multi-Agent Distributed Implicit Mapping,"Multi-agent distributed collaborative mapping provides comprehensive and
efficient representations for robots. However, existing approaches lack
instance-level awareness and semantic understanding of environments, limiting
their effectiveness for downstream applications. To address this issue, we
propose OpenMulti, an open-vocabulary instance-level multi-agent distributed
implicit mapping framework. Specifically, we introduce a Cross-Agent Instance
Alignment module, which constructs an Instance Collaborative Graph to ensure
consistent instance understanding across agents. To alleviate the degradation
of mapping accuracy due to the blind-zone optimization trap, we leverage Cross
Rendering Supervision to enhance distributed learning of the scene.
Experimental results show that OpenMulti outperforms related algorithms in both
fine-grained geometric accuracy and zero-shot semantic accuracy. In addition,
OpenMulti supports instance-level retrieval tasks, delivering semantic
annotations for downstream applications. The project website of OpenMulti is
publicly available at https://openmulti666.github.io/.","Jianyu Dou, Yinan Deng, Jiahui Wang, Xingsi Tang, Yi Yang, Yufeng Yue",2025-09-01 08:15:53+00:00,http://arxiv.org/abs/2509.01228v1,http://arxiv.org/pdf/2509.01228v1,cs.RO
"Novel bio-inspired soft actuators for upper-limb exoskeletons: design, fabrication and feasibility study","Soft robots have been increasingly utilized as sophisticated tools in
physical rehabilitation, particularly for assisting patients with neuromotor
impairments. However, many soft robotics for rehabilitation applications are
characterized by limitations such as slow response times, restricted range of
motion, and low output force. There are also limited studies on the precise
position and force control of wearable soft actuators. Furthermore, not many
studies articulate how bellow-structured actuator designs quantitatively
contribute to the robots' capability. This study introduces a paradigm of upper
limb soft actuator design. This paradigm comprises two actuators: the
Lobster-Inspired Silicone Pneumatic Robot (LISPER) for the elbow and the
Scallop-Shaped Pneumatic Robot (SCASPER) for the shoulder. LISPER is
characterized by higher bandwidth, increased output force/torque, and high
linearity. SCASPER is characterized by high output force/torque and simplified
fabrication processes. Comprehensive analytical models that describe the
relationship between pressure, bending angles, and output force for both
actuators were presented so the geometric configuration of the actuators can be
set to modify the range of motion and output forces. The preliminary test on a
dummy arm is conducted to test the capability of the actuators.","Haiyun Zhang, Gabrielle Naquila, Jung Hyun Bae, Zonghuan Wu, Ashwin Hingwe, Ashish Deshpande",2025-09-01 05:48:49+00:00,http://arxiv.org/abs/2509.01145v1,http://arxiv.org/pdf/2509.01145v1,cs.RO
A novel parameter estimation method for pneumatic soft hand control applying logarithmic decrement for pseudo rigid body modeling,"The rapid advancement in physical human-robot interaction (HRI) has
accelerated the development of soft robot designs and controllers. Controlling
soft robots, especially soft hand grasping, is challenging due to their
continuous deformation, motivating the use of reduced model-based controllers
for real-time dynamic performance. Most existing models, however, suffer from
computational inefficiency and complex parameter identification, limiting their
real-time applicability. To address this, we propose a paradigm coupling
Pseudo-Rigid Body Modeling with the Logarithmic Decrement Method for parameter
estimation (PRBM plus LDM). Using a soft robotic hand test bed, we validate
PRBM plus LDM for predicting position and force output from pressure input and
benchmark its performance. We then implement PRBM plus LDM as the basis for
closed-loop position and force controllers. Compared to a simple PID
controller, the PRBM plus LDM position controller achieves lower error (average
maximum error across all fingers: 4.37 degrees versus 20.38 degrees). For force
control, PRBM plus LDM outperforms constant pressure grasping in pinching tasks
on delicate objects: potato chip 86 versus 82.5, screwdriver 74.42 versus 70,
brass coin 64.75 versus 35. These results demonstrate PRBM plus LDM as a
computationally efficient and accurate modeling technique for soft actuators,
enabling stable and flexible grasping with precise force regulation.","Haiyun Zhang, Kelvin HoLam Heung, Gabrielle J. Naquila, Ashwin Hingwe, Ashish D. Deshpande",2025-09-01 04:15:25+00:00,http://arxiv.org/abs/2509.01113v1,http://arxiv.org/pdf/2509.01113v1,cs.RO
SR-SLAM: Scene-reliability Based RGB-D SLAM in Diverse Environments,"Visual simultaneous localization and mapping (SLAM) plays a critical role in
autonomous robotic systems, especially where accurate and reliable measurements
are essential for navigation and sensing. In feature-based SLAM, the
quantityand quality of extracted features significantly influence system
performance. Due to the variations in feature quantity and quality across
diverse environments, current approaches face two major challenges: (1) limited
adaptability in dynamic feature culling and pose estimation, and (2)
insufficient environmental awareness in assessment and optimization strategies.
To address these issues, we propose SRR-SLAM, a scene-reliability based
framework that enhances feature-based SLAM through environment-aware
processing. Our method introduces a unified scene reliability assessment
mechanism that incorporates multiple metrics and historical observations to
guide system behavior. Based on this assessment, we develop: (i) adaptive
dynamic region selection with flexible geometric constraints, (ii)
depth-assisted self-adjusting clustering for efficient dynamic feature removal
in high-dimensional settings, and (iii) reliability-aware pose refinement that
dynamically integrates direct methods when features are insufficient.
Furthermore, we propose (iv) reliability-based keyframe selection and a
weighted optimization scheme to reduce computational overhead while improving
estimation accuracy. Extensive experiments on public datasets and real world
scenarios show that SRR-SLAM outperforms state-of-the-art dynamic SLAM methods,
achieving up to 90% improvement in accuracy and robustness across diverse
environments. These improvements directly contribute to enhanced measurement
precision and reliability in autonomous robotic sensing systems.","Haolan Zhang, Chenghao Li, Thanh Nguyen Canh, Lijun Wang, Nak Young Chong",2025-09-01 04:09:00+00:00,http://arxiv.org/abs/2509.01111v1,http://arxiv.org/pdf/2509.01111v1,cs.RO
"Robix: A Unified Model for Robot Interaction, Reasoning and Planning","We introduce Robix, a unified model that integrates robot reasoning, task
planning, and natural language interaction within a single vision-language
architecture. Acting as the high-level cognitive layer in a hierarchical robot
system, Robix dynamically generates atomic commands for the low-level
controller and verbal responses for human interaction, enabling robots to
follow complex instructions, plan long-horizon tasks, and interact naturally
with human within an end-to-end framework. Robix further introduces novel
capabilities such as proactive dialogue, real-time interruption handling, and
context-aware commonsense reasoning during task execution. At its core, Robix
leverages chain-of-thought reasoning and adopts a three-stage training
strategy: (1) continued pretraining to enhance foundational embodied reasoning
abilities including 3D spatial understanding, visual grounding, and
task-centric reasoning; (2) supervised finetuning to model human-robot
interaction and task planning as a unified reasoning-action sequence; and (3)
reinforcement learning to improve reasoning-action consistency and long-horizon
task coherence. Extensive experiments demonstrate that Robix outperforms both
open-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in
interactive task execution, demonstrating strong generalization across diverse
instruction types (e.g., open-ended, multi-stage, constrained, invalid, and
interrupted) and various user-involved tasks such as table bussing, grocery
shopping, and dietary filtering.","Huang Fang, Mengxi Zhang, Heng Dong, Wei Li, Zixuan Wang, Qifeng Zhang, Xueyun Tian, Yucheng Hu, Hang Li",2025-09-01 03:53:47+00:00,http://arxiv.org/abs/2509.01106v1,http://arxiv.org/pdf/2509.01106v1,"cs.AI, cs.CV, cs.RO"
Model Predictive Control for a Soft Robotic Finger with Stochastic Behavior based on Fokker-Planck Equation,"The inherent flexibility of soft robots offers numerous advantages, such as
enhanced adaptability and improved safety. However, this flexibility can also
introduce challenges regarding highly uncertain and nonlinear motion. These
challenges become particularly problematic when using open-loop control
methods, which lack a feedback mechanism and are commonly employed in soft
robot control. Though one potential solution is model-based control, typical
deterministic models struggle with uncertainty as mentioned above. The idea is
to use the Fokker-Planck Equation (FPE), a master equation of a stochastic
process, to control not the state of soft robots but the probabilistic
distribution. In this study, we propose and implement a stochastic-based
control strategy, termed FPE-based Model Predictive Control (FPE-MPC), for a
soft robotic finger. Two numerical simulation case studies examine the
performance and characteristics of this control method, revealing its efficacy
in managing the uncertainty inherent in soft robotic systems.","Sumitaka Honji, Takahiro Wada",2025-09-01 02:12:06+00:00,http://arxiv.org/abs/2509.01065v1,http://arxiv.org/pdf/2509.01065v1,cs.RO
A Reactive Grasping Framework for Multi-DoF Grippers via Task Space Velocity Fields and Joint Space QP,"We present a fast and reactive grasping framework for multi-DoF grippers that
combines task-space velocity fields with a joint-space Quadratic Program (QP)
in a hierarchical structure. Reactive, collision-free global motion planning is
particularly challenging for high-DoF systems, since simultaneous increases in
state dimensionality and planning horizon trigger a combinatorial explosion of
the search space, making real-time planning intractable. To address this, we
plan globally in a lower-dimensional task space, such as fingertip positions,
and track locally in the full joint space while enforcing all constraints. This
approach is realized by constructing velocity fields in multiple task-space
coordinates (or in some cases a subset of joint coordinates) and solving a
weighted joint-space QP to compute joint velocities that track these fields
with appropriately assigned priorities. Through simulation experiments with
privileged knowledge and real-world tests using the recent pose-tracking
algorithm FoundationPose, we verify that our method enables high-DoF arm-hand
systems to perform real-time, collision-free reaching motions while adapting to
dynamic environments and external disturbances.","Yonghyeon Lee, Tzu-Yuan Lin, Alexander Alexiev, Sangbae Kim",2025-09-01 00:52:01+00:00,http://arxiv.org/abs/2509.01044v1,http://arxiv.org/pdf/2509.01044v1,cs.RO
TARA: A Low-Cost 3D-Printed Robotic Arm for Accessible Robotics Education,"The high cost of robotic platforms limits students' ability to gain practical
skills directly applicable in real-world scenarios. To address this challenge,
this paper presents TARA, a low-cost, 3D-printed robotic arm designed for
accessible robotics education. TARA includes an open-source repository with
design files, assembly instructions, and baseline code, enabling users to build
and customize the platform. The system balances affordability and
functionality, offering a highly capable robotic arm for approximately 200 USD,
significantly lower than industrial systems that often cost thousands of
dollars. Experimental validation confirmed accurate performance in basic
manipulation tasks. Rather than focusing on performance benchmarking, this work
prioritizes educational reproducibility, providing a platform that students and
educators can reliably replicate and extend.",Thays Leach Mitre,2025-09-01 00:50:20+00:00,http://arxiv.org/abs/2509.01043v1,http://arxiv.org/pdf/2509.01043v1,cs.RO
Symbolic Planning and Multi-Agent Path Finding in Extremely Dense Environments with Movable Obstacles,"We introduce the Block Rearrangement Problem (BRaP), a challenging component
of large warehouse management which involves rearranging storage blocks within
dense grids to achieve a target state. We formally define the BRaP as a graph
search problem. Building on intuitions from sliding puzzle problems, we propose
five search-based solution algorithms, leveraging joint configuration space
search, classical planning, multi-agent pathfinding, and expert heuristics. We
evaluate the five approaches empirically for plan quality and scalability.
Despite the exponential relation between search space size and block number,
our methods demonstrate efficiency in creating rearrangement plans for deeply
buried blocks in up to 80x80 grids.","Bo Fu, Zhe Chen, Rahul Chandan, Alex Barbosa, Michael Caldara, Joey Durham, Federico Pecora",2025-08-31 23:27:27+00:00,http://arxiv.org/abs/2509.01022v1,http://arxiv.org/pdf/2509.01022v1,"cs.AI, cs.MA, cs.RO, 93A16 93A16"
AI-driven Dispensing of Coral Reseeding Devices for Broad-scale Restoration of the Great Barrier Reef,"Coral reefs are on the brink of collapse, with climate change, ocean
acidification, and pollution leading to a projected 70-90% loss of coral
species within the next decade. Restoration efforts are crucial, but their
success hinges on introducing automation to upscale efforts. We present
automated deployment of coral re-seeding devices powered by artificial
intelligence, computer vision, and robotics. Specifically, we perform automated
substrate classification, enabling detection of areas of the seafloor suitable
for coral growth, thus significantly reducing reliance on human experts and
increasing the range and efficiency of restoration. Real-world testing of the
algorithms on the Great Barrier Reef leads to deployment accuracy of 77.8%,
sub-image patch classification of 89.1%, and real-time model inference at 5.5
frames per second. Further, we present and publicly contribute a large
collection of annotated substrate image data to foster future research in this
area.","Scarlett Raine, Benjamin Moshirian, Tobias Fischer",2025-08-31 23:09:51+00:00,http://arxiv.org/abs/2509.01019v1,http://arxiv.org/pdf/2509.01019v1,"cs.CV, cs.LG, cs.RO"
A Robust Numerical Method for Solving Trigonometric Equations in Robotic Kinematics,"This paper presents a robust numerical method for solving systems of
trigonometric equations commonly encountered in robotic kinematics. Our
approach employs polynomial substitution techniques combined with eigenvalue
decomposition to handle singular matrices and edge cases effectively. The
method demonstrates superior numerical stability compared to traditional
approaches and has been implemented as an open-source Python package. For
non-singular matrices, we employ Weierstrass substitution to transform the
system into a quartic polynomial, ensuring all analytical solutions are found.
For singular matrices, we develop specialized geometric constraint methods
using SVD analysis. The solver demonstrates machine precision accuracy ($<
10^{-15}$ error) with 100\% success rate on extensive test cases, making it
particularly valuable for robotics applications such as inverse kinematics
problems.",Hai-Jun Su,2025-08-31 22:13:42+00:00,http://arxiv.org/abs/2509.01010v1,http://arxiv.org/pdf/2509.01010v1,"cs.RO, math.AG"
Enhanced Mean Field Game for Interactive Decision-Making with Varied Stylish Multi-Vehicles,"This paper presents an MFG-based decision-making framework for autonomous
driving in heterogeneous traffic. To capture diverse human behaviors, we
propose a quantitative driving style representation that maps abstract traits
to parameters such as speed, safety factors, and reaction time. These
parameters are embedded into the MFG through a spatial influence field model.
To ensure safe operation in dense traffic, we introduce a safety-critical
lane-changing algorithm that leverages dynamic safety margins,
time-to-collision analysis, and multi-layered constraints. Real-world NGSIM
data is employed for style calibration and empirical validation. Experimental
results demonstrate zero collisions across six style combinations, two
15-vehicle scenarios, and NGSIM-based trials, consistently outperforming
conventional game-theoretic baselines. Overall, our approach provides a
scalable, interpretable, and behavior-aware planning framework for real-world
autonomous driving applications.","Liancheng Zheng, Zhen Tian, Yangfan He, Shuo Liu, Huilin Chen, Fujiang Yuan, Yanhong Peng",2025-08-31 20:24:53+00:00,http://arxiv.org/abs/2509.00981v2,http://arxiv.org/pdf/2509.00981v2,cs.RO
One-Step Model Predictive Path Integral for Manipulator Motion Planning Using Configuration Space Distance Fields,"Motion planning for robotic manipulators is a fundamental problem in
robotics. Classical optimization-based methods typically rely on the gradients
of signed distance fields (SDFs) to impose collision-avoidance constraints.
However, these methods are susceptible to local minima and may fail when the
SDF gradients vanish. Recently, Configuration Space Distance Fields (CDFs) have
been introduced, which directly model distances in the robot's configuration
space. Unlike workspace SDFs, CDFs are differentiable almost everywhere and
thus provide reliable gradient information. On the other hand, gradient-free
approaches such as Model Predictive Path Integral (MPPI) control leverage
long-horizon rollouts to achieve collision avoidance. While effective, these
methods are computationally expensive due to the large number of trajectory
samples, repeated collision checks, and the difficulty of designing cost
functions with heterogeneous physical units. In this paper, we propose a
framework that integrates CDFs with MPPI to enable direct navigation in the
robot's configuration space. Leveraging CDF gradients, we unify the MPPI cost
in joint-space and reduce the horizon to one step, substantially cutting
computation while preserving collision avoidance in practice. We demonstrate
that our approach achieves nearly 100% success rates in 2D environments and
consistently high success rates in challenging 7-DOF Franka manipulator
simulations with complex obstacles. Furthermore, our method attains control
frequencies exceeding 750 Hz, substantially outperforming both
optimization-based and standard MPPI baselines. These results highlight the
effectiveness and efficiency of the proposed CDF-MPPI framework for
high-dimensional motion planning.","Yulin Li, Tetsuro Miyazaki, Kenji Kawashima",2025-08-31 13:09:24+00:00,http://arxiv.org/abs/2509.00836v1,http://arxiv.org/pdf/2509.00836v1,cs.RO
An Effective Trajectory Planning and an Optimized Path Planning for a 6-Degree-of-Freedom Robot Manipulator,"An effective method for optimizing path planning for a specific model of a
6-degree-of-freedom (6-DOF) robot manipulator is presented as part of the
motion planning of the manipulator using computer algebra. We assume that we
are given a path in the form of a set of line segments that the end-effector
should follow. We also assume that we have a method to solve the inverse
kinematic problem of the manipulator at each via-point of the trajectory. The
proposed method consists of three steps. First, we calculate the feasible
region of the manipulator under a specific configuration of the end-effector.
Next, we aim to find a trajectory on the line segments and a sequence of joint
configurations the manipulator should follow to move the end-effector along the
specified trajectory. Finally, we find the optimal combination of solutions to
the inverse kinematic problem at each via-point along the trajectory by
reducing the problem to a shortest-path problem of the graph and applying
Dijkstra's algorithm. We show the effectiveness of the proposed method by
experiments.","Takumu Okazaki, Akira Terui, Masahiko Mikawa",2025-08-31 12:59:46+00:00,http://arxiv.org/abs/2509.00828v2,http://arxiv.org/pdf/2509.00828v2,"cs.RO, cs.SC, math.AC, 68W30, 13P10, 13P25, 68U07, 68R10"
Inverse Kinematics for a 6-Degree-of-Freedom Robot Manipulator Using Comprehensive Gröbner Systems,"We propose an effective method for solving the inverse kinematic problem of a
specific model of 6-degree-of-freedom (6-DOF) robot manipulator using computer
algebra. It is known that when the rotation axes of three consecutive
rotational joints of a manipulator intersect at a single point, the inverse
kinematics problem can be divided into determining position and orientation. We
extend this method to more general manipulators in which the rotational axes of
two consecutive joints intersect. This extension broadens the class of 6-DOF
manipulators for which the inverse kinematics problem can be solved, and is
expected to enable more efficient solutions. The inverse kinematic problem is
solved using the Comprehensive Gr\""obner System (CGS) with joint parameters of
the robot appearing as parameters in the coefficients to prevent repetitive
calculations of the Gr\""obner bases. The effectiveness of the proposed method
is shown by experiments.","Takumu Okazaki, Akira Terui, Masahiko Mikawa",2025-08-31 12:41:32+00:00,http://arxiv.org/abs/2509.00823v2,http://arxiv.org/pdf/2509.00823v2,"cs.RO, cs.SC, math.AC, 68W30, 13P10, 13P25"
DyPho-SLAM : Real-time Photorealistic SLAM in Dynamic Environments,"Visual SLAM algorithms have been enhanced through the exploration of Gaussian
Splatting representations, particularly in generating high-fidelity dense maps.
While existing methods perform reliably in static environments, they often
encounter camera tracking drift and fuzzy mapping when dealing with the
disturbances caused by moving objects. This paper presents DyPho-SLAM, a
real-time, resource-efficient visual SLAM system designed to address the
challenges of localization and photorealistic mapping in environments with
dynamic objects. Specifically, the proposed system integrates prior image
information to generate refined masks, effectively minimizing noise from mask
misjudgment. Additionally, to enhance constraints for optimization after
removing dynamic obstacles, we devise adaptive feature extraction strategies
significantly improving the system's resilience. Experiments conducted on
publicly dynamic RGB-D datasets demonstrate that the proposed system achieves
state-of-the-art performance in camera pose estimation and dense map
reconstruction, while operating in real-time in dynamic scenes.","Yi Liu, Keyu Fan, Bin Lan, Houde Liu",2025-08-31 08:22:29+00:00,http://arxiv.org/abs/2509.00741v1,http://arxiv.org/pdf/2509.00741v1,cs.RO
ER-LoRA: Effective-Rank Guided Adaptation for Weather-Generalized Depth Estimation,"Monocular depth estimation under adverse weather conditions (e.g.\ rain, fog,
snow, and nighttime) remains highly challenging due to the lack of reliable
ground truth and the difficulty of learning from unlabeled real-world data.
Existing methods often rely on synthetic adverse data with pseudo-labels, which
suffer from domain gaps, or employ self-supervised learning, which violates
photometric assumptions in adverse scenarios. In this work, we propose to
achieve weather-generalized depth estimation by Parameter-Efficient Fine-Tuning
(PEFT) of Vision Foundation Models (VFMs), using only a small amount of
high-visibility (normal) data. While PEFT has shown strong performance in
semantic tasks such as segmentation, it remains underexplored for geometry --
centric tasks like depth estimation -- especially in terms of balancing
effective adaptation with the preservation of pretrained knowledge. To this
end, we introduce the Selecting-Tuning-Maintaining (STM) strategy, which
structurally decomposes the pretrained weights of VFMs based on two kinds of
effective ranks (entropy-rank and stable-rank). In the tuning phase, we
adaptively select the proper rank number as well as the task-aware singular
directions for initialization, based on the entropy-rank and full-tuned weight;
while in the maintaining stage, we enforce a principal direction regularization
based on the stable-rank. This design guarantees flexible task adaptation while
preserving the strong generalization capability of the pretrained VFM.
Extensive experiments on four real-world benchmarks across diverse weather
conditions demonstrate that STM not only outperforms existing PEFT methods and
full fine-tuning but also surpasses methods trained with adverse synthetic
data, and even the depth foundation model","Weilong Yan, Xin Zhang, Robby T. Tan",2025-08-31 02:24:00+00:00,http://arxiv.org/abs/2509.00665v2,http://arxiv.org/pdf/2509.00665v2,"cs.CV, cs.RO"
CARIS: A Context-Adaptable Robot Interface System for Personalized and Scalable Human-Robot Interaction,"The human-robot interaction (HRI) field has traditionally used Wizard-of-Oz
(WoZ) controlled robots to explore navigation, conversational dynamics,
human-in-the-loop interactions, and more to explore appropriate robot behaviors
in everyday settings. However, existing WoZ tools are often limited to one
context, making them less adaptable across different settings, users, and
robotic platforms. To mitigate these issues, we introduce a Context-Adaptable
Robot Interface System (CARIS) that combines advanced robotic capabilities such
teleoperation, human perception, human-robot dialogue, and multimodal data
recording. Through pilot studies, we demonstrate the potential of CARIS to WoZ
control a robot in two contexts: 1) mental health companion and as a 2) tour
guide. Furthermore, we identified areas of improvement for CARIS, including
smoother integration between movement and communication, clearer functionality
separation, recommended prompts, and one-click communication options to enhance
the usability wizard control of CARIS. This project offers a publicly
available, context-adaptable tool for the HRI community, enabling researchers
to streamline data-driven approaches to intelligent robot behavior.","Felipe Arias-Russi, Yuanchen Bai, Angelique Taylor",2025-08-31 02:11:18+00:00,http://arxiv.org/abs/2509.00660v1,http://arxiv.org/pdf/2509.00660v1,"cs.RO, cs.HC"
MV-SSM: Multi-View State Space Modeling for 3D Human Pose Estimation,"While significant progress has been made in single-view 3D human pose
estimation, multi-view 3D human pose estimation remains challenging,
particularly in terms of generalizing to new camera configurations. Existing
attention-based transformers often struggle to accurately model the spatial
arrangement of keypoints, especially in occluded scenarios. Additionally, they
tend to overfit specific camera arrangements and visual scenes from training
data, resulting in substantial performance drops in new settings. In this
study, we introduce a novel Multi-View State Space Modeling framework, named
MV-SSM, for robustly estimating 3D human keypoints. We explicitly model the
joint spatial sequence at two distinct levels: the feature level from
multi-view images and the person keypoint level. We propose a Projective State
Space (PSS) block to learn a generalized representation of joint spatial
arrangements using state space modeling. Moreover, we modify Mamba's
traditional scanning into an effective Grid Token-guided Bidirectional Scanning
(GTBS), which is integral to the PSS block. Multiple experiments demonstrate
that MV-SSM achieves strong generalization, outperforming state-of-the-art
methods: +10.8 on AP25 (+24%) on the challenging three-camera setting in CMU
Panoptic, +7.0 on AP25 (+13%) on varying camera arrangements, and +15.3 PCP
(+38%) on Campus A1 in cross-dataset evaluations. Project Website:
https://aviralchharia.github.io/MV-SSM","Aviral Chharia, Wenbo Gou, Haoye Dong",2025-08-31 00:57:41+00:00,http://arxiv.org/abs/2509.00649v1,http://arxiv.org/pdf/2509.00649v1,"cs.CV, cs.RO"
A Risk-aware Spatial-temporal Trajectory Planning Framework for Autonomous Vehicles Using QP-MPC and Dynamic Hazard Fields,"Trajectory planning is a critical component in ensuring the safety,
stability, and efficiency of autonomous vehicles. While existing trajectory
planning methods have achieved progress, they often suffer from high
computational costs, unstable performance in dynamic environments, and limited
validation across diverse scenarios. To overcome these challenges, we propose
an enhanced QP-MPC-based framework that incorporates three key innovations: (i)
a novel cost function designed with a dynamic hazard field, which explicitly
balances safety, efficiency, and comfort; (ii) seamless integration of this
cost function into the QP-MPC formulation, enabling direct optimization of
desired driving behaviors; and (iii) extensive validation of the proposed
framework across complex tasks. The spatial safe planning is guided by a
dynamic hazard field (DHF) for risk assessment, while temporal safe planning is
based on a space-time graph. Besides, the quintic polynomial sampling and
sub-reward of comforts are used to ensure comforts during lane-changing. The
sub-reward of efficiency is used to maintain driving efficiency. Finally, the
proposed DHF-enhanced objective function integrates multiple objectives,
providing a proper optimization tasks for QP-MPC. Extensive simulations
demonstrate that the proposed framework outperforms benchmark optimization
methods in terms of efficiency, stability, and comfort across a variety of
scenarios likes lane-changing, overtaking, and crossing intersections.","Zhen Tian, Zhihao Lin, Dezong Zhao, Christos Anagnostopoulos, Qiyuan Wang, Wenjing Zhao, Xiaodan Wang, Chongfeng Wei",2025-08-31 00:35:55+00:00,http://arxiv.org/abs/2509.00643v1,http://arxiv.org/pdf/2509.00643v1,"cs.RO, cs.SY, eess.SY"
Vehicle-in-Virtual-Environment (VVE) Method for Developing and Evaluating VRU Safety of Connected and Autonomous Driving with Focus on Bicyclist Safety,"Extensive research has already been conducted in the autonomous driving field
to help vehicles navigate safely and efficiently. At the same time, plenty of
current research on vulnerable road user (VRU) safety is performed which
largely concentrates on perception, localization, or trajectory prediction of
VRUs. However, existing research still exhibits several gaps, including the
lack of a unified planning and collision avoidance system for autonomous
vehicles, limited investigation into delay tolerant control strategies, and the
absence of an efficient and standardized testing methodology. Ensuring VRU
safety remains one of the most pressing challenges in autonomous driving,
particularly in dynamic and unpredictable environments. In this two year
project, we focused on applying the Vehicle in Virtual Environment (VVE) method
to develop, evaluate, and demonstrate safety functions for Vulnerable Road
Users (VRUs) using automated steering and braking of ADS. In this current
second year project report, our primary focus was on enhancing the previous
year results while also considering bicyclist safety.","Haochong Chen, Xincheng Cao, Bilin Aksun-Guvenc, Levent Guvenc",2025-08-30 22:43:14+00:00,http://arxiv.org/abs/2509.00624v1,http://arxiv.org/pdf/2509.00624v1,"cs.RO, cs.SY, eess.SY"
Safe and Efficient Lane-Changing for Autonomous Vehicles: An Improved Double Quintic Polynomial Approach with Time-to-Collision Evaluation,"Autonomous driving technology has made significant advancements in recent
years, yet challenges remain in ensuring safe and comfortable interactions with
human-driven vehicles (HDVs), particularly during lane-changing maneuvers. This
paper proposes an improved double quintic polynomial approach for safe and
efficient lane-changing in mixed traffic environments. The proposed method
integrates a time-to-collision (TTC) based evaluation mechanism directly into
the trajectory optimization process, ensuring that the ego vehicle proactively
maintains a safe gap from surrounding HDVs throughout the maneuver. The
framework comprises state estimation for both the autonomous vehicle (AV) and
HDVs, trajectory generation using double quintic polynomials, real-time TTC
computation, and adaptive trajectory evaluation. To the best of our knowledge,
this is the first work to embed an analytic TTC penalty directly into the
closed-form double-quintic polynomial solver, enabling real-time safety-aware
trajectory generation without post-hoc validation. Extensive simulations
conducted under diverse traffic scenarios demonstrate the safety, efficiency,
and comfort of the proposed approach compared to conventional methods such as
quintic polynomials, Bezier curves, and B-splines. The results highlight that
the improved method not only avoids collisions but also ensures smooth
transitions and adaptive decision-making in dynamic environments. This work
bridges the gap between model-based and adaptive trajectory planning
approaches, offering a stable solution for real-world autonomous driving
applications.","Rui Bai, Rui Xu, Teng Rui, Jiale Liu, Qi Wei Oung, Hoi Leong Lee, Zhen Tian, Fujiang Yuan",2025-08-30 18:31:29+00:00,http://arxiv.org/abs/2509.00582v1,http://arxiv.org/pdf/2509.00582v1,"cs.RO, cs.SY, eess.SY"
Galaxea Open-World Dataset and G0 Dual-System VLA Model,"We present Galaxea Open-World Dataset, a large-scale, diverse collection of
robot behaviors recorded in authentic human living and working environments.
All demonstrations are gathered using a consistent robotic embodiment, paired
with precise subtask-level language annotations to facilitate both training and
evaluation. Building on this dataset, we introduce G0, a dual-system framework
that couples a Vision-Language Model (VLM) for multimodal planning with a
Vision-Language-Action (VLA) model for fine-grained execution. G0 is trained
using a three-stage curriculum: cross-embodiment pre-training,
single-embodiment pre-training, and task-specific post-training. A
comprehensive benchmark spanning tabletop manipulation, few-shot learning, and
long-horizon mobile manipulation, demonstrates the effectiveness of our
approach. In particular, we find that the single-embodiment pre-training stage,
together with the Galaxea Open-World Dataset, plays a critical role in
achieving strong performance.","Tao Jiang, Tianyuan Yuan, Yicheng Liu, Chenhao Lu, Jianning Cui, Xiao Liu, Shuiqi Cheng, Jiyang Gao, Huazhe Xu, Hang Zhao",2025-08-30 18:04:19+00:00,http://arxiv.org/abs/2509.00576v1,http://arxiv.org/pdf/2509.00576v1,"cs.RO, cs.CV"
Learning Dolly-In Filming From Demonstration Using a Ground-Based Robot,"Cinematic camera control demands a balance of precision and artistry -
qualities that are difficult to encode through handcrafted reward functions.
While reinforcement learning (RL) has been applied to robotic filmmaking, its
reliance on bespoke rewards and extensive tuning limits creative usability. We
propose a Learning from Demonstration (LfD) approach using Generative
Adversarial Imitation Learning (GAIL) to automate dolly-in shots with a
free-roaming, ground-based filming robot. Expert trajectories are collected via
joystick teleoperation in simulation, capturing smooth, expressive motion
without explicit objective design.
  Trained exclusively on these demonstrations, our GAIL policy outperforms a
PPO baseline in simulation, achieving higher rewards, faster convergence, and
lower variance. Crucially, it transfers directly to a real-world robot without
fine-tuning, achieving more consistent framing and subject alignment than a
prior TD3-based method. These results show that LfD offers a robust,
reward-free alternative to RL in cinematic domains, enabling real-time
deployment with minimal technical effort. Our pipeline brings intuitive,
stylized camera control within reach of creative professionals, bridging the
gap between artistic intent and robotic autonomy.","Philip Lorimer, Alan Hunter, Wenbin Li",2025-08-30 17:54:48+00:00,http://arxiv.org/abs/2509.00574v1,http://arxiv.org/pdf/2509.00574v1,"cs.RO, cs.LG"
Gray-Box Computed Torque Control for Differential-Drive Mobile Robot Tracking,"This study presents a learning-based nonlinear algorithm for tracking control
of differential-drive mobile robots. The Computed Torque Method (CTM) suffers
from inaccurate knowledge of system parameters, while Deep Reinforcement
Learning (DRL) algorithms are known for sample inefficiency and weak stability
guarantees. The proposed method replaces the black-box policy network of a DRL
agent with a gray-box Computed Torque Controller (CTC) to improve sample
efficiency and ensure closed-loop stability. This approach enables finding an
optimal set of controller parameters for an arbitrary reward function using
only a few short learning episodes. The Twin-Delayed Deep Deterministic Policy
Gradient (TD3) algorithm is used for this purpose. Additionally, some
controller parameters are constrained to lie within known value ranges,
ensuring the RL agent learns physically plausible values. A technique is also
applied to enforce a critically damped closed-loop time response. The
controller's performance is evaluated on a differential-drive mobile robot
simulated in the MuJoCo physics engine and compared against the raw CTC and a
conventional kinematic controller.",Arman Javan Sekhavat Pishkhani,2025-08-30 17:38:17+00:00,http://arxiv.org/abs/2509.00571v1,http://arxiv.org/pdf/2509.00571v1,"cs.RO, cs.SY, eess.SY"
ConceptBot: Enhancing Robot's Autonomy through Task Decomposition with Large Language Models and Knowledge Graph,"ConceptBot is a modular robotic planning framework that combines Large
Language Models and Knowledge Graphs to generate feasible and risk-aware plans
despite ambiguities in natural language instructions and correctly analyzing
the objects present in the environment - challenges that typically arise from a
lack of commonsense reasoning. To do that, ConceptBot integrates (i) an Object
Property Extraction (OPE) module that enriches scene understanding with
semantic concepts from ConceptNet, (ii) a User Request Processing (URP) module
that disambiguates and structures instructions, and (iii) a Planner that
generates context-aware, feasible pick-and-place policies. In comparative
evaluations against Google SayCan, ConceptBot achieved 100% success on explicit
tasks, maintained 87% accuracy on implicit tasks (versus 31% for SayCan),
reached 76% on risk-aware tasks (versus 15%), and outperformed SayCan in
application-specific scenarios, including material classification (70% vs. 20%)
and toxicity detection (86% vs. 36%). On SafeAgentBench, ConceptBot achieved an
overall score of 80% (versus 46% for the next-best baseline). These results,
validated in both simulation and laboratory experiments, demonstrate
ConceptBot's ability to generalize without domain-specific training and to
significantly improve the reliability of robotic policies in unstructured
environments. Website: https://sites.google.com/view/conceptbot","Alessandro Leanza, Angelo Moroncelli, Giuseppe Vizzari, Francesco Braghin, Loris Roveda, Blerina Spahiu",2025-08-30 17:31:52+00:00,http://arxiv.org/abs/2509.00570v1,http://arxiv.org/pdf/2509.00570v1,cs.RO
Reinforcement Learning of Dolly-In Filming Using a Ground-Based Robot,"Free-roaming dollies enhance filmmaking with dynamic movement, but challenges
in automated camera control remain unresolved. Our study advances this field by
applying Reinforcement Learning (RL) to automate dolly-in shots using
free-roaming ground-based filming robots, overcoming traditional control
hurdles. We demonstrate the effectiveness of combined control for precise film
tasks by comparing it to independent control strategies. Our robust RL pipeline
surpasses traditional Proportional-Derivative controller performance in
simulation and proves its efficacy in real-world tests on a modified ROSBot 2.0
platform equipped with a camera turret. This validates our approach's
practicality and sets the stage for further research in complex filming
scenarios, contributing significantly to the fusion of technology with
cinematic creativity. This work presents a leap forward in the field and opens
new avenues for research and development, effectively bridging the gap between
technological advancement and creative filmmaking.","Philip Lorimer, Jack Saunders, Alan Hunter, Wenbin Li",2025-08-30 17:14:11+00:00,http://arxiv.org/abs/2509.00564v1,http://arxiv.org/pdf/2509.00564v1,"cs.RO, cs.CV, cs.LG"
Needle Biopsy And Fiber-Optic Compatible Robotic Insertion Platform,"Tissue biopsy is the gold standard for diagnosing many diseases, involving
the extraction of diseased tissue for histopathology analysis by expert
pathologists. However, this procedure has two main limitations: 1) Manual
sampling through tissue biopsy is prone to inaccuracies; 2) The extraction
process is followed by a time-consuming pathology test. To address these
limitations, we present a compact, accurate, and maneuverable robotic insertion
platform to overcome the limitations in traditional histopathology. Our
platform is capable of steering a variety of tools with different sizes,
including needle for tissue extraction and optical fibers for vibrational
spectroscopy applications. This system facilitates the guidance of end-effector
to the tissue and assists surgeons in navigating to the biopsy target area for
multi-modal diagnosis. In this paper, we outline the general concept of our
device, followed by a detailed description of its mechanical design and control
scheme. We conclude with the validation of the system through a series of
tests, including positioning accuracy, admittance performance, and tool
insertion efficacy.","Fanxin Wang, Yikun Cheng, Chuyuan Tao, Rohit Bhargava, Thenkurussi Kesavadas",2025-08-30 15:24:47+00:00,http://arxiv.org/abs/2509.00530v1,http://arxiv.org/pdf/2509.00530v1,"cs.RO, cs.SY, eess.SY"
NeuralSVCD for Efficient Swept Volume Collision Detection,"Robot manipulation in unstructured environments requires efficient and
reliable Swept Volume Collision Detection (SVCD) for safe motion planning.
Traditional discrete methods potentially miss collisions between these points,
whereas SVCD continuously checks for collisions along the entire trajectory.
Existing SVCD methods typically face a trade-off between efficiency and
accuracy, limiting practical use. In this paper, we introduce NeuralSVCD, a
novel neural encoder-decoder architecture tailored to overcome this trade-off.
Our approach leverages shape locality and temporal locality through distributed
geometric representations and temporal optimization. This enhances
computational efficiency without sacrificing accuracy. Comprehensive
experiments show that NeuralSVCD consistently outperforms existing
state-of-the-art SVCD methods in terms of both collision detection accuracy and
computational efficiency, demonstrating its robust applicability across diverse
robotic manipulation scenarios. Code and videos are available at
https://neuralsvcd.github.io/.","Dongwon Son, Hojin Jung, Beomjoon Kim",2025-08-30 13:43:11+00:00,http://arxiv.org/abs/2509.00499v1,http://arxiv.org/pdf/2509.00499v1,"cs.RO, cs.AI, cs.LG"
FLUID: A Fine-Grained Lightweight Urban Signalized-Intersection Dataset of Dense Conflict Trajectories,"The trajectory data of traffic participants (TPs) is a fundamental resource
for evaluating traffic conditions and optimizing policies, especially at urban
intersections. Although data acquisition using drones is efficient, existing
datasets still have limitations in scene representativeness, information
richness, and data fidelity. This study introduces FLUID, comprising a
fine-grained trajectory dataset that captures dense conflicts at typical urban
signalized intersections, and a lightweight, full-pipeline framework for
drone-based trajectory processing. FLUID covers three distinct intersection
types, with approximately 5 hours of recording time and featuring over 20,000
TPs across 8 categories. Notably, the dataset averages two vehicle conflicts
per minute, involving roughly 25% of all motor vehicles. FLUID provides
comprehensive data, including trajectories, traffic signals, maps, and raw
videos. Comparison with the DataFromSky platform and ground-truth measurements
validates its high spatio-temporal accuracy. Through a detailed classification
of motor vehicle conflicts and violations, FLUID reveals a diversity of
interactive behaviors, demonstrating its value for human preference mining,
traffic behavior modeling, and autonomous driving research.","Yiyang Chen, Zhigang Wu, Guohong Zheng, Xuesong Wu, Liwen Xu, Haoyuan Tang, Zhaocheng He, Haipeng Zeng",2025-08-30 13:38:42+00:00,http://arxiv.org/abs/2509.00497v1,http://arxiv.org/pdf/2509.00497v1,"cs.RO, cs.CV"
Extended Diffeomorphism for Real-Time Motion Replication in Workspaces with Different Spatial Arrangements,"This paper presents two types of extended diffeomorphism designs to
compensate for spatial placement differences between robot workspaces.
Teleoperation of multiple robots is attracting attention to expand the
utilization of the robot embodiment. Real-time reproduction of robot motion
would facilitate the efficient execution of similar tasks by multiple robots. A
challenge in the motion reproduction is compensating for the spatial
arrangement errors of target keypoints in robot workspaces. This paper proposes
a methodology for smooth mappings that transform primary robot poses into
follower robot poses based on the predefined key points in each workspace.
Through a picking task experiment using a dual-arm UR5 robot, this study
demonstrates that the proposed mapping generation method can balance lower
mapping errors for precise operation and lower mapping gradients for smooth
replicated movement.","Masaki Saito, Shunki Itadera, Toshiyuki Murakami",2025-08-30 13:24:42+00:00,http://arxiv.org/abs/2509.00491v1,http://arxiv.org/pdf/2509.00491v1,cs.RO
Embodied Spatial Intelligence: from Implicit Scene Modeling to Spatial Reasoning,"This thesis introduces ""Embodied Spatial Intelligence"" to address the
challenge of creating robots that can perceive and act in the real world based
on natural language instructions. To bridge the gap between Large Language
Models (LLMs) and physical embodiment, we present contributions on two fronts:
scene representation and spatial reasoning. For perception, we develop robust,
scalable, and accurate scene representations using implicit neural models, with
contributions in self-supervised camera calibration, high-fidelity depth field
generation, and large-scale reconstruction. For spatial reasoning, we enhance
the spatial capabilities of LLMs by introducing a novel navigation benchmark, a
method for grounding language in 3D, and a state-feedback mechanism to improve
long-horizon decision-making. This work lays a foundation for robots that can
robustly perceive their surroundings and intelligently act upon complex,
language-based commands.",Jiading Fang,2025-08-30 11:42:26+00:00,http://arxiv.org/abs/2509.00465v1,http://arxiv.org/pdf/2509.00465v1,"cs.RO, cs.AI, cs.CV"
AGS: Accelerating 3D Gaussian Splatting SLAM via CODEC-Assisted Frame Covisibility Detection,"Simultaneous Localization and Mapping (SLAM) is a critical task that enables
autonomous vehicles to construct maps and localize themselves in unknown
environments. Recent breakthroughs combine SLAM with 3D Gaussian Splatting
(3DGS) to achieve exceptional reconstruction fidelity. However, existing
3DGS-SLAM systems provide insufficient throughput due to the need for multiple
training iterations per frame and the vast number of Gaussians.
  In this paper, we propose AGS, an algorithm-hardware co-design framework to
boost the efficiency of 3DGS-SLAM based on the intuition that SLAM systems
process frames in a streaming manner, where adjacent frames exhibit high
similarity that can be utilized for acceleration. On the software level: 1) We
propose a coarse-then-fine-grained pose tracking method with respect to the
robot's movement. 2) We avoid redundant computations of Gaussians by sharing
their contribution information across frames. On the hardware level, we propose
a frame covisibility detection engine to extract intermediate data from the
video CODEC. We also implement a pose tracking engine and a mapping engine with
workload schedulers to efficiently deploy the AGS algorithm. Our evaluation
shows that AGS achieves up to $17.12\times$, $6.71\times$, and $5.41\times$
speedups against the mobile and high-end GPUs, and a state-of-the-art 3DGS
accelerator, GSCore.","Houshu He, Naifeng Jing, Li Jiang, Xiaoyao Liang, Zhuoran Song",2025-08-30 09:29:35+00:00,http://arxiv.org/abs/2509.00433v1,http://arxiv.org/pdf/2509.00433v1,"cs.AR, cs.RO"
Domain Adaptation-Based Crossmodal Knowledge Distillation for 3D Semantic Segmentation,"Semantic segmentation of 3D LiDAR data plays a pivotal role in autonomous
driving. Traditional approaches rely on extensive annotated data for point
cloud analysis, incurring high costs and time investments. In contrast,
realworld image datasets offer abundant availability and substantial scale. To
mitigate the burden of annotating 3D LiDAR point clouds, we propose two
crossmodal knowledge distillation methods: Unsupervised Domain Adaptation
Knowledge Distillation (UDAKD) and Feature and Semantic-based Knowledge
Distillation (FSKD). Leveraging readily available spatio-temporally
synchronized data from cameras and LiDARs in autonomous driving scenarios, we
directly apply a pretrained 2D image model to unlabeled 2D data. Through
crossmodal knowledge distillation with known 2D-3D correspondence, we actively
align the output of the 3D network with the corresponding points of the 2D
network, thereby obviating the necessity for 3D annotations. Our focus is on
preserving modality-general information while filtering out modality-specific
details during crossmodal distillation. To achieve this, we deploy
self-calibrated convolution on 3D point clouds as the foundation of our domain
adaptation module. Rigorous experimentation validates the effectiveness of our
proposed methods, consistently surpassing the performance of state-of-the-art
approaches in the field.","Jialiang Kang, Jiawen Wang, Dingsheng Luo",2025-08-30 06:34:39+00:00,http://arxiv.org/abs/2509.00379v1,http://arxiv.org/pdf/2509.00379v1,"cs.CV, cs.RO"
Generative Visual Foresight Meets Task-Agnostic Pose Estimation in Robotic Table-Top Manipulation,"Robotic manipulation in unstructured environments requires systems that can
generalize across diverse tasks while maintaining robust and reliable
performance. We introduce {GVF-TAPE}, a closed-loop framework that combines
generative visual foresight with task-agnostic pose estimation to enable
scalable robotic manipulation. GVF-TAPE employs a generative video model to
predict future RGB-D frames from a single side-view RGB image and a task
description, offering visual plans that guide robot actions. A decoupled pose
estimation model then extracts end-effector poses from the predicted frames,
translating them into executable commands via low-level controllers. By
iteratively integrating video foresight and pose estimation in a closed loop,
GVF-TAPE achieves real-time, adaptive manipulation across a broad range of
tasks. Extensive experiments in both simulation and real-world settings
demonstrate that our approach reduces reliance on task-specific action data and
generalizes effectively, providing a practical and scalable solution for
intelligent robotic systems.","Chuye Zhang, Xiaoxiong Zhang, Wei Pan, Linfang Zheng, Wei Zhang",2025-08-30 04:53:32+00:00,http://arxiv.org/abs/2509.00361v1,http://arxiv.org/pdf/2509.00361v1,cs.RO
Autonomous Aggregate Sorting in Construction and Mining via Computer Vision-Aided Robotic Arm Systems,"Traditional aggregate sorting methods, whether manual or mechanical, often
suffer from low precision, limited flexibility, and poor adaptability to
diverse material properties such as size, shape, and lithology. To address
these limitations, this study presents a computer vision-aided robotic arm
system designed for autonomous aggregate sorting in construction and mining
applications. The system integrates a six-degree-of-freedom robotic arm, a
binocular stereo camera for 3D perception, and a ROS-based control framework.
Core techniques include an attention-augmented YOLOv8 model for aggregate
detection, stereo matching for 3D localization, Denavit-Hartenberg kinematic
modeling for arm motion control, minimum enclosing rectangle analysis for size
estimation, and hand-eye calibration for precise coordinate alignment.
Experimental validation with four aggregate types achieved an average grasping
and sorting success rate of 97.5%, with comparable classification accuracy.
Remaining challenges include the reliable handling of small aggregates and
texture-based misclassification. Overall, the proposed system demonstrates
significant potential to enhance productivity, reduce operational costs, and
improve safety in aggregate handling, while providing a scalable framework for
advancing smart automation in construction, mining, and recycling industries.","Md. Taherul Islam Shawon, Yuan Li, Yincai Cai, Junjie Niu, Ting Peng",2025-08-30 03:44:11+00:00,http://arxiv.org/abs/2509.00339v1,http://arxiv.org/pdf/2509.00339v1,cs.RO
Jacobian Exploratory Dual-Phase Reinforcement Learning for Dynamic Endoluminal Navigation of Deformable Continuum Robots,"Deformable continuum robots (DCRs) present unique planning challenges due to
nonlinear deformation mechanics and partial state observability, violating the
Markov assumptions of conventional reinforcement learning (RL) methods. While
Jacobian-based approaches offer theoretical foundations for rigid manipulators,
their direct application to DCRs remains limited by time-varying kinematics and
underactuated deformation dynamics. This paper proposes Jacobian Exploratory
Dual-Phase RL (JEDP-RL), a framework that decomposes planning into phased
Jacobian estimation and policy execution. During each training step, we first
perform small-scale local exploratory actions to estimate the deformation
Jacobian matrix, then augment the state representation with Jacobian features
to restore approximate Markovianity. Extensive SOFA surgical dynamic
simulations demonstrate JEDP-RL's three key advantages over proximal policy
optimization (PPO) baselines: 1) Convergence speed: 3.2x faster policy
convergence, 2) Navigation efficiency: requires 25% fewer steps to reach the
target, and 3) Generalization ability: achieve 92% success rate under material
property variations and achieve 83% (33% higher than PPO) success rate in the
unseen tissue environment.","Yu Tian, Chi Kit Ng, Hongliang Ren",2025-08-30 03:04:35+00:00,http://arxiv.org/abs/2509.00329v1,http://arxiv.org/pdf/2509.00329v1,"cs.RO, cs.AI, cs.SY, eess.SY"
Mechanistic interpretability for steering vision-language-action models,"Vision-Language-Action (VLA) models are a promising path to realizing
generalist embodied agents that can quickly adapt to new tasks, modalities, and
environments. However, methods for interpreting and steering VLAs fall far
short of classical robotics pipelines, which are grounded in explicit models of
kinematics, dynamics, and control. This lack of mechanistic insight is a
central challenge for deploying learned policies in real-world robotics, where
robustness and explainability are critical. Motivated by advances in
mechanistic interpretability for large language models, we introduce the first
framework for interpreting and steering VLAs via their internal
representations, enabling direct intervention in model behavior at inference
time. We project feedforward activations within transformer layers onto the
token embedding basis, identifying sparse semantic directions - such as speed
and direction - that are causally linked to action selection. Leveraging these
findings, we introduce a general-purpose activation steering method that
modulates behavior in real time, without fine-tuning, reward signals, or
environment interaction. We evaluate this method on two recent open-source
VLAs, Pi0 and OpenVLA, and demonstrate zero-shot behavioral control in
simulation (LIBERO) and on a physical robot (UR5). This work demonstrates that
interpretable components of embodied VLAs can be systematically harnessed for
control - establishing a new paradigm for transparent and steerable foundation
models in robotics.","Bear Häon, Kaylene Stocking, Ian Chuang, Claire Tomlin",2025-08-30 03:01:57+00:00,http://arxiv.org/abs/2509.00328v1,http://arxiv.org/pdf/2509.00328v1,"cs.RO, cs.LG"
Contact-Aided Navigation of Flexible Robotic Endoscope Using Deep Reinforcement Learning in Dynamic Stomach,"Navigating a flexible robotic endoscope (FRE) through the gastrointestinal
tract is critical for surgical diagnosis and treatment. However, navigation in
the dynamic stomach is particularly challenging because the FRE must learn to
effectively use contact with the deformable stomach walls to reach target
locations. To address this, we introduce a deep reinforcement learning (DRL)
based Contact-Aided Navigation (CAN) strategy for FREs, leveraging contact
force feedback to enhance motion stability and navigation precision. The
training environment is established using a physics-based finite element method
(FEM) simulation of a deformable stomach. Trained with the Proximal Policy
Optimization (PPO) algorithm, our approach achieves high navigation success
rates (within 3 mm error between the FRE's end-effector and target) and
significantly outperforms baseline policies. In both static and dynamic stomach
environments, the CAN agent achieved a 100% success rate with 1.6 mm average
error, and it maintained an 85% success rate in challenging unseen scenarios
with stronger external disturbances. These results validate that the DRL-based
CAN strategy substantially enhances FRE navigation performance over prior
methods.","Chi Kit Ng, Huxin Gao, Tian-Ao Ren, Jiewen Lai, Hongliang Ren",2025-08-30 02:42:06+00:00,http://arxiv.org/abs/2509.00319v1,http://arxiv.org/pdf/2509.00319v1,"cs.RO, cs.AI, cs.SY, eess.SY"
A Framework for Task and Motion Planning based on Expanding AND/OR Graphs,"Robot autonomy in space environments presents unique challenges, including
high perception and motion uncertainty, strict kinematic constraints, and
limited opportunities for human intervention. Therefore, Task and Motion
Planning (TMP) may be critical for autonomous servicing, surface operations, or
even in-orbit missions, just to name a few, as it models tasks as discrete
action sequencing integrated with continuous motion feasibility assessments. In
this paper, we introduce a TMP framework based on expanding AND/OR graphs,
referred to as TMP-EAOG, and demonstrate its adaptability to different
scenarios. TMP-EAOG encodes task-level abstractions within an AND/OR graph,
which expands iteratively as the plan is executed, and performs in-the-loop
motion planning assessments to ascertain their feasibility. As a consequence,
TMP-EAOG is characterised by the desirable properties of (i) robustness to a
certain degree of uncertainty, because AND/OR graph expansion can accommodate
for unpredictable information about the robot environment, (ii) controlled
autonomy, since an AND/OR graph can be validated by human experts, and (iii)
bounded flexibility, in that unexpected events, including the assessment of
unfeasible motions, can lead to different courses of action as alternative
paths in the AND/OR graph. We evaluate TMP-EAOG on two benchmark domains. We
use a simulated mobile manipulator as a proxy for space-grade autonomous
robots. Our evaluation shows that TMP-EAOG can deal with a wide range of
challenges in the benchmarks.","Fulvio Mastrogiovanni, Antony Thomas",2025-08-30 02:28:25+00:00,http://arxiv.org/abs/2509.00317v1,http://arxiv.org/pdf/2509.00317v1,"cs.RO, cs.AI"
TReF-6: Inferring Task-Relevant Frames from a Single Demonstration for One-Shot Skill Generalization,"Robots often struggle to generalize from a single demonstration due to the
lack of a transferable and interpretable spatial representation. In this work,
we introduce TReF-6, a method that infers a simplified, abstracted 6DoF
Task-Relevant Frame from a single trajectory. Our approach identifies an
influence point purely from the trajectory geometry to define the origin for a
local frame, which serves as a reference for parameterizing a Dynamic Movement
Primitive (DMP). This influence point captures the task's spatial structure,
extending the standard DMP formulation beyond start-goal imitation. The
inferred frame is semantically grounded via a vision-language model and
localized in novel scenes by Grounded-SAM, enabling functionally consistent
skill generalization. We validate TReF-6 in simulation and demonstrate
robustness to trajectory noise. We further deploy an end-to-end pipeline on
real-world manipulation tasks, showing that TReF-6 supports one-shot imitation
learning that preserves task intent across diverse object configurations.","Yuxuan Ding, Shuangge Wang, Tesca Fitzgerald",2025-08-30 01:54:28+00:00,http://arxiv.org/abs/2509.00310v1,http://arxiv.org/pdf/2509.00310v1,"cs.RO, cs.AI"
A Layered Control Perspective on Legged Locomotion: Embedding Reduced Order Models via Hybrid Zero Dynamics,"Reduced-order models (ROMs) provide a powerful means of synthesizing dynamic
walking gaits on legged robots. Yet this approach lacks the formal guarantees
enjoyed by methods that utilize the full-order model (FOM) for gait synthesis,
e.g., hybrid zero dynamics. This paper aims to unify these approaches through a
layered control perspective. In particular, we establish conditions on when a
ROM of locomotion yields stable walking on the full-order hybrid dynamics. To
achieve this result, given an ROM we synthesize a zero dynamics manifold
encoding the behavior of the ROM -- controllers can be synthesized that drive
the FOM to this surface, yielding hybrid zero dynamics. We prove that a stable
periodic orbit in the ROM implies an input-to-state stable periodic orbit of
the FOM's hybrid zero dynamics, and hence the FOM dynamics. This result is
demonstrated in simulation on a linear inverted pendulum ROM and a 5-link
planar walking FOM.","Sergio A. Esteban, Max H. Cohen, Adrian B. Ghansah, Aaron D. Ames",2025-08-30 01:09:51+00:00,http://arxiv.org/abs/2509.00294v1,http://arxiv.org/pdf/2509.00294v1,"eess.SY, cs.RO, cs.SY"
Learn from What We HAVE: History-Aware VErifier that Reasons about Past Interactions Online,"We introduce a novel History-Aware VErifier (HAVE) to disambiguate uncertain
scenarios online by leveraging past interactions. Robots frequently encounter
visually ambiguous objects whose manipulation outcomes remain uncertain until
physically interacted with. While generative models alone could theoretically
adapt to such ambiguity, in practice they obtain suboptimal performance in
ambiguous cases, even when conditioned on action history. To address this, we
propose explicitly decoupling action generation from verification: we use an
unconditional diffusion-based generator to propose multiple candidate actions
and employ our history-aware verifier to select the most promising action by
reasoning about past interactions. Through theoretical analysis, we demonstrate
that employing a verifier significantly improves expected action quality.
Empirical evaluations and analysis across multiple simulated and real-world
environments including articulated objects, multi-modal doors, and uneven
object pick-up confirm the effectiveness of our method and improvements over
baselines. Our project website is available at:
https://liy1shu.github.io/HAVE_CoRL25/","Yishu Li, Xinyi Mao, Ying Yuan, Kyutae Sim, Ben Eisner, David Held",2025-08-29 22:56:32+00:00,http://arxiv.org/abs/2509.00271v1,http://arxiv.org/pdf/2509.00271v1,cs.RO
Embodied AI in Social Spaces: Responsible and Adaptive Robots in Complex Setting -- UKAIRS 2025 (Copy),"This paper introduces and overviews a multidisciplinary project aimed at
developing responsible and adaptive multi-human multi-robot (MHMR) systems for
complex, dynamic settings. The project integrates co-design, ethical
frameworks, and multimodal sensing to create AI-driven robots that are
emotionally responsive, context-aware, and aligned with the needs of diverse
users. We outline the project's vision, methodology, and early outcomes,
demonstrating how embodied AI can support sustainable, ethical, and
human-centred futures.","Aleksandra Landowska, Aislinn D Gomez Bergin, Ayodeji O. Abioye, Jayati Deshmukh, Andriana Bouadouki, Maria Wheadon, Athina Georgara, Dominic Price, Tuyen Nguyen, Shuang Ao, Lokesh Singh, Yi Long, Raffaele Miele, Joel E. Fischer, Sarvapali D. Ramchurn",2025-08-29 20:01:56+00:00,http://arxiv.org/abs/2509.00218v1,http://arxiv.org/pdf/2509.00218v1,"cs.RO, cs.AI"
First Order Model-Based RL through Decoupled Backpropagation,"There is growing interest in reinforcement learning (RL) methods that
leverage the simulator's derivatives to improve learning efficiency. While
early gradient-based approaches have demonstrated superior performance compared
to derivative-free methods, accessing simulator gradients is often impractical
due to their implementation cost or unavailability. Model-based RL (MBRL) can
approximate these gradients via learned dynamics models, but the solver
efficiency suffers from compounding prediction errors during training rollouts,
which can degrade policy performance. We propose an approach that decouples
trajectory generation from gradient computation: trajectories are unrolled
using a simulator, while gradients are computed via backpropagation through a
learned differentiable model of the simulator. This hybrid design enables
efficient and consistent first-order policy optimization, even when simulator
gradients are unavailable, as well as learning a critic from simulation
rollouts, which is more accurate. Our method achieves the sample efficiency and
speed of specialized optimizers such as SHAC, while maintaining the generality
of standard approaches like PPO and avoiding ill behaviors observed in other
first-order MBRL methods. We empirically validate our algorithm on benchmark
control tasks and demonstrate its effectiveness on a real Go2 quadruped robot,
across both quadrupedal and bipedal locomotion tasks.","Joseph Amigo, Rooholla Khorrambakht, Elliot Chane-Sane, Nicolas Mansard, Ludovic Righetti",2025-08-29 19:55:25+00:00,http://arxiv.org/abs/2509.00215v2,http://arxiv.org/pdf/2509.00215v2,"cs.RO, cs.AI, cs.LG"
Poke and Strike: Learning Task-Informed Exploration Policies,"In many dynamic robotic tasks, such as striking pucks into a goal outside the
reachable workspace, the robot must first identify the relevant physical
properties of the object for successful task execution, as it is unable to
recover from failure or retry without human intervention. To address this
challenge, we propose a task-informed exploration approach, based on
reinforcement learning, that trains an exploration policy using rewards
automatically generated from the sensitivity of a privileged task policy to
errors in estimated properties. We also introduce an uncertainty-based
mechanism to determine when to transition from exploration to task execution,
ensuring sufficient property estimation accuracy with minimal exploration time.
Our method achieves a 90% success rate on the striking task with an average
exploration time under 1.2 seconds, significantly outperforming baselines that
achieve at most 40% success or require inefficient querying and retraining in a
simulator at test time. Additionally, we demonstrate that our task-informed
rewards capture the relative importance of physical properties in both the
striking task and the classical CartPole example. Finally, we validate our
approach by demonstrating its ability to identify object properties and adjust
task execution in a physical setup using the KUKA iiwa robot arm.","Marina Y. Aoyama, Joao Moura, Juan Del Aguila Ferrandis, Sethu Vijayakumar",2025-08-29 18:26:05+00:00,http://arxiv.org/abs/2509.00178v1,http://arxiv.org/pdf/2509.00178v1,cs.RO
Tree-Guided Diffusion Planner,"Planning with pretrained diffusion models has emerged as a promising approach
for solving test-time guided control problems. However, standard gradient
guidance typically performs optimally under convex and differentiable reward
landscapes, showing substantially reduced effectiveness in real-world scenarios
involving non-convex objectives, non-differentiable constraints, and
multi-reward structures. Furthermore, recent supervised planning approaches
require task-specific training or value estimators, which limits test-time
flexibility and zero-shot generalization. We propose a Tree-guided Diffusion
Planner (TDP), a zero-shot test-time planning framework that balances
exploration and exploitation through structured trajectory generation. We frame
test-time planning as a tree search problem using a bi-level sampling process:
(1) diverse parent trajectories are produced via training-free particle
guidance to encourage broad exploration, and (2) sub-trajectories are refined
through fast conditional denoising guided by task objectives. TDP addresses the
limitations of gradient guidance by exploring diverse trajectory regions and
harnessing gradient information across this expanded solution space using only
pretrained models and test-time reward signals. We evaluate TDP on three
diverse tasks: maze gold-picking, robot arm block manipulation, and AntMaze
multi-goal exploration. TDP consistently outperforms state-of-the-art
approaches on all tasks. The project page can be found at:
tree-diffusion-planner.github.io.","Hyeonseong Jeon, Cheolhong Min, Jaesik Park",2025-08-29 17:27:44+00:00,http://arxiv.org/abs/2508.21800v1,http://arxiv.org/pdf/2508.21800v1,"cs.AI, cs.RO"
Can a mobile robot learn from a pedestrian model to prevent the sidewalk salsa?,"Pedestrians approaching each other on a sidewalk sometimes end up in an
awkward interaction known as the ""sidewalk salsa"": they both (repeatedly)
deviate to the same side to avoid a collision. This provides an interesting use
case to study interactions between pedestrians and mobile robots because, in
the vast majority of cases, this phenomenon is avoided through a negotiation
based on implicit communication. Understanding how it goes wrong and how
pedestrians end up in the sidewalk salsa will therefore provide insight into
the implicit communication. This understanding can be used to design safe and
acceptable robotic behaviour. In a previous attempt to gain this understanding,
a model of pedestrian behaviour based on the Communication-Enabled Interaction
(CEI) framework was developed that can replicate the sidewalk salsa. However,
it is unclear how to leverage this model in robotic planning and
decision-making since it violates the assumptions of game theory, a much-used
framework in planning and decision-making. Here, we present a proof-of-concept
for an approach where a Reinforcement Learning (RL) agent leverages the model
to learn how to interact with pedestrians. The results show that a basic RL
agent successfully learned to interact with the CEI model. Furthermore, a
risk-averse RL agent that had access to the perceived risk of the CEI model
learned how to effectively communicate its intention through its motion and
thereby substantially lowered the perceived risk, and displayed effort by the
modelled pedestrian. These results show this is a promising approach and
encourage further exploration.","Olger Siebinga, David Abbink",2025-08-29 14:56:48+00:00,http://arxiv.org/abs/2508.21690v1,http://arxiv.org/pdf/2508.21690v1,cs.RO
Robust Convex Model Predictive Control with collision avoidance guarantees for robot manipulators,"Industrial manipulators are normally operated in cluttered environments,
making safe motion planning important. Furthermore, the presence of
model-uncertainties make safe motion planning more difficult. Therefore, in
practice the speed is limited in order to reduce the effect of disturbances.
There is a need for control methods that can guarantee safe motions that can be
executed fast. We address this need by suggesting a novel model predictive
control (MPC) solution for manipulators, where our two main components are a
robust tube MPC and a corridor planning algorithm to obtain collision-free
motion. Our solution results in a convex MPC, which we can solve fast, making
our method practically useful. We demonstrate the efficacy of our method in a
simulated environment with a 6 DOF industrial robot operating in cluttered
environments with uncertainties in model parameters. We outperform benchmark
methods, both in terms of being able to work under higher levels of model
uncertainties, while also yielding faster motion.","Bernhard Wullt, Johannes Köhler, Per Mattsson, Mikeal Norrlöf, Thomas B. Schön",2025-08-29 14:45:54+00:00,http://arxiv.org/abs/2508.21677v1,http://arxiv.org/pdf/2508.21677v1,cs.RO
A-MHA*: Anytime Multi-Heuristic A*,"Designing good heuristic functions for graph search requires adequate domain
knowledge. It is often easy to design heuristics that perform well and
correlate with the underlying true cost-to-go values in certain parts of the
search space but these may not be admissible throughout the domain thereby
affecting the optimality guarantees of the search. Bounded suboptimal search
using several such partially good but inadmissible heuristics was developed in
Multi-Heuristic A* (MHA*). Although MHA* leverages multiple inadmissible
heuristics to potentially generate a faster suboptimal solution, the original
version does not improve the solution over time. It is a one shot algorithm
that requires careful setting of inflation factors to obtain a desired one time
solution. In this work, we tackle this issue by extending MHA* to an anytime
version that finds a feasible suboptimal solution quickly and continually
improves it until time runs out. Our work is inspired from the Anytime
Repairing A* (ARA*) algorithm. We prove that our precise adaptation of ARA*
concepts in the MHA* framework preserves the original suboptimal and
completeness guarantees and enhances MHA* to perform in an anytime fashion.
Furthermore, we report the performance of A-MHA* in 3-D path planning domain
and sliding tiles puzzle and compare against MHA* and other anytime algorithms.","Ramkumar Natarajan, Muhammad Suhail Saleem, William Xiao, Sandip Aine, Howie Choset, Maxim Likhachev",2025-08-29 14:00:45+00:00,http://arxiv.org/abs/2508.21637v1,http://arxiv.org/pdf/2508.21637v1,"cs.AI, cs.RO"
The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics,"We present a multi-modal dataset collected in a soybean crop field,
comprising over two hours of recorded data from sensors such as stereo infrared
camera, color camera, accelerometer, gyroscope, magnetometer, GNSS (Single
Point Positioning, Real-Time Kinematic and Post-Processed Kinematic), and wheel
odometry. This dataset captures key challenges inherent to robotics in
agricultural environments, including variations in natural lighting, motion
blur, rough terrain, and long, perceptually aliased sequences. By addressing
these complexities, the dataset aims to support the development and
benchmarking of advanced algorithms for localization, mapping, perception, and
navigation in agricultural robotics. The platform and data collection system is
designed to meet the key requirements for evaluating multi-modal SLAM systems,
including hardware synchronization of sensors, 6-DOF ground truth and loops on
long trajectories.
  We run multimodal state-of-the art SLAM methods on the dataset, showcasing
the existing limitations in their application on agricultural settings. The
dataset and utilities to work with it are released on
https://cifasis.github.io/rosariov2/.","Nicolas Soncini, Javier Cremona, Erica Vidal, Maximiliano García, Gastón Castro, Taihú Pire",2025-08-29 13:58:55+00:00,http://arxiv.org/abs/2508.21635v1,http://arxiv.org/pdf/2508.21635v1,"cs.RO, cs.CV, cs.SY, eess.SY, I.2.9"
Evaluation of Large Language Models for Anomaly Detection in Autonomous Vehicles,"The rapid evolution of large language models (LLMs) has pushed their
boundaries to many applications in various domains. Recently, the research
community has started to evaluate their potential adoption in autonomous
vehicles and especially as complementary modules in the perception and planning
software stacks. However, their evaluation is limited in synthetic datasets or
manually driving datasets without the ground truth knowledge and more
precisely, how the current perception and planning algorithms would perform in
the cases under evaluation. For this reason, this work evaluates LLMs on
real-world edge cases where current autonomous vehicles have been proven to
fail. The proposed architecture consists of an open vocabulary object detector
coupled with prompt engineering and large language model contextual reasoning.
We evaluate several state-of-the-art models against real edge cases and provide
qualitative comparison results along with a discussion on the findings for the
potential application of LLMs as anomaly detectors in autonomous vehicles.","Petros Loukas, David Bassir, Savvas Chatzichristofis, Angelos Amanatiadis",2025-08-29 13:05:13+00:00,http://arxiv.org/abs/2509.05315v1,http://arxiv.org/pdf/2509.05315v1,"cs.RO, cs.AI, cs.CV"
Learning Agile Gate Traversal via Analytical Optimal Policy Gradient,"Traversing narrow gates presents a significant challenge and has become a
standard benchmark for evaluating agile and precise quadrotor flight.
Traditional modularized autonomous flight stacks require extensive design and
parameter tuning, while end-to-end reinforcement learning (RL) methods often
suffer from low sample efficiency and limited interpretability. In this work,
we present a novel hybrid framework that adaptively fine-tunes model predictive
control (MPC) parameters online using outputs from a neural network (NN)
trained offline. The NN jointly predicts a reference pose and cost-function
weights, conditioned on the coordinates of the gate corners and the current
drone state. To achieve efficient training, we derive analytical policy
gradients not only for the MPC module but also for an optimization-based gate
traversal detection module. Furthermore, we introduce a new formulation of the
attitude tracking error that admits a simplified representation, facilitating
effective learning with bounded gradients. Hardware experiments demonstrate
that our method enables fast and accurate quadrotor traversal through narrow
gates in confined environments. It achieves several orders of magnitude
improvement in sample efficiency compared to naive end-to-end RL approaches.","Tianchen Sun, Bingheng Wang, Longbin Tang, Yichao Gao, Lin Zhao",2025-08-29 12:48:39+00:00,http://arxiv.org/abs/2508.21592v1,http://arxiv.org/pdf/2508.21592v1,cs.RO
Estimated Informed Anytime Search for Sampling-Based Planning via Adaptive Sampler,"Path planning in robotics often involves solving continuously valued,
high-dimensional problems. Popular informed approaches include graph-based
searches, such as A*, and sampling-based methods, such as Informed RRT*, which
utilize informed set and anytime strategies to expedite path optimization
incrementally. Informed sampling-based planners define informed sets as subsets
of the problem domain based on the current best solution cost. However, when no
solution is found, these planners re-sample and explore the entire
configuration space, which is time-consuming and computationally expensive.
This article introduces Multi-Informed Trees (MIT*), a novel planner that
constructs estimated informed sets based on prior admissible solution costs
before finding the initial solution, thereby accelerating the initial
convergence rate. Moreover, MIT* employs an adaptive sampler that dynamically
adjusts the sampling strategy based on the exploration process. Furthermore,
MIT* utilizes length-related adaptive sparse collision checks to guide lazy
reverse search. These features enhance path cost efficiency and computation
times while ensuring high success rates in confined scenarios. Through a series
of simulations and real-world experiments, it is confirmed that MIT*
outperforms existing single-query, sampling-based planners for problems in R^4
to R^16 and has been successfully applied to real-world robot manipulation
tasks. A video showcasing our experimental results is available at:
https://youtu.be/30RsBIdexTU","Liding Zhang, Kuanqi Cai, Yu Zhang, Zhenshan Bing, Chaoqun Wang, Fan Wu, Sami Haddadin, Alois Knoll",2025-08-29 12:05:01+00:00,http://arxiv.org/abs/2508.21549v1,http://arxiv.org/pdf/2508.21549v1,cs.RO
Complete Gaussian Splats from a Single Image with Denoising Diffusion Models,"Gaussian splatting typically requires dense observations of the scene and can
fail to reconstruct occluded and unobserved areas. We propose a latent
diffusion model to reconstruct a complete 3D scene with Gaussian splats,
including the occluded parts, from only a single image during inference.
Completing the unobserved surfaces of a scene is challenging due to the
ambiguity of the plausible surfaces. Conventional methods use a
regression-based formulation to predict a single ""mode"" for occluded and
out-of-frustum surfaces, leading to blurriness, implausibility, and failure to
capture multiple possible explanations. Thus, they often address this problem
partially, focusing either on objects isolated from the background,
reconstructing only visible surfaces, or failing to extrapolate far from the
input views. In contrast, we propose a generative formulation to learn a
distribution of 3D representations of Gaussian splats conditioned on a single
input image. To address the lack of ground-truth training data, we propose a
Variational AutoReconstructor to learn a latent space only from 2D images in a
self-supervised manner, over which a diffusion model is trained. Our method
generates faithful reconstructions and diverse samples with the ability to
complete the occluded surfaces for high-quality 360-degree renderings.","Ziwei Liao, Mohamed Sayed, Steven L. Waslander, Sara Vicente, Daniyar Turmukhambetov, Michael Firman",2025-08-29 11:55:47+00:00,http://arxiv.org/abs/2508.21542v1,http://arxiv.org/pdf/2508.21542v1,"cs.CV, cs.AI, cs.RO"
ManipDreamer3D : Synthesizing Plausible Robotic Manipulation Video with Occupancy-aware 3D Trajectory,"Data scarcity continues to be a major challenge in the field of robotic
manipulation. Although diffusion models provide a promising solution for
generating robotic manipulation videos, existing methods largely depend on 2D
trajectories, which inherently face issues with 3D spatial ambiguity. In this
work, we present a novel framework named ManipDreamer3D for generating
plausible 3D-aware robotic manipulation videos from the input image and the
text instruction. Our method combines 3D trajectory planning with a
reconstructed 3D occupancy map created from a third-person perspective, along
with a novel trajectory-to-video diffusion model. Specifically, ManipDreamer3D
first reconstructs the 3D occupancy representation from the input image and
then computes an optimized 3D end-effector trajectory, minimizing path length
while avoiding collisions. Next, we employ a latent editing technique to create
video sequences from the initial image latent and the optimized 3D trajectory.
This process conditions our specially trained trajectory-to-video diffusion
model to produce robotic pick-and-place videos. Our method generates robotic
videos with autonomously planned plausible 3D trajectories, significantly
reducing human intervention requirements. Experimental results demonstrate
superior visual quality compared to existing methods.","Ying Li, Xiaobao Wei, Xiaowei Chi, Yuming Li, Zhongyu Zhao, Hao Wang, Ningning Ma, Ming Lu, Shanghang Zhang",2025-08-29 10:39:06+00:00,http://arxiv.org/abs/2509.05314v1,http://arxiv.org/pdf/2509.05314v1,"cs.RO, cs.AI, cs.CV"
Few-Shot Neuro-Symbolic Imitation Learning for Long-Horizon Planning and Acting,"Imitation learning enables intelligent systems to acquire complex behaviors
with minimal supervision. However, existing methods often focus on
short-horizon skills, require large datasets, and struggle to solve
long-horizon tasks or generalize across task variations and distribution
shifts. We propose a novel neuro-symbolic framework that jointly learns
continuous control policies and symbolic domain abstractions from a few skill
demonstrations. Our method abstracts high-level task structures into a graph,
discovers symbolic rules via an Answer Set Programming solver, and trains
low-level controllers using diffusion policy imitation learning. A high-level
oracle filters task-relevant information to focus each controller on a minimal
observation and action space. Our graph-based neuro-symbolic framework enables
capturing complex state transitions, including non-spatial and temporal
relations, that data-driven learning or clustering techniques often fail to
discover in limited demonstration datasets. We validate our approach in six
domains that involve four robotic arms, Stacking, Kitchen, Assembly, and Towers
of Hanoi environments, and a distinct Automated Forklift domain with two
environments. The results demonstrate high data efficiency with as few as five
skill demonstrations, strong zero- and few-shot generalizations, and
interpretable decision making.","Pierrick Lorang, Hong Lu, Johannes Huemer, Patrik Zips, Matthias Scheutz",2025-08-29 10:30:58+00:00,http://arxiv.org/abs/2508.21501v1,http://arxiv.org/pdf/2508.21501v1,cs.RO
Assessing Human Cooperation for Enhancing Social Robot Navigation,"Socially aware robot navigation is a planning paradigm where the robot
navigates in human environments and tries to adhere to social constraints while
interacting with the humans in the scene. These navigation strategies were
further improved using human prediction models, where the robot takes the
potential future trajectory of humans while computing its own. Though these
strategies significantly improve the robot's behavior, it faces difficulties
from time to time when the human behaves in an unexpected manner. This happens
as the robot fails to understand human intentions and cooperativeness, and the
human does not have a clear idea of what the robot is planning to do. In this
paper, we aim to address this gap through effective communication at an
appropriate time based on a geometric analysis of the context and human
cooperativeness in head-on crossing scenarios. We provide an assessment
methodology and propose some evaluation metrics that could distinguish a
cooperative human from a non-cooperative one. Further, we also show how
geometric reasoning can be used to generate appropriate verbal responses or
robot actions.","Hariharan Arunachalam, Phani Teja Singamaneni, Rachid Alami",2025-08-29 09:38:21+00:00,http://arxiv.org/abs/2508.21455v1,http://arxiv.org/pdf/2508.21455v1,cs.RO
RoboInspector: Unveiling the Unreliability of Policy Code for LLM-enabled Robotic Manipulation,"Large language models (LLMs) demonstrate remarkable capabilities in reasoning
and code generation, enabling robotic manipulation to be initiated with just a
single instruction. The LLM carries out various tasks by generating policy code
required to control the robot. Despite advances in LLMs, achieving reliable
policy code generation remains a significant challenge due to the diverse
requirements of real-world tasks and the inherent complexity of user
instructions. In practice, different users may provide distinct instructions to
drive the robot for the same task, which may cause the unreliability of policy
code generation. To bridge this gap, we design RoboInspector, a pipeline to
unveil and characterize the unreliability of the policy code for LLM-enabled
robotic manipulation from two perspectives: the complexity of the manipulation
task and the granularity of the instruction. We perform comprehensive
experiments with 168 distinct combinations of tasks, instructions, and LLMs in
two prominent frameworks. The RoboInspector identifies four main unreliable
behaviors that lead to manipulation failure. We provide a detailed
characterization of these behaviors and their underlying causes, giving insight
for practical development to reduce unreliability. Furthermore, we introduce a
refinement approach guided by failure policy code feedback that improves the
reliability of policy code generation by up to 35% in LLM-enabled robotic
manipulation, evaluated in both simulation and real-world environments.","Chenduo Ying, Linkang Du, Peng Cheng, Yuanchao Shu",2025-08-29 07:47:17+00:00,http://arxiv.org/abs/2508.21378v1,http://arxiv.org/pdf/2508.21378v1,"cs.RO, cs.AI"
Dynamics-Compliant Trajectory Diffusion for Super-Nominal Payload Manipulation,"Nominal payload ratings for articulated robots are typically derived from
worst-case configurations, resulting in uniform payload constraints across the
entire workspace. This conservative approach severely underutilizes the robot's
inherent capabilities -- our analysis demonstrates that manipulators can safely
handle payloads well above nominal capacity across broad regions of their
workspace while staying within joint angle, velocity, acceleration, and torque
limits. To address this gap between assumed and actual capability, we propose a
novel trajectory generation approach using denoising diffusion models that
explicitly incorporates payload constraints into the planning process. Unlike
traditional sampling-based methods that rely on inefficient trial-and-error,
optimization-based methods that are prohibitively slow, or kinodynamic planners
that struggle with problem dimensionality, our approach generates dynamically
feasible joint-space trajectories in constant time that can be directly
executed on physical hardware without post-processing. Experimental validation
on a 7 DoF Franka Emika Panda robot demonstrates that up to 67.6% of the
workspace remains accessible even with payloads exceeding 3 times the nominal
capacity. This expanded operational envelope highlights the importance of a
more nuanced consideration of payload dynamics in motion planning algorithms.","Anuj Pasricha, Joewie Koh, Jay Vakil, Alessandro Roncone",2025-08-29 07:40:00+00:00,http://arxiv.org/abs/2508.21375v1,http://arxiv.org/pdf/2508.21375v1,cs.RO
Multi-Modal Model Predictive Path Integral Control for Collision Avoidance,"This paper proposes a novel approach to motion planning and decision-making
for automated vehicles, using a multi-modal Model Predictive Path Integral
control algorithm. The method samples with Sobol sequences around the prior
input and incorporates analytical solutions for collision avoidance. By
leveraging multiple modes, the multi-modal control algorithm explores diverse
trajectories, such as manoeuvring around obstacles or stopping safely before
them, mitigating the risk of sub-optimal solutions. A non-linear single-track
vehicle model with a Fiala tyre serves as the prediction model, and tyre force
constraints within the friction circle are enforced to ensure vehicle stability
during evasive manoeuvres. The optimised steering angle and longitudinal
acceleration are computed to generate a collision-free trajectory and to
control the vehicle. In a high-fidelity simulation environment, we demonstrate
that the proposed algorithm can successfully avoid obstacles, keeping the
vehicle stable while driving a double lane change manoeuvre on high and
low-friction road surfaces and occlusion scenarios with moving obstacles,
outperforming a standard Model Predictive Path Integral approach.","Alberto Bertipaglia, Dariu M. Gavrila, Barys Shyrokau",2025-08-29 07:13:17+00:00,http://arxiv.org/abs/2508.21364v1,http://arxiv.org/pdf/2508.21364v1,"cs.RO, cs.SY, eess.SY"
Robust Real-Time Coordination of CAVs: A Distributed Optimization Framework under Uncertainty,"Achieving both safety guarantees and real-time performance in cooperative
vehicle coordination remains a fundamental challenge, particularly in dynamic
and uncertain environments. This paper presents a novel coordination framework
that resolves this challenge through three key innovations: 1) direct control
of vehicles' trajectory distributions during coordination, formulated as a
robust cooperative planning problem with adaptive enhanced safety constraints,
ensuring a specified level of safety regarding the uncertainty of the
interactive trajectory, 2) a fully parallel ADMM-based distributed trajectory
negotiation (ADMM-DTN) algorithm that efficiently solves the optimization
problem while allowing configurable negotiation rounds to balance solution
quality and computational resources, and 3) an interactive attention mechanism
that selectively focuses on critical interactive participants to further
enhance computational efficiency. Both simulation results and practical
experiments demonstrate that our framework achieves significant advantages in
safety (reducing collision rates by up to 40.79\% in various scenarios) and
real-time performance compared to state-of-the-art methods, while maintaining
strong scalability with increasing vehicle numbers. The proposed interactive
attention mechanism further reduces the computational demand by 14.1\%. The
framework's effectiveness is further validated through real-world experiments
with unexpected dynamic obstacles, demonstrating robust coordination in complex
environments. The experiment demo could be found at
https://youtu.be/4PZwBnCsb6Q.","Haojie Bai, Yang Wang, Cong Guo, Xiongwei Zhao, Hai Zhu",2025-08-29 04:24:49+00:00,http://arxiv.org/abs/2508.21322v1,http://arxiv.org/pdf/2508.21322v1,cs.RO
Cooperative Sensing Enhanced UAV Path-Following and Obstacle Avoidance with Variable Formation,"The high mobility of unmanned aerial vehicles (UAVs) enables them to be used
in various civilian fields, such as rescue and cargo transport. Path-following
is a crucial way to perform these tasks while sensing and collision avoidance
are essential for safe flight. In this paper, we investigate how to efficiently
and accurately achieve path-following, obstacle sensing and avoidance subtasks,
as well as their conflict-free fusion scheduling. Firstly, a high precision
deep reinforcement learning (DRL)-based UAV formation path-following model is
developed, and the reward function with adaptive weights is designed from the
perspective of distance and velocity errors. Then, we use integrated sensing
and communication (ISAC) signals to detect the obstacle and derive the
Cramer-Rao lower bound (CRLB) for obstacle sensing by information-level fusion,
based on which we propose the variable formation enhanced obstacle position
estimation (VFEO) algorithm. In addition, an online obstacle avoidance scheme
without pretraining is designed to solve the sparse reward. Finally, with the
aid of null space based (NSB) behavioral method, we present a hierarchical
subtasks fusion strategy. Simulation results demonstrate the effectiveness and
superiority of the subtask algorithms and the hierarchical fusion strategy.","Changheng Wang, Zhiqing Wei, Wangjun Jiang, Haoyue Jiang, Zhiyong Feng",2025-08-29 02:48:41+00:00,http://arxiv.org/abs/2508.21316v1,http://arxiv.org/pdf/2508.21316v1,"eess.SY, cs.RO, cs.SY"
Observability-driven Assignment of Heterogeneous Sensors for Multi-Target Tracking,"This paper addresses the challenge of assigning heterogeneous sensors (i.e.,
robots with varying sensing capabilities) for multi-target tracking. We
classify robots into two categories: (1) sufficient sensing robots, equipped
with range and bearing sensors, capable of independently tracking targets, and
(2) limited sensing robots, which are equipped with only range or bearing
sensors and need to at least form a pair to collaboratively track a target. Our
objective is to optimize tracking quality by minimizing uncertainty in target
state estimation through efficient robot-to-target assignment. By leveraging
matroid theory, we propose a greedy assignment algorithm that dynamically
allocates robots to targets to maximize tracking quality. The algorithm
guarantees constant-factor approximation bounds of 1/3 for arbitrary tracking
quality functions and 1/2 for submodular functions, while maintaining
polynomial-time complexity. Extensive simulations demonstrate the algorithm's
effectiveness in accurately estimating and tracking targets over extended
periods. Furthermore, numerical results confirm that the algorithm's
performance is close to that of the optimal assignment, highlighting its
robustness and practical applicability.","Seyed Ali Rakhshan, Mehdi Golestani, He Kong",2025-08-29 02:22:29+00:00,http://arxiv.org/abs/2508.21309v1,http://arxiv.org/pdf/2508.21309v1,cs.RO
Detecting Domain Shifts in Myoelectric Activations: Challenges and Opportunities in Stream Learning,"Detecting domain shifts in myoelectric activations poses a significant
challenge due to the inherent non-stationarity of electromyography (EMG)
signals. This paper explores the detection of domain shifts using data stream
(DS) learning techniques, focusing on the DB6 dataset from the Ninapro
database. We define domains as distinct time-series segments based on different
subjects and recording sessions, applying Kernel Principal Component Analysis
(KPCA) with a cosine kernel to pre-process and highlight these shifts. By
evaluating multiple drift detection methods such as CUSUM, Page-Hinckley, and
ADWIN, we reveal the limitations of current techniques in achieving high
performance for real-time domain shift detection in EMG signals. Our results
underscore the potential of streaming-based approaches for maintaining stable
EMG decoding models, while highlighting areas for further research to enhance
robustness and accuracy in real-world scenarios.","Yibin Sun, Nick Lim, Guilherme Weigert Cassales, Heitor Murilo Gomes, Bernhard Pfahringer, Albert Bifet, Anany Dwivedi",2025-08-29 00:47:28+00:00,http://arxiv.org/abs/2508.21278v1,http://arxiv.org/pdf/2508.21278v1,"cs.LG, cs.RO"
Learning to Assemble the Soma Cube with Legal-Action Masked DQN and Safe ZYZ Regrasp on a Doosan M0609,"This paper presents the first comprehensive application of legal-action
masked Deep Q-Networks with safe ZYZ regrasp strategies to an underactuated
gripper-equipped 6-DOF collaborative robot for autonomous Soma cube assembly
learning. Our approach represents the first systematic integration of
constraint-aware reinforcement learning with singularity-safe motion planning
on a Doosan M0609 collaborative robot. We address critical challenges in
robotic manipulation: combinatorial action space explosion, unsafe motion
planning, and systematic assembly strategy learning. Our system integrates a
legal-action masked DQN with hierarchical architecture that decomposes
Q-function estimation into orientation and position components, reducing
computational complexity from $O(3,132)$ to $O(116) + O(27)$ while maintaining
solution completeness. The robot-friendly reward function encourages
ground-first, vertically accessible assembly sequences aligned with
manipulation constraints. Curriculum learning across three progressive
difficulty levels (2-piece, 3-piece, 7-piece) achieves remarkable training
efficiency: 100\% success rate for Level 1 within 500 episodes, 92.9\% for
Level 2, and 39.9\% for Level 3 over 105,300 total training episodes.","Jaehong Oh, Seungjun Jung, Sawoong Kim",2025-08-29 00:27:03+00:00,http://arxiv.org/abs/2508.21272v1,http://arxiv.org/pdf/2508.21272v1,"cs.RO, stat.CO"
Mini Autonomous Car Driving based on 3D Convolutional Neural Networks,"Autonomous driving applications have become increasingly relevant in the
automotive industry due to their potential to enhance vehicle safety,
efficiency, and user experience, thereby meeting the growing demand for
sophisticated driving assistance features. However, the development of reliable
and trustworthy autonomous systems poses challenges such as high complexity,
prolonged training periods, and intrinsic levels of uncertainty. Mini
Autonomous Cars (MACs) are used as a practical testbed, enabling validation of
autonomous control methodologies on small-scale setups. This simplified and
cost-effective environment facilitates rapid evaluation and comparison of
machine learning models, which is particularly useful for algorithms requiring
online training. To address these challenges, this work presents a methodology
based on RGB-D information and three-dimensional convolutional neural networks
(3D CNNs) for MAC autonomous driving in simulated environments. We evaluate the
proposed approach against recurrent neural networks (RNNs), with architectures
trained and tested on two simulated tracks with distinct environmental
features. Performance was assessed using task completion success, lap-time
metrics, and driving consistency. Results highlight how architectural
modifications and track complexity influence the models' generalization
capability and vehicle control performance. The proposed 3D CNN demonstrated
promising results when compared with RNNs.","Pablo Moraes, Monica Rodriguez, Kristofer S. Kappel, Hiago Sodre, Santiago Fernandez, Igor Nunes, Bruna Guterres, Ricardo Grando",2025-08-29 00:21:34+00:00,http://arxiv.org/abs/2508.21271v1,http://arxiv.org/pdf/2508.21271v1,"cs.RO, cs.CV"
Remarks on stochastic cloning and delayed-state filtering,"Many estimation problems in robotics and navigation involve measurements that
depend on prior states. A prominent example is odometry, which measures the
relative change between states over time. Accurately handling these
delayed-state measurements requires capturing their correlations with prior
state estimates, and a widely used approach is stochastic cloning (SC), which
augments the state vector to account for these correlations.
  This work revisits a long-established but often overlooked alternative--the
delayed-state Kalman filter--and demonstrates that a properly derived filter
yields exactly the same state and covariance update as SC, without requiring
state augmentation. Moreover, the generalized Kalman filter formulation
provides computational advantages, while also reducing memory requirements for
higher-dimensional states.
  Our findings clarify a common misconception that Kalman filter variants are
inherently unable to handle correlated delayed-state measurements,
demonstrating that an alternative formulation achieves the same results more
efficiently.","Tara Mina, Lindsey Marinello, John Christian",2025-08-28 23:17:42+00:00,http://arxiv.org/abs/2508.21260v1,http://arxiv.org/pdf/2508.21260v1,"cs.RO, eess.SP, math.ST, stat.TH"
Uncertainty-Aware Ankle Exoskeleton Control,"Lower limb exoskeletons show promise to assist human movement, but their
utility is limited by controllers designed for discrete, predefined actions in
controlled environments, restricting their real-world applicability. We present
an uncertainty-aware control framework that enables ankle exoskeletons to
operate safely across diverse scenarios by automatically disengaging when
encountering unfamiliar movements. Our approach uses an uncertainty estimator
to classify movements as similar (in-distribution) or different
(out-of-distribution) relative to actions in the training set. We evaluated
three architectures (model ensembles, autoencoders, and generative adversarial
networks) on an offline dataset and tested the strongest performing
architecture (ensemble of gait phase estimators) online. The online test
demonstrated the ability of our uncertainty estimator to turn assistance on and
off as the user transitioned between in-distribution and out-of-distribution
tasks (F1: 89.2). This new framework provides a path for exoskeletons to safely
and autonomously support human movement in unstructured, everyday environments.","Fatima Mumtaza Tourk, Bishoy Galoaa, Sanat Shajan, Aaron J. Young, Michael Everett, Max K. Shepherd",2025-08-28 21:24:04+00:00,http://arxiv.org/abs/2508.21221v1,http://arxiv.org/pdf/2508.21221v1,cs.RO
Multi-robot Path Planning and Scheduling via Model Predictive Optimal Transport (MPC-OT),"In this paper, we propose a novel methodology for path planning and
scheduling for multi-robot navigation that is based on optimal transport theory
and model predictive control. We consider a setup where $N$ robots are tasked
to navigate to $M$ targets in a common space with obstacles. Mapping robots to
targets first and then planning paths can result in overlapping paths that lead
to deadlocks. We derive a strategy based on optimal transport that not only
provides minimum cost paths from robots to targets but also guarantees
non-overlapping trajectories. We achieve this by discretizing the space of
interest into $K$ cells and by imposing a ${K\times K}$ cost structure that
describes the cost of transitioning from one cell to another. Optimal transport
then provides \textit{optimal and non-overlapping} cell transitions for the
robots to reach the targets that can be readily deployed without any scheduling
considerations. The proposed solution requires $\unicode{x1D4AA}(K^3\log K)$
computations in the worst-case and $\unicode{x1D4AA}(K^2\log K)$ for
well-behaved problems. To further accommodate potentially overlapping
trajectories (unavoidable in certain situations) as well as robot dynamics, we
show that a temporal structure can be integrated into optimal transport with
the help of \textit{replans} and \textit{model predictive control}.","Usman A. Khan, Mouhacine Benosman, Wenliang Liu, Federico Pecora, Joseph W. Durham",2025-08-28 20:47:33+00:00,http://arxiv.org/abs/2508.21205v1,http://arxiv.org/pdf/2508.21205v1,"cs.RO, cs.LG"
Observer Design for Optical Flow-Based Visual-Inertial Odometry with Almost-Global Convergence,"This paper presents a novel cascaded observer architecture that combines
optical flow and IMU measurements to perform continuous monocular
visual-inertial odometry (VIO). The proposed solution estimates body-frame
velocity and gravity direction simultaneously by fusing velocity direction
information from optical flow measurements with gyro and accelerometer data.
This fusion is achieved using a globally exponentially stable Riccati observer,
which operates under persistently exciting translational motion conditions. The
estimated gravity direction in the body frame is then employed, along with an
optional magnetometer measurement, to design a complementary observer on
$\mathbf{SO}(3)$ for attitude estimation. The resulting interconnected observer
architecture is shown to be almost globally asymptotically stable. To extract
the velocity direction from sparse optical flow data, a gradient descent
algorithm is developed to solve a constrained minimization problem on the unit
sphere. The effectiveness of the proposed algorithms is validated through
simulation results.","Tarek Bouazza, Soulaimane Berkane, Minh-Duc Hua, Tarek Hamel",2025-08-28 18:55:16+00:00,http://arxiv.org/abs/2508.21163v1,http://arxiv.org/pdf/2508.21163v1,"cs.RO, cs.SY, eess.SY"
A Comparative Study of Spline-Based Trajectory Reconstruction Methods Across Varying Automatic Vehicle Location Data Densities,"Automatic vehicle location (AVL) data offers insights into transit dynamics,
but its effectiveness is often hampered by inconsistent update frequencies,
necessitating trajectory reconstruction. This research evaluates 13 trajectory
reconstruction methods, including several novel approaches, using
high-resolution AVL data from Austin, Texas. We examine the interplay of four
critical factors -- velocity, position, smoothing, and data density -- on
reconstruction performance. A key contribution of this study is evaluation of
these methods across sparse and dense datasets, providing insights into the
trade-off between accuracy and resource allocation. Our evaluation framework
combines traditional mathematical error metrics for positional and velocity
with practical considerations, such as physical realism (e.g., aligning
velocity and acceleration with stopped states, deceleration rates, and speed
variability). In addition, we provide insight into the relative value of each
method in calculating realistic metrics for infrastructure evaluations. Our
findings indicate that velocity-aware methods consistently outperform
position-only approaches. Interestingly, we discovered that smoothing-based
methods can degrade overall performance in complex, congested urban
environments, although enforcing monotonicity remains critical. The velocity
constrained Hermite interpolation with monotonicity enforcement (VCHIP-ME)
yields optimal results, offering a balance between high accuracy and
computational efficiency. Its minimal overhead makes it suitable for both
historical analysis and real-time applications, providing significant
predictive power when combined with dense datasets. These findings offer
practical guidance for researchers and practitioners implementing trajectory
reconstruction systems and emphasize the importance of investing in
higher-frequency AVL data collection for improved analysis.","Jake Robbennolt, Sirajum Munira, Stephen D. Boyles",2025-08-28 18:45:29+00:00,http://arxiv.org/abs/2509.00119v1,http://arxiv.org/pdf/2509.00119v1,cs.RO
Learning on the Fly: Rapid Policy Adaptation via Differentiable Simulation,"Learning control policies in simulation enables rapid, safe, and
cost-effective development of advanced robotic capabilities. However,
transferring these policies to the real world remains difficult due to the
sim-to-real gap, where unmodeled dynamics and environmental disturbances can
degrade policy performance. Existing approaches, such as domain randomization
and Real2Sim2Real pipelines, can improve policy robustness, but either struggle
under out-of-distribution conditions or require costly offline retraining. In
this work, we approach these problems from a different perspective. Instead of
relying on diverse training conditions before deployment, we focus on rapidly
adapting the learned policy in the real world in an online fashion. To achieve
this, we propose a novel online adaptive learning framework that unifies
residual dynamics learning with real-time policy adaptation inside a
differentiable simulation. Starting from a simple dynamics model, our framework
refines the model continuously with real-world data to capture unmodeled
effects and disturbances such as payload changes and wind. The refined dynamics
model is embedded in a differentiable simulation framework, enabling gradient
backpropagation through the dynamics and thus rapid, sample-efficient policy
updates beyond the reach of classical RL methods like PPO. All components of
our system are designed for rapid adaptation, enabling the policy to adjust to
unseen disturbances within 5 seconds of training. We validate the approach on
agile quadrotor control under various disturbances in both simulation and the
real world. Our framework reduces hovering error by up to 81% compared to
L1-MPC and 55% compared to DATT, while also demonstrating robustness in
vision-based control without explicit state estimation.","Jiahe Pan, Jiaxu Xing, Rudolf Reiter, Yifan Zhai, Elie Aljalbout, Davide Scaramuzza",2025-08-28 17:59:34+00:00,http://arxiv.org/abs/2508.21065v1,http://arxiv.org/pdf/2508.21065v1,cs.RO
Embodied AI: Emerging Risks and Opportunities for Policy Action,"The field of embodied AI (EAI) is rapidly advancing. Unlike virtual AI, EAI
systems can exist in, learn from, reason about, and act in the physical world.
With recent advances in AI models and hardware, EAI systems are becoming
increasingly capable across wider operational domains. While EAI systems can
offer many benefits, they also pose significant risks, including physical harm
from malicious use, mass surveillance, as well as economic and societal
disruption. These risks require urgent attention from policymakers, as existing
policies governing industrial robots and autonomous vehicles are insufficient
to address the full range of concerns EAI systems present. To help address this
issue, this paper makes three contributions. First, we provide a taxonomy of
the physical, informational, economic, and social risks EAI systems pose.
Second, we analyze policies in the US, EU, and UK to assess how existing
frameworks address these risks and to identify critical gaps. We conclude by
offering policy recommendations for the safe and beneficial deployment of EAI
systems, such as mandatory testing and certification schemes, clarified
liability frameworks, and strategies to manage EAI's potentially transformative
economic and societal impacts.","Jared Perlo, Alexander Robey, Fazl Barez, Luciano Floridi, Jakob Mökander",2025-08-28 17:59:07+00:00,http://arxiv.org/abs/2509.00117v2,http://arxiv.org/pdf/2509.00117v2,"cs.CY, cs.AI, cs.RO"
Prompt-to-Product: Generative Assembly via Bimanual Manipulation,"Creating assembly products demands significant manual effort and expert
knowledge in 1) designing the assembly and 2) constructing the product. This
paper introduces Prompt-to-Product, an automated pipeline that generates
real-world assembly products from natural language prompts. Specifically, we
leverage LEGO bricks as the assembly platform and automate the process of
creating brick assembly structures. Given the user design requirements,
Prompt-to-Product generates physically buildable brick designs, and then
leverages a bimanual robotic system to construct the real assembly products,
bringing user imaginations into the real world. We conduct a comprehensive user
study, and the results demonstrate that Prompt-to-Product significantly lowers
the barrier and reduces manual effort in creating assembly products from
imaginative ideas.","Ruixuan Liu, Philip Huang, Ava Pun, Kangle Deng, Shobhit Aggarwal, Kevin Tang, Michelle Liu, Deva Ramanan, Jun-Yan Zhu, Jiaoyang Li, Changliu Liu",2025-08-28 17:59:05+00:00,http://arxiv.org/abs/2508.21063v1,http://arxiv.org/pdf/2508.21063v1,"cs.RO, cs.AI"
CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification,"Recent Vision-Language-Action (VLA) models built on pre-trained
Vision-Language Models (VLMs) require extensive post-training, resulting in
high computational overhead that limits scalability and deployment.We propose
CogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages
instruction-driven routing and sparsification to improve both efficiency and
performance. CogVLA draws inspiration from human multimodal coordination and
introduces a 3-stage progressive architecture. 1) Encoder-FiLM based
Aggregation Routing (EFA-Routing) injects instruction information into the
vision encoder to selectively aggregate and compress dual-stream visual tokens,
forming a instruction-aware latent representation. 2) Building upon this
compact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing)
introduces action intent into the language model by pruning
instruction-irrelevant visually grounded tokens, thereby achieving token-level
sparsity. 3) To ensure that compressed perception inputs can still support
accurate and coherent action generation, we introduce V-L-A Coupled Attention
(CAtten), which combines causal vision-language attention with bidirectional
action parallel decoding. Extensive experiments on the LIBERO benchmark and
real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art
performance with success rates of 97.4% and 70.0%, respectively, while reducing
training costs by 2.5-fold and decreasing inference latency by 2.8-fold
compared to OpenVLA. CogVLA is open-sourced and publicly available at
https://github.com/JiuTian-VL/CogVLA.","Wei Li, Renshan Zhang, Rui Shao, Jie He, Liqiang Nie",2025-08-28 17:50:58+00:00,http://arxiv.org/abs/2508.21046v1,http://arxiv.org/pdf/2508.21046v1,"cs.CV, cs.RO"
HITTER: A HumanoId Table TEnnis Robot via Hierarchical Planning and Learning,"Humanoid robots have recently achieved impressive progress in locomotion and
whole-body control, yet they remain constrained in tasks that demand rapid
interaction with dynamic environments through manipulation. Table tennis
exemplifies such a challenge: with ball speeds exceeding 5 m/s, players must
perceive, predict, and act within sub-second reaction times, requiring both
agility and precision. To address this, we present a hierarchical framework for
humanoid table tennis that integrates a model-based planner for ball trajectory
prediction and racket target planning with a reinforcement learning-based
whole-body controller. The planner determines striking position, velocity and
timing, while the controller generates coordinated arm and leg motions that
mimic human strikes and maintain stability and agility across consecutive
rallies. Moreover, to encourage natural movements, human motion references are
incorporated during training. We validate our system on a general-purpose
humanoid robot, achieving up to 106 consecutive shots with a human opponent and
sustained exchanges against another humanoid. These results demonstrate
real-world humanoid table tennis with sub-second reactive control, marking a
step toward agile and interactive humanoid behaviors.","Zhi Su, Bike Zhang, Nima Rahmanian, Yuman Gao, Qiayuan Liao, Caitlin Regan, Koushil Sreenath, S. Shankar Sastry",2025-08-28 17:49:12+00:00,http://arxiv.org/abs/2508.21043v2,http://arxiv.org/pdf/2508.21043v2,cs.RO
EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control,"The human ability to seamlessly perform multimodal reasoning and physical
interaction in the open world is a core goal for general-purpose embodied
intelligent systems. Recent vision-language-action (VLA) models, which are
co-trained on large-scale robot and visual-text data, have demonstrated notable
progress in general robot control. However, they still fail to achieve
human-level flexibility in interleaved reasoning and interaction. In this work,
introduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is
a unified embodied foundation model that achieves superior performance in
multimodal embodied reasoning and robot control through interleaved
vision-text-action pre-training. The development of EO-1 is based on two key
pillars: (i) a unified architecture that processes multimodal inputs
indiscriminately (image, text, video, and action), and (ii) a massive,
high-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains
over 1.5 million samples with emphasis on interleaved vision-text-action
comprehension. EO-1 is trained through synergies between auto-regressive
decoding and flow matching denoising on EO-Data1.5M, enabling seamless robot
action generation and multimodal embodied reasoning. Extensive experiments
demonstrate the effectiveness of interleaved vision-text-action learning for
open-world understanding and generalization, validated through a variety of
long-horizon, dexterous manipulation tasks across multiple embodiments. This
paper details the architecture of EO-1, the data construction strategy of
EO-Data1.5M, and the training methodology, offering valuable insights for
developing advanced embodied foundation models.","Delin Qu, Haoming Song, Qizhi Chen, Zhaoqing Chen, Xianqiang Gao, Xinyi Ye, Qi Lv, Modi Shi, Guanghui Ren, Cheng Ruan, Maoqing Yao, Haoran Yang, Jiacheng Bao, Bin Zhao, Dong Wang",2025-08-28 17:26:15+00:00,http://arxiv.org/abs/2508.21112v3,http://arxiv.org/pdf/2508.21112v3,"cs.RO, cs.AI"
Rapid Mismatch Estimation via Neural Network Informed Variational Inference,"With robots increasingly operating in human-centric environments, ensuring
soft and safe physical interactions, whether with humans, surroundings, or
other machines, is essential. While compliant hardware can facilitate such
interactions, this work focuses on impedance controllers that allow
torque-controlled robots to safely and passively respond to contact while
accurately executing tasks. From inverse dynamics to quadratic
programming-based controllers, the effectiveness of these methods relies on
accurate dynamics models of the robot and the object it manipulates. Any model
mismatch results in task failures and unsafe behaviors. Thus, we introduce
Rapid Mismatch Estimation (RME), an adaptive, controller-agnostic,
probabilistic framework that estimates end-effector dynamics mismatches online,
without relying on external force-torque sensors. From the robot's
proprioceptive feedback, a Neural Network Model Mismatch Estimator generates a
prior for a Variational Inference solver, which rapidly converges to the
unknown parameters while quantifying uncertainty. With a real 7-DoF manipulator
driven by a state-of-the-art passive impedance controller, RME adapts to sudden
changes in mass and center of mass at the end-effector in $\sim400$ ms, in
static and dynamic settings. We demonstrate RME in a collaborative scenario
where a human attaches an unknown basket to the robot's end-effector and
dynamically adds/removes heavy items, showcasing fast and safe adaptation to
changing dynamics during physical interaction without any external sensory
system.","Mateusz Jaszczuk, Nadia Figueroa",2025-08-28 17:09:05+00:00,http://arxiv.org/abs/2508.21007v1,http://arxiv.org/pdf/2508.21007v1,cs.RO
Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees,"Kinodynamic motion planning is concerned with computing collision-free
trajectories while abiding by the robot's dynamic constraints. This critical
problem is often tackled using sampling-based planners (SBPs) that explore the
robot's high-dimensional state space by constructing a search tree via action
propagations. Although SBPs can offer global guarantees on completeness and
solution quality, their performance is often hindered by slow exploration due
to uninformed action sampling. Learning-based approaches can yield
significantly faster runtimes, yet they fail to generalize to
out-of-distribution (OOD) scenarios and lack critical guarantees, e.g., safety,
thus limiting their deployment on physical robots. We present Diffusion Tree
(DiTree): a provably-generalizable framework leveraging diffusion policies
(DPs) as informed samplers to efficiently guide state-space search within SBPs.
DiTree combines DP's ability to model complex distributions of expert
trajectories, conditioned on local observations, with the completeness of SBPs
to yield provably-safe solutions within a few action propagation iterations for
complex dynamical systems. We demonstrate DiTree's power with an implementation
combining the popular RRT planner with a DP action sampler trained on a single
environment. In comprehensive evaluations on OOD scenarios, DiTree achieves on
average a 30% higher success rate compared to standalone DP or SBPs, on a
dynamic car and Mujoco's ant robot settings (for the latter, SBPs fail
completely). Beyond simulation, real-world car experiments confirm DiTree's
applicability, demonstrating superior trajectory quality and robustness even
under severe sim-to-real gaps. Project webpage:
https://sites.google.com/view/ditree.","Yaniv Hassidof, Tom Jurgenson, Kiril Solovey",2025-08-28 17:04:00+00:00,http://arxiv.org/abs/2508.21001v2,http://arxiv.org/pdf/2508.21001v2,"cs.LG, cs.AI, cs.RO"
UltraTac: Integrated Ultrasound-Augmented Visuotactile Sensor for Enhanced Robotic Perception,"Visuotactile sensors provide high-resolution tactile information but are
incapable of perceiving the material features of objects. We present UltraTac,
an integrated sensor that combines visuotactile imaging with ultrasound sensing
through a coaxial optoacoustic architecture. The design shares structural
components and achieves consistent sensing regions for both modalities.
Additionally, we incorporate acoustic matching into the traditional
visuotactile sensor structure, enabling integration of the ultrasound sensing
modality without compromising visuotactile performance. Through tactile
feedback, we dynamically adjust the operating state of the ultrasound module to
achieve flexible functional coordination. Systematic experiments demonstrate
three key capabilities: proximity sensing in the 3-8 cm range ($R^2=0.90$),
material classification (average accuracy: 99.20%), and texture-material
dual-mode object recognition achieving 92.11% accuracy on a 15-class task.
Finally, we integrate the sensor into a robotic manipulation system to
concurrently detect container surface patterns and internal content, which
verifies its potential for advanced human-machine interaction and precise
robotic manipulation.","Junhao Gong, Kit-Wa Sou, Shoujie Li, Changqing Guo, Yan Huang, Chuqiao Lyu, Ziwu Song, Wenbo Ding",2025-08-28 16:37:00+00:00,http://arxiv.org/abs/2508.20982v2,http://arxiv.org/pdf/2508.20982v2,cs.RO
ActLoc: Learning to Localize on the Move via Active Viewpoint Selection,"Reliable localization is critical for robot navigation, yet most existing
systems implicitly assume that all viewing directions at a location are equally
informative. In practice, localization becomes unreliable when the robot
observes unmapped, ambiguous, or uninformative regions. To address this, we
present ActLoc, an active viewpoint-aware planning framework for enhancing
localization accuracy for general robot navigation tasks. At its core, ActLoc
employs a largescale trained attention-based model for viewpoint selection. The
model encodes a metric map and the camera poses used during map construction,
and predicts localization accuracy across yaw and pitch directions at arbitrary
3D locations. These per-point accuracy distributions are incorporated into a
path planner, enabling the robot to actively select camera orientations that
maximize localization robustness while respecting task and motion constraints.
ActLoc achieves stateof-the-art results on single-viewpoint selection and
generalizes effectively to fulltrajectory planning. Its modular design makes it
readily applicable to diverse robot navigation and inspection tasks.","Jiajie Li, Boyang Sun, Luca Di Giammarino, Hermann Blum, Marc Pollefeys",2025-08-28 16:36:02+00:00,http://arxiv.org/abs/2508.20981v1,http://arxiv.org/pdf/2508.20981v1,"cs.RO, cs.CV, cs.LG"
Scaling Fabric-Based Piezoresistive Sensor Arrays for Whole-Body Tactile Sensing,"Scaling tactile sensing for robust whole-body manipulation is a significant
challenge, often limited by wiring complexity, data throughput, and system
reliability. This paper presents a complete architecture designed to overcome
these barriers. Our approach pairs open-source, fabric-based sensors with
custom readout electronics that reduce signal crosstalk to less than 3.3%
through hardware-based mitigation. Critically, we introduce a novel,
daisy-chained SPI bus topology that avoids the practical limitations of common
wireless protocols and the prohibitive wiring complexity of USB hub-based
systems. This architecture streams synchronized data from over 8,000 taxels
across 1 square meter of sensing area at update rates exceeding 50 FPS,
confirming its suitability for real-time control. We validate the system's
efficacy in a whole-body grasping task where, without feedback, the robot's
open-loop trajectory results in an uncontrolled application of force that
slowly crushes a deformable cardboard box. With real-time tactile feedback, the
robot transforms this motion into a gentle, stable grasp, successfully
manipulating the object without causing structural damage. This work provides a
robust and well-characterized platform to enable future research in advanced
whole-body control and physical human-robot interaction.","Curtis C. Johnson, Daniel Webb, David Hill, Marc D. Killpack",2025-08-28 16:19:16+00:00,http://arxiv.org/abs/2508.20959v1,http://arxiv.org/pdf/2508.20959v1,"cs.RO, eess.SP"
PLUME: Procedural Layer Underground Modeling Engine,"As space exploration advances, underground environments are becoming
increasingly attractive due to their potential to provide shelter, easier
access to resources, and enhanced scientific opportunities. Although such
environments exist on Earth, they are often not easily accessible and do not
accurately represent the diversity of underground environments found throughout
the solar system. This paper presents PLUME, a procedural generation framework
aimed at easily creating 3D underground environments. Its flexible structure
allows for the continuous enhancement of various underground features, aligning
with our expanding understanding of the solar system. The environments
generated using PLUME can be used for AI training, evaluating robotics
algorithms, 3D rendering, and facilitating rapid iteration on developed
exploration algorithms. In this paper, it is demonstrated that PLUME has been
used along with a robotic simulator. PLUME is open source and has been released
on Github. https://github.com/Gabryss/P.L.U.M.E","Gabriel Manuel Garcia, Antoine Richard, Miguel Olivares-Mendez",2025-08-28 15:55:36+00:00,http://arxiv.org/abs/2508.20926v1,http://arxiv.org/pdf/2508.20926v1,cs.RO
COMETH: Convex Optimization for Multiview Estimation and Tracking of Humans,"In the era of Industry 5.0, monitoring human activity is essential for
ensuring both ergonomic safety and overall well-being. While multi-camera
centralized setups improve pose estimation accuracy, they often suffer from
high computational costs and bandwidth requirements, limiting scalability and
real-time applicability. Distributing processing across edge devices can reduce
network bandwidth and computational load. On the other hand, the constrained
resources of edge devices lead to accuracy degradation, and the distribution of
computation leads to temporal and spatial inconsistencies. We address this
challenge by proposing COMETH (Convex Optimization for Multiview Estimation and
Tracking of Humans), a lightweight algorithm for real-time multi-view human
pose fusion that relies on three concepts: it integrates kinematic and
biomechanical constraints to increase the joint positioning accuracy; it
employs convex optimization-based inverse kinematics for spatial fusion; and it
implements a state observer to improve temporal consistency. We evaluate COMETH
on both public and industrial datasets, where it outperforms state-of-the-art
methods in localization, detection, and tracking accuracy. The proposed fusion
pipeline enables accurate and scalable human motion tracking, making it
well-suited for industrial and safety-critical applications. The code is
publicly available at https://github.com/PARCO-LAB/COMETH.","Enrico Martini, Ho Jin Choi, Nadia Figueroa, Nicola Bombieri",2025-08-28 15:50:29+00:00,http://arxiv.org/abs/2508.20920v1,http://arxiv.org/pdf/2508.20920v1,"cs.CV, cs.RO"
Language-Enhanced Mobile Manipulation for Efficient Object Search in Indoor Environments,"Enabling robots to efficiently search for and identify objects in complex,
unstructured environments is critical for diverse applications ranging from
household assistance to industrial automation. However, traditional scene
representations typically capture only static semantics and lack interpretable
contextual reasoning, limiting their ability to guide object search in
completely unfamiliar settings. To address this challenge, we propose a
language-enhanced hierarchical navigation framework that tightly integrates
semantic perception and spatial reasoning. Our method, Goal-Oriented
Dynamically Heuristic-Guided Hierarchical Search (GODHS), leverages large
language models (LLMs) to infer scene semantics and guide the search process
through a multi-level decision hierarchy. Reliability in reasoning is achieved
through the use of structured prompts and logical constraints applied at each
stage of the hierarchy. For the specific challenges of mobile manipulation, we
introduce a heuristic-based motion planner that combines polar angle sorting
with distance prioritization to efficiently generate exploration paths.
Comprehensive evaluations in Isaac Sim demonstrate the feasibility of our
framework, showing that GODHS can locate target objects with higher search
efficiency compared to conventional, non-semantic search strategies. Website
and Video are available at: https://drapandiger.github.io/GODHS","Liding Zhang, Zeqi Li, Kuanqi Cai, Qian Huang, Zhenshan Bing, Alois Knoll",2025-08-28 15:27:35+00:00,http://arxiv.org/abs/2508.20899v1,http://arxiv.org/pdf/2508.20899v1,cs.RO
CoCoL: A Communication Efficient Decentralized Collaborative Method for Multi-Robot Systems,"Collaborative learning enhances the performance and adaptability of
multi-robot systems in complex tasks but faces significant challenges due to
high communication overhead and data heterogeneity inherent in multi-robot
tasks. To this end, we propose CoCoL, a Communication efficient decentralized
Collaborative Learning method tailored for multi-robot systems with
heterogeneous local datasets. Leveraging a mirror descent framework, CoCoL
achieves remarkable communication efficiency with approximate Newton-type
updates by capturing the similarity between objective functions of robots, and
reduces computational costs through inexact sub-problem solutions. Furthermore,
the integration of a gradient tracking scheme ensures its robustness against
data heterogeneity. Experimental results on three representative multi robot
collaborative learning tasks show the superiority of the proposed CoCoL in
significantly reducing both the number of communication rounds and total
bandwidth consumption while maintaining state-of-the-art accuracy. These
benefits are particularly evident in challenging scenarios involving non-IID
(non-independent and identically distributed) data distribution, streaming
data, and time-varying network topologies.","Jiaxi Huang, Yan Huang, Yixian Zhao, Wenchao Meng, Jinming Xu",2025-08-28 15:25:48+00:00,http://arxiv.org/abs/2508.20898v1,http://arxiv.org/pdf/2508.20898v1,"cs.RO, cs.LG, cs.MA"
To New Beginnings: A Survey of Unified Perception in Autonomous Vehicle Software,"Autonomous vehicle perception typically relies on modular pipelines that
decompose the task into detection, tracking, and prediction. While
interpretable, these pipelines suffer from error accumulation and limited
inter-task synergy. Unified perception has emerged as a promising paradigm that
integrates these sub-tasks within a shared architecture, potentially improving
robustness, contextual reasoning, and efficiency while retaining interpretable
outputs. In this survey, we provide a comprehensive overview of unified
perception, introducing a holistic and systemic taxonomy that categorizes
methods along task integration, tracking formulation, and representation flow.
We define three paradigms -Early, Late, and Full Unified Perception- and
systematically review existing methods, their architectures, training
strategies, datasets used, and open-source availability, while highlighting
future research directions. This work establishes the first comprehensive
framework for understanding and advancing unified perception, consolidates
fragmented efforts, and guides future research toward more robust,
generalizable, and interpretable perception.","Loïc Stratil, Felix Fent, Esteban Rivera, Markus Lienkamp",2025-08-28 15:20:35+00:00,http://arxiv.org/abs/2508.20892v1,http://arxiv.org/pdf/2508.20892v1,"cs.CV, cs.RO"
Deep Fuzzy Optimization for Batch-Size and Nearest Neighbors in Optimal Robot Motion Planning,"Efficient motion planning algorithms are essential in robotics. Optimizing
essential parameters, such as batch size and nearest neighbor selection in
sampling-based methods, can enhance performance in the planning process.
However, existing approaches often lack environmental adaptability. Inspired by
the method of the deep fuzzy neural networks, this work introduces
Learning-based Informed Trees (LIT*), a sampling-based deep fuzzy
learning-based planner that dynamically adjusts batch size and nearest neighbor
parameters to obstacle distributions in the configuration spaces. By encoding
both global and local ratios via valid and invalid states, LIT* differentiates
between obstacle-sparse and obstacle-dense regions, leading to lower-cost paths
and reduced computation time. Experimental results in high-dimensional spaces
demonstrate that LIT* achieves faster convergence and improved solution
quality. It outperforms state-of-the-art single-query, sampling-based planners
in environments ranging from R^8 to R^14 and is successfully validated on a
dual-arm robot manipulation task. A video showcasing our experimental results
is available at: https://youtu.be/NrNs9zebWWk","Liding Zhang, Qiyang Zong, Yu Zhang, Zhenshan Bing, Alois Knoll",2025-08-28 15:14:15+00:00,http://arxiv.org/abs/2508.20884v1,http://arxiv.org/pdf/2508.20884v1,cs.RO
Genetic Informed Trees (GIT*): Path Planning via Reinforced Genetic Programming Heuristics,"Optimal path planning involves finding a feasible state sequence between a
start and a goal that optimizes an objective. This process relies on heuristic
functions to guide the search direction. While a robust function can improve
search efficiency and solution quality, current methods often overlook
available environmental data and simplify the function structure due to the
complexity of information relationships. This study introduces Genetic Informed
Trees (GIT*), which improves upon Effort Informed Trees (EIT*) by integrating a
wider array of environmental data, such as repulsive forces from obstacles and
the dynamic importance of vertices, to refine heuristic functions for better
guidance. Furthermore, we integrated reinforced genetic programming (RGP),
which combines genetic programming with reward system feedback to mutate
genotype-generative heuristic functions for GIT*. RGP leverages a multitude of
data types, thereby improving computational efficiency and solution quality
within a set timeframe. Comparative analyses demonstrate that GIT* surpasses
existing single-query, sampling-based planners in problems ranging from R^4 to
R^16 and was tested on a real-world mobile manipulation task. A video
showcasing our experimental results is available at
https://youtu.be/URjXbc_BiYg","Liding Zhang, Kuanqi Cai, Zhenshan Bing, Chaoqun Wang, Alois Knoll",2025-08-28 15:02:02+00:00,http://arxiv.org/abs/2508.20871v1,http://arxiv.org/pdf/2508.20871v1,cs.RO
Encoding Tactile Stimuli for Organoid Intelligence in Braille Recognition,"This study proposes a generalizable encoding strategy that maps tactile
sensor data to electrical stimulation patterns, enabling neural organoids to
perform an open-loop artificial tactile Braille classification task. Human
forebrain organoids cultured on a low-density microelectrode array (MEA) are
systematically stimulated to characterize the relationship between electrical
stimulation parameters (number of pulse, phase amplitude, phase duration, and
trigger delay) and organoid responses, measured as spike activity and spatial
displacement of the center of activity. Implemented on event-based tactile
inputs recorded from the Evetac sensor, our system achieved an average Braille
letter classification accuracy of 61 percent with a single organoid, which
increased significantly to 83 percent when responses from a three-organoid
ensemble were combined. Additionally, the multi-organoid configuration
demonstrated enhanced robustness against various types of artificially
introduced noise. This research demonstrates the potential of organoids as
low-power, adaptive bio-hybrid computational elements and provides a
foundational encoding framework for future scalable bio-hybrid computing
architectures.","Tianyi Liu, Hemma Philamore, Benjamin Ward-Cherrier",2025-08-28 14:44:25+00:00,http://arxiv.org/abs/2508.20850v1,http://arxiv.org/pdf/2508.20850v1,"cs.NE, cs.ET, cs.RO"
Learning Primitive Embodied World Models: Towards Scalable Robotic Learning,"While video-generation-based embodied world models have gained increasing
attention, their reliance on large-scale embodied interaction data remains a
key bottleneck. The scarcity, difficulty of collection, and high dimensionality
of embodied data fundamentally limit the alignment granularity between language
and actions and exacerbate the challenge of long-horizon video
generation--hindering generative models from achieving a ""GPT moment"" in the
embodied domain. There is a naive observation: the diversity of embodied data
far exceeds the relatively small space of possible primitive motions. Based on
this insight, we propose a novel paradigm for world modeling--Primitive
Embodied World Models (PEWM). By restricting video generation to fixed short
horizons, our approach 1) enables fine-grained alignment between linguistic
concepts and visual representations of robotic actions, 2) reduces learning
complexity, 3) improves data efficiency in embodied data collection, and 4)
decreases inference latency. By equipping with a modular Vision-Language Model
(VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further
enables flexible closed-loop control and supports compositional generalization
of primitive-level policies over extended, complex tasks. Our framework
leverages the spatiotemporal vision priors in video models and the semantic
awareness of VLMs to bridge the gap between fine-grained physical interaction
and high-level reasoning, paving the way toward scalable, interpretable, and
general-purpose embodied intelligence.","Qiao Sun, Liujia Yang, Wei Tang, Wei Huang, Kaixin Xu, Yongchao Chen, Mingyu Liu, Jiange Yang, Haoyi Zhu, Yating Wang, Tong He, Yilun Chen, Xili Dai, Nanyang Ye, Qinying Gu",2025-08-28 14:31:48+00:00,http://arxiv.org/abs/2508.20840v1,http://arxiv.org/pdf/2508.20840v1,"cs.RO, cs.AI, cs.MM"
Model-Free Hovering and Source Seeking via Extremum Seeking Control: Experimental Demonstration,"In a recent effort, we successfully proposed a categorically novel approach
to mimic the phenomenoa of hovering and source seeking by flapping insects and
hummingbirds using a new extremum seeking control (ESC) approach. Said ESC
approach was shown capable of characterizing the physics of hovering and source
seeking by flapping systems, providing at the same time uniquely novel
opportunity for a model-free, real-time biomimicry control design. In this
paper, we experimentally test and verify, for the first time in the literature,
the potential of ESC in flapping robots to achieve model-free, real-time
controlled hovering and source seeking. The results of this paper, while being
restricted to 1D, confirm the premise of introducing ESC as a natural control
method and biomimicry mechanism to the field of flapping flight and robotics.","Ahmed A. Elgohary, Rohan Palanikumar, Sameh A. Eisa",2025-08-28 14:29:07+00:00,http://arxiv.org/abs/2508.20836v1,http://arxiv.org/pdf/2508.20836v1,"cs.RO, math.OC"
A Soft Fabric-Based Thermal Haptic Device for VR and Teleoperation,"This paper presents a novel fabric-based thermal-haptic interface for virtual
reality and teleoperation. It integrates pneumatic actuation and conductive
fabric with an innovative ultra-lightweight design, achieving only 2~g for each
finger unit. By embedding heating elements within textile pneumatic chambers,
the system delivers modulated pressure and thermal stimuli to fingerpads
through a fully soft, wearable interface.
  Comprehensive characterization demonstrates rapid thermal modulation with
heating rates up to 3$^{\circ}$C/s, enabling dynamic thermal feedback for
virtual or teleoperation interactions. The pneumatic subsystem generates forces
up to 8.93~N at 50~kPa, while optimization of fingerpad-actuator clearance
enhances cooling efficiency with minimal force reduction. Experimental
validation conducted with two different user studies shows high temperature
identification accuracy (0.98 overall) across three thermal levels, and
significant manipulation improvements in a virtual pick-and-place tasks.
Results show enhanced success rates (88.5\% to 96.4\%, p = 0.029) and improved
force control precision (p = 0.013) when haptic feedback is enabled, validating
the effectiveness of the integrated thermal-haptic approach for advanced
human-machine interaction applications.","Rui Chen, Domenico Chiaradia, Antonio Frisoli, Daniele Leonardis",2025-08-28 14:25:35+00:00,http://arxiv.org/abs/2508.20831v1,http://arxiv.org/pdf/2508.20831v1,cs.RO
Uncertainty Aware-Predictive Control Barrier Functions: Safer Human Robot Interaction through Probabilistic Motion Forecasting,"To enable flexible, high-throughput automation in settings where people and
robots share workspaces, collaborative robotic cells must reconcile stringent
safety guarantees with the need for responsive and effective behavior. A
dynamic obstacle is the stochastic, task-dependent variability of human motion:
when robots fall back on purely reactive or worst-case envelopes, they brake
unnecessarily, stall task progress, and tamper with the fluidity that true
Human-Robot Interaction demands. In recent years, learning-based human-motion
prediction has rapidly advanced, although most approaches produce worst-case
scenario forecasts that often do not treat prediction uncertainty in a
well-structured way, resulting in over-conservative planning algorithms,
limiting their flexibility. We introduce Uncertainty-Aware Predictive Control
Barrier Functions (UA-PCBFs), a unified framework that fuses probabilistic
human hand motion forecasting with the formal safety guarantees of Control
Barrier Functions. In contrast to other variants, our framework allows for
dynamic adjustment of the safety margin thanks to the human motion uncertainty
estimation provided by a forecasting module. Thanks to uncertainty estimation,
UA-PCBFs empower collaborative robots with a deeper understanding of future
human states, facilitating more fluid and intelligent interactions through
informed motion planning. We validate UA-PCBFs through comprehensive real-world
experiments with an increasing level of realism, including automated setups (to
perform exactly repeatable motions) with a robotic hand and direct human-robot
interactions (to validate promptness, usability, and human confidence).
Relative to state-of-the-art HRI architectures, UA-PCBFs show better
performance in task-critical metrics, significantly reducing the number of
violations of the robot's safe space during interaction with respect to the
state-of-the-art.","Lorenzo Busellato, Federico Cunico, Diego Dall'Alba, Marco Emporio, Andrea Giachetti, Riccardo Muradore, Marco Cristani",2025-08-28 14:11:26+00:00,http://arxiv.org/abs/2508.20812v1,http://arxiv.org/pdf/2508.20812v1,"cs.RO, cs.AI"
SKGE-SWIN: End-To-End Autonomous Vehicle Waypoint Prediction and Navigation Using Skip Stage Swin Transformer,"Focusing on the development of an end-to-end autonomous vehicle model with
pixel-to-pixel context awareness, this research proposes the SKGE-Swin
architecture. This architecture utilizes the Swin Transformer with a skip-stage
mechanism to broaden feature representation globally and at various network
levels. This approach enables the model to extract information from distant
pixels by leveraging the Swin Transformer's Shifted Window-based Multi-head
Self-Attention (SW-MSA) mechanism and to retain critical information from the
initial to the final stages of feature extraction, thereby enhancing its
capability to comprehend complex patterns in the vehicle's surroundings. The
model is evaluated on the CARLA platform using adversarial scenarios to
simulate real-world conditions. Experimental results demonstrate that the
SKGE-Swin architecture achieves a superior Driving Score compared to previous
methods. Furthermore, an ablation study will be conducted to evaluate the
contribution of each architectural component, including the influence of skip
connections and the use of the Swin Transformer, in improving model
performance.","Fachri Najm Noer Kartiman, Rasim, Yaya Wihardi, Nurul Hasanah, Oskar Natan, Bambang Wahono, Taufik Ibnu Salim",2025-08-28 13:17:35+00:00,http://arxiv.org/abs/2508.20762v1,http://arxiv.org/pdf/2508.20762v1,"cs.CV, cs.AI, cs.LG, cs.RO"
Non-expert to Expert Motion Translation Using Generative Adversarial Networks,"Decreasing skilled workers is a very serious problem in the world. To deal
with this problem, the skill transfer from experts to robots has been
researched. These methods which teach robots by human motion are called
imitation learning. Experts' skills generally appear in not only position data,
but also force data. Thus, position and force data need to be saved and
reproduced. To realize this, a lot of research has been conducted in the
framework of a motion-copying system. Recent research uses machine learning
methods to generate motion commands. However, most of them could not change
tasks by following human intention. Some of them can change tasks by
conditional training, but the labels are limited. Thus, we propose the flexible
motion translation method by using Generative Adversarial Networks. The
proposed method enables users to teach robots tasks by inputting data, and
skills by a trained model. We evaluated the proposed system with a 3-DOF
calligraphy robot.","Yuki Tanaka, Seiichiro Katsura",2025-08-28 13:03:30+00:00,http://arxiv.org/abs/2508.20740v1,http://arxiv.org/pdf/2508.20740v1,"cs.RO, cs.SY, eess.SY"
Task Allocation for Autonomous Machines using Computational Intelligence and Deep Reinforcement Learning,"Enabling multiple autonomous machines to perform reliably requires the
development of efficient cooperative control algorithms. This paper presents a
survey of algorithms that have been developed for controlling and coordinating
autonomous machines in complex environments. We especially focus on task
allocation methods using computational intelligence (CI) and deep reinforcement
learning (RL). The advantages and disadvantages of the surveyed methods are
analysed thoroughly. We also propose and discuss in detail various future
research directions that shed light on how to improve existing algorithms or
create new methods to enhance the employability and performance of autonomous
machines in real-world applications. The findings indicate that CI and deep RL
methods provide viable approaches to addressing complex task allocation
problems in dynamic and uncertain environments. The recent development of deep
RL has greatly contributed to the literature on controlling and coordinating
autonomous machines, and it has become a growing trend in this area. It is
envisaged that this paper will provide researchers and engineers with a
comprehensive overview of progress in machine learning research related to
autonomous machines. It also highlights underexplored areas, identifies
emerging methodologies, and suggests new avenues for exploration in future
research within this domain.","Thanh Thi Nguyen, Quoc Viet Hung Nguyen, Jonathan Kua, Imran Razzak, Dung Nguyen, Saeid Nahavandi",2025-08-28 11:48:55+00:00,http://arxiv.org/abs/2508.20688v1,http://arxiv.org/pdf/2508.20688v1,"cs.RO, cs.AI"
Task-Oriented Edge-Assisted Cross-System Design for Real-Time Human-Robot Interaction in Industrial Metaverse,"Real-time human-device interaction in industrial Metaverse faces challenges
such as high computational load, limited bandwidth, and strict latency. This
paper proposes a task-oriented edge-assisted cross-system framework using
digital twins (DTs) to enable responsive interactions. By predicting operator
motions, the system supports: 1) proactive Metaverse rendering for visual
feedback, and 2) preemptive control of remote devices. The DTs are decoupled
into two virtual functions-visual display and robotic control-optimizing both
performance and adaptability. To enhance generalizability, we introduce the
Human-In-The-Loop Model-Agnostic Meta-Learning (HITL-MAML) algorithm, which
dynamically adjusts prediction horizons. Evaluation on two tasks demonstrates
the framework's effectiveness: in a Trajectory-Based Drawing Control task, it
reduces weighted RMSE from 0.0712 m to 0.0101 m; in a real-time 3D scene
representation task for nuclear decommissioning, it achieves a PSNR of 22.11,
SSIM of 0.8729, and LPIPS of 0.1298. These results show the framework's
capability to ensure spatial precision and visual fidelity in real-time,
high-risk industrial environments.","Kan Chen, Zhen Meng, Xiangmin Xu, Jiaming Yang, Emma Li, Philip G. Zhao",2025-08-28 11:10:41+00:00,http://arxiv.org/abs/2508.20664v1,http://arxiv.org/pdf/2508.20664v1,"cs.RO, cs.AI, cs.GR"
Traversing the Narrow Path: A Two-Stage Reinforcement Learning Framework for Humanoid Beam Walking,"Traversing narrow beams is challenging for humanoids due to sparse,
safety-critical contacts and the fragility of purely learned policies. We
propose a physically grounded, two-stage framework that couples an XCoM/LIPM
footstep template with a lightweight residual planner and a simple low-level
tracker. Stage-1 is trained on flat ground: the tracker learns to robustly
follow footstep targets by adding small random perturbations to heuristic
footsteps, without any hand-crafted centerline locking, so it acquires stable
contact scheduling and strong target-tracking robustness. Stage-2 is trained in
simulation on a beam: a high-level planner predicts a body-frame residual
(Delta x, Delta y, Delta psi) for the swing foot only, refining the template
step to prioritize safe, precise placement under narrow support while
preserving interpretability. To ease deployment, sensing is kept minimal and
consistent between simulation and hardware: the planner consumes compact,
forward-facing elevation cues together with onboard IMU and joint signals. On a
Unitree G1, our system reliably traverses a 0.2 m-wide, 3 m-long beam. Across
simulation and real-world studies, residual refinement consistently outperforms
template-only and monolithic baselines in success rate, centerline adherence,
and safety margins, while the structured footstep interface enables transparent
analysis and low-friction sim-to-real transfer.","TianChen Huang, Wei Gao, Runchen Xu, Shiwu Zhang",2025-08-28 11:09:19+00:00,http://arxiv.org/abs/2508.20661v2,http://arxiv.org/pdf/2508.20661v2,cs.RO
SimShear: Sim-to-Real Shear-based Tactile Servoing,"We present SimShear, a sim-to-real pipeline for tactile control that enables
the use of shear information without explicitly modeling shear dynamics in
simulation. Shear, arising from lateral movements across contact surfaces, is
critical for tasks involving dynamic object interactions but remains
challenging to simulate. To address this, we introduce shPix2pix, a
shear-conditioned U-Net GAN that transforms simulated tactile images absent of
shear, together with a vector encoding shear information, into realistic
equivalents with shear deformations. This method outperforms baseline pix2pix
approaches in simulating tactile images and in pose/shear prediction. We apply
SimShear to two control tasks using a pair of low-cost desktop robotic arms
equipped with a vision-based tactile sensor: (i) a tactile tracking task, where
a follower arm tracks a surface moved by a leader arm, and (ii) a collaborative
co-lifting task, where both arms jointly hold an object while the leader
follows a prescribed trajectory. Our method maintains contact errors within 1
to 2 mm across varied trajectories where shear sensing is essential, validating
the feasibility of sim-to-real shear modeling with rigid-body simulators and
opening new directions for simulation in tactile robotics.","Kipp McAdam Freud, Yijiong Lin, Nathan F. Lepora",2025-08-28 08:54:06+00:00,http://arxiv.org/abs/2508.20561v1,http://arxiv.org/pdf/2508.20561v1,cs.RO
SPGrasp: Spatiotemporal Prompt-driven Grasp Synthesis in Dynamic Scenes,"Real-time interactive grasp synthesis for dynamic objects remains challenging
as existing methods fail to achieve low-latency inference while maintaining
promptability. To bridge this gap, we propose SPGrasp (spatiotemporal
prompt-driven dynamic grasp synthesis), a novel framework extending segment
anything model v2 (SAMv2) for video stream grasp estimation. Our core
innovation integrates user prompts with spatiotemporal context, enabling
real-time interaction with end-to-end latency as low as 59 ms while ensuring
temporal consistency for dynamic objects. In benchmark evaluations, SPGrasp
achieves instance-level grasp accuracies of 90.6% on OCID and 93.8% on
Jacquard. On the challenging GraspNet-1Billion dataset under continuous
tracking, SPGrasp achieves 92.0% accuracy with 73.1 ms per-frame latency,
representing a 58.5% reduction compared to the prior state-of-the-art
promptable method RoG-SAM while maintaining competitive accuracy. Real-world
experiments involving 13 moving objects demonstrate a 94.8% success rate in
interactive grasping scenarios. These results confirm SPGrasp effectively
resolves the latency-interactivity trade-off in dynamic grasp synthesis.","Yunpeng Mei, Hongjie Cao, Yinqiu Xia, Wei Xiao, Zhaohan Feng, Gang Wang, Jie Chen",2025-08-28 08:38:50+00:00,http://arxiv.org/abs/2508.20547v2,http://arxiv.org/pdf/2508.20547v2,"cs.RO, cs.AI, cs.CV"
GENNAV: Polygon Mask Generation for Generalized Referring Navigable Regions,"We focus on the task of identifying the location of target regions from a
natural language instruction and a front camera image captured by a mobility.
This task is challenging because it requires both existence prediction and
segmentation, particularly for stuff-type target regions with ambiguous
boundaries. Existing methods often underperform in handling stuff-type target
regions, in addition to absent or multiple targets. To overcome these
limitations, we propose GENNAV, which predicts target existence and generates
segmentation masks for multiple stuff-type target regions. To evaluate GENNAV,
we constructed a novel benchmark called GRiN-Drive, which includes three
distinct types of samples: no-target, single-target, and multi-target. GENNAV
achieved superior performance over baseline methods on standard evaluation
metrics. Furthermore, we conducted real-world experiments with four automobiles
operated in five geographically distinct urban areas to validate its zero-shot
transfer performance. In these experiments, GENNAV outperformed baseline
methods and demonstrated its robustness across diverse real-world environments.
The project page is available at https://gennav.vercel.app/.","Kei Katsumata, Yui Iioka, Naoki Hosomi, Teruhisa Misu, Kentaro Yamada, Komei Sugiura",2025-08-28 08:09:38+00:00,http://arxiv.org/abs/2508.21102v1,http://arxiv.org/pdf/2508.21102v1,"cs.CV, cs.RO"
"Learning Fast, Tool aware Collision Avoidance for Collaborative Robots","Ensuring safe and efficient operation of collaborative robots in human
environments is challenging, especially in dynamic settings where both obstacle
motion and tasks change over time. Current robot controllers typically assume
full visibility and fixed tools, which can lead to collisions or overly
conservative behavior. In our work, we introduce a tool-aware collision
avoidance system that adjusts in real time to different tool sizes and modes of
tool-environment interaction. Using a learned perception model, our system
filters out robot and tool components from the point cloud, reasons about
occluded area, and predicts collision under partial observability. We then use
a control policy trained via constrained reinforcement learning to produce
smooth avoidance maneuvers in under 10 milliseconds. In simulated and
real-world tests, our approach outperforms traditional approaches (APF, MPPI)
in dynamic environments, while maintaining sub-millimeter accuracy. Moreover,
our system operates with approximately 60% lower computational cost compared to
a state-of-the-art GPU-based planner. Our approach provides modular, efficient,
and effective collision avoidance for robots operating in dynamic environments.
We integrate our method into a collaborative robot application and demonstrate
its practical use for safe and responsive operation.","Joonho Lee, Yunho Kim, Seokjoon Kim, Quan Nguyen, Youngjin Heo",2025-08-28 06:10:29+00:00,http://arxiv.org/abs/2508.20457v1,http://arxiv.org/pdf/2508.20457v1,"cs.RO, cs.SY, eess.SY"
Regulation-Aware Game-Theoretic Motion Planning for Autonomous Racing,"This paper presents a regulation-aware motion planning framework for
autonomous racing scenarios. Each agent solves a Regulation-Compliant Model
Predictive Control problem, where racing rules - such as right-of-way and
collision avoidance responsibilities - are encoded using Mixed Logical
Dynamical constraints. We formalize the interaction between vehicles as a
Generalized Nash Equilibrium Problem (GNEP) and approximate its solution using
an Iterative Best Response scheme. Building on this, we introduce the
Regulation-Aware Game-Theoretic Planner (RA-GTP), in which the attacker reasons
over the defender's regulation-constrained behavior. This game-theoretic layer
enables the generation of overtaking strategies that are both safe and
non-conservative. Simulation results demonstrate that the RA-GTP outperforms
baseline methods that assume non-interacting or rule-agnostic opponent models,
leading to more effective maneuvers while consistently maintaining compliance
with racing regulations.","Francesco Prignoli, Francesco Borrelli, Paolo Falcone, Mark Pustilnik",2025-08-27 18:30:28+00:00,http://arxiv.org/abs/2508.20203v1,http://arxiv.org/pdf/2508.20203v1,"eess.SY, cs.RO, cs.SY"
Discrete-Guided Diffusion for Scalable and Safe Multi-Robot Motion Planning,"Multi-Robot Motion Planning (MRMP) involves generating collision-free
trajectories for multiple robots operating in a shared continuous workspace.
While discrete multi-agent path finding (MAPF) methods are broadly adopted due
to their scalability, their coarse discretization severely limits trajectory
quality. In contrast, continuous optimization-based planners offer
higher-quality paths but suffer from the curse of dimensionality, resulting in
poor scalability with respect to the number of robots. This paper tackles the
limitations of these two approaches by introducing a novel framework that
integrates discrete MAPF solvers with constrained generative diffusion models.
The resulting framework, called Discrete-Guided Diffusion (DGD), has three key
characteristics: (1) it decomposes the original nonconvex MRMP problem into
tractable subproblems with convex configuration spaces, (2) it combines
discrete MAPF solutions with constrained optimization techniques to guide
diffusion models capture complex spatiotemporal dependencies among robots, and
(3) it incorporates a lightweight constraint repair mechanism to ensure
trajectory feasibility. The proposed method sets a new state-of-the-art
performance in large-scale, complex environments, scaling to 100 robots while
achieving planning efficiency and high success rates.","Jinhao Liang, Sven Koenig, Ferdinando Fioretto",2025-08-27 17:59:36+00:00,http://arxiv.org/abs/2508.20095v1,http://arxiv.org/pdf/2508.20095v1,"cs.RO, cs.AI, cs.LG"
HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation,"Leveraging human motion data to impart robots with versatile manipulation
skills has emerged as a promising paradigm in robotic manipulation.
Nevertheless, translating multi-source human hand motions into feasible robot
behaviors remains challenging, particularly for robots equipped with
multi-fingered dexterous hands characterized by complex, high-dimensional
action spaces. Moreover, existing approaches often struggle to produce policies
capable of adapting to diverse environmental conditions. In this paper, we
introduce HERMES, a human-to-robot learning framework for mobile bimanual
dexterous manipulation. First, HERMES formulates a unified reinforcement
learning approach capable of seamlessly transforming heterogeneous human hand
motions from multiple sources into physically plausible robotic behaviors.
Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depth
image-based sim2real transfer method for improved generalization to real-world
scenarios. Furthermore, to enable autonomous operation in varied and
unstructured environments, we augment the navigation foundation model with a
closed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise
alignment of visual goals and effectively bridging autonomous navigation and
dexterous manipulation. Extensive experimental results demonstrate that HERMES
consistently exhibits generalizable behaviors across diverse, in-the-wild
scenarios, successfully performing numerous complex mobile bimanual dexterous
manipulation tasks. Project Page:https://gemcollector.github.io/HERMES/.","Zhecheng Yuan, Tianming Wei, Langzhe Gu, Pu Hua, Tianhai Liang, Yuanpei Chen, Huazhe Xu",2025-08-27 17:53:46+00:00,http://arxiv.org/abs/2508.20085v3,http://arxiv.org/pdf/2508.20085v3,cs.RO
Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies,"Vision-Language-Action (VLA) models adapt large vision-language backbones to
map images and instructions to robot actions. However, prevailing VLA decoders
either generate actions autoregressively in a fixed left-to-right order or
attach continuous diffusion or flow matching heads outside the backbone,
demanding specialized training and iterative sampling that hinder a unified,
scalable architecture. We present Discrete Diffusion VLA, a single-transformer
policy that models discretized action chunks with discrete diffusion and is
trained with the same cross-entropy objective as the VLM backbone. The design
retains diffusion's progressive refinement paradigm while remaining natively
compatible with the discrete token interface of VLMs. Our method achieves an
adaptive decoding order that resolves easy action elements before harder ones
and uses secondary remasking to revisit uncertain predictions across refinement
rounds, which improves consistency and enables robust error correction. This
unified decoder preserves pretrained vision language priors, supports parallel
decoding, breaks the autoregressive bottleneck, and reduces the number of
function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO,
71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv
Bridge, improving over both autoregressive and continuous diffusion baselines.
These findings indicate that discrete-diffusion action decoder supports precise
action modeling and consistent training, laying groundwork for scaling VLA to
larger models and datasets.","Zhixuan Liang, Yizhuo Li, Tianshuo Yang, Chengyue Wu, Sitong Mao, Liuao Pei, Xiaokang Yang, Jiangmiao Pang, Yao Mu, Ping Luo",2025-08-27 17:39:11+00:00,http://arxiv.org/abs/2508.20072v1,http://arxiv.org/pdf/2508.20072v1,"cs.CV, cs.LG, cs.RO"
Visio-Verbal Teleimpedance Interface: Enabling Semi-Autonomous Control of Physical Interaction via Eye Tracking and Speech,"The paper presents a visio-verbal teleimpedance interface for commanding 3D
stiffness ellipsoids to the remote robot with a combination of the operator's
gaze and verbal interaction. The gaze is detected by an eye-tracker, allowing
the system to understand the context in terms of what the operator is currently
looking at in the scene. Along with verbal interaction, a Visual Language Model
(VLM) processes this information, enabling the operator to communicate their
intended action or provide corrections. Based on these inputs, the interface
can then generate appropriate stiffness matrices for different physical
interaction actions. To validate the proposed visio-verbal teleimpedance
interface, we conducted a series of experiments on a setup including a Force
Dimension Sigma.7 haptic device to control the motion of the remote Kuka LBR
iiwa robotic arm. The human operator's gaze is tracked by Tobii Pro Glasses 2,
while human verbal commands are processed by a VLM using GPT-4o. The first
experiment explored the optimal prompt configuration for the interface. The
second and third experiments demonstrated different functionalities of the
interface on a slide-in-the-groove task.","Henk H. A. Jekel, Alejandro Díaz Rosales, Luka Peternel",2025-08-27 16:42:35+00:00,http://arxiv.org/abs/2508.20037v1,http://arxiv.org/pdf/2508.20037v1,cs.RO
Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation,"Vision-Language-Action (VLA) models have become a cornerstone in robotic
policy learning, leveraging large-scale multimodal data for robust and scalable
control. However, existing VLA frameworks primarily address short-horizon
tasks, and their effectiveness on long-horizon, multi-step robotic manipulation
remains limited due to challenges in skill chaining and subtask dependencies.
In this work, we introduce Long-VLA, the first end-to-end VLA model
specifically designed for long-horizon robotic tasks. Our approach features a
novel phase-aware input masking strategy that adaptively segments each subtask
into moving and interaction phases, enabling the model to focus on
phase-relevant sensory cues and enhancing subtask compatibility. This unified
strategy preserves the scalability and data efficiency of VLA training, and our
architecture-agnostic module can be seamlessly integrated into existing VLA
models. We further propose the L-CALVIN benchmark to systematically evaluate
long-horizon manipulation. Extensive experiments on both simulated and
real-world tasks demonstrate that Long-VLA significantly outperforms prior
state-of-the-art methods, establishing a new baseline for long-horizon robotic
control.","Yiguo Fan, Pengxiang Ding, Shuanghao Bai, Xinyang Tong, Yuyang Zhu, Hongchao Lu, Fengqi Dai, Wei Zhao, Yang Liu, Siteng Huang, Zhaoxin Fan, Badong Chen, Donglin Wang",2025-08-27 15:12:03+00:00,http://arxiv.org/abs/2508.19958v2,http://arxiv.org/pdf/2508.19958v2,cs.RO
"Divide, Discover, Deploy: Factorized Skill Learning with Symmetry and Style Priors","Unsupervised Skill Discovery (USD) allows agents to autonomously learn
diverse behaviors without task-specific rewards. While recent USD methods have
shown promise, their application to real-world robotics remains underexplored.
In this paper, we propose a modular USD framework to address the challenges in
the safety, interpretability, and deployability of the learned skills. Our
approach employs user-defined factorization of the state space to learn
disentangled skill representations. It assigns different skill discovery
algorithms to each factor based on the desired intrinsic reward function. To
encourage structured morphology-aware skills, we introduce symmetry-based
inductive biases tailored to individual factors. We also incorporate a style
factor and regularization penalties to promote safe and robust behaviors. We
evaluate our framework in simulation using a quadrupedal robot and demonstrate
zero-shot transfer of the learned skills to real hardware. Our results show
that factorization and symmetry lead to the discovery of structured
human-interpretable behaviors, while the style factor and penalties enhance
safety and diversity. Additionally, we show that the learned skills can be used
for downstream tasks and perform on par with oracle policies trained with
hand-crafted rewards.","Rafael Cathomen, Mayank Mittal, Marin Vlastelica, Marco Hutter",2025-08-27 15:05:49+00:00,http://arxiv.org/abs/2508.19953v2,http://arxiv.org/pdf/2508.19953v2,cs.RO
FARM: Frame-Accelerated Augmentation and Residual Mixture-of-Experts for Physics-Based High-Dynamic Humanoid Control,"Unified physics-based humanoid controllers are pivotal for robotics and
character animation, yet models that excel on gentle, everyday motions still
stumble on explosive actions, hampering real-world deployment. We bridge this
gap with FARM (Frame-Accelerated Augmentation and Residual Mixture-of-Experts),
an end-to-end framework composed of frame-accelerated augmentation, a robust
base controller, and a residual mixture-of-experts (MoE). Frame-accelerated
augmentation exposes the model to high-velocity pose changes by widening
inter-frame gaps. The base controller reliably tracks everyday low-dynamic
motions, while the residual MoE adaptively allocates additional network
capacity to handle challenging high-dynamic actions, significantly enhancing
tracking accuracy. In the absence of a public benchmark, we curate the
High-Dynamic Humanoid Motion (HDHM) dataset, comprising 3593 physically
plausible clips. On HDHM, FARM reduces the tracking failure rate by 42.8\% and
lowers global mean per-joint position error by 14.6\% relative to the baseline,
while preserving near-perfect accuracy on low-dynamic motions. These results
establish FARM as a new baseline for high-dynamic humanoid control and
introduce the first open benchmark dedicated to this challenge. The code and
dataset will be released at https://github.com/Colin-Jing/FARM.","Tan Jing, Shiting Chen, Yangfan Li, Weisheng Xu, Renjing Xu",2025-08-27 14:35:37+00:00,http://arxiv.org/abs/2508.19926v1,http://arxiv.org/pdf/2508.19926v1,cs.RO
A Standing Support Mobility Robot for Enhancing Independence in Elderly Daily Living,"This paper presents a standing support mobility robot ""Moby"" developed to
enhance independence and safety for elderly individuals during daily activities
such as toilet transfers. Unlike conventional seated mobility aids, the robot
maintains users in an upright posture, reducing physical strain, supporting
natural social interaction at eye level, and fostering a greater sense of
self-efficacy. Moby offers a novel alternative by functioning both passively
and with mobility support, enabling users to perform daily tasks more
independently. Its main advantages include ease of use, lightweight design,
comfort, versatility, and effective sit-to-stand assistance. The robot
leverages the Robot Operating System (ROS) for seamless control, featuring
manual and autonomous operation modes. A custom control system enables safe and
intuitive interaction, while the integration with NAV2 and LiDAR allows for
robust navigation capabilities. This paper reviews existing mobility solutions
and compares them to Moby, details the robot's design, and presents objective
and subjective experimental results using the NASA-TLX method and time
comparisons to other methods to validate our design criteria and demonstrate
the advantages of our contribution.","Ricardo J. Manríquez-Cisterna, Ankit A. Ravankar, Jose V. Salazar Luces, Takuro Hatsukari, Yasuhisa Hirata",2025-08-27 12:02:02+00:00,http://arxiv.org/abs/2508.19816v1,http://arxiv.org/pdf/2508.19816v1,cs.RO
Beyond Pairwise Comparisons: Unveiling Structural Landscape of Mobile Robot Models,"Understanding the computational power of mobile robot systems is a
fundamental challenge in distributed computing. While prior work has focused on
pairwise separations between models, we explore how robot capabilities, light
observability, and scheduler synchrony interact in more complex ways.
  We first show that the Exponential Times Expansion (ETE) problem is solvable
only in the strongest model -- fully-synchronous robots with full mutual lights
($\mathcal{LUMT}^F$). We then introduce the Hexagonal Edge Traversal (HET) and
TAR(d)* problems to demonstrate how internal memory and lights interact with
synchrony: under weak synchrony, internal memory alone is insufficient, while
full synchrony can substitute for both lights and memory.
  In the asynchronous setting, we classify problems such as LP-MLCv, VEC, and
ZCC to show fine-grained separations between $\mathcal{FSTA}$ and
$\mathcal{FCOM}$ robots. We also analyze Vertex Traversal Rendezvous (VTR) and
Leave Place Convergence (LP-Cv), illustrating the limitations of internal
memory in symmetric settings.
  These results extend the known separation map of 14 canonical robot models,
revealing structural phenomena only visible through higher-order comparisons.
Our work provides new impossibility criteria and deepens the understanding of
how observability, memory, and synchrony collectively shape the computational
power of mobile robots.","Shota Naito, Tsukasa Ninomiya, Koichi Wada",2025-08-27 11:47:29+00:00,http://arxiv.org/abs/2508.19805v2,http://arxiv.org/pdf/2508.19805v2,"cs.DC, cs.RO"
APT*: Asymptotically Optimal Motion Planning via Adaptively Prolated Elliptical R-Nearest Neighbors,"Optimal path planning aims to determine a sequence of states from a start to
a goal while accounting for planning objectives. Popular methods often
integrate fixed batch sizes and neglect information on obstacles, which is not
problem-specific. This study introduces Adaptively Prolated Trees (APT*), a
novel sampling-based motion planner that extends based on Force Direction
Informed Trees (FDIT*), integrating adaptive batch-sizing and elliptical
$r$-nearest neighbor modules to dynamically modulate the path searching process
based on environmental feedback. APT* adjusts batch sizes based on the
hypervolume of the informed sets and considers vertices as electric charges
that obey Coulomb's law to define virtual forces via neighbor samples, thereby
refining the prolate nearest neighbor selection. These modules employ
non-linear prolate methods to adaptively adjust the electric charges of
vertices for force definition, thereby improving the convergence rate with
lower solution costs. Comparative analyses show that APT* outperforms existing
single-query sampling-based planners in dimensions from $\mathbb{R}^4$ to
$\mathbb{R}^{16}$, and it was further validated through a real-world robot
manipulation task. A video showcasing our experimental results is available at:
https://youtu.be/gCcUr8LiEw4","Liding Zhang, Sicheng Wang, Kuanqi Cai, Zhenshan Bing, Fan Wu, Chaoqun Wang, Sami Haddadin, Alois Knoll",2025-08-27 11:16:36+00:00,http://arxiv.org/abs/2508.19790v1,http://arxiv.org/pdf/2508.19790v1,cs.RO
Context-Aware Risk Estimation in Home Environments: A Probabilistic Framework for Service Robots,"We present a novel framework for estimating accident-prone regions in
everyday indoor scenes, aimed at improving real-time risk awareness in service
robots operating in human-centric environments. As robots become integrated
into daily life, particularly in homes, the ability to anticipate and respond
to environmental hazards is crucial for ensuring user safety, trust, and
effective human-robot interaction. Our approach models object-level risk and
context through a semantic graph-based propagation algorithm. Each object is
represented as a node with an associated risk score, and risk propagates
asymmetrically from high-risk to low-risk objects based on spatial proximity
and accident relationship. This enables the robot to infer potential hazards
even when they are not explicitly visible or labeled. Designed for
interpretability and lightweight onboard deployment, our method is validated on
a dataset with human-annotated risk regions, achieving a binary risk detection
accuracy of 75%. The system demonstrates strong alignment with human
perception, particularly in scenes involving sharp or unstable objects. These
results underline the potential of context-aware risk reasoning to enhance
robotic scene understanding and proactive safety behaviors in shared
human-robot spaces. This framework could serve as a foundation for future
systems that make context-driven safety decisions, provide real-time alerts, or
autonomously assist users in avoiding or mitigating hazards within home
environments.","Sena Ishii, Akash Chikhalikar, Ankit A. Ravankar, Jose Victorio Salazar Luces, Yasuhisa Hirata",2025-08-27 11:14:05+00:00,http://arxiv.org/abs/2508.19788v1,http://arxiv.org/pdf/2508.19788v1,"cs.RO, cs.CV"
Tree-Based Grafting Approach for Bidirectional Motion Planning with Local Subsets Optimization,"Bidirectional motion planning often reduces planning time compared to its
unidirectional counterparts. It requires connecting the forward and reverse
search trees to form a continuous path. However, this process could fail and
restart the asymmetric bidirectional search due to the limitations of
lazy-reverse search. To address this challenge, we propose Greedy GuILD
Grafting Trees (G3T*), a novel path planner that grafts invalid edge
connections at both ends to re-establish tree-based connectivity, enabling
rapid path convergence. G3T* employs a greedy approach using the minimum
Lebesgue measure of guided incremental local densification (GuILD) subsets to
optimize paths efficiently. Furthermore, G3T* dynamically adjusts the sampling
distribution between the informed set and GuILD subsets based on historical and
current cost improvements, ensuring asymptotic optimality. These features
enhance the forward search's growth towards the reverse tree, achieving faster
convergence and lower solution costs. Benchmark experiments across dimensions
from R^2 to R^8 and real-world robotic evaluations demonstrate G3T*'s superior
performance compared to existing single-query sampling-based planners. A video
showcasing our experimental results is available at:
https://youtu.be/3mfCRL5SQIU","Liding Zhang, Yao Ling, Zhenshan Bing, Fan Wu, Sami Haddadin, Alois Knoll",2025-08-27 11:00:25+00:00,http://arxiv.org/abs/2508.19776v1,http://arxiv.org/pdf/2508.19776v1,cs.RO
Elliptical K-Nearest Neighbors -- Path Optimization via Coulomb's Law and Invalid Vertices in C-space Obstacles,"Path planning has long been an important and active research area in
robotics. To address challenges in high-dimensional motion planning, this study
introduces the Force Direction Informed Trees (FDIT*), a sampling-based planner
designed to enhance speed and cost-effectiveness in pathfinding. FDIT* builds
upon the state-of-the-art informed sampling planner, the Effort Informed Trees
(EIT*), by capitalizing on often-overlooked information in invalid vertices. It
incorporates principles of physical force, particularly Coulomb's law. This
approach proposes the elliptical $k$-nearest neighbors search method, enabling
fast convergence navigation and avoiding high solution cost or infeasible paths
by exploring more problem-specific search-worthy areas. It demonstrates
benefits in search efficiency and cost reduction, particularly in confined,
high-dimensional environments. It can be viewed as an extension of nearest
neighbors search techniques. Fusing invalid vertex data with physical dynamics
facilitates force-direction-based search regions, resulting in an improved
convergence rate to the optimum. FDIT* outperforms existing single-query,
sampling-based planners on the tested problems in R^4 to R^16 and has been
demonstrated on a real-world mobile manipulation task.","Liding Zhang, Zhenshan Bing, Yu Zhang, Kuanqi Cai, Lingyun Chen, Fan Wu, Sami Haddadin, Alois Knoll",2025-08-27 10:57:50+00:00,http://arxiv.org/abs/2508.19771v1,http://arxiv.org/pdf/2508.19771v1,cs.RO
Efficient Human-Aware Task Allocation for Multi-Robot Systems in Shared Environments,"Multi-robot systems are increasingly deployed in applications, such as
intralogistics or autonomous delivery, where multiple robots collaborate to
complete tasks efficiently. One of the key factors enabling their efficient
cooperation is Multi-Robot Task Allocation (MRTA). Algorithms solving this
problem optimize task distribution among robots to minimize the overall
execution time. In shared environments, apart from the relative distance
between the robots and the tasks, the execution time is also significantly
impacted by the delay caused by navigating around moving people. However, most
existing MRTA approaches are dynamics-agnostic, relying on static maps and
neglecting human motion patterns, leading to inefficiencies and delays. In this
paper, we introduce \acrfull{method name}. This method leverages Maps of
Dynamics (MoDs), spatio-temporal queryable models designed to capture
historical human movement patterns, to estimate the impact of humans on the
task execution time during deployment. \acrshort{method name} utilizes a
stochastic cost function that includes MoDs. Experimental results show that
integrating MoDs enhances task allocation performance, resulting in reduced
mission completion times by up to $26\%$ compared to the dynamics-agnostic
method and up to $19\%$ compared to the baseline. This work underscores the
importance of considering human dynamics in MRTA within shared environments and
presents an efficient framework for deploying multi-robot systems in
environments populated by humans.","Maryam Kazemi Eskeri, Ville Kyrki, Dominik Baumann, Tomasz Piotr Kucner",2025-08-27 09:53:19+00:00,http://arxiv.org/abs/2508.19731v1,http://arxiv.org/pdf/2508.19731v1,cs.RO
Embodied Intelligence for Sustainable Flight: A Soaring Robot with Active Morphological Control,"Achieving both agile maneuverability and high energy efficiency in aerial
robots, particularly in dynamic wind environments, remains challenging.
Conventional thruster-powered systems offer agility but suffer from high energy
consumption, while fixed-wing designs are efficient but lack hovering and
maneuvering capabilities. We present Floaty, a shape-changing robot that
overcomes these limitations by passively soaring, harnessing wind energy
through intelligent morphological control inspired by birds. Floaty's design is
optimized for passive stability, and its control policy is derived from an
experimentally learned aerodynamic model, enabling precise attitude and
position control without active propulsion. Wind tunnel experiments demonstrate
Floaty's ability to hover, maneuver, and reject disturbances in vertical
airflows up to 10 m/s. Crucially, Floaty achieves this with a specific power
consumption of 10 W/kg, an order of magnitude lower than thruster-powered
systems. This introduces a paradigm for energy-efficient aerial robotics,
leveraging morphological intelligence and control to operate sustainably in
challenging wind conditions.","Ghadeer Elmkaiel, Syn Schmitt, Michael Muehlebach",2025-08-27 08:45:52+00:00,http://arxiv.org/abs/2508.19684v1,http://arxiv.org/pdf/2508.19684v1,"cs.RO, physics.app-ph"
Autonomous Aerial Manipulation at Arbitrary Pose in SE(3) with Robust Control and Whole-body Planning,"Aerial manipulators based on conventional multirotors can conduct
manipulation only in small roll and pitch angles due to the underactuatedness
of the multirotor base. If the multirotor base is capable of hovering at
arbitrary orientation, the robot can freely locate itself at any point in
$\mathsf{SE}(3)$, significantly extending its manipulation workspace and
enabling a manipulation task that was originally not viable. In this work, we
present a geometric robust control and whole-body motion planning framework for
an omnidirectional aerial manipulator (OAM). To maximize the strength of OAM,
we first propose a geometric robust controller for a floating base. Since the
motion of the robotic arm and the interaction forces during manipulation affect
the stability of the floating base, the base should be capable of mitigating
these adverse effects while controlling its 6D pose. We then design a two-step
optimization-based whole-body motion planner, jointly considering the pose of
the floating base and the joint angles of the robotic arm to harness the entire
configuration space. The devised two-step approach facilitates real-time
applicability and enhances convergence of the optimization problem with
non-convex and non-Euclidean search space. The proposed approach enables the
base to be stationary at any 6D pose while autonomously carrying out
sophisticated manipulation near obstacles without any collision. We demonstrate
the effectiveness of the proposed framework through experiments in which an OAM
performs grasping and pulling of an object in multiple scenarios, including
near $90^\circ$ and even $180^\circ$ pitch angles.","Dongjae Lee, Byeongjun Kim, H. Jin Kim",2025-08-27 06:44:23+00:00,http://arxiv.org/abs/2508.19608v1,http://arxiv.org/pdf/2508.19608v1,cs.RO
Impedance Primitive-augmented Hierarchical Reinforcement Learning for Sequential Tasks,"This paper presents an Impedance Primitive-augmented hierarchical
reinforcement learning framework for efficient robotic manipulation in
sequential contact tasks. We leverage this hierarchical structure to
sequentially execute behavior primitives with variable stiffness control
capabilities for contact tasks. Our proposed approach relies on three key
components: an action space enabling variable stiffness control, an adaptive
stiffness controller for dynamic stiffness adjustments during primitive
execution, and affordance coupling for efficient exploration while encouraging
compliance. Through comprehensive training and evaluation, our framework learns
efficient stiffness control capabilities and demonstrates improvements in
learning efficiency, compositionality in primitive selection, and success rates
compared to the state-of-the-art. The training environments include block
lifting, door opening, object pushing, and surface cleaning. Real world
evaluations further confirm the framework's sim2real capability. This work lays
the foundation for more adaptive and versatile robotic manipulation systems,
with potential applications in more complex contact-based tasks.","Amin Berjaoui Tahmaz, Ravi Prakash, Jens Kober",2025-08-27 06:43:08+00:00,http://arxiv.org/abs/2508.19607v1,http://arxiv.org/pdf/2508.19607v1,cs.RO
A Lightweight Crowd Model for Robot Social Navigation,"Robots operating in human-populated environments must navigate safely and
efficiently while minimizing social disruption. Achieving this requires
estimating crowd movement to avoid congested areas in real-time. Traditional
microscopic models struggle to scale in dense crowds due to high computational
cost, while existing macroscopic crowd prediction models tend to be either
overly simplistic or computationally intensive. In this work, we propose a
lightweight, real-time macroscopic crowd prediction model tailored for human
motion, which balances prediction accuracy and computational efficiency. Our
approach simplifies both spatial and temporal processing based on the inherent
characteristics of pedestrian flow, enabling robust generalization without the
overhead of complex architectures. We demonstrate a 3.6 times reduction in
inference time, while improving prediction accuracy by 3.1 %. Integrated into a
socially aware planning framework, the model enables efficient and socially
compliant robot navigation in dynamic environments. This work highlights that
efficient human crowd modeling enables robots to navigate dense environments
without costly computations.","Maryam Kazemi Eskeri, Thomas Wiedemann, Ville Kyrki, Dominik Baumann, Tomasz Piotr Kucner",2025-08-27 06:13:43+00:00,http://arxiv.org/abs/2508.19595v1,http://arxiv.org/pdf/2508.19595v1,"cs.RO, cs.LG"
DATR: Diffusion-based 3D Apple Tree Reconstruction Framework with Sparse-View,"Digital twin applications offered transformative potential by enabling
real-time monitoring and robotic simulation through accurate virtual replicas
of physical assets. The key to these systems is 3D reconstruction with high
geometrical fidelity. However, existing methods struggled under field
conditions, especially with sparse and occluded views. This study developed a
two-stage framework (DATR) for the reconstruction of apple trees from sparse
views. The first stage leverages onboard sensors and foundation models to
semi-automatically generate tree masks from complex field images. Tree masks
are used to filter out background information in multi-modal data for the
single-image-to-3D reconstruction at the second stage. This stage consists of a
diffusion model and a large reconstruction model for respective multi view and
implicit neural field generation. The training of the diffusion model and LRM
was achieved by using realistic synthetic apple trees generated by a Real2Sim
data generator. The framework was evaluated on both field and synthetic
datasets. The field dataset includes six apple trees with field-measured ground
truth, while the synthetic dataset featured structurally diverse trees.
Evaluation results showed that our DATR framework outperformed existing 3D
reconstruction methods across both datasets and achieved domain-trait
estimation comparable to industrial-grade stationary laser scanners while
improving the throughput by $\sim$360 times, demonstrating strong potential for
scalable agricultural digital twin systems.","Tian Qiu, Alan Zoubi, Yiyuan Lin, Ruiming Du, Lailiang Cheng, Yu Jiang",2025-08-27 01:45:54+00:00,http://arxiv.org/abs/2508.19508v1,http://arxiv.org/pdf/2508.19508v1,"cs.RO, cs.CV"
Gentle Object Retraction in Dense Clutter Using Multimodal Force Sensing and Imitation Learning,"Dense collections of movable objects are common in everyday spaces -- from
cabinets in a home to shelves in a warehouse. Safely retracting objects from
such collections is difficult for robots, yet people do it easily, using
non-prehensile tactile sensing on the sides and backs of their hands and arms.
We investigate the role of such sensing for training robots to gently reach
into constrained clutter and extract objects. The available sensing modalities
are (1) ""eye-in-hand"" vision, (2) proprioception, (3) non-prehensile triaxial
tactile sensing, (4) contact wrenches estimated from joint torques, and (5) a
measure of successful object acquisition obtained by monitoring the vacuum line
of a suction cup. We use imitation learning to train policies from a set of
demonstrations on randomly generated scenes, then conduct an ablation study of
wrench and tactile information. We evaluate each policy's performance across 40
unseen environment configurations. Policies employing any force sensing show
fewer excessive force failures, an increased overall success rate, and faster
completion times. The best performance is achieved using both tactile and
wrench information, producing an 80% improvement above the baseline without
force information.","Dane Brouwer, Joshua Citron, Heather Nolte, Jeannette Bohg, Mark Cutkosky",2025-08-26 23:38:52+00:00,http://arxiv.org/abs/2508.19476v1,http://arxiv.org/pdf/2508.19476v1,cs.RO
An Iterative Approach for Heterogeneous Multi-Agent Route Planning with Resource Transportation Uncertainty and Temporal Logic Goals,"This paper presents an iterative approach for heterogeneous multi-agent route
planning in environments with unknown resource distributions. We focus on a
team of robots with diverse capabilities tasked with executing missions
specified using Capability Temporal Logic (CaTL), a formal framework built on
Signal Temporal Logic to handle spatial, temporal, capability, and resource
constraints. The key challenge arises from the uncertainty in the initial
distribution and quantity of resources in the environment. To address this, we
introduce an iterative algorithm that dynamically balances exploration and task
fulfillment. Robots are guided to explore the environment, identifying resource
locations and quantities while progressively refining their understanding of
the resource landscape. At the same time, they aim to maximally satisfy the
mission objectives based on the current information, adapting their strategies
as new data is uncovered. This approach provides a robust solution for planning
in dynamic, resource-constrained environments, enabling efficient coordination
of heterogeneous teams even under conditions of uncertainty. Our method's
effectiveness and performance are demonstrated through simulated case studies.","Gustavo A. Cardona, Kaier Liang, Cristian-Ioan Vasile",2025-08-26 20:52:11+00:00,http://arxiv.org/abs/2508.19429v1,http://arxiv.org/pdf/2508.19429v1,"cs.RO, cs.FL"
From Stoplights to On-Ramps: A Comprehensive Set of Crash Rate Benchmarks for Freeway and Surface Street ADS Evaluation,"This paper presents crash rate benchmarks for evaluating US-based Automated
Driving Systems (ADS) for multiple urban areas. The purpose of this study was
to extend prior benchmarks focused only on surface streets to additionally
capture freeway crash risk for future ADS safety performance assessments. Using
publicly available police-reported crash and vehicle miles traveled (VMT) data,
the methodology details the isolation of in-transport passenger vehicles, road
type classification, and crash typology. Key findings revealed that freeway
crash rates exhibit large geographic dependence variations with
any-injury-reported crash rates being nearly 3.5 times higher in Atlanta (2.4
IPMM; the highest) when compared to Phoenix (0.7 IPMM; the lowest). The results
show the critical need for location-specific benchmarks to avoid biased safety
evaluations and provide insights into the vehicle miles traveled (VMT) required
to achieve statistical significance for various safety impact levels. The
distribution of crash types depended on the outcome severity level. Higher
severity outcomes (e.g., fatal crashes) had a larger proportion of
single-vehicle, vulnerable road users (VRU), and opposite-direction collisions
compared to lower severity (police-reported) crashes. Given heterogeneity in
crash types by severity, performance in low-severity scenarios may not be
predictive of high-severity outcomes. These benchmarks are additionally used to
quantify at the required mileage to show statistically significant deviations
from human performance. This is the first paper to generate freeway-specific
benchmarks for ADS evaluation and provides a foundational framework for future
ADS benchmarking by evaluators and developers.","John M. Scanlon, Timothy L McMurry, Yin-Hsiu Chen, Kristofer D. Kusano, Trent Victor",2025-08-26 20:48:24+00:00,http://arxiv.org/abs/2508.19425v1,http://arxiv.org/pdf/2508.19425v1,cs.RO
LaVA-Man: Learning Visual Action Representations for Robot Manipulation,"Visual-textual understanding is essential for language-guided robot
manipulation. Recent works leverage pre-trained vision-language models to
measure the similarity between encoded visual observations and textual
instructions, and then train a model to map this similarity to robot actions.
However, this two-step approach limits the model to capture the relationship
between visual observations and textual instructions, leading to reduced
precision in manipulation tasks. We propose to learn visual-textual
associations through a self-supervised pretext task: reconstructing a masked
goal image conditioned on an input image and textual instructions. This
formulation allows the model to learn visual-action representations without
robot action supervision. The learned representations can then be fine-tuned
for manipulation tasks with only a few demonstrations. We also introduce the
\textit{Omni-Object Pick-and-Place} dataset, which consists of annotated robot
tabletop manipulation episodes, including 180 object classes and 3,200
instances with corresponding textual instructions. This dataset enables the
model to acquire diverse object priors and allows for a more comprehensive
evaluation of its generalisation capability across object instances.
Experimental results on the five benchmarks, including both simulated and
real-robot validations, demonstrate that our method outperforms prior art.","Chaoran Zhu, Hengyi Wang, Yik Lung Pang, Changjae Oh",2025-08-26 19:34:54+00:00,http://arxiv.org/abs/2508.19391v1,http://arxiv.org/pdf/2508.19391v1,cs.RO
"FlipWalker: Jacob's Ladder toy-inspired robot for locomotion across diverse, complex terrain","This paper introduces FlipWalker, a novel underactuated robot locomotion
system inspired by Jacob's Ladder illusion toy, designed to traverse
challenging terrains where wheeled robots often struggle. Like the Jacob's
Ladder toy, FlipWalker features two interconnected segments joined by flexible
cables, enabling it to pivot and flip around singularities in a manner
reminiscent of the toy's cascading motion. Actuation is provided by
motor-driven legs within each segment that push off either the ground or the
opposing segment, depending on the robot's current configuration. A
physics-based model of the underactuated flipping dynamics is formulated to
elucidate the critical design parameters governing forward motion and obstacle
clearance or climbing. The untethered prototype weighs 0.78 kg, achieves a
maximum flipping speed of 0.2 body lengths per second. Experimental trials on
artificial grass, river rocks, and snow demonstrate that FlipWalker's flipping
strategy, which relies on ground reaction forces applied normal to the surface,
offers a promising alternative to traditional locomotion for navigating
irregular outdoor terrain.","Diancheng Li, Nia Ralston, Bastiaan Hagen, Phoebe Tan, Matthew A. Robertson",2025-08-26 19:16:21+00:00,http://arxiv.org/abs/2508.19380v1,http://arxiv.org/pdf/2508.19380v1,cs.RO
Inference of Human-derived Specifications of Object Placement via Demonstration,"As robots' manipulation capabilities improve for pick-and-place tasks (e.g.,
object packing, sorting, and kitting), methods focused on understanding
human-acceptable object configurations remain limited expressively with regard
to capturing spatial relationships important to humans. To advance robotic
understanding of human rules for object arrangement, we introduce
positionally-augmented RCC (PARCC), a formal logic framework based on region
connection calculus (RCC) for describing the relative position of objects in
space. Additionally, we introduce an inference algorithm for learning PARCC
specifications via demonstrations. Finally, we present the results from a human
study, which demonstrate our framework's ability to capture a human's intended
specification and the benefits of learning from demonstration approaches over
human-provided specifications.","Alex Cuellar, Ho Chit Siu, Julie A Shah",2025-08-26 18:57:21+00:00,http://arxiv.org/abs/2508.19367v1,http://arxiv.org/pdf/2508.19367v1,"cs.RO, cs.AI, cs.HC"
MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation,"Temporal context is essential for robotic manipulation because such tasks are
inherently non-Markovian, yet mainstream VLA models typically overlook it and
struggle with long-horizon, temporally dependent tasks. Cognitive science
suggests that humans rely on working memory to buffer short-lived
representations for immediate control, while the hippocampal system preserves
verbatim episodic details and semantic gist of past experience for long-term
memory. Inspired by these mechanisms, we propose MemoryVLA, a
Cognition-Memory-Action framework for long-horizon robotic manipulation. A
pretrained VLM encodes the observation into perceptual and cognitive tokens
that form working memory, while a Perceptual-Cognitive Memory Bank stores
low-level details and high-level semantics consolidated from it. Working memory
retrieves decision-relevant entries from the bank, adaptively fuses them with
current tokens, and updates the bank by merging redundancies. Using these
tokens, a memory-conditioned diffusion action expert yields temporally aware
action sequences. We evaluate MemoryVLA on 150+ simulation and real-world tasks
across three robots. On SimplerEnv-Bridge, Fractal, and LIBERO-5 suites, it
achieves 71.9%, 72.7%, and 96.5% success rates, respectively, all outperforming
state-of-the-art baselines CogACT and pi-0, with a notable +14.6 gain on
Bridge. On 12 real-world tasks spanning general skills and long-horizon
temporal dependencies, MemoryVLA achieves 84.0% success rate, with long-horizon
tasks showing a +26 improvement over state-of-the-art baseline. Project Page:
https://shihao1895.github.io/MemoryVLA","Hao Shi, Bin Xie, Yingfei Liu, Lin Sun, Fengrong Liu, Tiancai Wang, Erjin Zhou, Haoqiang Fan, Xiangyu Zhang, Gao Huang",2025-08-26 17:57:16+00:00,http://arxiv.org/abs/2508.19236v1,http://arxiv.org/pdf/2508.19236v1,"cs.RO, cs.CV"
Planning-Query-Guided Model Generation for Model-Based Deformable Object Manipulation,"Efficient planning in high-dimensional spaces, such as those involving
deformable objects, requires computationally tractable yet sufficiently
expressive dynamics models. This paper introduces a method that automatically
generates task-specific, spatially adaptive dynamics models by learning which
regions of the object require high-resolution modeling to achieve good task
performance for a given planning query. Task performance depends on the complex
interplay between the dynamics model, world dynamics, control, and task
requirements. Our proposed diffusion-based model generator predicts per-region
model resolutions based on start and goal pointclouds that define the planning
query. To efficiently collect the data for learning this mapping, a two-stage
process optimizes resolution using predictive dynamics as a prior before
directly optimizing using closed-loop performance. On a tree-manipulation task,
our method doubles planning speed with only a small decrease in task
performance over using a full-resolution model. This approach informs a path
towards using previous planning and control data to generate computationally
efficient yet sufficiently expressive dynamics models for new tasks.","Alex LaGrassa, Zixuan Huang, Dmitry Berenson, Oliver Kroemer",2025-08-26 17:03:39+00:00,http://arxiv.org/abs/2508.19199v1,http://arxiv.org/pdf/2508.19199v1,"cs.RO, cs.LG"
AutoRing: Imitation Learning--based Autonomous Intraocular Foreign Body Removal Manipulation with Eye Surgical Robot,"Intraocular foreign body removal demands millimeter-level precision in
confined intraocular spaces, yet existing robotic systems predominantly rely on
manual teleoperation with steep learning curves. To address the challenges of
autonomous manipulation (particularly kinematic uncertainties from variable
motion scaling and variation of the Remote Center of Motion (RCM) point), we
propose AutoRing, an imitation learning framework for autonomous intraocular
foreign body ring manipulation. Our approach integrates dynamic RCM calibration
to resolve coordinate-system inconsistencies caused by intraocular instrument
variation and introduces the RCM-ACT architecture, which combines
action-chunking transformers with real-time kinematic realignment. Trained
solely on stereo visual data and instrument kinematics from expert
demonstrations in a biomimetic eye model, AutoRing successfully completes ring
grasping and positioning tasks without explicit depth sensing. Experimental
validation demonstrates end-to-end autonomy under uncalibrated microscopy
conditions. The results provide a viable framework for developing intelligent
eye-surgical systems capable of complex intraocular procedures.","Yue Wang, Wenjie Deng, Haotian Xue, Di Cui, Yiqi Chen, Mingchuan Zhou, Haochao Ying, Jian Wu",2025-08-26 16:54:34+00:00,http://arxiv.org/abs/2508.19191v2,http://arxiv.org/pdf/2508.19191v2,cs.RO
Real-Time Model Checking for Closed-Loop Robot Reactive Planning,"We present a new application of model checking which achieves real-time
multi-step planning and obstacle avoidance on a real autonomous robot. We have
developed a small, purpose-built model checking algorithm which generates plans
in situ based on ""core"" knowledge and attention as found in biological agents.
This is achieved in real-time using no pre-computed data on a low-powered
device. Our approach is based on chaining temporary control systems which are
spawned to counteract disturbances in the local environment that disrupt an
autonomous agent from its preferred action (or resting state). A novel
discretization of 2D LiDAR data sensitive to bounded variations in the local
environment is used. Multi-step planning using model checking by forward
depth-first search is applied to cul-de-sac and playground scenarios. Both
empirical results and informal proofs of two fundamental properties of our
approach demonstrate that model checking can be used to create efficient
multi-step plans for local obstacle avoidance, improving on the performance of
a reactive agent which can only plan one step. Our approach is an instructional
case study for the development of safe, reliable and explainable planning in
the context of autonomous vehicles.","Christopher Chandler, Bernd Porr, Giulia Lafratta, Alice Miller",2025-08-26 16:49:30+00:00,http://arxiv.org/abs/2508.19186v1,http://arxiv.org/pdf/2508.19186v1,"cs.RO, cs.AI, cs.FL, I.2.9; I.2; D.2.4"
From Tabula Rasa to Emergent Abilities: Discovering Robot Skills via Real-World Unsupervised Quality-Diversity,"Autonomous skill discovery aims to enable robots to acquire diverse behaviors
without explicit supervision. Learning such behaviors directly on physical
hardware remains challenging due to safety and data efficiency constraints.
Existing methods, including Quality-Diversity Actor-Critic (QDAC), require
manually defined skill spaces and carefully tuned heuristics, limiting
real-world applicability. We propose Unsupervised Real-world Skill Acquisition
(URSA), an extension of QDAC that enables robots to autonomously discover and
master diverse, high-performing skills directly in the real world. We
demonstrate that URSA successfully discovers diverse locomotion skills on a
Unitree A1 quadruped in both simulation and the real world. Our approach
supports both heuristic-driven skill discovery and fully unsupervised settings.
We also show that the learned skill repertoire can be reused for downstream
tasks such as real-world damage adaptation, where URSA outperforms all
baselines in 5 out of 9 simulated and 3 out of 5 real-world damage scenarios.
Our results establish a new framework for real-world robot learning that
enables continuous skill discovery with limited human intervention,
representing a significant step toward more autonomous and adaptable robotic
systems. Demonstration videos are available at
https://adaptive-intelligent-robotics.github.io/URSA.","Luca Grillotti, Lisa Coiffard, Oscar Pang, Maxence Faldor, Antoine Cully",2025-08-26 16:20:41+00:00,http://arxiv.org/abs/2508.19172v3,http://arxiv.org/pdf/2508.19172v3,"cs.RO, cs.AI, cs.LG"
Direction Informed Trees (DIT*): Optimal Path Planning via Direction Filter and Direction Cost Heuristic,"Optimal path planning requires finding a series of feasible states from the
starting point to the goal to optimize objectives. Popular path planning
algorithms, such as Effort Informed Trees (EIT*), employ effort heuristics to
guide the search. Effective heuristics are accurate and computationally
efficient, but achieving both can be challenging due to their conflicting
nature. This paper proposes Direction Informed Trees (DIT*), a sampling-based
planner that focuses on optimizing the search direction for each edge,
resulting in goal bias during exploration. We define edges as generalized
vectors and integrate similarity indexes to establish a directional filter that
selects the nearest neighbors and estimates direction costs. The estimated
direction cost heuristics are utilized in edge evaluation. This strategy allows
the exploration to share directional information efficiently. DIT* convergence
faster than existing single-query, sampling-based planners on tested problems
in R^4 to R^16 and has been demonstrated in real-world environments with
various planning tasks. A video showcasing our experimental results is
available at: https://youtu.be/2SX6QT2NOek","Liding Zhang, Kejia Chen, Kuanqi Cai, Yu Zhang, Yixuan Dang, Yansong Wu, Zhenshan Bing, Fan Wu, Sami Haddadin, Alois Knoll",2025-08-26 16:16:18+00:00,http://arxiv.org/abs/2508.19168v1,http://arxiv.org/pdf/2508.19168v1,cs.RO
Real-time Testing of Satellite Attitude Control With a Reaction Wheel Hardware-In-the-Loop Platform,"We propose the Hardware-in-the-Loop (HIL) test of an adaptive satellite
attitude control system with reaction wheel health estimation capabilities.
Previous simulations and Software-in-the-Loop testing have prompted further
experiments to explore the validity of the controller with real momentum
exchange devices in the loop. This work is a step toward a comprehensive
testing framework for validation of spacecraft attitude control algorithms. The
proposed HIL testbed includes brushless DC motors and drivers that communicate
using a CAN bus, an embedded computer that executes control and adaptation
laws, and a satellite simulator that produces simulated sensor data, estimated
attitude states, and responds to actions of the external actuators. We propose
methods to artificially induce failures on the reaction wheels, and present
related issues and lessons learned.","Morokot Sakal, George Nehma, Camilo Riano-Rios, Madhur Tiwari",2025-08-26 16:12:25+00:00,http://arxiv.org/abs/2508.19164v1,http://arxiv.org/pdf/2508.19164v1,"cs.RO, cs.SY, eess.SY"
Safe Navigation under State Uncertainty: Online Adaptation for Robust Control Barrier Functions,"Measurements and state estimates are often imperfect in control practice,
posing challenges for safety-critical applications, where safety guarantees
rely on accurate state information. In the presence of estimation errors,
several prior robust control barrier function (R-CBF) formulations have imposed
strict conditions on the input. These methods can be overly conservative and
can introduce issues such as infeasibility, high control effort, etc. This work
proposes a systematic method to improve R-CBFs, and demonstrates its advantages
on a tracked vehicle that navigates among multiple obstacles. A primary
contribution is a new optimization-based online parameter adaptation scheme
that reduces the conservativeness of existing R-CBFs. In order to reduce the
complexity of the parameter optimization, we merge several safety constraints
into one unified numerical CBF via Poisson's equation. We further address the
dual relative degree issue that typically causes difficulty in vehicle
tracking. Experimental trials demonstrate the overall performance improvement
of our approach over existing formulations.","Ersin Das, Rahal Nanayakkara, Xiao Tan, Ryan M. Bena, Joel W. Burdick, Paulo Tabuada, Aaron D. Ames",2025-08-26 16:09:14+00:00,http://arxiv.org/abs/2508.19159v1,http://arxiv.org/pdf/2508.19159v1,"eess.SY, cs.RO, cs.SY"
QuadKAN: KAN-Enhanced Quadruped Motion Control via End-to-End Reinforcement Learning,"We address vision-guided quadruped motion control with reinforcement learning
(RL) and highlight the necessity of combining proprioception with vision for
robust control. We propose QuadKAN, a spline-parameterized cross-modal policy
instantiated with Kolmogorov-Arnold Networks (KANs). The framework incorporates
a spline encoder for proprioception and a spline fusion head for
proprioception-vision inputs. This structured function class aligns the
state-to-action mapping with the piecewise-smooth nature of gait, improving
sample efficiency, reducing action jitter and energy consumption, and providing
interpretable posture-action sensitivities. We adopt Multi-Modal Delay
Randomization (MMDR) and perform end-to-end training with Proximal Policy
Optimization (PPO). Evaluations across diverse terrains, including both even
and uneven surfaces and scenarios with static or dynamic obstacles, demonstrate
that QuadKAN achieves consistently higher returns, greater distances, and fewer
collisions than state-of-the-art (SOTA) baselines. These results show that
spline-parameterized policies offer a simple, effective, and interpretable
alternative for robust vision-guided locomotion. A repository will be made
available upon acceptance.","Yinuo Wang, Gavin Tao",2025-08-26 16:05:32+00:00,http://arxiv.org/abs/2508.19153v2,http://arxiv.org/pdf/2508.19153v2,"cs.RO, cs.AI, cs.CV, cs.SY, eess.IV, eess.SY"
Uncertainty-Resilient Active Intention Recognition for Robotic Assistants,"Purposeful behavior in robotic assistants requires the integration of
multiple components and technological advances. Often, the problem is reduced
to recognizing explicit prompts, which limits autonomy, or is oversimplified
through assumptions such as near-perfect information. We argue that a critical
gap remains unaddressed -- specifically, the challenge of reasoning about the
uncertain outcomes and perception errors inherent to human intention
recognition. In response, we present a framework designed to be resilient to
uncertainty and sensor noise, integrating real-time sensor data with a
combination of planners. Centered around an intention-recognition POMDP, our
approach addresses cooperative planning and acting under uncertainty. Our
integrated framework has been successfully tested on a physical robot with
promising results.","Juan Carlos Saborío, Marc Vinci, Oscar Lima, Sebastian Stock, Lennart Niecksch, Martin Günther, Alexander Sung, Joachim Hertzberg, Martin Atzmüller",2025-08-26 16:00:38+00:00,http://arxiv.org/abs/2508.19150v1,http://arxiv.org/pdf/2508.19150v1,"cs.RO, cs.AI"
ZeST: an LLM-based Zero-Shot Traversability Navigation for Unknown Environments,"The advancement of robotics and autonomous navigation systems hinges on the
ability to accurately predict terrain traversability. Traditional methods for
generating datasets to train these prediction models often involve putting
robots into potentially hazardous environments, posing risks to equipment and
safety. To solve this problem, we present ZeST, a novel approach leveraging
visual reasoning capabilities of Large Language Models (LLMs) to create a
traversability map in real-time without exposing robots to danger. Our approach
not only performs zero-shot traversability and mitigates the risks associated
with real-world data collection but also accelerates the development of
advanced navigation systems, offering a cost-effective and scalable solution.
To support our findings, we present navigation results, in both controlled
indoor and unstructured outdoor environments. As shown in the experiments, our
method provides safer navigation when compared to other state-of-the-art
methods, constantly reaching the final goal.","Shreya Gummadi, Mateus V. Gasparino, Gianluca Capezzuto, Marcelo Becker, Girish Chowdhary",2025-08-26 15:30:19+00:00,http://arxiv.org/abs/2508.19131v1,http://arxiv.org/pdf/2508.19131v1,"cs.RO, cs.AI, cs.CV"
DELIVER: A System for LLM-Guided Coordinated Multi-Robot Pickup and Delivery using Voronoi-Based Relay Planning,"We present DELIVER (Directed Execution of Language-instructed Item Via
Engineered Relay), a fully integrated framework for cooperative multi-robot
pickup and delivery driven by natural language commands. DELIVER unifies
natural language understanding, spatial decomposition, relay planning, and
motion execution to enable scalable, collision-free coordination in real-world
settings. Given a spoken or written instruction, a lightweight instance of
LLaMA3 interprets the command to extract pickup and delivery locations. The
environment is partitioned using a Voronoi tessellation to define
robot-specific operating regions. Robots then compute optimal relay points
along shared boundaries and coordinate handoffs. A finite-state machine governs
each robot's behavior, enabling robust execution. We implement DELIVER on the
MultiTRAIL simulation platform and validate it in both ROS2-based Gazebo
simulations and real-world hardware using TurtleBot3 robots. Empirical results
show that DELIVER maintains consistent mission cost across varying team sizes
while reducing per-agent workload by up to 55% compared to a single-agent
system. Moreover, the number of active relay agents remains low even as team
size increases, demonstrating the system's scalability and efficient agent
utilization. These findings underscore DELIVER's modular and extensible
architecture for language-guided multi-robot coordination, advancing the
frontiers of cyber-physical system integration.","Alkesh K. Srivastava, Jared Michael Levin, Alexander Derrico, Philip Dames",2025-08-26 15:17:08+00:00,http://arxiv.org/abs/2508.19114v1,http://arxiv.org/pdf/2508.19114v1,"cs.RO, cs.MA"
VibES: Induced Vibration for Persistent Event-Based Sensing,"Event cameras are a bio-inspired class of sensors that asynchronously measure
per-pixel intensity changes. Under fixed illumination conditions in static or
low-motion scenes, rigidly mounted event cameras are unable to generate any
events, becoming unsuitable for most computer vision tasks. To address this
limitation, recent work has investigated motion-induced event stimulation that
often requires complex hardware or additional optical components. In contrast,
we introduce a lightweight approach to sustain persistent event generation by
employing a simple rotating unbalanced mass to induce periodic vibrational
motion. This is combined with a motion-compensation pipeline that removes the
injected motion and yields clean, motion-corrected events for downstream
perception tasks. We demonstrate our approach with a hardware prototype and
evaluate it on real-world captured datasets. Our method reliably recovers
motion parameters and improves both image reconstruction and edge detection
over event-based sensing without motion induction.","Vincenzo Polizzi, Stephen Yang, Quentin Clark, Jonathan Kelly, Igor Gilitschenski, David B. Lindell",2025-08-26 14:58:51+00:00,http://arxiv.org/abs/2508.19094v1,http://arxiv.org/pdf/2508.19094v1,"cs.CV, cs.RO"
An LLM-powered Natural-to-Robotic Language Translation Framework with Correctness Guarantees,"The Large Language Models (LLM) are increasingly being deployed in robotics
to generate robot control programs for specific user tasks, enabling embodied
intelligence. Existing methods primarily focus on LLM training and prompt
design that utilize LLMs to generate executable programs directly from user
tasks in natural language. However, due to the inconsistency of the LLMs and
the high complexity of the tasks, such best-effort approaches often lead to
tremendous programming errors in the generated code, which significantly
undermines the effectiveness especially when the light-weight LLMs are applied.
This paper introduces a natural-robotic language translation framework that (i)
provides correctness verification for generated control programs and (ii)
enhances the performance of LLMs in program generation via feedback-based
fine-tuning for the programs. To achieve this, a Robot Skill Language (RSL) is
proposed to abstract away from the intricate details of the control programs,
bridging the natural language tasks with the underlying robot skills. Then, the
RSL compiler and debugger are constructed to verify RSL programs generated by
the LLM and provide error feedback to the LLM for refining the outputs until
being verified by the compiler. This provides correctness guarantees for the
LLM-generated programs before being offloaded to the robots for execution,
significantly enhancing the effectiveness of LLM-powered robotic applications.
Experiments demonstrate NRTrans outperforms the existing method under a range
of LLMs and tasks, and achieves a high success rate for light-weight LLMs.","ZhenDong Chen, ZhanShang Nie, ShiXing Wan, JunYi Li, YongTian Cheng, Shuai Zhao",2025-08-26 14:32:49+00:00,http://arxiv.org/abs/2508.19074v1,http://arxiv.org/pdf/2508.19074v1,"cs.RO, cs.AI, cs.PL"
HuBE: Cross-Embodiment Human-like Behavior Execution for Humanoid Robots,"Achieving both behavioral similarity and appropriateness in human-like motion
generation for humanoid robot remains an open challenge, further compounded by
the lack of cross-embodiment adaptability. To address this problem, we propose
HuBE, a bi-level closed-loop framework that integrates robot state, goal poses,
and contextual situations to generate human-like behaviors, ensuring both
behavioral similarity and appropriateness, and eliminating structural
mismatches between motion generation and execution. To support this framework,
we construct HPose, a context-enriched dataset featuring fine-grained
situational annotations. Furthermore, we introduce a bone scaling-based data
augmentation strategy that ensures millimeter-level compatibility across
heterogeneous humanoid robots. Comprehensive evaluations on multiple commercial
platforms demonstrate that HuBE significantly improves motion similarity,
behavioral appropriateness, and computational efficiency over state-of-the-art
baselines, establishing a solid foundation for transferable and human-like
behavior execution across diverse humanoid robots.","Shipeng Lyu, Fangyuan Wang, Weiwei Lin, Luhao Zhu, David Navarro-Alarcon, Guodong Guo",2025-08-26 13:01:07+00:00,http://arxiv.org/abs/2508.19002v1,http://arxiv.org/pdf/2508.19002v1,cs.RO
Enhanced UAV Path Planning Using the Tangent Intersection Guidance (TIG) Algorithm,"Efficient and safe navigation of Unmanned Aerial Vehicles (UAVs) is critical
for various applications, including combat support, package delivery and Search
and Rescue Operations. This paper introduces the Tangent Intersection Guidance
(TIG) algorithm, an advanced approach for UAV path planning in both static and
dynamic environments. The algorithm uses the elliptic tangent intersection
method to generate feasible paths. It generates two sub-paths for each threat,
selects the optimal route based on a heuristic rule, and iteratively refines
the path until the target is reached. Considering the UAV kinematic and dynamic
constraints, a modified smoothing technique based on quadratic B\'ezier curves
is adopted to generate a smooth and efficient route. Experimental results show
that the TIG algorithm can generate the shortest path in less time, starting
from 0.01 seconds, with fewer turning angles compared to A*, PRM, RRT*, Tangent
Graph, and Static APPATT algorithms in static environments. Furthermore, in
completely unknown and partially known environments, TIG demonstrates efficient
real-time path planning capabilities for collision avoidance, outperforming APF
and Dynamic APPATT algorithms.","Hichem Cheriet, Khellat Kihel Badra, Chouraqui Samira",2025-08-26 12:11:59+00:00,http://arxiv.org/abs/2508.18967v1,http://arxiv.org/pdf/2508.18967v1,"cs.RO, cs.CV"
VisionSafeEnhanced VPC: Cautious Predictive Control with Visibility Constraints under Uncertainty for Autonomous Robotic Surgery,"Autonomous control of the laparoscope in robot-assisted Minimally Invasive
Surgery (MIS) has received considerable research interest due to its potential
to improve surgical safety. Despite progress in pixel-level Image-Based Visual
Servoing (IBVS) control, the requirement of continuous visibility and the
existence of complex disturbances, such as parameterization error, measurement
noise, and uncertainties of payloads, could degrade the surgeon's visual
experience and compromise procedural safety. To address these limitations, this
paper proposes VisionSafeEnhanced Visual Predictive Control (VPC), a robust and
uncertainty-adaptive framework for autonomous laparoscope control that
guarantees Field of View (FoV) safety under uncertainty. Firstly, Gaussian
Process Regression (GPR) is utilized to perform hybrid (deterministic +
stochastic) quantification of operational uncertainties including residual
model uncertainties, stochastic uncertainties, and external disturbances. Based
on uncertainty quantification, a novel safety aware trajectory optimization
framework with probabilistic guarantees is proposed, where a
uncertainty-adaptive safety Control Barrier Function (CBF) condition is given
based on uncertainty propagation, and chance constraints are simultaneously
formulated based on probabilistic approximation. This uncertainty aware
formulation enables adaptive control effort allocation, minimizing unnecessary
camera motion while maintaining robustness. The proposed method is validated
through comparative simulations and experiments on a commercial surgical robot
platform (MicroPort MedBot Toumai) performing a sequential multi-target lymph
node dissection. Compared with baseline methods, the framework maintains
near-perfect target visibility (>99.9%), reduces tracking e","Wang Jiayin, Wei Yanran, Jiang Lei, Guo Xiaoyu, Zheng Ayong, Zhao Weidong, Li Zhongkui",2025-08-26 11:24:20+00:00,http://arxiv.org/abs/2508.18937v1,http://arxiv.org/pdf/2508.18937v1,cs.RO
Interpretable Decision-Making for End-to-End Autonomous Driving,"Trustworthy AI is mandatory for the broad deployment of autonomous vehicles.
Although end-to-end approaches derive control commands directly from raw data,
interpreting these decisions remains challenging, especially in complex urban
scenarios. This is mainly attributed to very deep neural networks with
non-linear decision boundaries, making it challenging to grasp the logic behind
AI-driven decisions. This paper presents a method to enhance interpretability
while optimizing control commands in autonomous driving. To address this, we
propose loss functions that promote the interpretability of our model by
generating sparse and localized feature maps. The feature activations allow us
to explain which image regions contribute to the predicted control command. We
conduct comprehensive ablation studies on the feature extraction step and
validate our method on the CARLA benchmarks. We also demonstrate that our
approach improves interpretability, which correlates with reducing infractions,
yielding a safer, high-performance driving model. Notably, our monocular,
non-ensemble model surpasses the top-performing approaches from the CARLA
Leaderboard by achieving lower infraction scores and the highest route
completion rate, all while ensuring interpretability.","Mona Mirzaie, Bodo Rosenhahn",2025-08-26 10:14:16+00:00,http://arxiv.org/abs/2508.18898v1,http://arxiv.org/pdf/2508.18898v1,"cs.CV, cs.AI, cs.LG, cs.RO"
AS2FM: Enabling Statistical Model Checking of ROS 2 Systems for Robust Autonomy,"Designing robotic systems to act autonomously in unforeseen environments is a
challenging task. This work presents a novel approach to use formal
verification, specifically Statistical Model Checking (SMC), to verify system
properties of autonomous robots at design-time. We introduce an extension of
the SCXML format, designed to model system components including both Robot
Operating System 2 (ROS 2) and Behavior Tree (BT) features. Further, we
contribute Autonomous Systems to Formal Models (AS2FM), a tool to translate the
full system model into JANI. The use of JANI, a standard format for
quantitative model checking, enables verification of system properties with
off-the-shelf SMC tools. We demonstrate the practical usability of AS2FM both
in terms of applicability to real-world autonomous robotic control systems, and
in terms of verification runtime scaling. We provide a case study, where we
successfully identify problems in a ROS 2-based robotic manipulation use case
that is verifiable in less than one second using consumer hardware.
Additionally, we compare to the state of the art and demonstrate that our
method is more comprehensive in system feature support, and that the
verification runtime scales linearly with the size of the model, instead of
exponentially.","Christian Henkel, Marco Lampacrescia, Michaela Klauck, Matteo Morelli",2025-08-26 09:00:18+00:00,http://arxiv.org/abs/2508.18820v1,http://arxiv.org/pdf/2508.18820v1,"cs.RO, cs.FL"
Learning Real-World Acrobatic Flight from Human Preferences,"Preference-based reinforcement learning (PbRL) enables agents to learn
control policies without requiring manually designed reward functions, making
it well-suited for tasks where objectives are difficult to formalize or
inherently subjective. Acrobatic flight poses a particularly challenging
problem due to its complex dynamics, rapid movements, and the importance of
precise execution. In this work, we explore the use of PbRL for agile drone
control, focusing on the execution of dynamic maneuvers such as powerloops.
Building on Preference-based Proximal Policy Optimization (Preference PPO), we
propose Reward Ensemble under Confidence (REC), an extension to the reward
learning objective that improves preference modeling and learning stability.
Our method achieves 88.4% of the shaped reward performance, compared to 55.2%
with standard Preference PPO. We train policies in simulation and successfully
transfer them to real-world drones, demonstrating multiple acrobatic maneuvers
where human preferences emphasize stylistic qualities of motion. Furthermore,
we demonstrate the applicability of our probabilistic reward model in a
representative MuJoCo environment for continuous control. Finally, we highlight
the limitations of manually designed rewards, observing only 60.7% agreement
with human preferences. These results underscore the effectiveness of PbRL in
capturing complex, human-centered objectives across both physical and simulated
domains.","Colin Merk, Ismail Geles, Jiaxu Xing, Angel Romero, Giorgia Ramponi, Davide Scaramuzza",2025-08-26 08:56:53+00:00,http://arxiv.org/abs/2508.18817v1,http://arxiv.org/pdf/2508.18817v1,"cs.RO, cs.LG"
HyperTASR: Hypernetwork-Driven Task-Aware Scene Representations for Robust Manipulation,"Effective policy learning for robotic manipulation requires scene
representations that selectively capture task-relevant environmental features.
Current approaches typically employ task-agnostic representation extraction,
failing to emulate the dynamic perceptual adaptation observed in human
cognition. We present HyperTASR, a hypernetwork-driven framework that modulates
scene representations based on both task objectives and the execution phase.
Our architecture dynamically generates representation transformation parameters
conditioned on task specifications and progression state, enabling
representations to evolve contextually throughout task execution. This approach
maintains architectural compatibility with existing policy learning frameworks
while fundamentally reconfiguring how visual features are processed. Unlike
methods that simply concatenate or fuse task embeddings with task-agnostic
representations, HyperTASR establishes computational separation between
task-contextual and state-dependent processing paths, enhancing learning
efficiency and representational quality. Comprehensive evaluations in both
simulation and real-world environments demonstrate substantial performance
improvements across different representation paradigms. Through ablation
studies and attention visualization, we confirm that our approach selectively
prioritizes task-relevant scene information, closely mirroring human adaptive
perception during manipulation tasks. The project website is at
[HyperTASR](https://lisunphil.github.io/HyperTASR_projectpage/
""lisunphil.github.io/HyperTASR_projectpage/"").","Li Sun, Jiefeng Wu, Feng Chen, Ruizhe Liu, Yanchao Yang",2025-08-26 08:35:17+00:00,http://arxiv.org/abs/2508.18802v2,http://arxiv.org/pdf/2508.18802v2,cs.RO
PseudoMapTrainer: Learning Online Mapping without HD Maps,"Online mapping models show remarkable results in predicting vectorized maps
from multi-view camera images only. However, all existing approaches still rely
on ground-truth high-definition maps during training, which are expensive to
obtain and often not geographically diverse enough for reliable generalization.
In this work, we propose PseudoMapTrainer, a novel approach to online mapping
that uses pseudo-labels generated from unlabeled sensor data. We derive those
pseudo-labels by reconstructing the road surface from multi-camera imagery
using Gaussian splatting and semantics of a pre-trained 2D segmentation
network. In addition, we introduce a mask-aware assignment algorithm and loss
function to handle partially masked pseudo-labels, allowing for the first time
the training of online mapping models without any ground-truth maps.
Furthermore, our pseudo-labels can be effectively used to pre-train an online
model in a semi-supervised manner to leverage large-scale unlabeled
crowdsourced data. The code is available at
github.com/boschresearch/PseudoMapTrainer.","Christian Löwens, Thorben Funke, Jingchao Xie, Alexandru Paul Condurache",2025-08-26 08:13:30+00:00,http://arxiv.org/abs/2508.18788v1,http://arxiv.org/pdf/2508.18788v1,"cs.CV, cs.LG, cs.RO"
Hybrid Perception and Equivariant Diffusion for Robust Multi-Node Rebar Tying,"Rebar tying is a repetitive but critical task in reinforced concrete
construction, typically performed manually at considerable ergonomic risk.
Recent advances in robotic manipulation hold the potential to automate the
tying process, yet face challenges in accurately estimating tying poses in
congested rebar nodes. In this paper, we introduce a hybrid perception and
motion planning approach that integrates geometry-based perception with
Equivariant Denoising Diffusion on SE(3) (Diffusion-EDFs) to enable robust
multi-node rebar tying with minimal training data. Our perception module
utilizes density-based clustering (DBSCAN), geometry-based node feature
extraction, and principal component analysis (PCA) to segment rebar bars,
identify rebar nodes, and estimate orientation vectors for sequential ranking,
even in complex, unstructured environments. The motion planner, based on
Diffusion-EDFs, is trained on as few as 5-10 demonstrations to generate
sequential end-effector poses that optimize collision avoidance and tying
efficiency. The proposed system is validated on various rebar meshes, including
single-layer, multi-layer, and cluttered configurations, demonstrating high
success rates in node detection and accurate sequential tying. Compared with
conventional approaches that rely on large datasets or extensive manual
parameter tuning, our method achieves robust, efficient, and adaptable
multi-node tying while significantly reducing data requirements. This result
underscores the potential of hybrid perception and diffusion-driven planning to
enhance automation in on-site construction tasks, improving both safety and
labor efficiency.","Zhitao Wang, Yirong Xiong, Roberto Horowitz, Yanke Wang, Yuxing Han",2025-08-26 07:09:27+00:00,http://arxiv.org/abs/2509.00065v1,http://arxiv.org/pdf/2509.00065v1,"cs.RO, cs.CV"
Are All Marine Species Created Equal? Performance Disparities in Underwater Object Detection,"Underwater object detection is critical for monitoring marine ecosystems but
poses unique challenges, including degraded image quality, imbalanced class
distribution, and distinct visual characteristics. Not every species is
detected equally well, yet underlying causes remain unclear. We address two key
research questions: 1) What factors beyond data quantity drive class-specific
performance disparities? 2) How can we systematically improve detection of
under-performing marine species? We manipulate the DUO dataset to separate the
object detection task into localization and classification and investigate the
under-performance of the scallop class. Localization analysis using YOLO11 and
TIDE finds that foreground-background discrimination is the most problematic
stage regardless of data quantity. Classification experiments reveal persistent
precision gaps even with balanced data, indicating intrinsic feature-based
challenges beyond data scarcity and inter-class dependencies. We recommend
imbalanced distributions when prioritizing precision, and balanced
distributions when prioritizing recall. Improving under-performing classes
should focus on algorithmic advances, especially within localization modules.
We publicly release our code and datasets.","Melanie Wille, Tobias Fischer, Scarlett Raine",2025-08-26 06:56:06+00:00,http://arxiv.org/abs/2508.18729v1,http://arxiv.org/pdf/2508.18729v1,"cs.CV, cs.LG, cs.RO"
OpenTie: Open-vocabulary Sequential Rebar Tying System,"Robotic practices on the construction site emerge as an attention-attracting
manner owing to their capability of tackle complex challenges, especially in
the rebar-involved scenarios. Most of existing products and research are mainly
focused on flat rebar setting with model training demands. To fulfill this gap,
we propose OpenTie, a 3D training-free rebar tying framework utilizing a
RGB-to-point-cloud generation and an open-vocabulary detection. We implements
the OpenTie via a robotic arm with a binocular camera and guarantees a high
accuracy by applying the prompt-based object detection method on the image
filtered by our propose post-processing procedure based a image to point cloud
generation framework. The system is flexible for horizontal and vertical rebar
tying tasks and the experiments on the real-world rebar setting verifies that
the effectiveness of the system in practice.","Mingze Liu, Sai Fan, Haozhen Li, Haobo Liang, Yixing Yuan, Yanke Wang",2025-08-26 06:25:54+00:00,http://arxiv.org/abs/2509.00064v1,http://arxiv.org/pdf/2509.00064v1,"cs.RO, cs.CV"
Enhancing Video-Based Robot Failure Detection Using Task Knowledge,"Robust robotic task execution hinges on the reliable detection of execution
failures in order to trigger safe operation modes, recovery strategies, or task
replanning. However, many failure detection methods struggle to provide
meaningful performance when applied to a variety of real-world scenarios. In
this paper, we propose a video-based failure detection approach that uses
spatio-temporal knowledge in the form of the actions the robot performs and
task-relevant objects within the field of view. Both pieces of information are
available in most robotic scenarios and can thus be readily obtained. We
demonstrate the effectiveness of our approach on three datasets that we amend,
in part, with additional annotations of the aforementioned task-relevant
knowledge. In light of the results, we also propose a data augmentation method
that improves performance by applying variable frame rates to different parts
of the video. We observe an improvement from 77.9 to 80.0 in F1 score on the
ARMBench dataset without additional computational expense and an additional
increase to 81.4 with test-time augmentation. The results emphasize the
importance of spatio-temporal information during failure detection and suggest
further investigation of suitable heuristics in future implementations. Code
and annotations are available.","Santosh Thoduka, Sebastian Houben, Juergen Gall, Paul G. Plöger",2025-08-26 06:10:46+00:00,http://arxiv.org/abs/2508.18705v1,http://arxiv.org/pdf/2508.18705v1,"cs.RO, cs.CV"
AgriChrono: A Multi-modal Dataset Capturing Crop Growth and Lighting Variability with a Field Robot,"Existing datasets for precision agriculture have primarily been collected in
static or controlled environments such as indoor labs or greenhouses, often
with limited sensor diversity and restricted temporal span. These conditions
fail to reflect the dynamic nature of real farmland, including illumination
changes, crop growth variation, and natural disturbances. As a result, models
trained on such data often lack robustness and generalization when applied to
real-world field scenarios. In this paper, we present AgriChrono, a novel
robotic data collection platform and multi-modal dataset designed to capture
the dynamic conditions of real-world agricultural environments. Our platform
integrates multiple sensors and enables remote, time-synchronized acquisition
of RGB, Depth, LiDAR, and IMU data, supporting efficient and repeatable
long-term data collection across varying illumination and crop growth stages.
We benchmark a range of state-of-the-art 3D reconstruction models on the
AgriChrono dataset, highlighting the difficulty of reconstruction in real-world
field environments and demonstrating its value as a research asset for
advancing model generalization under dynamic conditions. The code and dataset
are publicly available at: https://github.com/StructuresComp/agri-chrono","Jaehwan Jeong, Tuan-Anh Vu, Mohammad Jony, Shahab Ahmad, Md. Mukhlesur Rahman, Sangpil Kim, M. Khalid Jawed",2025-08-26 05:39:47+00:00,http://arxiv.org/abs/2508.18694v1,http://arxiv.org/pdf/2508.18694v1,"cs.RO, cs.AI, cs.SY, eess.SY"
Deep Sensorimotor Control by Imitating Predictive Models of Human Motion,"As the embodiment gap between a robot and a human narrows, new opportunities
arise to leverage datasets of humans interacting with their surroundings for
robot learning. We propose a novel technique for training sensorimotor policies
with reinforcement learning by imitating predictive models of human motions.
Our key insight is that the motion of keypoints on human-inspired robot
end-effectors closely mirrors the motion of corresponding human body keypoints.
This enables us to use a model trained to predict future motion on human data
\emph{zero-shot} on robot data. We train sensorimotor policies to track the
predictions of such a model, conditioned on a history of past robot states,
while optimizing a relatively sparse task reward. This approach entirely
bypasses gradient-based kinematic retargeting and adversarial losses, which
limit existing methods from fully leveraging the scale and diversity of modern
human-scene interaction datasets. Empirically, we find that our approach can
work across robots and tasks, outperforming existing baselines by a large
margin. In addition, we find that tracking a human motion model can substitute
for carefully designed dense rewards and curricula in manipulation tasks. Code,
data and qualitative results available at
https://jirl-upenn.github.io/track_reward/.","Himanshu Gaurav Singh, Pieter Abbeel, Jitendra Malik, Antonio Loquercio",2025-08-26 05:25:57+00:00,http://arxiv.org/abs/2508.18691v1,http://arxiv.org/pdf/2508.18691v1,cs.RO
Engineering Automotive Digital Twins on Standardized Architectures: A Case Study,"Digital twin (DT) technology has become of interest in the automotive
industry. There is a growing need for smarter services that utilize the unique
capabilities of DTs, ranging from computer-aided remote control to cloud-based
fleet coordination. Developing such services starts with the software
architecture. However, the scarcity of DT architectural guidelines poses a
challenge for engineering automotive DTs. Currently, the only DT architectural
standard is the one defined in ISO 23247. Though not developed for automotive
systems, it is one of the few feasible starting points for automotive DTs. In
this work, we investigate the suitability of the ISO 23247 reference
architecture for developing automotive DTs. Through the case study of
developing an Adaptive Cruise Control DT for a 1/10\textsuperscript{th}-scale
autonomous vehicle, we identify some strengths and limitations of the reference
architecture and begin distilling future directions for researchers,
practitioners, and standard developers.","Stefan Ramdhan, Winnie Trandinh, Istvan David, Vera Pantelic, Mark Lawford",2025-08-26 04:09:13+00:00,http://arxiv.org/abs/2508.18662v1,http://arxiv.org/pdf/2508.18662v1,cs.RO
Integration of Robot and Scene Kinematics for Sequential Mobile Manipulation Planning,"We present a Sequential Mobile Manipulation Planning (SMMP) framework that
can solve long-horizon multi-step mobile manipulation tasks with coordinated
whole-body motion, even when interacting with articulated objects. By
abstracting environmental structures as kinematic models and integrating them
with the robot's kinematics, we construct an Augmented Configuration Apace
(A-Space) that unifies the previously separate task constraints for navigation
and manipulation, while accounting for the joint reachability of the robot
base, arm, and manipulated objects. This integration facilitates efficient
planning within a tri-level framework: a task planner generates symbolic action
sequences to model the evolution of A-Space, an optimization-based motion
planner computes continuous trajectories within A-Space to achieve desired
configurations for both the robot and scene elements, and an intermediate plan
refinement stage selects action goals that ensure long-horizon feasibility. Our
simulation studies first confirm that planning in A-Space achieves an 84.6\%
higher task success rate compared to baseline methods. Validation on real
robotic systems demonstrates fluid mobile manipulation involving (i) seven
types of rigid and articulated objects across 17 distinct contexts, and (ii)
long-horizon tasks of up to 14 sequential steps. Our results highlight the
significance of modeling scene kinematics into planning entities, rather than
encoding task-specific constraints, offering a scalable and generalizable
approach to complex robotic manipulation.","Ziyuan Jiao, Yida Niu, Zeyu Zhang, Yangyang Wu, Yao Su, Yixin Zhu, Hangxin Liu, Song-Chun Zhu",2025-08-26 03:08:54+00:00,http://arxiv.org/abs/2508.18627v1,http://arxiv.org/pdf/2508.18627v1,cs.RO
Design and Testing of a Low-Cost 3D-Printed Servo Gimbal for Thrust Vector Control in Model Rockets,"Thrust vector control (TVC) is a key mechanism for stabilizing rockets during
flight, yet conventional implementations remain costly and technically
inaccessible to students and hobbyists. This paper presents the design,
fabrication, and testing of a low-cost, 3D-printed, servo-driven
two-dimensional gimbal developed for model rocket applications. The gimbal
underwent more than 60 CAD iterations, with servo selection guided by torque,
response time, and stability requirements. A high-speed camera and Fusion 360
parameter simulations were used to emulate dynamic instability, enabling
evaluation of angular deflection, servo responsiveness, and structural
durability. The results demonstrated stable actuation within plus or minus 5
degrees, with response times on the average order of 44.5 ms, while limitations
included servo fatigue and pin-joint stress under extended loading. The project
highlights the feasibility of student-accessible thrust vector control systems
and their potential as a reproducible platform for STEM education and
experimental aerospace research.",Ekansh Singh,2025-08-26 02:45:26+00:00,http://arxiv.org/abs/2509.00061v1,http://arxiv.org/pdf/2509.00061v1,"eess.SY, cs.RO, cs.SY"
SignLoc: Robust Localization using Navigation Signs and Public Maps,"Navigation signs and maps, such as floor plans and street maps, are widely
available and serve as ubiquitous aids for way-finding in human environments.
Yet, they are rarely used by robot systems. This paper presents SignLoc, a
global localization method that leverages navigation signs to localize the
robot on publicly available maps -- specifically floor plans and OpenStreetMap
(OSM) graphs -- without prior sensor-based mapping. SignLoc first extracts a
navigation graph from the input map. It then employs a probabilistic
observation model to match directional and locational cues from the detected
signs to the graph, enabling robust topo-semantic localization within a Monte
Carlo framework. We evaluated SignLoc in diverse large-scale environments: part
of a university campus, a shopping mall, and a hospital complex. Experimental
results show that SignLoc reliably localizes the robot after observing only one
to two signs.","Nicky Zimmerman, Joel Loo, Ayush Agrawal, David Hsu",2025-08-26 02:24:04+00:00,http://arxiv.org/abs/2508.18606v2,http://arxiv.org/pdf/2508.18606v2,cs.RO
Mimicking associative learning of rats via a neuromorphic robot in open field maze using spatial cell models,"Data-driven Artificial Intelligence (AI) approaches have exhibited remarkable
prowess across various cognitive tasks using extensive training data. However,
the reliance on large datasets and neural networks presents challenges such as
highpower consumption and limited adaptability, particularly in
SWaP-constrained applications like planetary exploration. To address these
issues, we propose enhancing the autonomous capabilities of intelligent robots
by emulating the associative learning observed in animals. Associative learning
enables animals to adapt to their environment by memorizing concurrent events.
By replicating this mechanism, neuromorphic robots can navigate dynamic
environments autonomously, learning from interactions to optimize performance.
This paper explores the emulation of associative learning in rodents using
neuromorphic robots within open-field maze environments, leveraging insights
from spatial cells such as place and grid cells. By integrating these models,
we aim to enable online associative learning for spatial tasks in real-time
scenarios, bridging the gap between biological spatial cognition and robotics
for advancements in autonomous systems.","Tianze Liu, Md Abu Bakr Siddique, Hongyu An",2025-08-25 20:17:38+00:00,http://arxiv.org/abs/2508.18460v1,http://arxiv.org/pdf/2508.18460v1,"cs.RO, cs.SY, eess.SY"
PneuGelSight: Soft Robotic Vision-Based Proprioception and Tactile Sensing,"Soft pneumatic robot manipulators are popular in industrial and
human-interactive applications due to their compliance and flexibility.
However, deploying them in real-world scenarios requires advanced sensing for
tactile feedback and proprioception. Our work presents a novel vision-based
approach for sensorizing soft robots. We demonstrate our approach on
PneuGelSight, a pioneering pneumatic manipulator featuring high-resolution
proprioception and tactile sensing via an embedded camera. To optimize the
sensor's performance, we introduce a comprehensive pipeline that accurately
simulates its optical and dynamic properties, facilitating a zero-shot
knowledge transition from simulation to real-world applications. PneuGelSight
and our sim-to-real pipeline provide a novel, easily implementable, and robust
sensing methodology for soft robots, paving the way for the development of more
advanced soft robots with enhanced sensory capabilities.","Ruohan Zhang, Uksang Yoo, Yichen Li, Arpit Argawal, Wenzhen Yuan",2025-08-25 19:47:45+00:00,http://arxiv.org/abs/2508.18443v1,http://arxiv.org/pdf/2508.18443v1,cs.RO
Efficient task and path planning for maintenance automation using a robot system,"The research and development of intelligent automation solutions is a
ground-breaking point for the factory of the future. A promising and
challenging mission is the use of autonomous robot systems to automate tasks in
the field of maintenance. For this purpose, the robot system must be able to
plan autonomously the different manipulation tasks and the corresponding paths.
Basic requirements are the development of algorithms with a low computational
complexity and the possibility to deal with environmental uncertainties. In
this work, an approach is presented, which is especially suited to solve the
problem of maintenance automation. For this purpose, offline data from CAD is
combined with online data from an RGBD vision system via a probabilistic
filter, to compensate uncertainties from offline data. For planning the
different tasks, a method is explained, which use a symbolic description,
founded on a novel sampling-based method to compute the disassembly space. For
path planning we use global state-of-the art algorithms with a method that
allows the adaption of the exploration stepsize in order to reduce the planning
time. Every method is experimentally validated and discussed.","Christian Friedrich, Akos Csiszar, Armin Lechler, Alexander Verl",2025-08-25 18:40:27+00:00,http://arxiv.org/abs/2508.18400v1,http://arxiv.org/pdf/2508.18400v1,cs.RO
Maintenance automation: methods for robotics manipulation planning and execution,"Automating complex tasks using robotic systems requires skills for planning,
control and execution. This paper proposes a complete robotic system for
maintenance automation, which can automate disassembly and assembly operations
under environmental uncertainties (e.g. deviations between prior plan
information). The cognition of the robotic system is based on a planning
approach (using CAD and RGBD data) and includes a method to interpret a
symbolic plan and transform it to a set of executable robot instructions. The
complete system is experimentally evaluated using real-world applications. This
work shows the first step to transfer these theoretical results into a
practical robotic solution.","Christian Friedrich, Ralf Gulde, Armin Lechler, Alexander Verl",2025-08-25 18:38:38+00:00,http://arxiv.org/abs/2508.18399v1,http://arxiv.org/pdf/2508.18399v1,cs.RO
Mining the Long Tail: A Comparative Study of Data-Centric Criticality Metrics for Robust Offline Reinforcement Learning in Autonomous Motion Planning,"Offline Reinforcement Learning (RL) presents a promising paradigm for
training autonomous vehicle (AV) planning policies from large-scale, real-world
driving logs. However, the extreme data imbalance in these logs, where mundane
scenarios vastly outnumber rare ""long-tail"" events, leads to brittle and unsafe
policies when using standard uniform data sampling. In this work, we address
this challenge through a systematic, large-scale comparative study of data
curation strategies designed to focus the learning process on information-rich
samples. We investigate six distinct criticality weighting schemes which are
categorized into three families: heuristic-based, uncertainty-based, and
behavior-based. These are evaluated at two temporal scales, the individual
timestep and the complete scenario. We train seven goal-conditioned
Conservative Q-Learning (CQL) agents with a state-of-the-art, attention-based
architecture and evaluate them in the high-fidelity Waymax simulator. Our
results demonstrate that all data curation methods significantly outperform the
baseline. Notably, data-driven curation using model uncertainty as a signal
achieves the most significant safety improvements, reducing the collision rate
by nearly three-fold (from 16.0% to 5.5%). Furthermore, we identify a clear
trade-off where timestep-level weighting excels at reactive safety while
scenario-level weighting improves long-horizon planning. Our work provides a
comprehensive framework for data curation in Offline RL and underscores that
intelligent, non-uniform sampling is a critical component for building safe and
reliable autonomous agents.",Antonio Guillen-Perez,2025-08-25 18:37:29+00:00,http://arxiv.org/abs/2508.18397v1,http://arxiv.org/pdf/2508.18397v1,"cs.RO, cs.AI, cs.LG"
FlowVLA: Thinking in Motion with a Visual Chain of Thought,"Many Vision-Language-Action (VLA) models are built upon an internal world
model trained via direct next-frame prediction ($v_t \rightarrow v_{t+1}$).
This paradigm, however, presents a fundamental challenge: it \textbf{conflates}
the task of predicting physical motion with that of rendering static
appearance, forcing a single mechanism to handle both. This inherent coupling
often leads to physically implausible forecasts and inefficient policy
learning. To address this limitation, we introduce the \textbf{Visual Chain of
Thought (Visual CoT)}, a framework that disentangles these processes by
compelling the model to first reason about \textbf{motion dynamics} before
generating the future frame's \textbf{visual appearance}. We instantiate this
principle by proposing \textbf{FlowVLA}, an autoregressive Transformer that
explicitly materializes this reasoning process as ``$v_t \rightarrow f_t
\rightarrow v_{t+1}$'', where $f_t$ is an intermediate optical flow prediction.
By forcing the model to first commit to a motion plan ($f_t$), FlowVLA learns
disentangled dynamics, resulting in more coherent visual predictions and
significantly more efficient policy learning. Experiments on challenging
robotics manipulation benchmarks demonstrate that FlowVLA achieves
state-of-the-art performance with substantially improved sample efficiency,
pointing toward a more principled foundation for world modeling in VLAs.
Project page: https://irpn-lab.github.io/FlowVLA/","Zhide Zhong, Haodong Yan, Junfeng Li, Xiangchen Liu, Xin Gong, Wenxuan Song, Jiayi Chen, Haoang Li",2025-08-25 17:59:21+00:00,http://arxiv.org/abs/2508.18269v2,http://arxiv.org/pdf/2508.18269v2,cs.RO
SafeBimanual: Diffusion-based Trajectory Optimization for Safe Bimanual Manipulation,"Bimanual manipulation has been widely applied in household services and
manufacturing, which enables the complex task completion with coordination
requirements. Recent diffusion-based policy learning approaches have achieved
promising performance in modeling action distributions for bimanual
manipulation. However, they ignored the physical safety constraints of bimanual
manipulation, which leads to the dangerous behaviors with damage to robots and
objects. To this end, we propose a test-time trajectory optimization framework
named SafeBimanual for any pre-trained diffusion-based bimanual manipulation
policies, which imposes the safety constraints on bimanual actions to avoid
dangerous robot behaviors with improved success rate. Specifically, we design
diverse cost functions for safety constraints in different dual-arm cooperation
patterns including avoidance of tearing objects and collision between arms and
objects, which optimizes the manipulator trajectories with guided sampling of
diffusion denoising process. Moreover, we employ a vision-language model (VLM)
to schedule the cost functions by specifying keypoints and corresponding
pairwise relationship, so that the optimal safety constraint is dynamically
generated in the entire bimanual manipulation process. SafeBimanual
demonstrates superiority on 8 simulated tasks in RoboTwin with a 13.7% increase
in success rate and a 18.8% reduction in unsafe interactions over
state-of-the-art diffusion-based methods. Extensive experiments on 4 real-world
tasks further verify its practical value by improving the success rate by
32.5%.","Haoyuan Deng, Wenkai Guo, Qianzhun Wang, Zhenyu Wu, Ziwei Wang",2025-08-25 17:59:02+00:00,http://arxiv.org/abs/2508.18268v1,http://arxiv.org/pdf/2508.18268v1,"cs.RO, cs.AI"
"Correspondence-Free, Function-Based Sim-to-Real Learning for Deformable Surface Control","This paper presents a correspondence-free, function-based sim-to-real
learning method for controlling deformable freeform surfaces. Unlike
traditional sim-to-real transfer methods that strongly rely on marker points
with full correspondences, our approach simultaneously learns a deformation
function space and a confidence map -- both parameterized by a neural network
-- to map simulated shapes to their real-world counterparts. As a result, the
sim-to-real learning can be conducted by input from either a 3D scanner as
point clouds (without correspondences) or a motion capture system as marker
points (tolerating missed markers). The resultant sim-to-real transfer can be
seamlessly integrated into a neural network-based computational pipeline for
inverse kinematics and shape control. We demonstrate the versatility and
adaptability of our method on both vision devices and across four pneumatically
actuated soft robots: a deformable membrane, a robotic mannequin, and two soft
manipulators.","Yingjun Tian, Guoxin Fang, Renbo Su, Aoran Lyu, Neelotpal Dutta, Simeon Gill, Andrew Weightman, Charlie C. L. Wang",2025-08-25 17:54:24+00:00,http://arxiv.org/abs/2509.00060v1,http://arxiv.org/pdf/2509.00060v1,cs.RO
Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework,"Traversability estimation is critical for enabling robots to navigate across
diverse terrains and environments. While recent self-supervised learning
methods achieve promising results, they often fail to capture the
characteristics of non-traversable regions. Moreover, most prior works
concentrate on a single modality, overlooking the complementary strengths
offered by integrating heterogeneous sensory modalities for more robust
traversability estimation. To address these limitations, we propose a
multimodal self-supervised framework for traversability labeling and
estimation. First, our annotation pipeline integrates footprint, LiDAR, and
camera data as prompts for a vision foundation model, generating traversability
labels that account for both semantic and geometric cues. Then, leveraging
these labels, we train a dual-stream network that jointly learns from different
modalities in a decoupled manner, enhancing its capacity to recognize diverse
traversability patterns. In addition, we incorporate sparse LiDAR-based
supervision to mitigate the noise introduced by pseudo labels. Finally,
extensive experiments conducted across urban, off-road, and campus environments
demonstrate the effectiveness of our approach. The proposed automatic labeling
method consistently achieves around 88% IoU across diverse datasets. Compared
to existing self-supervised state-of-the-art methods, our multimodal
traversability estimation network yields consistently higher IoU, improving by
1.6-3.5% on all evaluated datasets.","Zipeng Fang, Yanbo Wang, Lei Zhao, Weidong Chen",2025-08-25 17:40:16+00:00,http://arxiv.org/abs/2508.18249v1,http://arxiv.org/pdf/2508.18249v1,"cs.RO, cs.CV"
DANCeRS: A Distributed Algorithm for Negotiating Consensus in Robot Swarms with Gaussian Belief Propagation,"Robot swarms require cohesive collective behaviour to address diverse
challenges, including shape formation and decision-making. Existing approaches
often treat consensus in discrete and continuous decision spaces as distinct
problems. We present DANCeRS, a unified, distributed algorithm leveraging
Gaussian Belief Propagation (GBP) to achieve consensus in both domains. By
representing a swarm as a factor graph our method ensures scalability and
robustness in dynamic environments, relying on purely peer-to-peer message
passing. We demonstrate the effectiveness of our general framework through two
applications where agents in a swarm must achieve consensus on global behaviour
whilst relying on local communication. In the first, robots must perform path
planning and collision avoidance to create shape formations. In the second, we
show how the same framework can be used by a group of robots to form a
consensus over a set of discrete decisions. Experimental results highlight our
method's scalability and efficiency compared to recent approaches to these
problems making it a promising solution for multi-robot systems requiring
distributed consensus. We encourage the reader to see the supplementary video
demo.","Aalok Patwardhan, Andrew J. Davison",2025-08-25 15:58:19+00:00,http://arxiv.org/abs/2508.18153v1,http://arxiv.org/pdf/2508.18153v1,cs.RO
Analysis of Harpy's Constrained Trotting and Jumping Maneuver,"This study presents an analysis of experimental data from Harpy, a
thruster-assisted bipedal robot developed at Northeastern University. The study
examines data sets from trotting and jumping experiments to understand the
fundamental principles governing hybrid leg-thruster locomotion. Through data
analysis across multiple locomotion modes, this research reveals that Harpy
achieves stable locomotion with bounded trajectories and consistent foot
placement through strategic leg-thruster synergy. The results demonstrate
controlled joint behavior with low torques and symmetric tracking, accurate
foot placement within kinematic constraints despite phase-transition
perturbations, and underactuated degree-of-freedom stability without
divergence. Energy level analysis reveals that legs provide primary propulsion,
while the thrusters enable additional aerial phase control. The analysis
identifies critical body-leg coupling dynamics during aerial phases that
require phase-specific control strategies. Consistent repeatability and
symmetry across experiments validate the robustness of the hybrid actuation
approach.",Prathima Ananda Kumar,2025-08-25 15:44:01+00:00,http://arxiv.org/abs/2508.18139v1,http://arxiv.org/pdf/2508.18139v1,cs.RO
BirdRecorder's AI on Sky: Safeguarding birds of prey by detection and classification of tiny objects around wind turbines,"The urgent need for renewable energy expansion, particularly wind power, is
hindered by conflicts with wildlife conservation. To address this, we developed
BirdRecorder, an advanced AI-based anti-collision system to protect endangered
birds, especially the red kite (Milvus milvus). Integrating robotics,
telemetry, and high-performance AI algorithms, BirdRecorder aims to detect,
track, and classify avian species within a range of 800 m to minimize
bird-turbine collisions.
  BirdRecorder integrates advanced AI methods with optimized hardware and
software architectures to enable real-time image processing. Leveraging Single
Shot Detector (SSD) for detection, combined with specialized hardware
acceleration and tracking algorithms, our system achieves high detection
precision while maintaining the speed necessary for real-time decision-making.
By combining these components, BirdRecorder outperforms existing approaches in
both accuracy and efficiency.
  In this paper, we summarize results on field tests and performance of the
BirdRecorder system. By bridging the gap between renewable energy expansion and
wildlife conservation, BirdRecorder contributes to a more sustainable
coexistence of technology and nature.","Nico Klar, Nizam Gifary, Felix P. G. Ziegler, Frank Sehnke, Anton Kaifel, Eric Price, Aamir Ahmad",2025-08-25 15:41:36+00:00,http://arxiv.org/abs/2508.18136v1,http://arxiv.org/pdf/2508.18136v1,"cs.CV, cs.LG, cs.RO, cs.SY, eess.SY"
The Effects of Communication Delay on Human Performance and Neurocognitive Responses in Mobile Robot Teleoperation,"Communication delays in mobile robot teleoperation adversely affect
human-machine collaboration. Understanding delay effects on human operational
performance and neurocognition is essential for resolving this issue. However,
no previous research has explored this. To fill this gap, we conduct a
human-in-the-loop experiment involving 10 participants, integrating
electroencephalography (EEG) and robot behavior data under varying delays
(0-500 ms in 100 ms increments) to systematically investigate these effects.
Behavior analysis reveals significant performance degradation at 200-300 ms
delays, affecting both task efficiency and accuracy. EEG analysis discovers
features with significant delay dependence: frontal $\theta/\beta$-band and
parietal $\alpha$-band power. We also identify a threshold window (100-200 ms)
for early perception of delay in humans, during which these EEG features first
exhibit significant differences. When delay exceeds 400 ms, all features
plateau, indicating saturation of cognitive resource allocation at
physiological limits. These findings provide the first evidence of perceptual
and cognitive delay thresholds during teleoperation tasks in humans, offering
critical neurocognitive insights for the design of delay compensation
strategies.","Zhaokun Chen, Wenshuo Wang, Wenzhuo Liu, Yichen Liu, Junqiang Xi",2025-08-25 14:39:00+00:00,http://arxiv.org/abs/2508.18074v1,http://arxiv.org/pdf/2508.18074v1,cs.RO
Arnold: a generalist muscle transformer policy,"Controlling high-dimensional and nonlinear musculoskeletal models of the
human body is a foundational scientific challenge. Recent machine learning
breakthroughs have heralded policies that master individual skills like
reaching, object manipulation and locomotion in musculoskeletal systems with
many degrees of freedom. However, these agents are merely ""specialists"",
achieving high performance for a single skill. In this work, we develop Arnold,
a generalist policy that masters multiple tasks and embodiments. Arnold
combines behavior cloning and fine-tuning with PPO to achieve expert or
super-expert performance in 14 challenging control tasks from dexterous object
manipulation to locomotion. A key innovation is Arnold's sensorimotor
vocabulary, a compositional representation of the semantics of heterogeneous
sensory modalities, objectives, and actuators. Arnold leverages this vocabulary
via a transformer architecture to deal with the variable observation and action
spaces of each task. This framework supports efficient multi-task,
multi-embodiment learning and facilitates rapid adaptation to novel tasks.
Finally, we analyze Arnold to provide insights into biological motor control,
corroborating recent findings on the limited transferability of muscle
synergies across tasks.","Alberto Silvio Chiappa, Boshi An, Merkourios Simos, Chengkun Li, Alexander Mathis",2025-08-25 14:22:40+00:00,http://arxiv.org/abs/2508.18066v1,http://arxiv.org/pdf/2508.18066v1,"cs.RO, cs.AI, cs.LG, q-bio.QM"
Modeling and Control Framework for Autonomous Space Manipulator Handover Operations,"Autonomous space robotics is poised to play a vital role in future space
missions, particularly for In-space Servicing, Assembly, and Manufacturing
(ISAM). A key capability in such missions is the Robot-to-Robot (R2R) handover
of mission-critical objects. This work presents a dynamic model of a dual-arm
space manipulator system and compares various tracking control laws. The key
contributions of this work are the development of a cooperative manipulator
dynamic model and the comparative analysis of control laws to support
autonomous R2R handovers in ISAM scenarios.","Diego Quevedo, Sarah Hudson, Donghoon Kim",2025-08-25 13:56:36+00:00,http://arxiv.org/abs/2508.18039v1,http://arxiv.org/pdf/2508.18039v1,"cs.RO, cs.SY, eess.SY"
No Need to Look! Locating and Grasping Objects by a Robot Arm Covered with Sensitive Skin,"Locating and grasping of objects by robots is typically performed using
visual sensors. Haptic feedback from contacts with the environment is only
secondary if present at all. In this work, we explored an extreme case of
searching for and grasping objects in complete absence of visual input, relying
on haptic feedback only. The main novelty lies in the use of contacts over the
complete surface of a robot manipulator covered with sensitive skin. The search
is divided into two phases: (1) coarse workspace exploration with the complete
robot surface, followed by (2) precise localization using the end-effector
equipped with a force/torque sensor. We systematically evaluated this method in
simulation and on the real robot, demonstrating that diverse objects can be
located, grasped, and put in a basket. The overall success rate on the real
robot for one object was 85.7\% with failures mainly while grasping specific
objects. The method using whole-body contacts is six times faster compared to a
baseline that uses haptic feedback only on the end-effector. We also show
locating and grasping multiple objects on the table. This method is not
restricted to our specific setup and can be deployed on any platform with the
ability of sensing contacts over the entire body surface. This work holds
promise for diverse applications in areas with challenging visual perception
(due to lighting, dust, smoke, occlusion) such as in agriculture when fruits or
vegetables need to be located inside foliage and picked.","Karel Bartunek, Lukas Rustler, Matej Hoffmann",2025-08-25 12:55:38+00:00,http://arxiv.org/abs/2508.17986v1,http://arxiv.org/pdf/2508.17986v1,cs.RO
Integration of Computer Vision with Adaptive Control for Autonomous Driving Using ADORE,"Ensuring safety in autonomous driving requires a seamless integration of
perception and decision making under uncertain conditions. Although computer
vision (CV) models such as YOLO achieve high accuracy in detecting traffic
signs and obstacles, their performance degrades in drift scenarios caused by
weather variations or unseen objects. This work presents a simulated autonomous
driving system that combines a context aware CV model with adaptive control
using the ADORE framework. The CARLA simulator was integrated with ADORE via
the ROS bridge, allowing real-time communication between perception, decision,
and control modules. A simulated test case was designed in both clear and drift
weather conditions to demonstrate the robust detection performance of the
perception model while ADORE successfully adapted vehicle behavior to speed
limits and obstacles with low response latency. The findings highlight the
potential of coupling deep learning-based perception with rule-based adaptive
decision making to improve automotive safety critical system.","Abu Shad Ahammed, Md Shahi Amran Hossain, Sayeri Mukherjee, Roman Obermaisser, Md. Ziaur Rahman",2025-08-25 12:55:30+00:00,http://arxiv.org/abs/2508.17985v2,http://arxiv.org/pdf/2508.17985v2,cs.RO
Neural Algorithmic Reasoners informed Large Language Model for Multi-Agent Path Finding,"The development and application of large language models (LLM) have
demonstrated that foundational models can be utilized to solve a wide array of
tasks. However, their performance in multi-agent path finding (MAPF) tasks has
been less than satisfactory, with only a few studies exploring this area. MAPF
is a complex problem requiring both planning and multi-agent coordination. To
improve the performance of LLM in MAPF tasks, we propose a novel framework,
LLM-NAR, which leverages neural algorithmic reasoners (NAR) to inform LLM for
MAPF. LLM-NAR consists of three key components: an LLM for MAPF, a pre-trained
graph neural network-based NAR, and a cross-attention mechanism. This is the
first work to propose using a neural algorithmic reasoner to integrate GNNs
with the map information for MAPF, thereby guiding LLM to achieve superior
performance. LLM-NAR can be easily adapted to various LLM models. Both
simulation and real-world experiments demonstrate that our method significantly
outperforms existing LLM-based approaches in solving MAPF problems.","Pu Feng, Size Wang, Yuhong Cao, Junkang Liang, Rongye Shi, Wenjun Wu",2025-08-25 12:38:08+00:00,http://arxiv.org/abs/2508.17971v1,http://arxiv.org/pdf/2508.17971v1,"cs.AI, cs.RO"
A holistic perception system of internal and external monitoring for ground autonomous vehicles: AutoTRUST paradigm,"This paper introduces a holistic perception system for internal and external
monitoring of autonomous vehicles, with the aim of demonstrating a novel
AI-leveraged self-adaptive framework of advanced vehicle technologies and
solutions that optimize perception and experience on-board. Internal monitoring
system relies on a multi-camera setup designed for predicting and identifying
driver and occupant behavior through facial recognition, exploiting in addition
a large language model as virtual assistant. Moreover, the in-cabin monitoring
system includes AI-empowered smart sensors that measure air-quality and perform
thermal comfort analysis for efficient on and off-boarding. On the other hand,
external monitoring system perceives the surrounding environment of vehicle,
through a LiDAR-based cost-efficient semantic segmentation approach, that
performs highly accurate and efficient super-resolution on low-quality raw 3D
point clouds. The holistic perception framework is developed in the context of
EU's Horizon Europe programm AutoTRUST, and has been integrated and deployed on
a real electric vehicle provided by ALKE. Experimental validation and
evaluation at the integration site of Joint Research Centre at Ispra, Italy,
highlights increased performance and efficiency of the modular blocks of the
proposed perception architecture.","Alexandros Gkillas, Christos Anagnostopoulos, Nikos Piperigkos, Dimitris Tsiktsiris, Theofilos Christodoulou, Theofanis Siamatras, Dimitrios Triantafyllou, Christos Basdekis, Theoktisti Marinopoulou, Panagiotis Lepentsiotis, Elefterios Blitsis, Aggeliki Zacharaki, Nearchos Stylianidis, Leonidas Katelaris, Lamberto Salvan, Aris S. Lalos, Christos Laoudias, Antonios Lalas, Konstantinos Votis",2025-08-25 12:32:13+00:00,http://arxiv.org/abs/2508.17969v1,http://arxiv.org/pdf/2508.17969v1,"cs.RO, cs.CV"
Egocentric Instruction-oriented Affordance Prediction via Large Multimodal Model,"Affordance is crucial for intelligent robots in the context of object
manipulation. In this paper, we argue that affordance should be
task-/instruction-dependent, which is overlooked by many previous works. That
is, different instructions can lead to different manipulation regions and
directions even for the same object. According to this observation, we present
a new dataset comprising fifteen thousand object-instruction-affordance
triplets. All scenes in the dataset are from an egocentric viewpoint, designed
to approximate the perspective of a human-like robot. Furthermore, we
investigate how to enable large multimodal models (LMMs) to serve as affordance
predictors by implementing a ``search against verifiers'' pipeline. An LMM is
asked to progressively predict affordances, with the output at each step being
verified by itself during the iterative process, imitating a reasoning process.
Experiments show that our method not only unlocks new instruction-oriented
affordance prediction capabilities, but also achieves outstanding performance
broadly.","Bokai Ji, Jie Gu, Xiaokang Ma, Chu Tang, Jingmin Chen, Guangxia Li",2025-08-25 11:40:31+00:00,http://arxiv.org/abs/2508.17922v1,http://arxiv.org/pdf/2508.17922v1,"cs.RO, cs.CV"
Physical Embodiment Enables Information Processing Beyond Explicit Sensing in Active Matter,"Living microorganisms have evolved dedicated sensory machinery to detect
environmental perturbations, processing these signals through biochemical
networks to guide behavior. Replicating such capabilities in synthetic active
matter remains a fundamental challenge. Here, we demonstrate that synthetic
active particles can adapt to hidden hydrodynamic perturbations through
physical embodiment alone, without explicit sensing mechanisms. Using
reinforcement learning to control self-thermophoretic particles, we show that
they learn navigation strategies to counteract unobserved flow fields by
exploiting information encoded in their physical dynamics. Remarkably,
particles successfully navigate perturbations that are not included in their
state inputs, revealing that embodied dynamics can serve as an implicit sensing
mechanism. This discovery establishes physical embodiment as a computational
resource for information processing in active matter, with implications for
autonomous microrobotic systems and bio-inspired computation.","Diptabrata Paul, Nikola Milosevic, Nico Scherf, Frank Cichos",2025-08-25 11:39:36+00:00,http://arxiv.org/abs/2508.17921v1,http://arxiv.org/pdf/2508.17921v1,"cond-mat.soft, cs.RO"
CubeDN: Real-time Drone Detection in 3D Space from Dual mmWave Radar Cubes,"As drone use has become more widespread, there is a critical need to ensure
safety and security. A key element of this is robust and accurate drone
detection and localization. While cameras and other optical sensors like LiDAR
are commonly used for object detection, their performance degrades under
adverse lighting and environmental conditions. Therefore, this has generated
interest in finding more reliable alternatives, such as millimeter-wave
(mmWave) radar. Recent research on mmWave radar object detection has
predominantly focused on 2D detection of road users. Although these systems
demonstrate excellent performance for 2D problems, they lack the sensing
capability to measure elevation, which is essential for 3D drone detection. To
address this gap, we propose CubeDN, a single-stage end-to-end radar object
detection network specifically designed for flying drones. CubeDN overcomes
challenges such as poor elevation resolution by utilizing a dual radar
configuration and a novel deep learning pipeline. It simultaneously detects,
localizes, and classifies drones of two sizes, achieving decimeter-level
tracking accuracy at closer ranges with overall $95\%$ average precision (AP)
and $85\%$ average recall (AR). Furthermore, CubeDN completes data processing
and inference at 10Hz, making it highly suitable for practical applications.","Yuan Fang, Fangzhan Shi, Xijia Wei, Qingchao Chen, Kevin Chetty, Simon Julier",2025-08-25 09:32:15+00:00,http://arxiv.org/abs/2508.17831v1,http://arxiv.org/pdf/2508.17831v1,cs.RO
Effect of Performance Feedback Timing on Motor Learning for a Surgical Training Task,"Objective: Robot-assisted minimally invasive surgery (RMIS) has become the
gold standard for a variety of surgical procedures, but the optimal method of
training surgeons for RMIS is unknown. We hypothesized that real-time, rather
than post-task, error feedback would better increase learning speed and reduce
errors. Methods: Forty-two surgical novices learned a virtual version of the
ring-on-wire task, a canonical task in RMIS training. We investigated the
impact of feedback timing with multi-sensory (haptic and visual) cues in three
groups: (1) real-time error feedback, (2) trial replay with error feedback, and
(3) no error feedback. Results: Participant performance was evaluated based on
the accuracy of ring position and orientation during the task. Participants who
received real-time feedback outperformed other groups in ring orientation.
Additionally, participants who received feedback in replay outperformed
participants who did not receive any error feedback on ring orientation during
long, straight path sections. There were no significant differences between
groups for ring position overall, but participants who received real-time
feedback outperformed the other groups in positional accuracy on tightly curved
path sections. Conclusion: The addition of real-time haptic and visual error
feedback improves learning outcomes in a virtual surgical task over error
feedback in replay or no error feedback at all. Significance: This work
demonstrates that multi-sensory error feedback delivered in real time leads to
better training outcomes as compared to the same feedback delivered after task
completion. This novel method of training may enable surgical trainees to
develop skills with greater speed and accuracy.","Mary Kate Gale, Kailana Baker-Matsuoka, Ilana Nisky, Allison Okamura",2025-08-25 09:30:17+00:00,http://arxiv.org/abs/2508.17830v1,http://arxiv.org/pdf/2508.17830v1,cs.RO
Adaptive Output Steps: FlexiSteps Network for Dynamic Trajectory Prediction,"Accurate trajectory prediction is vital for autonomous driving, robotics, and
intelligent decision-making systems, yet traditional models typically rely on
fixed-length output predictions, limiting their adaptability to dynamic
real-world scenarios. In this paper, we introduce the FlexiSteps Network (FSN),
a novel framework that dynamically adjusts prediction output time steps based
on varying contextual conditions. Inspired by recent advancements addressing
observation length discrepancies and dynamic feature extraction, FSN
incorporates an pre-trained Adaptive Prediction Module (APM) to evaluate and
adjust the output steps dynamically, ensuring optimal prediction accuracy and
efficiency. To guarantee the plug-and-play of our FSN, we also design a Dynamic
Decoder(DD). Additionally, to balance the prediction time steps and prediction
accuracy, we design a scoring mechanism, which not only introduces the
Fr\'echet distance to evaluate the geometric similarity between the predicted
trajectories and the ground truth trajectories but the length of predicted
steps is also considered. Extensive experiments conducted on benchmark datasets
including Argoverse and INTERACTION demonstrate the effectiveness and
flexibility of our proposed FSN framework.","Yunxiang Liu, Hongkuo Niu, Jianlin Zhu",2025-08-25 08:43:08+00:00,http://arxiv.org/abs/2508.17797v1,http://arxiv.org/pdf/2508.17797v1,"cs.RO, cs.AI"
Talking to Robots: A Practical Examination of Speech Foundation Models for HRI Applications,"Automatic Speech Recognition (ASR) systems in real-world settings need to
handle imperfect audio, often degraded by hardware limitations or environmental
noise, while accommodating diverse user groups. In human-robot interaction
(HRI), these challenges intersect to create a uniquely challenging recognition
environment. We evaluate four state-of-the-art ASR systems on eight publicly
available datasets that capture six dimensions of difficulty: domain-specific,
accented, noisy, age-variant, impaired, and spontaneous speech. Our analysis
demonstrates significant variations in performance, hallucination tendencies,
and inherent biases, despite similar scores on standard benchmarks. These
limitations have serious implications for HRI, where recognition errors can
interfere with task performance, user trust, and safety.","Theresa Pekarek Rosin, Julia Gachot, Henri-Leon Kordt, Matthias Kerzel, Stefan Wermter",2025-08-25 07:45:20+00:00,http://arxiv.org/abs/2508.17753v1,http://arxiv.org/pdf/2508.17753v1,"cs.RO, cs.AI, cs.CL, cs.HC"
U2UData-2: A Scalable Swarm UAVs Autonomous Flight Dataset for Long-horizon Tasks,"Swarm UAV autonomous flight for Long-Horizon (LH) tasks is crucial for
advancing the low-altitude economy. However, existing methods focus only on
specific basic tasks due to dataset limitations, failing in real-world
deployment for LH tasks. LH tasks are not mere concatenations of basic tasks,
requiring handling long-term dependencies, maintaining persistent states, and
adapting to dynamic goal shifts. This paper presents U2UData-2, the first
large-scale swarm UAV autonomous flight dataset for LH tasks and the first
scalable swarm UAV data online collection and algorithm closed-loop
verification platform. The dataset is captured by 15 UAVs in autonomous
collaborative flights for LH tasks, comprising 12 scenes, 720 traces, 120
hours, 600 seconds per trajectory, 4.32M LiDAR frames, and 12.96M RGB frames.
This dataset also includes brightness, temperature, humidity, smoke, and
airflow values covering all flight routes. The platform supports the
customization of simulators, UAVs, sensors, flight algorithms, formation modes,
and LH tasks. Through a visual control window, this platform allows users to
collect customized datasets through one-click deployment online and to verify
algorithms by closed-loop simulation. U2UData-2 also introduces an LH task for
wildlife conservation and provides comprehensive benchmarks with 9 SOTA models.
U2UData-2 can be found at https://fengtt42.github.io/U2UData-2/.","Tongtong Feng, Xin Wang, Feilin Han, Leping Zhang, Wenwu Zhu",2025-08-25 07:39:36+00:00,http://arxiv.org/abs/2509.00055v1,http://arxiv.org/pdf/2509.00055v1,"cs.RO, cs.AI, cs.MA, cs.MM"
Robotic Fire Risk Detection based on Dynamic Knowledge Graph Reasoning: An LLM-Driven Approach with Graph Chain-of-Thought,"Fire is a highly destructive disaster, but effective prevention can
significantly reduce its likelihood of occurrence. When it happens, deploying
emergency robots in fire-risk scenarios can help minimize the danger to human
responders. However, current research on pre-disaster warnings and
disaster-time rescue still faces significant challenges due to incomplete
perception, inadequate fire situational awareness, and delayed response. To
enhance intelligent perception and response planning for robots in fire
scenarios, we first construct a knowledge graph (KG) by leveraging large
language models (LLMs) to integrate fire domain knowledge derived from fire
prevention guidelines and fire rescue task information from robotic emergency
response documents. We then propose a new framework called Insights-on-Graph
(IOG), which integrates the structured fire information of KG and Large
Multimodal Models (LMMs). The framework generates perception-driven risk graphs
from real-time scene imagery to enable early fire risk detection and provide
interpretable emergency responses for task module and robot component
configuration based on the evolving risk situation. Extensive simulations and
real-world experiments show that IOG has good applicability and practical
application value in fire risk detection and rescue decision-making.","Haimei Pan, Jiyun Zhang, Qinxi Wei, Xiongnan Jin, Chen Xinkai, Jie Cheng",2025-08-25 07:10:16+00:00,http://arxiv.org/abs/2509.00054v2,http://arxiv.org/pdf/2509.00054v2,"cs.RO, cs.AI"
MEVITA: Open-Source Bipedal Robot Assembled from E-Commerce Components via Sheet Metal Welding,"Various bipedal robots have been developed to date, and in recent years,
there has been a growing trend toward releasing these robots as open-source
platforms. This shift is fostering an environment in which anyone can freely
develop bipedal robots and share their knowledge, rather than relying solely on
commercial products. However, most existing open-source bipedal robots are
designed to be fabricated using 3D printers, which limits their scalability in
size and often results in fragile structures. On the other hand, some
metal-based bipedal robots have been developed, but they typically involve a
large number of components, making assembly difficult, and in some cases, the
parts themselves are not readily available through e-commerce platforms. To
address these issues, we developed MEVITA, an open-source bipedal robot that
can be built entirely from components available via e-commerce. Aiming for the
minimal viable configuration for a bipedal robot, we utilized sheet metal
welding to integrate complex geometries into single parts, thereby
significantly reducing the number of components and enabling easy assembly for
anyone. Through reinforcement learning in simulation and Sim-to-Real transfer,
we demonstrated robust walking behaviors across various environments,
confirming the effectiveness of our approach. All hardware, software, and
training environments can be obtained from https://github.com/haraduka/mevita .","Kento Kawaharazuka, Shogo Sawaguchi, Ayumu Iwata, Keita Yoneda, Temma Suzuki, Kei Okada",2025-08-25 05:41:53+00:00,http://arxiv.org/abs/2508.17684v1,http://arxiv.org/pdf/2508.17684v1,cs.RO
SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and Manipulation,"Event cameras offer microsecond latency, high dynamic range, and low power
consumption, making them ideal for real-time robotic perception under
challenging conditions such as motion blur, occlusion, and illumination
changes. However, despite their advantages, synthetic event-based vision
remains largely unexplored in mainstream robotics simulators. This lack of
simulation setup hinders the evaluation of event-driven approaches for robotic
manipulation and navigation tasks. This work presents an open-source,
user-friendly v2e robotics operating system (ROS) package for Gazebo simulation
that enables seamless event stream generation from RGB camera feeds. The
package is used to investigate event-based robotic policies (ERP) for real-time
navigation and manipulation. Two representative scenarios are evaluated: (1)
object following with a mobile robot and (2) object detection and grasping with
a robotic manipulator. Transformer-based ERPs are trained by behavior cloning
and compared to RGB-based counterparts under various operating conditions.
Experimental results show that event-guided policies consistently deliver
competitive advantages. The results highlight the potential of event-driven
perception to improve real-time robotic navigation and manipulation, providing
a foundation for broader integration of event cameras into robotic policy
learning. The GitHub repo for the dataset and code:
https://eventbasedvision.github.io/SEBVS/","Krishna Vinod, Prithvi Jai Ramesh, Pavan Kumar B N, Bharatesh Chakravarthi",2025-08-25 04:14:04+00:00,http://arxiv.org/abs/2508.17643v1,http://arxiv.org/pdf/2508.17643v1,"cs.RO, cs.CV"
GWM: Towards Scalable Gaussian World Models for Robotic Manipulation,"Training robot policies within a learned world model is trending due to the
inefficiency of real-world interactions. The established image-based world
models and policies have shown prior success, but lack robust geometric
information that requires consistent spatial and physical understanding of the
three-dimensional world, even pre-trained on internet-scale video sources. To
this end, we propose a novel branch of world model named Gaussian World Model
(GWM) for robotic manipulation, which reconstructs the future state by
inferring the propagation of Gaussian primitives under the effect of robot
actions. At its core is a latent Diffusion Transformer (DiT) combined with a 3D
variational autoencoder, enabling fine-grained scene-level future state
reconstruction with Gaussian Splatting. GWM can not only enhance the visual
representation for imitation learning agent by self-supervised future
prediction training, but can serve as a neural simulator that supports
model-based reinforcement learning. Both simulated and real-world experiments
depict that GWM can precisely predict future scenes conditioned on diverse
robot actions, and can be further utilized to train policies that outperform
the state-of-the-art by impressive margins, showcasing the initial data scaling
potential of 3D world model.","Guanxing Lu, Baoxiong Jia, Puhao Li, Yixin Chen, Ziwei Wang, Yansong Tang, Siyuan Huang",2025-08-25 02:01:09+00:00,http://arxiv.org/abs/2508.17600v1,http://arxiv.org/pdf/2508.17600v1,"cs.RO, cs.AI, cs.CV, cs.LG"
LodeStar: Long-horizon Dexterity via Synthetic Data Augmentation from Human Demonstrations,"Developing robotic systems capable of robustly executing long-horizon
manipulation tasks with human-level dexterity is challenging, as such tasks
require both physical dexterity and seamless sequencing of manipulation skills
while robustly handling environment variations. While imitation learning offers
a promising approach, acquiring comprehensive datasets is resource-intensive.
In this work, we propose a learning framework and system LodeStar that
automatically decomposes task demonstrations into semantically meaningful
skills using off-the-shelf foundation models, and generates diverse synthetic
demonstration datasets from a few human demos through reinforcement learning.
These sim-augmented datasets enable robust skill training, with a Skill Routing
Transformer (SRT) policy effectively chaining the learned skills together to
execute complex long-horizon manipulation tasks. Experimental evaluations on
three challenging real-world long-horizon dexterous manipulation tasks
demonstrate that our approach significantly improves task performance and
robustness compared to previous baselines. Videos are available at
lodestar-robot.github.io.","Weikang Wan, Jiawei Fu, Xiaodi Yuan, Yifeng Zhu, Hao Su",2025-08-24 22:57:16+00:00,http://arxiv.org/abs/2508.17547v1,http://arxiv.org/pdf/2508.17547v1,"cs.RO, cs.AI, cs.LG"
Variational Shape Inference for Grasp Diffusion on SE(3),"Grasp synthesis is a fundamental task in robotic manipulation which usually
has multiple feasible solutions. Multimodal grasp synthesis seeks to generate
diverse sets of stable grasps conditioned on object geometry, making the robust
learning of geometric features crucial for success. To address this challenge,
we propose a framework for learning multimodal grasp distributions that
leverages variational shape inference to enhance robustness against shape noise
and measurement sparsity. Our approach first trains a variational autoencoder
for shape inference using implicit neural representations, and then uses these
learned geometric features to guide a diffusion model for grasp synthesis on
the SE(3) manifold. Additionally, we introduce a test-time grasp optimization
technique that can be integrated as a plugin to further enhance grasping
performance. Experimental results demonstrate that our shape inference for
grasp synthesis formulation outperforms state-of-the-art multimodal grasp
synthesis methods on the ACRONYM dataset by 6.3%, while demonstrating
robustness to deterioration in point cloud density compared to other
approaches. Furthermore, our trained model achieves zero-shot transfer to
real-world manipulation of household objects, generating 34% more successful
grasps than baselines despite measurement noise and point cloud calibration
errors.","S. Talha Bukhari, Kaivalya Agrawal, Zachary Kingston, Aniket Bera",2025-08-24 18:14:11+00:00,http://arxiv.org/abs/2508.17482v1,http://arxiv.org/pdf/2508.17482v1,cs.RO
SoK: Cybersecurity Assessment of Humanoid Ecosystem,"Humanoids are progressing toward practical deployment across healthcare,
industrial, defense, and service sectors. While typically considered
cyber-physical systems (CPSs), their dependence on traditional networked
software stacks (e.g., Linux operating systems), robot operating system (ROS)
middleware, and over-the-air update channels, creates a distinct security
profile that exposes them to vulnerabilities conventional CPS models do not
fully address. Prior studies have mainly examined specific threats, such as
LiDAR spoofing or adversarial machine learning (AML). This narrow focus
overlooks how an attack targeting one component can cascade harm throughout the
robot's interconnected systems. We address this gap through a systematization
of knowledge (SoK) that takes a comprehensive approach, consolidating
fragmented research from robotics, CPS, and network security domains. We
introduce a seven-layer security model for humanoid robots, organizing 39 known
attacks and 35 defenses across the humanoid ecosystem-from hardware to
human-robot interaction. Building on this security model, we develop a
quantitative 39x35 attack-defense matrix with risk-weighted scoring, validated
through Monte Carlo analysis. We demonstrate our method by evaluating three
real-world robots: Pepper, G1 EDU, and Digit. The scoring analysis revealed
varying security maturity levels, with scores ranging from 39.9% to 79.5%
across the platforms. This work introduces a structured, evidence-based
assessment method that enables systematic security evaluation, supports
cross-platform benchmarking, and guides prioritization of security investments
in humanoid robotics.","Priyanka Prakash Surve, Asaf Shabtai, Yuval Elovici",2025-08-24 18:13:33+00:00,http://arxiv.org/abs/2508.17481v2,http://arxiv.org/pdf/2508.17481v2,"cs.CR, cs.RO"
Harnessing ADAS for Pedestrian Safety: A Data-Driven Exploration of Fatality Reduction,"Pedestrian fatalities continue to rise in the United States, driven by
factors such as human distraction, increased vehicle size, and complex traffic
environments. Advanced Driver Assistance Systems (ADAS) offer a promising
avenue for improving pedestrian safety by enhancing driver awareness and
vehicle responsiveness. This study conducts a comprehensive data-driven
analysis utilizing the Fatality Analysis Reporting System (FARS) to quantify
the effectiveness of specific ADAS features like Pedestrian Automatic Emergency
Braking (PAEB), Forward Collision Warning (FCW), and Lane Departure Warning
(LDW), in lowering pedestrian fatalities. By linking vehicle specifications
with crash data, we assess how ADAS performance varies under different
environmental and behavioral conditions, such as lighting, weather, and
driver/pedestrian distraction. Results indicate that while ADAS can reduce
crash severity and prevent some fatalities, its effectiveness is diminished in
low-light and adverse weather. The findings highlight the need for enhanced
sensor technologies and improved driver education. This research informs
policymakers, transportation planners, and automotive manufacturers on
optimizing ADAS deployment to improve pedestrian safety and reduce
traffic-related deaths.","Methusela Sulle, Judith Mwakalonge, Gurcan Comert, Saidi Siuhi, Nana Kankam Gyimah",2025-08-24 17:58:55+00:00,http://arxiv.org/abs/2509.00048v1,http://arxiv.org/pdf/2509.00048v1,"cs.CY, cs.RO"
Morphological Cognition: Classifying MNIST Digits Through Morphological Computation Alone,"With the rise of modern deep learning, neural networks have become an
essential part of virtually every artificial intelligence system, making it
difficult even to imagine different models for intelligent behavior. In
contrast, nature provides us with many different mechanisms for intelligent
behavior, most of which we have yet to replicate. One of such underinvestigated
aspects of intelligence is embodiment and the role it plays in intelligent
behavior. In this work, we focus on how the simple and fixed behavior of
constituent parts of a simulated physical body can result in an emergent
behavior that can be classified as cognitive by an outside observer.
Specifically, we show how simulated voxels with fixed behaviors can be combined
to create a robot such that, when presented with an image of an MNIST digit
zero, it moves towards the left; and when it is presented with an image of an
MNIST digit one, it moves towards the right. Such robots possess what we refer
to as ``morphological cognition'' -- the ability to perform cognitive behavior
as a result of morphological processes. To the best of our knowledge, this is
the first demonstration of a high-level mental faculty such as image
classification performed by a robot without any neural circuitry. We hope that
this work serves as a proof-of-concept and fosters further research into
different models of intelligence.","Alican Mertan, Nick Cheney",2025-08-24 17:53:55+00:00,http://arxiv.org/abs/2508.17469v1,http://arxiv.org/pdf/2508.17469v1,"cs.RO, cs.NE"
A Synthetic Dataset for Manometry Recognition in Robotic Applications,"This work addresses the challenges of data scarcity and high acquisition
costs for training robust object detection models in complex industrial
environments, such as offshore oil platforms. The practical and economic
barriers to collecting real-world data in these hazardous settings often hamper
the development of autonomous inspection systems. To overcome this, in this
work we propose and validate a hybrid data synthesis pipeline that combines
procedural rendering with AI-driven video generation. Our methodology leverages
BlenderProc to create photorealistic images with precise annotations and
controlled domain randomization, and integrates NVIDIA's Cosmos-Predict2
world-foundation model to synthesize physically plausible video sequences with
temporal diversity, capturing rare viewpoints and adverse conditions. We
demonstrate that a YOLO-based detection network trained on a composite dataset,
blending real images with our synthetic data, achieves superior performance
compared to models trained exclusively on real-world data. Notably, a 1:1
mixture of real and synthetic data yielded the highest accuracy, surpassing the
real-only baseline. These findings highlight the viability of a synthetic-first
approach as an efficient, cost-effective, and safe alternative for developing
reliable perception systems in safety-critical and resource-constrained
industrial applications.","Pedro Antonio Rabelo Saraiva, Enzo Ferreira de Souza, Joao Manoel Herrera Pinheiro, Thiago H. Segreto, Ricardo V. Godoy, Marcelo Becker",2025-08-24 17:52:13+00:00,http://arxiv.org/abs/2508.17468v1,http://arxiv.org/pdf/2508.17468v1,"cs.CV, cs.AI, cs.LG, cs.RO"
Optimizing Grasping in Legged Robots: A Deep Learning Approach to Loco-Manipulation,"Quadruped robots have emerged as highly efficient and versatile platforms,
excelling in navigating complex and unstructured terrains where traditional
wheeled robots might fail. Equipping these robots with manipulator arms unlocks
the advanced capability of loco-manipulation to perform complex physical
interaction tasks in areas ranging from industrial automation to
search-and-rescue missions. However, achieving precise and adaptable grasping
in such dynamic scenarios remains a significant challenge, often hindered by
the need for extensive real-world calibration and pre-programmed grasp
configurations. This paper introduces a deep learning framework designed to
enhance the grasping capabilities of quadrupeds equipped with arms, focusing on
improved precision and adaptability. Our approach centers on a sim-to-real
methodology that minimizes reliance on physical data collection. We developed a
pipeline within the Genesis simulation environment to generate a synthetic
dataset of grasp attempts on common objects. By simulating thousands of
interactions from various perspectives, we created pixel-wise annotated
grasp-quality maps to serve as the ground truth for our model. This dataset was
used to train a custom CNN with a U-Net-like architecture that processes
multi-modal input from an onboard RGB and depth cameras, including RGB images,
depth maps, segmentation masks, and surface normal maps. The trained model
outputs a grasp-quality heatmap to identify the optimal grasp point. We
validated the complete framework on a four-legged robot. The system
successfully executed a full loco-manipulation task: autonomously navigating to
a target object, perceiving it with its sensors, predicting the optimal grasp
pose using our model, and performing a precise grasp. This work proves that
leveraging simulated training with advanced sensing offers a scalable and
effective solution for object handling.","Dilermando Almeida, Guilherme Lazzarini, Juliano Negri, Thiago H. Segreto, Ricardo V. Godoy, Marcelo Becker",2025-08-24 17:47:56+00:00,http://arxiv.org/abs/2508.17466v1,http://arxiv.org/pdf/2508.17466v1,"cs.RO, cs.AI, cs.CV, cs.LG, cs.SY, eess.SY"
Evolutionary Brain-Body Co-Optimization Consistently Fails to Select for Morphological Potential,"Brain-body co-optimization remains a challenging problem, despite increasing
interest from the community in recent years. To understand and overcome the
challenges, we propose exhaustively mapping a morphology-fitness landscape to
study it. To this end, we train controllers for each feasible morphology in a
design space of 1,305,840 distinct morphologies, constrained by a computational
budget. First, we show that this design space constitutes a good model for
studying the brain-body co-optimization problem, and our attempt to
exhaustively map it roughly captures the landscape. We then proceed to analyze
how evolutionary brain-body co-optimization algorithms work in this design
space. The complete knowledge of the morphology-fitness landscape facilitates a
better understanding of the results of evolutionary brain-body co-optimization
algorithms and how they unfold over evolutionary time in the morphology space.
This investigation shows that the experimented algorithms cannot consistently
find near-optimal solutions. The search, at times, gets stuck on morphologies
that are sometimes one mutation away from better morphologies, and the
algorithms cannot efficiently track the fitness gradient in the
morphology-fitness landscape. We provide evidence that experimented algorithms
regularly undervalue the fitness of individuals with newly mutated bodies and,
as a result, eliminate promising morphologies throughout evolution. Our work
provides the most concrete demonstration of the challenges of evolutionary
brain-body co-optimization. Our findings ground the trends in the literature
and provide valuable insights for future work.","Alican Mertan, Nick Cheney",2025-08-24 17:47:07+00:00,http://arxiv.org/abs/2508.17464v1,http://arxiv.org/pdf/2508.17464v1,"cs.RO, cs.NE"
"Robotic Manipulation via Imitation Learning: Taxonomy, Evolution, Benchmark, and Challenges","Robotic Manipulation (RM) is central to the advancement of autonomous robots,
enabling them to interact with and manipulate objects in real-world
environments. This survey focuses on RM methodologies that leverage imitation
learning, a powerful technique that allows robots to learn complex manipulation
skills by mimicking human demonstrations. We identify and analyze the most
influential studies in this domain, selected based on community impact and
intrinsic quality. For each paper, we provide a structured summary, covering
the research purpose, technical implementation, hierarchical classification,
input formats, key priors, strengths and limitations, and citation metrics.
Additionally, we trace the chronological development of imitation learning
techniques within RM policy (RMP), offering a timeline of key technological
advancements. Where available, we report benchmark results and perform
quantitative evaluations to compare existing methods. By synthesizing these
insights, this review provides a comprehensive resource for researchers and
practitioners, highlighting both the state of the art and the challenges that
lie ahead in the field of robotic manipulation through imitation learning.","Zezeng Li, Alexandre Chapin, Enda Xiang, Rui Yang, Bruno Machado, Na Lei, Emmanuel Dellandrea, Di Huang, Liming Chen",2025-08-24 17:01:15+00:00,http://arxiv.org/abs/2508.17449v2,http://arxiv.org/pdf/2508.17449v2,cs.RO
Robust Point Cloud Registration via Geometric Overlapping Guided Rotation Search,"Point cloud registration based on correspondences computes the rigid
transformation that maximizes the number of inliers constrained within the
noise threshold. Current state-of-the-art (SOTA) methods employing spatial
compatibility graphs or branch-and-bound (BnB) search mainly focus on
registration under high outlier ratios. However, graph-based methods require at
least quadratic space and time complexity for graph construction, while
multi-stage BnB search methods often suffer from inaccuracy due to local optima
between decomposed stages. This paper proposes a geometric maximum overlapping
registration framework via rotation-only BnB search. The rigid transformation
is decomposed using Chasles' theorem into a translation along rotation axis and
a 2D rigid transformation. The optimal rotation axis and angle are searched via
BnB, with residual parameters formulated as range maximum query (RMQ) problems.
Firstly, the top-k candidate rotation axes are searched within a hemisphere
parameterized by cube mapping, and the translation along each axis is estimated
through interval stabbing of the correspondences projected onto that axis.
Secondly, the 2D registration is relaxed to 1D rotation angle search with 2D
RMQ of geometric overlapping for axis-aligned rectangles, which is solved
deterministically in polynomial time using sweep line algorithm with segment
tree. Experimental results on 3DMatch, 3DLoMatch, and KITTI datasets
demonstrate superior accuracy and efficiency over SOTA methods, while the time
complexity is polynomial and the space complexity increases linearly with the
number of points, even in the worst case.","Zhao Zheng, Jingfan Fan, Long Shao, Hong Song, Danni Ai, Tianyu Fu, Deqiang Xiao, Yongtian Wang, Jian Yang",2025-08-24 16:01:31+00:00,http://arxiv.org/abs/2508.17427v1,http://arxiv.org/pdf/2508.17427v1,"cs.CV, cs.RO"
OVITA: Open-Vocabulary Interpretable Trajectory Adaptations,"Adapting trajectories to dynamic situations and user preferences is crucial
for robot operation in unstructured environments with non-expert users. Natural
language enables users to express these adjustments in an interactive manner.
We introduce OVITA, an interpretable, open-vocabulary, language-driven
framework designed for adapting robot trajectories in dynamic and novel
situations based on human instructions. OVITA leverages multiple pre-trained
Large Language Models (LLMs) to integrate user commands into trajectories
generated by motion planners or those learned through demonstrations. OVITA
employs code as an adaptation policy generated by an LLM, enabling users to
adjust individual waypoints, thus providing flexible control. Another LLM,
which acts as a code explainer, removes the need for expert users, enabling
intuitive interactions. The efficacy and significance of the proposed OVITA
framework is demonstrated through extensive simulations and real-world
environments with diverse tasks involving spatiotemporal variations on
heterogeneous robotic platforms such as a KUKA IIWA robot manipulator,
Clearpath Jackal ground robot, and CrazyFlie drone.","Anurag Maurya, Tashmoy Ghosh, Anh Nguyen, Ravi Prakash",2025-08-24 09:01:40+00:00,http://arxiv.org/abs/2508.17260v1,http://arxiv.org/pdf/2508.17260v1,cs.RO
SEER-VAR: Semantic Egocentric Environment Reasoner for Vehicle Augmented Reality,"We present SEER-VAR, a novel framework for egocentric vehicle-based augmented
reality (AR) that unifies semantic decomposition, Context-Aware SLAM Branches
(CASB), and LLM-driven recommendation. Unlike existing systems that assume
static or single-view settings, SEER-VAR dynamically separates cabin and road
scenes via depth-guided vision-language grounding. Two SLAM branches track
egocentric motion in each context, while a GPT-based module generates
context-aware overlays such as dashboard cues and hazard alerts. To support
evaluation, we introduce EgoSLAM-Drive, a real-world dataset featuring
synchronized egocentric views, 6DoF ground-truth poses, and AR annotations
across diverse driving scenarios. Experiments demonstrate that SEER-VAR
achieves robust spatial alignment and perceptually coherent AR rendering across
varied environments. As one of the first to explore LLM-based AR recommendation
in egocentric driving, we address the lack of comparable systems through
structured prompting and detailed user studies. Results show that SEER-VAR
enhances perceived scene understanding, overlay relevance, and driver ease,
providing an effective foundation for future research in this direction. Code
and dataset will be made open source.","Yuzhi Lai, Shenghai Yuan, Peizheng Li, Jun Lou, Andreas Zell",2025-08-24 08:45:15+00:00,http://arxiv.org/abs/2508.17255v1,http://arxiv.org/pdf/2508.17255v1,"cs.CV, cs.RO"
Collaborative-Online-Learning-Enabled Distributionally Robust Motion Control for Multi-Robot Systems,"This paper develops a novel COllaborative-Online-Learning (COOL)-enabled
motion control framework for multi-robot systems to avoid collision amid
randomly moving obstacles whose motion distributions are partially observable
through decentralized data streams. To address the notable challenge of data
acquisition due to occlusion, a COOL approach based on the Dirichlet process
mixture model is proposed to efficiently extract motion distribution
information by exchanging among robots selected learning structures. By
leveraging the fine-grained local-moment information learned through COOL, a
data-stream-driven ambiguity set for obstacle motion is constructed. We then
introduce a novel ambiguity set propagation method, which theoretically admits
the derivation of the ambiguity sets for obstacle positions over the entire
prediction horizon by utilizing obstacle current positions and the ambiguity
set for obstacle motion. Additionally, we develop a compression scheme with its
safety guarantee to automatically adjust the complexity and granularity of the
ambiguity set by aggregating basic ambiguity sets that are close in a measure
space, thereby striking an attractive trade-off between control performance and
computation time. Then the probabilistic collision-free trajectories are
generated through distributionally robust optimization problems. The
distributionally robust obstacle avoidance constraints based on the compressed
ambiguity set are equivalently reformulated by deriving separating hyperplanes
through tractable semi-definite programming. Finally, we establish the
probabilistic collision avoidance guarantee and the long-term tracking
performance guarantee for the proposed framework. The numerical simulations are
used to demonstrate the efficacy and superiority of the proposed approach
compared with state-of-the-art methods.","Chao Ning, Han Wang, Longyan Li, Yang Shi",2025-08-24 00:59:48+00:00,http://arxiv.org/abs/2508.17173v1,http://arxiv.org/pdf/2508.17173v1,"math.OC, cs.RO, cs.SY, eess.SY"
LaGarNet: Goal-Conditioned Recurrent State-Space Models for Pick-and-Place Garment Flattening,"We present a novel goal-conditioned recurrent state space (GC-RSSM) model
capable of learning latent dynamics of pick-and-place garment manipulation. Our
proposed method LaGarNet matches the state-of-the-art performance of mesh-based
methods, marking the first successful application of state-space models on
complex garments. LaGarNet trains on a coverage-alignment reward and a dataset
collected through a general procedure supported by a random policy and a
diffusion policy learned from few human demonstrations; it substantially
reduces the inductive biases introduced in the previous similar methods. We
demonstrate that a single-policy LaGarNet achieves flattening on four different
types of garments in both real-world and simulation settings.","Halid Abdulrahim Kadi, Kasim Terzić",2025-08-23 15:52:48+00:00,http://arxiv.org/abs/2508.17070v1,http://arxiv.org/pdf/2508.17070v1,cs.RO
DeltaFlow: An Efficient Multi-frame Scene Flow Estimation Method,"Previous dominant methods for scene flow estimation focus mainly on input
from two consecutive frames, neglecting valuable information in the temporal
domain. While recent trends shift towards multi-frame reasoning, they suffer
from rapidly escalating computational costs as the number of frames grows. To
leverage temporal information more efficiently, we propose DeltaFlow
($\Delta$Flow), a lightweight 3D framework that captures motion cues via a
$\Delta$ scheme, extracting temporal features with minimal computational cost,
regardless of the number of frames. Additionally, scene flow estimation faces
challenges such as imbalanced object class distributions and motion
inconsistency. To tackle these issues, we introduce a Category-Balanced Loss to
enhance learning across underrepresented classes and an Instance Consistency
Loss to enforce coherent object motion, improving flow accuracy. Extensive
evaluations on the Argoverse 2 and Waymo datasets show that $\Delta$Flow
achieves state-of-the-art performance with up to 22% lower error and $2\times$
faster inference compared to the next-best multi-frame supervised method, while
also demonstrating a strong cross-domain generalization ability. The code is
open-sourced at https://github.com/Kin-Zhang/DeltaFlow along with trained model
weights.","Qingwen Zhang, Xiaomeng Zhu, Yushan Zhang, Yixi Cai, Olov Andersson, Patric Jensfelt",2025-08-23 15:06:59+00:00,http://arxiv.org/abs/2508.17054v1,http://arxiv.org/pdf/2508.17054v1,"cs.CV, cs.RO"
M3DMap: Object-aware Multimodal 3D Mapping for Dynamic Environments,"3D mapping in dynamic environments poses a challenge for modern researchers
in robotics and autonomous transportation. There are no universal
representations for dynamic 3D scenes that incorporate multimodal data such as
images, point clouds, and text. This article takes a step toward solving this
problem. It proposes a taxonomy of methods for constructing multimodal 3D maps,
classifying contemporary approaches based on scene types and representations,
learning methods, and practical applications. Using this taxonomy, a brief
structured analysis of recent methods is provided. The article also describes
an original modular method called M3DMap, designed for object-aware
construction of multimodal 3D maps for both static and dynamic scenes. It
consists of several interconnected components: a neural multimodal object
segmentation and tracking module; an odometry estimation module, including
trainable algorithms; a module for 3D map construction and updating with
various implementations depending on the desired scene representation; and a
multimodal data retrieval module. The article highlights original
implementations of these modules and their advantages in solving various
practical tasks, from 3D object grounding to mobile manipulation. Additionally,
it presents theoretical propositions demonstrating the positive effect of using
multimodal data and modern foundational models in 3D mapping methods. Details
of the taxonomy and method implementation are available at
https://yuddim.github.io/M3DMap.",Dmitry Yudin,2025-08-23 14:45:48+00:00,http://arxiv.org/abs/2508.17044v1,http://arxiv.org/pdf/2508.17044v1,"cs.CV, cs.RO"
A Rapid Iterative Trajectory Planning Method for Automated Parking through Differential Flatness,"As autonomous driving continues to advance, automated parking is becoming
increasingly essential. However, significant challenges arise when implementing
path velocity decomposition (PVD) trajectory planning for automated parking.
The primary challenge is ensuring rapid and precise collision-free trajectory
planning, which is often in conflict. The secondary challenge involves
maintaining sufficient control feasibility of the planned trajectory,
particularly at gear shifting points (GSP). This paper proposes a PVD-based
rapid iterative trajectory planning (RITP) method to solve the above
challenges. The proposed method effectively balances the necessity for time
efficiency and precise collision avoidance through a novel collision avoidance
framework. Moreover, it enhances the overall control feasibility of the planned
trajectory by incorporating the vehicle kinematics model and including terminal
smoothing constraints (TSC) at GSP during path planning. Specifically, the
proposed method leverages differential flatness to ensure the planned path
adheres to the vehicle kinematic model. Additionally, it utilizes TSC to
maintain curvature continuity at GSP, thereby enhancing the control feasibility
of the overall trajectory. The simulation results demonstrate superior time
efficiency and tracking errors compared to model-integrated and other
iteration-based trajectory planning methods. In the real-world experiment, the
proposed method was implemented and validated on a ROS-based vehicle,
demonstrating the applicability of the RITP method for real vehicles.","Zhouheng Li, Lei Xie, Cheng Hu, Hongye Su",2025-08-23 14:36:48+00:00,http://arxiv.org/abs/2508.17038v1,http://arxiv.org/pdf/2508.17038v1,"cs.RO, cs.SY, eess.SY"
DualReg: Dual-Space Filtering and Reinforcement for Rigid Registration,"Rigid registration, aiming to estimate a rigid transformation to align source
and target data, play a crucial role in applications such as SLAM and 3D
reconstruction. However, noisy, partially overlapping data and the need for
real-time processing pose major challenges for rigid registration. Considering
that feature-based matching can handle large transformation differences but
suffers from limited accuracy, while local geometry-based matching can achieve
fine-grained local alignment but relies heavily on a good initial
transformation, we propose a novel dual-space paradigm to fully leverage the
strengths of both approaches. First, we introduce an efficient filtering
mechanism that incorporates a computationally lightweight single-point RANSAC
algorithm followed by a refinement module to eliminate unreliable feature-based
correspondences. Subsequently, we treat filtered correspondences as anchor
points, extract geometric proxies, and formulates an effective objective
function with a tailored solver to estimate the transformation. Experiments
verify our method's effectiveness, as shown by achieving up to a 32x CPU-time
speedup over MAC on KITTI with comparable accuracy.","Jiayi Li, Yuxin Yao, Qiuhang Lu, Juyong Zhang",2025-08-23 14:24:20+00:00,http://arxiv.org/abs/2508.17034v1,http://arxiv.org/pdf/2508.17034v1,cs.RO
Curve-based slicer for multi-axis DLP 3D printing,"This paper introduces a novel curve-based slicing method for generating
planar layers with dynamically varying orientations in digital light processing
(DLP) 3D printing. Our approach effectively addresses key challenges in DLP
printing, such as regions with large overhangs and staircase artifacts, while
preserving its intrinsic advantages of high resolution and fast printing
speeds. We formulate the slicing problem as an optimization task, in which
parametric curves are computed to define both the slicing layers and the model
partitioning through their tangent planes. These curves inherently define
motion trajectories for the build platform and can be optimized to meet
critical manufacturing objectives, including collision-free motion and
floating-free deposition. We validate our method through physical experiments
on a robotic multi-axis DLP printing setup, demonstrating that the optimized
curves can robustly guide smooth, high-quality fabrication of complex
geometries.","Chengkai Dai, Tao Liu, Dezhao Guo, Binzhi Sun, Guoxin Fang, Yeung Yam, Charlie C. L. Wang",2025-08-23 13:06:29+00:00,http://arxiv.org/abs/2509.00040v1,http://arxiv.org/pdf/2509.00040v1,"cs.GR, cs.RO"
Fiducial Marker Splatting for High-Fidelity Robotics Simulations,"High-fidelity 3D simulation is critical for training mobile robots, but its
traditional reliance on mesh-based representations often struggle in complex
environments, such as densely packed greenhouses featuring occlusions and
repetitive structures. Recent neural rendering methods, like Gaussian Splatting
(GS), achieve remarkable visual realism but lack flexibility to incorporate
fiducial markers, which are essential for robotic localization and control. We
propose a hybrid framework that combines the photorealism of GS with structured
marker representations. Our core contribution is a novel algorithm for
efficiently generating GS-based fiducial markers (e.g., AprilTags) within
cluttered scenes. Experiments show that our approach outperforms traditional
image-fitting techniques in both efficiency and pose-estimation accuracy. We
further demonstrate the framework's potential in a greenhouse simulation. This
agricultural setting serves as a challenging testbed, as its combination of
dense foliage, similar-looking elements, and occlusions pushes the limits of
perception, thereby highlighting the framework's value for real-world
applications.","Diram Tabaa, Gianni Di Caro",2025-08-23 12:53:51+00:00,http://arxiv.org/abs/2508.17012v1,http://arxiv.org/pdf/2508.17012v1,"cs.CV, cs.RO"
LLM-based Human-like Traffic Simulation for Self-driving Tests,"Ensuring realistic traffic dynamics is a prerequisite for simulation
platforms to evaluate the reliability of self-driving systems before deployment
in the real world. Because most road users are human drivers, reproducing their
diverse behaviors within simulators is vital. Existing solutions, however,
typically rely on either handcrafted heuristics or narrow data-driven models,
which capture only fragments of real driving behaviors and offer limited
driving style diversity and interpretability. To address this gap, we introduce
HDSim, an HD traffic generation framework that combines cognitive theory with
large language model (LLM) assistance to produce scalable and realistic traffic
scenarios within simulation platforms. The framework advances the state of the
art in two ways: (i) it introduces a hierarchical driver model that represents
diverse driving style traits, and (ii) it develops a Perception-Mediated
Behavior Influence strategy, where LLMs guide perception to indirectly shape
driver actions. Experiments reveal that embedding HDSim into simulation
improves detection of safety-critical failures in self-driving systems by up to
68% and yields realism-consistent accident interpretability.","Wendi Li, Hao Wu, Han Gao, Bing Mao, Fengyuan Xu, Sheng Zhong",2025-08-23 09:30:55+00:00,http://arxiv.org/abs/2508.16962v1,http://arxiv.org/pdf/2508.16962v1,"cs.RO, cs.AI"
Drive As You Like: Strategy-Level Motion Planning Based on A Multi-Head Diffusion Model,"Recent advances in motion planning for autonomous driving have led to models
capable of generating high-quality trajectories. However, most existing
planners tend to fix their policy after supervised training, leading to
consistent but rigid driving behaviors. This limits their ability to reflect
human preferences or adapt to dynamic, instruction-driven demands. In this
work, we propose a diffusion-based multi-head trajectory planner(M-diffusion
planner). During the early training stage, all output heads share weights to
learn to generate high-quality trajectories. Leveraging the probabilistic
nature of diffusion models, we then apply Group Relative Policy Optimization
(GRPO) to fine-tune the pre-trained model for diverse policy-specific
behaviors. At inference time, we incorporate a large language model (LLM) to
guide strategy selection, enabling dynamic, instruction-aware planning without
switching models. Closed-loop simulation demonstrates that our post-trained
planner retains strong planning capability while achieving state-of-the-art
(SOTA) performance on the nuPlan val14 benchmark. Open-loop results further
show that the generated trajectories exhibit clear diversity, effectively
satisfying multi-modal driving behavior requirements. The code and related
experiments will be released upon acceptance of the paper.","Fan Ding, Xuewen Luo, Hwa Hui Tew, Ruturaj Reddy, Xikun Wang, Junn Yong Loo",2025-08-23 08:33:11+00:00,http://arxiv.org/abs/2508.16947v1,http://arxiv.org/pdf/2508.16947v1,"cs.RO, cs.AI"
HumanoidVerse: A Versatile Humanoid for Vision-Language Guided Multi-Object Rearrangement,"We introduce HumanoidVerse, a novel framework for vision-language guided
humanoid control that enables a single physically simulated robot to perform
long-horizon, multi-object rearrangement tasks across diverse scenes. Unlike
prior methods that operate in fixed settings with single-object interactions,
our approach supports consecutive manipulation of multiple objects, guided only
by natural language instructions and egocentric camera RGB observations.
HumanoidVerse is trained via a multi-stage curriculum using a dual-teacher
distillation pipeline, enabling fluid transitions between sub-tasks without
requiring environment resets. To support this, we construct a large-scale
dataset comprising 350 multi-object tasks spanning four room layouts. Extensive
experiments in the Isaac Gym simulator demonstrate that our method
significantly outperforms prior state-of-the-art in both task success rate and
spatial precision, and generalizes well to unseen environments and
instructions. Our work represents a key step toward robust, general-purpose
humanoid agents capable of executing complex, sequential tasks under real-world
sensory constraints. The video visualization results can be found on the
project page: https://haozhuo-zhang.github.io/HumanoidVerse-project-page/.","Haozhuo Zhang, Jingkai Sun, Michele Caprio, Jian Tang, Shanghang Zhang, Qiang Zhang, Wei Pan",2025-08-23 08:23:14+00:00,http://arxiv.org/abs/2508.16943v1,http://arxiv.org/pdf/2508.16943v1,"cs.RO, cs.AI"
Relative Navigation and Dynamic Target Tracking for Autonomous Underwater Proximity Operations,"Estimating a target's 6-DoF motion in underwater proximity operations is
difficult because the chaser lacks target-side proprioception and the available
relative observations are sparse, noisy, and often partial (e.g., Ultra-Short
Baseline (USBL) positions). Without a motion prior, factor-graph maximum a
posteriori estimation is underconstrained: consecutive target states are weakly
linked and orientation can drift. We propose a generalized constant-twist
motion prior defined on the tangent space of Lie groups that enforces
temporally consistent trajectories across all degrees of freedom; in SE(3) it
couples translation and rotation in the body frame. We present a ternary factor
and derive its closed-form Jacobians based on standard Lie group operations,
enabling drop-in use for trajectories on arbitrary Lie groups. We evaluate two
deployment modes: (A) an SE(3)-only representation that regularizes orientation
even when only position is measured, and (B) a mode with boundary factors that
switches the target representation between SE(3) and 3D position while applying
the same generalized constant-twist prior across representation changes.
Validation on a real-world dynamic docking scenario dataset shows consistent
ego-target trajectory estimation through USBL-only and optical relative
measurement segments with an improved relative tracking accuracy compared to
the noisy measurements to the target. Because the construction relies on
standard Lie group primitives, it is portable across state manifolds and
sensing modalities.","David Baxter, Aldo Terán Espinoza, Antonio Terán Espinoza, Amy Loutfi, John Folkesson, Peter Sigray, Stephanie Lowry, Jakob Kuttenkeuler",2025-08-23 05:19:50+00:00,http://arxiv.org/abs/2508.16901v1,http://arxiv.org/pdf/2508.16901v1,"cs.RO, cs.SY, eess.SP, eess.SY, I.2.9; I.2.8; F.2.2"
A Workflow for Map Creation in Autonomous Vehicle Simulations,"The fast development of technology and artificial intelligence has
significantly advanced Autonomous Vehicle (AV) research, emphasizing the need
for extensive simulation testing. Accurate and adaptable maps are critical in
AV development, serving as the foundation for localization, path planning, and
scenario testing. However, creating simulation-ready maps is often difficult
and resource-intensive, especially with simulators like CARLA (CAR Learning to
Act). Many existing workflows require significant computational resources or
rely on specific simulators, limiting flexibility for developers. This paper
presents a custom workflow to streamline map creation for AV development,
demonstrated through the generation of a 3D map of a parking lot at Ontario
Tech University. Future work will focus on incorporating SLAM technologies,
optimizing the workflow for broader simulator compatibility, and exploring more
flexible handling of latitude and longitude values to enhance map generation
accuracy.","Zubair Islam, Ahmaad Ansari, George Daoud, Mohamed El-Darieby",2025-08-23 00:58:09+00:00,http://arxiv.org/abs/2508.16856v1,http://arxiv.org/pdf/2508.16856v1,"cs.RO, cs.AI, cs.GR"
Autonomous UAV Flight Navigation in Confined Spaces: A Reinforcement Learning Approach,"Inspecting confined industrial infrastructure, such as ventilation shafts, is
a hazardous and inefficient task for humans. Unmanned Aerial Vehicles (UAVs)
offer a promising alternative, but GPS-denied environments require robust
control policies to prevent collisions. Deep Reinforcement Learning (DRL) has
emerged as a powerful framework for developing such policies, and this paper
provides a comparative study of two leading DRL algorithms for this task: the
on-policy Proximal Policy Optimization (PPO) and the off-policy Soft
Actor-Critic (SAC). The training was conducted with procedurally generated duct
environments in Genesis simulation environment. A reward function was designed
to guide a drone through a series of waypoints while applying a significant
penalty for collisions. PPO learned a stable policy that completed all
evaluation episodes without collision, producing smooth trajectories. By
contrast, SAC consistently converged to a suboptimal behavior that traversed
only the initial segments before failure. These results suggest that, in
hazard-dense navigation, the training stability of on-policy methods can
outweigh the nominal sample efficiency of off-policy algorithms. More broadly,
the study provides evidence that procedurally generated, high-fidelity
simulations are effective testbeds for developing and benchmarking robust
navigation policies.","Marco S. Tayar, Lucas K. de Oliveira, Juliano D. Negri, Thiago H. Segreto, Ricardo V. Godoy, Marcelo Becker",2025-08-22 21:29:59+00:00,http://arxiv.org/abs/2508.16807v1,http://arxiv.org/pdf/2508.16807v1,"cs.RO, cs.AI, cs.LG, cs.SY, eess.SY"
A Dataset and Benchmark for Robotic Cloth Unfolding Grasp Selection: The ICRA 2024 Cloth Competition,"Robotic cloth manipulation suffers from a lack of standardized benchmarks and
shared datasets for evaluating and comparing different approaches. To address
this, we created a benchmark and organized the ICRA 2024 Cloth Competition, a
unique head-to-head evaluation focused on grasp pose selection for in-air
robotic cloth unfolding. Eleven diverse teams participated in the competition,
utilizing our publicly released dataset of real-world robotic cloth unfolding
attempts and a variety of methods to design their unfolding approaches.
Afterwards, we also expanded our dataset with 176 competition evaluation
trials, resulting in a dataset of 679 unfolding demonstrations across 34
garments. Analysis of the competition results revealed insights about the
trade-off between grasp success and coverage, the surprisingly strong
achievements of hand-engineered methods and a significant discrepancy between
competition performance and prior work, underscoring the importance of
independent, out-of-the-lab evaluation in robotic cloth manipulation. The
associated dataset is a valuable resource for developing and evaluating grasp
selection methods, particularly for learning-based approaches. We hope that our
benchmark, dataset and competition results can serve as a foundation for future
benchmarks and drive further progress in data-driven robotic cloth
manipulation. The dataset and benchmarking code are available at
https://airo.ugent.be/cloth_competition.","Victor-Louis De Gusseme, Thomas Lips, Remko Proesmans, Julius Hietala, Giwan Lee, Jiyoung Choi, Jeongil Choi, Geon Kim, Phayuth Yonrith, Domen Tabernik, Andrej Gams, Peter Nimac, Matej Urbas, Jon Muhovič, Danijel Skočaj, Matija Mavsar, Hyojeong Yu, Minseo Kwon, Young J. Kim, Yang Cong, Ronghan Chen, Yu Ren, Supeng Diao, Jiawei Weng, Jiayue Liu, Haoran Sun, Linhan Yang, Zeqing Zhang, Ning Guo, Lei Yang, Fang Wan, Chaoyang Song, Jia Pan, Yixiang Jin, Yong A, Jun Shi, Dingzhe Li, Yong Yang, Kakeru Yamasaki, Takumi Kajiwara, Yuki Nakadera, Krati Saxena, Tomohiro Shibata, Chongkun Xia, Kai Mo, Yanzhao Yu, Qihao Lin, Binqiang Ma, Uihun Sagong, JungHyun Choi, JeongHyun Park, Dongwoo Lee, Yeongmin Kim, Myun Joong Hwang, Yusuke Kuribayashi, Naoki Hiratsuka, Daisuke Tanaka, Solvi Arnold, Kimitoshi Yamazaki, Carlos Mateo-Agullo, Andreas Verleysen, Francis Wyffels",2025-08-22 19:03:37+00:00,http://arxiv.org/abs/2508.16749v1,http://arxiv.org/pdf/2508.16749v1,cs.RO
COSMO-Bench: A Benchmark for Collaborative SLAM Optimization,"Recent years have seen a focus on research into distributed optimization
algorithms for multi-robot Collaborative Simultaneous Localization and Mapping
(C-SLAM). Research in this domain, however, is made difficult by a lack of
standard benchmark datasets. Such datasets have been used to great effect in
the field of single-robot SLAM, and researchers focused on multi-robot problems
would benefit greatly from dedicated benchmark datasets. To address this gap,
we design and release the Collaborative Open-Source Multi-robot Optimization
Benchmark (COSMO-Bench) -- a suite of 24 datasets derived from a
state-of-the-art C-SLAM front-end and real-world LiDAR data. Data DOI:
https://doi.org/10.1184/R1/29652158","Daniel McGann, Easton R. Potokar, Michael Kaess",2025-08-22 18:13:03+00:00,http://arxiv.org/abs/2508.16731v1,http://arxiv.org/pdf/2508.16731v1,cs.RO
Hierarchical Decision-Making for Autonomous Navigation: Integrating Deep Reinforcement Learning and Fuzzy Logic in Four-Wheel Independent Steering and Driving Systems,"This paper presents a hierarchical decision-making framework for autonomous
navigation in four-wheel independent steering and driving (4WISD) systems. The
proposed approach integrates deep reinforcement learning (DRL) for high-level
navigation with fuzzy logic for low-level control to ensure both task
performance and physical feasibility. The DRL agent generates global motion
commands, while the fuzzy logic controller enforces kinematic constraints to
prevent mechanical strain and wheel slippage. Simulation experiments
demonstrate that the proposed framework outperforms traditional navigation
methods, offering enhanced training efficiency and stability and mitigating
erratic behaviors compared to purely DRL-based solutions. Real-world
validations further confirm the framework's ability to navigate safely and
effectively in dynamic industrial settings. Overall, this work provides a
scalable and reliable solution for deploying 4WISD mobile robots in complex,
real-world scenarios.","Yizhi Wang, Degang Xu, Yongfang Xie, Shuzhong Tan, Xianan Zhou, Peng Chen",2025-08-22 17:57:56+00:00,http://arxiv.org/abs/2508.16574v1,http://arxiv.org/pdf/2508.16574v1,"cs.RO, cs.AI"
Comparative Analysis of UAV Path Planning Algorithms for Efficient Navigation in Urban 3D Environments,"The most crucial challenges for UAVs are planning paths and avoiding
obstacles in their way. In recent years, a wide variety of path-planning
algorithms have been developed. These algorithms have successfully solved
path-planning problems; however, they suffer from multiple challenges and
limitations. To test the effectiveness and efficiency of three widely used
algorithms, namely A*, RRT*, and Particle Swarm Optimization (PSO), this paper
conducts extensive experiments in 3D urban city environments cluttered with
obstacles. Three experiments were designed with two scenarios each to test the
aforementioned algorithms. These experiments consider different city map sizes,
different altitudes, and varying obstacle densities and sizes in the
environment. According to the experimental results, the A* algorithm
outperforms the others in both computation efficiency and path quality. PSO is
especially suitable for tight turns and dense environments, and RRT* offers a
balance and works well across all experiments due to its randomized approach to
finding solutions.","Hichem Cheriet, Khellat Kihel Badra, Chouraqui Samira",2025-08-22 16:37:59+00:00,http://arxiv.org/abs/2508.16515v2,http://arxiv.org/pdf/2508.16515v2,"cs.RO, cs.AI"
On Kinodynamic Global Planning in a Simplicial Complex Environment: A Mixed Integer Approach,"This work casts the kinodynamic planning problem for car-like vehicles as an
optimization task to compute a minimum-time trajectory and its associated
velocity profile, subject to boundary conditions on velocity, acceleration, and
steering. The approach simultaneously optimizes both the spatial path and the
sequence of acceleration and steering controls, ensuring continuous motion from
a specified initial position and velocity to a target end position and
velocity.The method analyzes the admissible control space and terrain to avoid
local minima. The proposed method operates efficiently in simplicial complex
environments, a preferred terrain representation for capturing intricate 3D
landscapes. The problem is initially posed as a mixed-integer fractional
program with quadratic constraints, which is then reformulated into a
mixed-integer bilinear objective through a variable transformation and
subsequently relaxed to a mixed-integer linear program using McCormick
envelopes. Comparative simulations against planners such as MPPI and log-MPPI
demonstrate that the proposed approach generates solutions 104 times faster
while strictly adhering to the specified constraints","Otobong Jerome, Alexandr Klimchik, Alexander Maloletov, Geesara Kulathunga",2025-08-22 16:35:01+00:00,http://arxiv.org/abs/2508.16511v1,http://arxiv.org/pdf/2508.16511v1,"cs.RO, math.OC"
Terrain Classification for the Spot Quadrupedal Mobile Robot Using Only Proprioceptive Sensing,"Quadrupedal mobile robots can traverse a wider range of terrain types than
their wheeled counterparts but do not perform the same on all terrain types.
These robots are prone to undesirable behaviours like sinking and slipping on
challenging terrains. To combat this issue, we propose a terrain classifier
that provides information on terrain type that can be used in robotic systems
to create a traversability map to plan safer paths for the robot to navigate.
The work presented here is a terrain classifier developed for a Boston Dynamics
Spot robot. Spot provides over 100 measured proprioceptive signals describing
the motions of the robot and its four legs (e.g., foot penetration, forces,
joint angles, etc.). The developed terrain classifier combines dimensionality
reduction techniques to extract relevant information from the signals and then
applies a classification technique to differentiate terrain based on
traversability. In representative field testing, the resulting terrain
classifier was able to identify three different terrain types with an accuracy
of approximately 97%","Sophie Villemure, Jefferson Silveira, Joshua A. Marshall",2025-08-22 16:29:11+00:00,http://arxiv.org/abs/2508.16504v1,http://arxiv.org/pdf/2508.16504v1,"cs.RO, eess.SP"
HOSt3R: Keypoint-free Hand-Object 3D Reconstruction from RGB images,"Hand-object 3D reconstruction has become increasingly important for
applications in human-robot interaction and immersive AR/VR experiences. A
common approach for object-agnostic hand-object reconstruction from RGB
sequences involves a two-stage pipeline: hand-object 3D tracking followed by
multi-view 3D reconstruction. However, existing methods rely on keypoint
detection techniques, such as Structure from Motion (SfM) and hand-keypoint
optimization, which struggle with diverse object geometries, weak textures, and
mutual hand-object occlusions, limiting scalability and generalization. As a
key enabler to generic and seamless, non-intrusive applicability, we propose in
this work a robust, keypoint detector-free approach to estimating hand-object
3D transformations from monocular motion video/images. We further integrate
this with a multi-view reconstruction pipeline to accurately recover
hand-object 3D shape. Our method, named HOSt3R, is unconstrained, does not rely
on pre-scanned object templates or camera intrinsics, and reaches
state-of-the-art performance for the tasks of object-agnostic hand-object 3D
transformation and shape estimation on the SHOWMe benchmark. We also experiment
on sequences from the HO3D dataset, demonstrating generalization to unseen
object categories.","Anilkumar Swamy, Vincent Leroy, Philippe Weinzaepfel, Jean-Sébastien Franco, Grégory Rogez",2025-08-22 15:30:40+00:00,http://arxiv.org/abs/2508.16465v2,http://arxiv.org/pdf/2508.16465v2,"cs.CV, cs.AI, cs.HC, cs.LG, cs.RO"
Swarming Without an Anchor (SWA): Robot Swarms Adapt Better to Localization Dropouts Then a Single Robot,"In this paper, we present the Swarming Without an Anchor (SWA) approach to
state estimation in swarms of Unmanned Aerial Vehicles (UAVs) experiencing
ego-localization dropout, where individual agents are laterally stabilized
using relative information only. We propose to fuse decentralized state
estimation with robust mutual perception and onboard sensor data to maintain
accurate state awareness despite intermittent localization failures. Thus, the
relative information used to estimate the lateral state of UAVs enables the
identification of the unambiguous state of UAVs with respect to the local
constellation. The resulting behavior reaches velocity consensus, as this task
can be referred to as the double integrator synchronization problem. All
disturbances and performance degradations except a uniform translation drift of
the swarm as a whole is attenuated which is enabling new opportunities in using
tight cooperation for increasing reliability and resilience of multi-UAV
systems. Simulations and real-world experiments validate the effectiveness of
our approach, demonstrating its capability to sustain cohesive swarm behavior
in challenging conditions of unreliable or unavailable primary localization.","Jiri Horyna, Roland Jung, Stephan Weiss, Eliseo Ferrante, Martin Saska",2025-08-22 15:20:54+00:00,http://arxiv.org/abs/2508.16460v1,http://arxiv.org/pdf/2508.16460v1,"cs.RO, cs.MA"
GPL-SLAM: A Laser SLAM Framework with Gaussian Process Based Extended Landmarks,"We present a novel Simultaneous Localization and Mapping (SLAM) method that
employs Gaussian Process (GP) based landmark (object) representations. Instead
of conventional grid maps or point cloud registration, we model the environment
on a per object basis using GP based contour representations. These contours
are updated online through a recursive scheme, enabling efficient memory usage.
The SLAM problem is formulated within a fully Bayesian framework, allowing
joint inference over the robot pose and object based map. This representation
provides semantic information such as the number of objects and their areas,
while also supporting probabilistic measurement to object associations.
Furthermore, the GP based contours yield confidence bounds on object shapes,
offering valuable information for downstream tasks like safe navigation and
exploration. We validate our method on synthetic and real world experiments,
and show that it delivers accurate localization and mapping performance across
diverse structured environments.","Ali Emre Balcı, Erhan Ege Keyvan, Emre Özkan",2025-08-22 15:20:43+00:00,http://arxiv.org/abs/2508.16459v1,http://arxiv.org/pdf/2508.16459v1,cs.RO
Sound and Solution-Complete CCBS,"Continuous-time Conflict Based-Search (CCBS) has long been viewed as the
de-facto optimal solver for multi-agent path finding in continuous time
(MAPFR). Recent findings, however, show that the original theoretical variant
of CCBS can suffer from non-termination, while the widely used implementation
can return sub-optimal solutions. We introduce an analytical framework that
yields simple and sufficient conditions under which any CCBS-style algorithm is
both sound, i.e., returns only optimal solutions, and solution complete, i.e.,
terminates on every solvable MAPFR instance. Investigating the publicly
available implementation of CCBS reveals that it violates these conditions.
Though this merely indicates that CCBS might be unsound, this indication is
supported by counter-examples.
  Leveraging the analytical framework, we propose a novel branching rule and
prove that it satisfies the sufficient conditions, thereby restoring soundness
and termination guarantees. Consequently, the resulting CCBS variant is both
sound and solution complete, matching the guarantees of the discrete-time CBS
for the first time in the continuous domain. We experimentally apply standard
CCBS and CCBS under our branching rule to an example problem, with our
branching rule returning a solution with lower sum-of-costs than standard CCBS.
Because the branching rule largely only affects the branching step, it can be
adopted as a drop-in replacement in existing code-bases, as we show in our
provided implementation. Beyond CCBS, the analytical framework and termination
criterion provide a systematic way to evaluate other CCBS-like MAPFR solvers
and future extensions.","Alvin Combrink, Sabino Francesco Roselli, Martin Fabian",2025-08-22 14:23:15+00:00,http://arxiv.org/abs/2508.16410v1,http://arxiv.org/pdf/2508.16410v1,"cs.MA, cs.DM, cs.RO"
Towards Training-Free Underwater 3D Object Detection from Sonar Point Clouds: A Comparison of Traditional and Deep Learning Approaches,"Underwater 3D object detection remains one of the most challenging frontiers
in computer vision, where traditional approaches struggle with the harsh
acoustic environment and scarcity of training data. While deep learning has
revolutionized terrestrial 3D detection, its application underwater faces a
critical bottleneck: obtaining sufficient annotated sonar data is prohibitively
expensive and logistically complex, often requiring specialized vessels, expert
surveyors, and favorable weather conditions. This work addresses a fundamental
question: Can we achieve reliable underwater 3D object detection without
real-world training data? We tackle this challenge by developing and comparing
two paradigms for training-free detection of artificial structures in multibeam
echo-sounder point clouds. Our dual approach combines a physics-based sonar
simulation pipeline that generates synthetic training data for state-of-the-art
neural networks, with a robust model-based template matching system that
leverages geometric priors of target objects. Evaluation on real bathymetry
surveys from the Baltic Sea reveals surprising insights: while neural networks
trained on synthetic data achieve 98% mean Average Precision (mAP) on simulated
scenes, they drop to 40% mAP on real sonar data due to domain shift.
Conversely, our template matching approach maintains 83% mAP on real data
without requiring any training, demonstrating remarkable robustness to acoustic
noise and environmental variations. Our findings challenge conventional wisdom
about data-hungry deep learning in underwater domains and establish the first
large-scale benchmark for training-free underwater 3D detection. This work
opens new possibilities for autonomous underwater vehicle navigation, marine
archaeology, and offshore infrastructure monitoring in data-scarce environments
where traditional machine learning approaches fail.","M. Salman Shaukat, Yannik Käckenmeister, Sebastian Bader, Thomas Kirste",2025-08-22 12:08:21+00:00,http://arxiv.org/abs/2508.18293v1,http://arxiv.org/pdf/2508.18293v1,"cs.CV, cs.AI, cs.LG, cs.RO"
Do What? Teaching Vision-Language-Action Models to Reject the Impossible,"Recently, Vision-Language-Action (VLA) models have demonstrated strong
performance on a range of robotic tasks. These models rely on multimodal
inputs, with language instructions playing a crucial role -- not only in
predicting actions, but also in robustly interpreting user intent, even when
the requests are impossible to fulfill. In this work, we investigate how VLAs
can recognize, interpret, and respond to false-premise instructions: natural
language commands that reference objects or conditions absent from the
environment. We propose Instruct-Verify-and-Act (IVA), a unified framework that
(i) detects when an instruction cannot be executed due to a false premise, (ii)
engages in language-based clarification or correction, and (iii) grounds
plausible alternatives in perception and action. Towards this end, we construct
a large-scale instruction tuning setup with structured language prompts and
train a VLA model capable of handling both accurate and erroneous requests. Our
approach leverages a contextually augmented, semi-synthetic dataset containing
paired positive and false-premise instructions, enabling robust detection and
natural language correction. Our experiments show that IVA improves false
premise detection accuracy by 97.56% over baselines, while increasing
successful responses in false-premise scenarios by 50.78%.","Wen-Han Hsieh, Elvis Hsieh, Dantong Niu, Trevor Darrell, Roei Herzig, David M. Chan",2025-08-22 10:54:33+00:00,http://arxiv.org/abs/2508.16292v1,http://arxiv.org/pdf/2508.16292v1,"cs.AI, cs.RO"
Take That for Me: Multimodal Exophora Resolution with Interactive Questioning for Ambiguous Out-of-View Instructions,"Daily life support robots must interpret ambiguous verbal instructions
involving demonstratives such as ``Bring me that cup,'' even when objects or
users are out of the robot's view. Existing approaches to exophora resolution
primarily rely on visual data and thus fail in real-world scenarios where the
object or user is not visible. We propose Multimodal Interactive Exophora
resolution with user Localization (MIEL), which is a multimodal exophora
resolution framework leveraging sound source localization (SSL), semantic
mapping, visual-language models (VLMs), and interactive questioning with
GPT-4o. Our approach first constructs a semantic map of the environment and
estimates candidate objects from a linguistic query with the user's skeletal
data. SSL is utilized to orient the robot toward users who are initially
outside its visual field, enabling accurate identification of user gestures and
pointing directions. When ambiguities remain, the robot proactively interacts
with the user, employing GPT-4o to formulate clarifying questions. Experiments
in a real-world environment showed results that were approximately 1.3 times
better when the user was visible to the robot and 2.0 times better when the
user was not visible to the robot, compared to the methods without SSL and
interactive questioning. The project website is
https://emergentsystemlabstudent.github.io/MIEL/.","Akira Oyama, Shoichi Hasegawa, Akira Taniguchi, Yoshinobu Hagiwara, Tadahiro Taniguchi",2025-08-22 07:09:06+00:00,http://arxiv.org/abs/2508.16143v1,http://arxiv.org/pdf/2508.16143v1,"cs.RO, cs.AI"
Validating Terrain Models in Digital Twins for Trustworthy sUAS Operations,"With the increasing deployment of small Unmanned Aircraft Systems (sUAS) in
unfamiliar and complex environments, Environmental Digital Twins (EDT) that
comprise weather, airspace, and terrain data are critical for safe flight
planning and for maintaining appropriate altitudes during search and
surveillance operations. With the expansion of sUAS capabilities through edge
and cloud computing, accurate EDT are also vital for advanced sUAS
capabilities, like geolocation. However, real-world sUAS deployment introduces
significant sources of uncertainty, necessitating a robust validation process
for EDT components. This paper focuses on the validation of terrain models, one
of the key components of an EDT, for real-world sUAS tasks. These models are
constructed by fusing U.S. Geological Survey (USGS) datasets and satellite
imagery, incorporating high-resolution environmental data to support mission
tasks. Validating both the terrain models and their operational use by sUAS
under real-world conditions presents significant challenges, including limited
data granularity, terrain discontinuities, GPS and sensor inaccuracies, visual
detection uncertainties, as well as onboard resources and timing constraints.
We propose a 3-Dimensions validation process grounded in software engineering
principles, following a workflow across granularity of tests, simulation to
real world, and the analysis of simple to edge conditions. We demonstrate our
approach using a multi-sUAS platform equipped with a Terrain-Aware Digital
Shadow.","Arturo Miguel Russell Bernal, Maureen Petterson, Pedro Antonio Alarcon Granadeno, Michael Murphy, James Mason, Jane Cleland-Huang",2025-08-22 05:42:55+00:00,http://arxiv.org/abs/2508.16104v1,http://arxiv.org/pdf/2508.16104v1,"cs.SE, cs.RO"
NeuralMeshing: Complete Object Mesh Extraction from Casual Captures,"How can we extract complete geometric models of objects that we encounter in
our daily life, without having access to commercial 3D scanners? In this paper
we present an automated system for generating geometric models of objects from
two or more videos. Our system requires the specification of one known point in
at least one frame of each video, which can be automatically determined using a
fiducial marker such as a checkerboard or Augmented Reality (AR) marker. The
remaining frames are automatically positioned in world space by using
Structure-from-Motion techniques. By using multiple videos and merging results,
a complete object mesh can be generated, without having to rely on hole
filling. Code for our system is available from
https://github.com/FlorisE/NeuralMeshing.","Floris Erich, Naoya Chiba, Abdullah Mustafa, Ryo Hanai, Noriaki Ando, Yusuke Yoshiyasu, Yukiyasu Domae",2025-08-22 01:06:18+00:00,http://arxiv.org/abs/2508.16026v1,http://arxiv.org/pdf/2508.16026v1,"cs.CV, cs.RO"
Self-Aligning EPM Connector: A Versatile Solution for Adaptive and Multi-Modal Interfaces,"This paper presents a multifunctional connector based on electro-permanent
magnet (EPM) technology, integrating self-alignment, mechanical coupling, fluid
transfer, and data communication within a compact SLA-3D printed structure.
Experimental results demonstrate reliable self-alignment, efficient fluid
transfer in single-loop and dual-channel modes, and robust data transmission
via integrated electronic control. The connector exhibits high flexibility in
accommodating axial, angular, and lateral misalignments while maintaining low
energy consumption. These features make it highly suitable for modular
robotics, electric vehicle charging, household robotic platforms, and aerospace
docking applications.","Bingchao Wang, Adam A. Stokes",2025-08-21 23:59:39+00:00,http://arxiv.org/abs/2508.16008v1,http://arxiv.org/pdf/2508.16008v1,cs.RO
"GelSLAM: A Real-time, High-Fidelity, and Robust 3D Tactile SLAM System","Accurately perceiving an object's pose and shape is essential for precise
grasping and manipulation. Compared to common vision-based methods, tactile
sensing offers advantages in precision and immunity to occlusion when tracking
and reconstructing objects in contact. This makes it particularly valuable for
in-hand and other high-precision manipulation tasks. In this work, we present
GelSLAM, a real-time 3D SLAM system that relies solely on tactile sensing to
estimate object pose over long periods and reconstruct object shapes with high
fidelity. Unlike traditional point cloud-based approaches, GelSLAM uses
tactile-derived surface normals and curvatures for robust tracking and loop
closure. It can track object motion in real time with low error and minimal
drift, and reconstruct shapes with submillimeter accuracy, even for low-texture
objects such as wooden tools. GelSLAM extends tactile sensing beyond local
contact to enable global, long-horizon spatial perception, and we believe it
will serve as a foundation for many precise manipulation tasks involving
interaction with objects in hand. The video demo is available on our website:
https://joehjhuang.github.io/gelslam.","Hung-Jui Huang, Mohammad Amin Mirzaee, Michael Kaess, Wenzhen Yuan",2025-08-21 22:20:43+00:00,http://arxiv.org/abs/2508.15990v1,http://arxiv.org/pdf/2508.15990v1,"cs.RO, cs.CV"
UnPose: Uncertainty-Guided Diffusion Priors for Zero-Shot Pose Estimation,"Estimating the 6D pose of novel objects is a fundamental yet challenging
problem in robotics, often relying on access to object CAD models. However,
acquiring such models can be costly and impractical. Recent approaches aim to
bypass this requirement by leveraging strong priors from foundation models to
reconstruct objects from single or multi-view images, but typically require
additional training or produce hallucinated geometry. To this end, we propose
UnPose, a novel framework for zero-shot, model-free 6D object pose estimation
and reconstruction that exploits 3D priors and uncertainty estimates from a
pre-trained diffusion model. Specifically, starting from a single-view RGB-D
frame, UnPose uses a multi-view diffusion model to estimate an initial 3D model
using 3D Gaussian Splatting (3DGS) representation, along with pixel-wise
epistemic uncertainty estimates. As additional observations become available,
we incrementally refine the 3DGS model by fusing new views guided by the
diffusion model's uncertainty, thereby continuously improving the pose
estimation accuracy and 3D reconstruction quality. To ensure global
consistency, the diffusion prior-generated views and subsequent observations
are further integrated in a pose graph and jointly optimized into a coherent
3DGS field. Extensive experiments demonstrate that UnPose significantly
outperforms existing approaches in both 6D pose estimation accuracy and 3D
reconstruction quality. We further showcase its practical applicability in
real-world robotic manipulation tasks.","Zhaodong Jiang, Ashish Sinha, Tongtong Cao, Yuan Ren, Bingbing Liu, Binbin Xu",2025-08-21 21:31:04+00:00,http://arxiv.org/abs/2508.15972v1,http://arxiv.org/pdf/2508.15972v1,"cs.RO, cs.CV"
Neural Robot Dynamics,"Accurate and efficient simulation of modern robots remains challenging due to
their high degrees of freedom and intricate mechanisms. Neural simulators have
emerged as a promising alternative to traditional analytical simulators,
capable of efficiently predicting complex dynamics and adapting to real-world
data; however, existing neural simulators typically require
application-specific training and fail to generalize to novel tasks and/or
environments, primarily due to inadequate representations of the global state.
In this work, we address the problem of learning generalizable neural
simulators for robots that are structured as articulated rigid bodies. We
propose NeRD (Neural Robot Dynamics), learned robot-specific dynamics models
for predicting future states for articulated rigid bodies under contact
constraints. NeRD uniquely replaces the low-level dynamics and contact solvers
in an analytical simulator and employs a robot-centric and spatially-invariant
simulation state representation. We integrate the learned NeRD models as an
interchangeable backend solver within a state-of-the-art robotics simulator. We
conduct extensive experiments to show that the NeRD simulators are stable and
accurate over a thousand simulation steps; generalize across tasks and
environment configurations; enable policy learning exclusively in a neural
engine; and, unlike most classical simulators, can be fine-tuned from
real-world data to bridge the gap between simulation and reality.","Jie Xu, Eric Heiden, Iretiayo Akinola, Dieter Fox, Miles Macklin, Yashraj Narang",2025-08-21 17:54:41+00:00,http://arxiv.org/abs/2508.15755v1,http://arxiv.org/pdf/2508.15755v1,"cs.RO, cs.AI, cs.GR, cs.LG"
Understanding and Utilizing Dynamic Coupling in Free-Floating Space Manipulators for On-Orbit Servicing,"This study proposes a dynamic coupling-informed trajectory optimization
algorithm for free-floating space manipulator systems (SMSs). Dynamic coupling
between the base and the manipulator arms plays a critical role in influencing
the system's behavior. While prior research has predominantly focused on
minimizing this coupling, often overlooking its potential advantages, this work
investigates how dynamic coupling can instead be leveraged to improve
trajectory planning. Singular value decomposition (SVD) of the dynamic coupling
matrix is employed to identify the dominant components governing coupling
behavior. A quantitative metric is then formulated to characterize the strength
and directionality of the coupling and is incorporated into a trajectory
optimization framework. To assess the feasibility of the optimized trajectory,
a sliding mode control-based tracking controller is designed to generate the
required joint torque inputs. Simulation results demonstrate that explicitly
accounting for dynamic coupling in trajectory planning enables more informed
and potentially more efficient operation, offering new directions for the
control of free-floating SMSs.","Gargi Das, Daegyun Choi, Donghoon Kim",2025-08-21 17:20:52+00:00,http://arxiv.org/abs/2508.15732v1,http://arxiv.org/pdf/2508.15732v1,"cs.RO, cs.SY, eess.SY"
Exploiting Policy Idling for Dexterous Manipulation,"Learning-based methods for dexterous manipulation have made notable progress
in recent years. However, learned policies often still lack reliability and
exhibit limited robustness to important factors of variation. One failure
pattern that can be observed across many settings is that policies idle, i.e.
they cease to move beyond a small region of states when they reach certain
states. This policy idling is often a reflection of the training data. For
instance, it can occur when the data contains small actions in areas where the
robot needs to perform high-precision motions, e.g., when preparing to grasp an
object or object insertion. Prior works have tried to mitigate this phenomenon
e.g. by filtering the training data or modifying the control frequency.
However, these approaches can negatively impact policy performance in other
ways. As an alternative, we investigate how to leverage the detectability of
idling behavior to inform exploration and policy improvement. Our approach,
Pause-Induced Perturbations (PIP), applies perturbations at detected idling
states, thus helping it to escape problematic basins of attraction. On a range
of challenging simulated dual-arm tasks, we find that this simple approach can
already noticeably improve test-time performance, with no additional
supervision or training. Furthermore, since the robot tends to idle at critical
points in a movement, we also find that learning from the resulting episodes
leads to better iterative policy improvement compared to prior approaches. Our
perturbation strategy also leads to a 15-35% improvement in absolute success
rate on a real-world insertion task that requires complex multi-finger
manipulation.","Annie S. Chen, Philemon Brakel, Antonia Bronars, Annie Xie, Sandy Huang, Oliver Groth, Maria Bauza, Markus Wulfmeier, Nicolas Heess, Dushyant Rao",2025-08-21 15:52:45+00:00,http://arxiv.org/abs/2508.15669v1,http://arxiv.org/pdf/2508.15669v1,"cs.RO, cs.LG, 68T40, I.2.9"
Mind and Motion Aligned: A Joint Evaluation IsaacSim Benchmark for Task Planning and Low-Level Policies in Mobile Manipulation,"Benchmarks are crucial for evaluating progress in robotics and embodied AI.
However, a significant gap exists between benchmarks designed for high-level
language instruction following, which often assume perfect low-level execution,
and those for low-level robot control, which rely on simple, one-step commands.
This disconnect prevents a comprehensive evaluation of integrated systems where
both task planning and physical execution are critical. To address this, we
propose Kitchen-R, a novel benchmark that unifies the evaluation of task
planning and low-level control within a simulated kitchen environment. Built as
a digital twin using the Isaac Sim simulator and featuring more than 500
complex language instructions, Kitchen-R supports a mobile manipulator robot.
We provide baseline methods for our benchmark, including a task-planning
strategy based on a vision-language model and a low-level control policy based
on diffusion policy. We also provide a trajectory collection system. Our
benchmark offers a flexible framework for three evaluation modes: independent
assessment of the planning module, independent assessment of the control
policy, and, crucially, an integrated evaluation of the whole system. Kitchen-R
bridges a key gap in embodied AI research, enabling more holistic and realistic
benchmarking of language-guided robotic agents.","Nikita Kachaev, Andrei Spiridonov, Andrey Gorodetsky, Kirill Muravyev, Nikita Oskolkov, Aditya Narendra, Vlad Shakhuro, Dmitry Makarov, Aleksandr I. Panov, Polina Fedotova, Alexey K. Kovalev",2025-08-21 15:48:51+00:00,http://arxiv.org/abs/2508.15663v1,http://arxiv.org/pdf/2508.15663v1,"cs.RO, cs.AI"
LLM-Driven Self-Refinement for Embodied Drone Task Planning,"We introduce SRDrone, a novel system designed for self-refinement task
planning in industrial-grade embodied drones. SRDrone incorporates two key
technical contributions: First, it employs a continuous state evaluation
methodology to robustly and accurately determine task outcomes and provide
explanatory feedback. This approach supersedes conventional reliance on
single-frame final-state assessment for continuous, dynamic drone operations.
Second, SRDrone implements a hierarchical Behavior Tree (BT) modification
model. This model integrates multi-level BT plan analysis with a constrained
strategy space to enable structured reflective learning from experience.
Experimental results demonstrate that SRDrone achieves a 44.87% improvement in
Success Rate (SR) over baseline methods. Furthermore, real-world deployment
utilizing an experience base optimized through iterative self-refinement
attains a 96.25% SR. By embedding adaptive task refinement capabilities within
an industrial-grade BT planning framework, SRDrone effectively integrates the
general reasoning intelligence of Large Language Models (LLMs) with the
stringent physical execution constraints inherent to embodied drones. Code is
available at https://github.com/ZXiiiC/SRDrone.","Deyu Zhang, Xicheng Zhang, Jiahao Li, Tingting Long, Xunhua Dai, Yongjian Fu, Jinrui Zhang, Ju Ren, Yaoxue Zhang",2025-08-21 12:29:01+00:00,http://arxiv.org/abs/2508.15501v1,http://arxiv.org/pdf/2508.15501v1,"cs.RO, cs.AI"
Lang2Lift: A Framework for Language-Guided Pallet Detection and Pose Estimation Integrated in Autonomous Outdoor Forklift Operation,"The logistics and construction industries face persistent challenges in
automating pallet handling, especially in outdoor environments with variable
payloads, inconsistencies in pallet quality and dimensions, and unstructured
surroundings. In this paper, we tackle automation of a critical step in pallet
transport: the pallet pick-up operation. Our work is motivated by labor
shortages, safety concerns, and inefficiencies in manually locating and
retrieving pallets under such conditions. We present Lang2Lift, a framework
that leverages foundation models for natural language-guided pallet detection
and 6D pose estimation, enabling operators to specify targets through intuitive
commands such as ""pick up the steel beam pallet near the crane."" The perception
pipeline integrates Florence-2 and SAM-2 for language-grounded segmentation
with FoundationPose for robust pose estimation in cluttered, multi-pallet
outdoor scenes under variable lighting. The resulting poses feed into a motion
planning module for fully autonomous forklift operation. We validate Lang2Lift
on the ADAPT autonomous forklift platform, achieving 0.76 mIoU pallet
segmentation accuracy on a real-world test dataset. Timing and error analysis
demonstrate the system's robustness and confirm its feasibility for deployment
in operational logistics and construction environments. Video demonstrations
are available at https://eric-nguyen1402.github.io/lang2lift.github.io/","Huy Hoang Nguyen, Johannes Huemer, Markus Murschitz, Tobias Glueck, Minh Nhat Vu, Andreas Kugi",2025-08-21 10:28:39+00:00,http://arxiv.org/abs/2508.15427v1,http://arxiv.org/pdf/2508.15427v1,"cs.RO, cs.CV"
Spatial Policy: Guiding Visuomotor Robotic Manipulation with Spatial-Aware Modeling and Reasoning,"Vision-centric hierarchical embodied models have demonstrated strong
potential for long-horizon robotic control. However, existing methods lack
spatial awareness capabilities, limiting their effectiveness in bridging visual
plans to actionable control in complex environments. To address this problem,
we propose Spatial Policy (SP), a unified spatial-aware visuomotor robotic
manipulation framework via explicit spatial modeling and reasoning.
Specifically, we first design a spatial-conditioned embodied video generation
module to model spatially guided predictions through a spatial plan table.
Then, we propose a spatial-based action prediction module to infer executable
actions with coordination. Finally, we propose a spatial reasoning feedback
policy to refine the spatial plan table via dual-stage replanning. Extensive
experiments show that SP significantly outperforms state-of-the-art baselines,
achieving a 33.0% average improvement over the best baseline. With an 86.7%
average success rate across 11 diverse tasks, SP substantially enhances the
practicality of embodied models for robotic control applications. Code and
checkpoints are maintained at
https://plantpotatoonmoon.github.io/SpatialPolicy/.","Yijun Liu, Yuwei Liu, Yuan Meng, Jieheng Zhang, Yuwei Zhou, Ye Li, Jiacheng Jiang, Kangye Ji, Shijia Ge, Zhi Wang, Wenwu Zhu",2025-08-21 10:24:18+00:00,http://arxiv.org/abs/2508.15874v1,http://arxiv.org/pdf/2508.15874v1,"cs.RO, cs.AI, cs.CV"
Active Prostate Phantom with Multiple Chambers,"Prostate cancer is a major global health concern, requiring advancements in
robotic surgery and diagnostics to improve patient outcomes. A phantom is a
specially designed object that simulates human tissues or organs. It can be
used for calibrating and testing a medical process, as well as for training and
research purposes. Existing prostate phantoms fail to simulate dynamic
scenarios. This paper presents a pneumatically actuated prostate phantom with
multiple independently controlled chambers, allowing for precise volumetric
adjustments to replicate asymmetric and symmetric benign prostatic hyperplasia
(BPH). The phantom is designed based on shape analysis of magnetic resonance
imaging (MRI) datasets, modeled with finite element method (FEM), and validated
through 3D reconstruction. The simulation results showed strong agreement with
physical measurements, achieving average errors of 3.47% in forward modeling
and 1.41% in inverse modeling. These results demonstrate the phantom's
potential as a platform for validating robotic-assisted systems and for further
development toward realistic simulation-based medical training.","Sizhe Tian, Yinoussa Adagolodjo, Jeremie Dequidt",2025-08-21 08:51:34+00:00,http://arxiv.org/abs/2508.15873v1,http://arxiv.org/pdf/2508.15873v1,"physics.med-ph, cs.RO"
"Sensing, Social, and Motion Intelligence in Embodied Navigation: A Comprehensive Survey","Embodied navigation (EN) advances traditional navigation by enabling robots
to perform complex egocentric tasks through sensing, social, and motion
intelligence. In contrast to classic methodologies that rely on explicit
localization and pre-defined maps, EN leverages egocentric perception and
human-like interaction strategies. This survey introduces a comprehensive EN
formulation structured into five stages: Transition, Observation, Fusion,
Reward-policy construction, and Action (TOFRA). The TOFRA framework serves to
synthesize the current state of the art, provide a critical review of relevant
platforms and evaluation metrics, and identify critical open research
challenges. A list of studies is available at
https://github.com/Franky-X/Awesome-Embodied-Navigation.","Chaoran Xiong, Yulong Huang, Fangwen Yu, Changhao Chen, Yue Wang, Songpengchen Xia, Ling Pei",2025-08-21 08:33:51+00:00,http://arxiv.org/abs/2508.15354v1,http://arxiv.org/pdf/2508.15354v1,cs.RO
Mag-Match: Magnetic Vector Field Features for Map Matching and Registration,"Map matching and registration are essential tasks in robotics for
localisation and integration of multi-session or multi-robot data. Traditional
methods rely on cameras or LiDARs to capture visual or geometric information
but struggle in challenging conditions like smoke or dust. Magnetometers, on
the other hand, detect magnetic fields, revealing features invisible to other
sensors and remaining robust in such environments. In this paper, we introduce
Mag-Match, a novel method for extracting and describing features in 3D magnetic
vector field maps to register different maps of the same area. Our feature
descriptor, based on higher-order derivatives of magnetic field maps, is
invariant to global orientation, eliminating the need for gravity-aligned
mapping. To obtain these higher-order derivatives map-wide given point-wise
magnetometer data, we leverage a physics-informed Gaussian Process to perform
efficient and recursive probabilistic inference of both the magnetic field and
its derivatives. We evaluate Mag-Match in simulated and real-world experiments
against a SIFT-based approach, demonstrating accurate map-to-map, robot-to-map,
and robot-to-robot transformations - even without initial gravitational
alignment.","William McDonald, Cedric Le Gentil, Jennifer Wakulicz, Teresa Vidal-Calleja",2025-08-21 06:41:41+00:00,http://arxiv.org/abs/2508.15300v1,http://arxiv.org/pdf/2508.15300v1,cs.RO
Survey of Vision-Language-Action Models for Embodied Manipulation,"Embodied intelligence systems, which enhance agent capabilities through
continuous environment interactions, have garnered significant attention from
both academia and industry. Vision-Language-Action models, inspired by
advancements in large foundation models, serve as universal robotic control
frameworks that substantially improve agent-environment interaction
capabilities in embodied intelligence systems. This expansion has broadened
application scenarios for embodied AI robots. This survey comprehensively
reviews VLA models for embodied manipulation. Firstly, it chronicles the
developmental trajectory of VLA architectures. Subsequently, we conduct a
detailed analysis of current research across 5 critical dimensions: VLA model
structures, training datasets, pre-training methods, post-training methods, and
model evaluation. Finally, we synthesize key challenges in VLA development and
real-world deployment, while outlining promising future research directions.","Haoran Li, Yuhui Chen, Wenbo Cui, Weiheng Liu, Kai Liu, Mingcai Zhou, Zhengtao Zhang, Dongbin Zhao",2025-08-21 03:30:04+00:00,http://arxiv.org/abs/2508.15201v1,http://arxiv.org/pdf/2508.15201v1,"cs.RO, cs.AI"
Hardware Implementation of a Zero-Prior-Knowledge Approach to Lifelong Learning in Kinematic Control of Tendon-Driven Quadrupeds,"Like mammals, robots must rapidly learn to control their bodies and interact
with their environment despite incomplete knowledge of their body structure and
surroundings. They must also adapt to continuous changes in both. This work
presents a bio-inspired learning algorithm, General-to-Particular (G2P),
applied to a tendon-driven quadruped robotic system developed and fabricated
in-house. Our quadruped robot undergoes an initial five-minute phase of
generalized motor babbling, followed by 15 refinement trials (each lasting 20
seconds) to achieve specific cyclical movements. This process mirrors the
exploration-exploitation paradigm observed in mammals. With each refinement,
the robot progressively improves upon its initial ""good enough"" solution. Our
results serve as a proof-of-concept, demonstrating the hardware-in-the-loop
system's ability to learn the control of a tendon-driven quadruped with
redundancies in just a few minutes to achieve functional and adaptive cyclical
non-convex movements. By advancing autonomous control in robotic locomotion,
our approach paves the way for robots capable of dynamically adjusting to new
environments, ensuring sustained adaptability and performance.","Hesam Azadjou, Suraj Chakravarthi Raja, Ali Marjaninejad, Francisco J. Valero-Cuevas",2025-08-21 01:49:28+00:00,http://arxiv.org/abs/2508.15160v1,http://arxiv.org/pdf/2508.15160v1,cs.RO
Open-Universe Assistance Games,"Embodied AI agents must infer and act in an interpretable way on diverse
human goals and preferences that are not predefined. To formalize this setting,
we introduce Open-Universe Assistance Games (OU-AGs), a framework where the
agent must reason over an unbounded and evolving space of possible goals. In
this context, we introduce GOOD (GOals from Open-ended Dialogue), a
data-efficient, online method that extracts goals in the form of natural
language during an interaction with a human, and infers a distribution over
natural language goals. GOOD prompts an LLM to simulate users with different
complex intents, using its responses to perform probabilistic inference over
candidate goals. This approach enables rich goal representations and
uncertainty estimation without requiring large offline datasets. We evaluate
GOOD in a text-based grocery shopping domain and in a text-operated simulated
household robotics environment (AI2Thor), using synthetic user profiles. Our
method outperforms a baseline without explicit goal tracking, as confirmed by
both LLM-based and human evaluations.","Rachel Ma, Jingyi Qu, Andreea Bobu, Dylan Hadfield-Menell",2025-08-20 23:07:10+00:00,http://arxiv.org/abs/2508.15119v1,http://arxiv.org/pdf/2508.15119v1,"cs.AI, cs.CL, cs.LG, cs.RO"
Discrete VHCs for Propeller Motion of a Devil-Stick using purely Impulsive Inputs,"The control problem of realizing propeller motion of a devil-stick in the
vertical plane using impulsive forces applied normal to the stick is
considered. This problem is an example of underactuated robotic juggling and
has not been considered in the literature before. Inspired by virtual holonomic
constraints, the concept of discrete virtual holonomic constraints (DVHC) is
introduced for the first time to solve this orbital stabilization problem. At
the discrete instants when impulsive inputs are applied, the location of the
center-of-mass of the devil-stick is specified in terms of its orientation
angle. This yields the discrete zero dynamics (DZD), which provides conditions
for stable propeller motion. In the limiting case, when the rotation angle
between successive applications of impulsive inputs is chosen to be arbitrarily
small, the problem reduces to that of propeller motion under continuous
forcing. A controller that enforces the DVHC, and an orbit stabilizing
controller based on the impulse controlled Poincar\'e map approach are
presented. The efficacy of the approach to trajectory design and stabilization
is validated through simulations.","Aakash Khandelwal, Ranjan Mukherjee",2025-08-20 20:08:14+00:00,http://arxiv.org/abs/2508.15040v1,http://arxiv.org/pdf/2508.15040v1,"eess.SY, cs.RO, cs.SY, math.DS"
Decentralized Vision-Based Autonomous Aerial Wildlife Monitoring,"Wildlife field operations demand efficient parallel deployment methods to
identify and interact with specific individuals, enabling simultaneous
collective behavioral analysis, and health and safety interventions. Previous
robotics solutions approach the problem from the herd perspective, or are
manually operated and limited in scale. We propose a decentralized vision-based
multi-quadrotor system for wildlife monitoring that is scalable, low-bandwidth,
and sensor-minimal (single onboard RGB camera). Our approach enables robust
identification and tracking of large species in their natural habitat. We
develop novel vision-based coordination and tracking algorithms designed for
dynamic, unstructured environments without reliance on centralized
communication or control. We validate our system through real-world
experiments, demonstrating reliable deployment in diverse field conditions.","Makram Chahine, William Yang, Alaa Maalouf, Justin Siriska, Ninad Jadhav, Daniel Vogt, Stephanie Gil, Robert Wood, Daniela Rus",2025-08-20 20:05:05+00:00,http://arxiv.org/abs/2508.15038v1,http://arxiv.org/pdf/2508.15038v1,"cs.RO, cs.AI, cs.CV, cs.MA, I.2.9"
In-Context Iterative Policy Improvement for Dynamic Manipulation,"Attention-based architectures trained on internet-scale language data have
demonstrated state of the art reasoning ability for various language-based
tasks, such as logic problems and textual reasoning. Additionally, these Large
Language Models (LLMs) have exhibited the ability to perform few-shot
prediction via in-context learning, in which input-output examples provided in
the prompt are generalized to new inputs. This ability furthermore extends
beyond standard language tasks, enabling few-shot learning for general
patterns. In this work, we consider the application of in-context learning with
pre-trained language models for dynamic manipulation. Dynamic manipulation
introduces several crucial challenges, including increased dimensionality,
complex dynamics, and partial observability. To address this, we take an
iterative approach, and formulate our in-context learning problem to predict
adjustments to a parametric policy based on previous interactions. We show
across several tasks in simulation and on a physical robot that utilizing
in-context learning outperforms alternative methods in the low data regime.
Video summary of this work and experiments can be found
https://youtu.be/2inxpdrq74U?si=dAdDYsUEr25nZvRn.","Mark Van der Merwe, Devesh Jha",2025-08-20 19:24:49+00:00,http://arxiv.org/abs/2508.15021v1,http://arxiv.org/pdf/2508.15021v1,cs.RO
GraspQP: Differentiable Optimization of Force Closure for Diverse and Robust Dexterous Grasping,"Dexterous robotic hands enable versatile interactions due to the flexibility
and adaptability of multi-fingered designs, allowing for a wide range of
task-specific grasp configurations in diverse environments. However, to fully
exploit the capabilities of dexterous hands, access to diverse and high-quality
grasp data is essential -- whether for developing grasp prediction models from
point clouds, training manipulation policies, or supporting high-level task
planning with broader action options. Existing approaches for dataset
generation typically rely on sampling-based algorithms or simplified
force-closure analysis, which tend to converge to power grasps and often
exhibit limited diversity. In this work, we propose a method to synthesize
large-scale, diverse, and physically feasible grasps that extend beyond simple
power grasps to include refined manipulations, such as pinches and tri-finger
precision grasps. We introduce a rigorous, differentiable energy formulation of
force closure, implicitly defined through a Quadratic Program (QP).
Additionally, we present an adjusted optimization method (MALA*) that improves
performance by dynamically rejecting gradient steps based on the distribution
of energy values across all samples. We extensively evaluate our approach and
demonstrate significant improvements in both grasp diversity and the stability
of final grasp predictions. Finally, we provide a new, large-scale grasp
dataset for 5,700 objects from DexGraspNet, comprising five different grippers
and three distinct grasp types.
  Dataset and Code:https://graspqp.github.io/","René Zurbrügg, Andrei Cramariuc, Marco Hutter",2025-08-20 18:43:16+00:00,http://arxiv.org/abs/2508.15002v1,http://arxiv.org/pdf/2508.15002v1,cs.RO
A Vision-Based Shared-Control Teleoperation Scheme for Controlling the Robotic Arm of a Four-Legged Robot,"In hazardous and remote environments, robotic systems perform critical tasks
demanding improved safety and efficiency. Among these, quadruped robots with
manipulator arms offer mobility and versatility for complex operations.
However, teleoperating quadruped robots is challenging due to the lack of
integrated obstacle detection and intuitive control methods for the robotic
arm, increasing collision risks in confined or dynamically changing workspaces.
Teleoperation via joysticks or pads can be non-intuitive and demands a high
level of expertise due to its complexity, culminating in a high cognitive load
on the operator. To address this challenge, a teleoperation approach that
directly maps human arm movements to the robotic manipulator offers a simpler
and more accessible solution. This work proposes an intuitive remote control by
leveraging a vision-based pose estimation pipeline that utilizes an external
camera with a machine learning-based model to detect the operator's wrist
position. The system maps these wrist movements into robotic arm commands to
control the robot's arm in real-time. A trajectory planner ensures safe
teleoperation by detecting and preventing collisions with both obstacles and
the robotic arm itself. The system was validated on the real robot,
demonstrating robust performance in real-time control. This teleoperation
approach provides a cost-effective solution for industrial applications where
safety, precision, and ease of use are paramount, ensuring reliable and
intuitive robotic control in high-risk environments.","Murilo Vinicius da Silva, Matheus Hipolito Carvalho, Juliano Negri, Thiago Segreto, Gustavo J. G. Lahr, Ricardo V. Godoy, Marcelo Becker",2025-08-20 18:31:57+00:00,http://arxiv.org/abs/2508.14994v1,http://arxiv.org/pdf/2508.14994v1,"cs.RO, cs.CV, cs.LG, cs.SY, eess.SY"
You Only Pose Once: A Minimalist's Detection Transformer for Monocular RGB Category-level 9D Multi-Object Pose Estimation,"Accurately recovering the full 9-DoF pose of unseen instances within specific
categories from a single RGB image remains a core challenge for robotics and
automation. Most existing solutions still rely on pseudo-depth, CAD models, or
multi-stage cascades that separate 2D detection from pose estimation. Motivated
by the need for a simpler, RGB-only alternative that learns directly at the
category level, we revisit a longstanding question: Can object detection and
9-DoF pose estimation be unified with high performance, without any additional
data? We show that they can with our method, YOPO, a single-stage, query-based
framework that treats category-level 9-DoF estimation as a natural extension of
2D detection. YOPO augments a transformer detector with a lightweight pose
head, a bounding-box-conditioned translation module, and a 6D-aware Hungarian
matching cost. The model is trained end-to-end only with RGB images and
category-level pose labels. Despite its minimalist design, YOPO sets a new
state of the art on three benchmarks. On the REAL275 dataset, it achieves 79.6%
$\rm{IoU}_{50}$ and 54.1% under the $10^\circ$$10{\rm{cm}}$ metric, surpassing
prior RGB-only methods and closing much of the gap to RGB-D systems. The code,
models, and additional qualitative results can be found on our project.","Hakjin Lee, Junghoon Seo, Jaehoon Sim",2025-08-20 18:00:01+00:00,http://arxiv.org/abs/2508.14965v1,http://arxiv.org/pdf/2508.14965v1,"cs.CV, cs.RO"
"Virtual Community: An Open World for Humans, Robots, and Society","The rapid progress in AI and Robotics may lead to a profound societal
transformation, as humans and robots begin to coexist within shared
communities, introducing both opportunities and challenges. To explore this
future, we present Virtual Community-an open-world platform for humans, robots,
and society-built on a universal physics engine and grounded in real-world 3D
scenes. With Virtual Community, we aim to study embodied social intelligence at
scale: 1) How robots can intelligently cooperate or compete; 2) How humans
develop social relations and build community; 3) More importantly, how
intelligent robots and humans can co-exist in an open world. To support these,
Virtual Community features: 1) An open-source multi-agent physics simulator
that supports robots, humans, and their interactions within a society; 2) A
large-scale, real-world aligned community generation pipeline, including vast
outdoor space, diverse indoor scenes, and a community of grounded agents with
rich characters and appearances. Leveraging Virtual Community, we propose two
novel challenges. The Community Planning Challenge evaluates multi-agent
reasoning and planning ability in open-world settings, such as cooperating to
help agents with daily activities and efficiently connecting other agents. The
Community Robot Challenge requires multiple heterogeneous robots to collaborate
in solving complex open-world tasks. We evaluate various baselines on these
tasks and demonstrate the challenges in both high-level open-world task
planning and low-level cooperation controls. We hope that Virtual Community
will unlock further study of human-robot coexistence within open-world
environments.","Qinhong Zhou, Hongxin Zhang, Xiangye Lin, Zheyuan Zhang, Yutian Chen, Wenjun Liu, Zunzhe Zhang, Sunli Chen, Lixing Fang, Qiushi Lyu, Xinyu Sun, Jincheng Yang, Zeyuan Wang, Bao Chi Dang, Zhehuan Chen, Daksha Ladia, Jiageng Liu, Chuang Gan",2025-08-20 17:59:32+00:00,http://arxiv.org/abs/2508.14893v1,http://arxiv.org/pdf/2508.14893v1,"cs.CV, cs.CL, cs.RO"
Fusing Monocular RGB Images with AIS Data to Create a 6D Pose Estimation Dataset for Marine Vessels,"The paper presents a novel technique for creating a 6D pose estimation
dataset for marine vessels by fusing monocular RGB images with Automatic
Identification System (AIS) data. The proposed technique addresses the
limitations of relying purely on AIS for location information, caused by issues
like equipment reliability, data manipulation, and transmission delays. By
combining vessel detections from monocular RGB images, obtained using an object
detection network (YOLOX-X), with AIS messages, the technique generates 3D
bounding boxes that represent the vessels' 6D poses, i.e. spatial and
rotational dimensions. The paper evaluates different object detection models to
locate vessels in image space. We also compare two transformation methods
(homography and Perspective-n-Point) for aligning AIS data with image
coordinates. The results of our work demonstrate that the Perspective-n-Point
(PnP) method achieves a significantly lower projection error compared to
homography-based approaches used before, and the YOLOX-X model achieves a mean
Average Precision (mAP) of 0.80 at an Intersection over Union (IoU) threshold
of 0.5 for relevant vessel classes. We show indication that our approach allows
the creation of a 6D pose estimation dataset without needing manual annotation.
Additionally, we introduce the Boats on Nordelbe Kehrwieder (BONK-pose), a
publicly available dataset comprising 3753 images with 3D bounding box
annotations for pose estimation, created by our data fusion approach. This
dataset can be used for training and evaluating 6D pose estimation networks. In
addition we introduce a set of 1000 images with 2D bounding box annotations for
ship detection from the same scene.","Fabian Holst, Emre Gülsoylu, Simone Frintrop",2025-08-20 15:16:33+00:00,http://arxiv.org/abs/2508.14767v1,http://arxiv.org/pdf/2508.14767v1,"cs.CV, cs.RO"
Safe and Transparent Robots for Human-in-the-Loop Meat Processing,"Labor shortages have severely affected the meat processing sector. Automated
technology has the potential to support the meat industry, assist workers, and
enhance job quality. However, existing automation in meat processing is highly
specialized, inflexible, and cost intensive. Instead of forcing manufacturers
to buy a separate device for each step of the process, our objective is to
develop general-purpose robotic systems that work alongside humans to perform
multiple meat processing tasks. Through a recently conducted survey of industry
experts, we identified two main challenges associated with integrating these
collaborative robots alongside human workers. First, there must be measures to
ensure the safety of human coworkers; second, the coworkers need to understand
what the robot is doing. This paper addresses both challenges by introducing a
safety and transparency framework for general-purpose meat processing robots.
For safety, we implement a hand-detection system that continuously monitors
nearby humans. This system can halt the robot in situations where the human
comes into close proximity of the operating robot. We also develop an
instrumented knife equipped with a force sensor that can differentiate contact
between objects such as meat, bone, or fixtures. For transparency, we introduce
a method that detects the robot's uncertainty about its performance and uses an
LED interface to communicate that uncertainty to the human. Additionally, we
design a graphical interface that displays the robot's plans and allows the
human to provide feedback on the planned cut. Overall, our framework can ensure
safe operation while keeping human workers in-the-loop about the robot's
actions which we validate through a user study.","Sagar Parekh, Casey Grothoff, Ryan Wright, Robin White, Dylan P. Losey",2025-08-20 15:10:01+00:00,http://arxiv.org/abs/2508.14763v1,http://arxiv.org/pdf/2508.14763v1,cs.RO
Consistent Pose Estimation of Unmanned Ground Vehicles through Terrain-Aided Multi-Sensor Fusion on Geometric Manifolds,"Aiming to enhance the consistency and thus long-term accuracy of Extended
Kalman Filters for terrestrial vehicle localization, this paper introduces the
Manifold Error State Extended Kalman Filter (M-ESEKF). By representing the
robot's pose in a space with reduced dimensionality, the approach ensures
feasible estimates on generic smooth surfaces, without introducing artificial
constraints or simplifications that may degrade a filter's performance. The
accompanying measurement models are compatible with common loosely- and
tightly-coupled sensor modalities and also implicitly account for the ground
geometry. We extend the formulation by introducing a novel correction scheme
that embeds additional domain knowledge into the sensor data, giving more
accurate uncertainty approximations and further enhancing filter consistency.
The proposed estimator is seamlessly integrated into a validated modular state
estimation framework, demonstrating compatibility with existing
implementations. Extensive Monte Carlo simulations across diverse scenarios and
dynamic sensor configurations show that the M-ESEKF outperforms classical
filter formulations in terms of consistency and stability. Moreover, it
eliminates the need for scenario-specific parameter tuning, enabling its
application in a variety of real-world settings.","Alexander Raab, Stephan Weiss, Alessandro Fornasier, Christian Brommer, Abdalrahman Ibrahim",2025-08-20 12:24:45+00:00,http://arxiv.org/abs/2508.14661v1,http://arxiv.org/pdf/2508.14661v1,cs.RO
An Informative Planning Framework for Target Tracking and Active Mapping in Dynamic Environments with ASVs,"Mobile robot platforms are increasingly being used to automate information
gathering tasks such as environmental monitoring. Efficient target tracking in
dynamic environments is critical for applications such as search and rescue and
pollutant cleanups. In this letter, we study active mapping of floating targets
that drift due to environmental disturbances such as wind and currents. This is
a challenging problem as it involves predicting both spatial and temporal
variations in the map due to changing conditions. We propose an informative
path planning framework to map an arbitrary number of moving targets with
initially unknown positions in dynamic environments. A key component of our
approach is a spatiotemporal prediction network that predicts target position
distributions over time. We propose an adaptive planning objective for target
tracking that leverages these predictions. Simulation experiments show that our
proposed planning objective improves target tracking performance compared to
existing methods that consider only entropy reduction as the planning
objective. Finally, we validate our approach in field tests using an autonomous
surface vehicle, showcasing its ability to track targets in real-world
monitoring scenarios.","Sanjeev Ramkumar Sudha, Marija Popović, Erlend M. Coates",2025-08-20 11:44:30+00:00,http://arxiv.org/abs/2508.14636v2,http://arxiv.org/pdf/2508.14636v2,cs.RO
Can LLM Agents Solve Collaborative Tasks? A Study on Urgency-Aware Planning and Coordination,"The ability to coordinate actions across multiple agents is critical for
solving complex, real-world problems. Large Language Models (LLMs) have shown
strong capabilities in communication, planning, and reasoning, raising the
question of whether they can also support effective collaboration in
multi-agent settings. In this work, we investigate the use of LLM agents to
solve a structured victim rescue task that requires division of labor,
prioritization, and cooperative planning. Agents operate in a fully known
graph-based environment and must allocate resources to victims with varying
needs and urgency levels. We systematically evaluate their performance using a
suite of coordination-sensitive metrics, including task success rate, redundant
actions, room conflicts, and urgency-weighted efficiency. This study offers new
insights into the strengths and failure modes of LLMs in physically grounded
multi-agent collaboration tasks, contributing to future benchmarks and
architectural improvements.","João Vitor de Carvalho Silva, Douglas G. Macharet",2025-08-20 11:44:10+00:00,http://arxiv.org/abs/2508.14635v1,http://arxiv.org/pdf/2508.14635v1,"cs.RO, cs.AI"
TRUST-Planner: Topology-guided Robust Trajectory Planner for AAVs with Uncertain Obstacle Spatial-temporal Avoidance,"Despite extensive developments in motion planning of autonomous aerial
vehicles (AAVs), existing frameworks faces the challenges of local minima and
deadlock in complex dynamic environments, leading to increased collision risks.
To address these challenges, we present TRUST-Planner, a topology-guided
hierarchical planning framework for robust spatial-temporal obstacle avoidance.
In the frontend, a dynamic enhanced visible probabilistic roadmap (DEV-PRM) is
proposed to rapidly explore topological paths for global guidance. The backend
utilizes a uniform terminal-free minimum control polynomial (UTF-MINCO) and
dynamic distance field (DDF) to enable efficient predictive obstacle avoidance
and fast parallel computation. Furthermore, an incremental multi-branch
trajectory management framework is introduced to enable spatio-temporal
topological decision-making, while efficiently leveraging historical
information to reduce replanning time. Simulation results show that
TRUST-Planner outperforms baseline competitors, achieving a 96\% success rate
and millisecond-level computation efficiency in tested complex environments.
Real-world experiments further validate the feasibility and practicality of the
proposed method.","Junzhi Li, Teng Long, Jingliang Sun, Jianxin Zhong",2025-08-20 10:52:28+00:00,http://arxiv.org/abs/2508.14610v1,http://arxiv.org/pdf/2508.14610v1,cs.RO
Making Pose Representations More Expressive and Disentangled via Residual Vector Quantization,"Recent progress in text-to-motion has advanced both 3D human motion
generation and text-based motion control. Controllable motion generation
(CoMo), which enables intuitive control, typically relies on pose code
representations, but discrete pose codes alone cannot capture fine-grained
motion details, limiting expressiveness. To overcome this, we propose a method
that augments pose code-based latent representations with continuous motion
features using residual vector quantization (RVQ). This design preserves the
interpretability and manipulability of pose codes while effectively capturing
subtle motion characteristics such as high-frequency details. Experiments on
the HumanML3D dataset show that our model reduces Frechet inception distance
(FID) from 0.041 to 0.015 and improves Top-1 R-Precision from 0.508 to 0.510.
Qualitative analysis of pairwise direction similarity between pose codes
further confirms the model's controllability for motion editing.","Sukhyun Jeong, Hong-Gi Shin, Yong-Hoon Choi",2025-08-20 09:29:21+00:00,http://arxiv.org/abs/2508.14561v1,http://arxiv.org/pdf/2508.14561v1,"cs.CV, cs.RO"
EAROL: Environmental Augmented Perception-Aware Planning and Robust Odometry via Downward-Mounted Tilted LiDAR,"To address the challenges of localization drift and perception-planning
coupling in unmanned aerial vehicles (UAVs) operating in open-top scenarios
(e.g., collapsed buildings, roofless mazes), this paper proposes EAROL, a novel
framework with a downward-mounted tilted LiDAR configuration (20{\deg}
inclination), integrating a LiDAR-Inertial Odometry (LIO) system and a
hierarchical trajectory-yaw optimization algorithm. The hardware innovation
enables constraint enhancement via dense ground point cloud acquisition and
forward environmental awareness for dynamic obstacle detection. A
tightly-coupled LIO system, empowered by an Iterative Error-State Kalman Filter
(IESKF) with dynamic motion compensation, achieves high level 6-DoF
localization accuracy in feature-sparse environments. The planner, augmented by
environment, balancing environmental exploration, target tracking precision,
and energy efficiency. Physical experiments demonstrate 81% tracking error
reduction, 22% improvement in perceptual coverage, and near-zero vertical drift
across indoor maze and 60-meter-scale outdoor scenarios. This work proposes a
hardware-algorithm co-design paradigm, offering a robust solution for UAV
autonomy in post-disaster search and rescue missions. We will release our
software and hardware as an open-source package for the community. Video:
https://youtu.be/7av2ueLSiYw.","Xinkai Liang, Yigu Ge, Yangxi Shi, Haoyu Yang, Xu Cao, Hao Fang",2025-08-20 09:16:29+00:00,http://arxiv.org/abs/2508.14554v1,http://arxiv.org/pdf/2508.14554v1,cs.RO
Taming VR Teleoperation and Learning from Demonstration for Multi-Task Bimanual Table Service Manipulation,"This technical report presents the champion solution of the Table Service
Track in the ICRA 2025 What Bimanuals Can Do (WBCD) competition. We tackled a
series of demanding tasks under strict requirements for speed, precision, and
reliability: unfolding a tablecloth (deformable-object manipulation), placing a
pizza into the container (pick-and-place), and opening and closing a food
container with the lid. Our solution combines VR-based teleoperation and
Learning from Demonstrations (LfD) to balance robustness and autonomy. Most
subtasks were executed through high-fidelity remote teleoperation, while the
pizza placement was handled by an ACT-based policy trained from 100 in-person
teleoperated demonstrations with randomized initial configurations. By
carefully integrating scoring rules, task characteristics, and current
technical capabilities, our approach achieved both high efficiency and
reliability, ultimately securing the first place in the competition.","Weize Li, Zhengxiao Han, Lixin Xu, Xiangyu Chen, Harrison Bounds, Chenrui Zhang, Yifan Xu",2025-08-20 08:47:40+00:00,http://arxiv.org/abs/2508.14542v2,http://arxiv.org/pdf/2508.14542v2,cs.RO
FBI: Learning Dexterous In-hand Manipulation with Dynamic Visuotactile Shortcut Policy,"Dexterous in-hand manipulation is a long-standing challenge in robotics due
to complex contact dynamics and partial observability. While humans synergize
vision and touch for such tasks, robotic approaches often prioritize one
modality, therefore limiting adaptability. This paper introduces Flow Before
Imitation (FBI), a visuotactile imitation learning framework that dynamically
fuses tactile interactions with visual observations through motion dynamics.
Unlike prior static fusion methods, FBI establishes a causal link between
tactile signals and object motion via a dynamics-aware latent model. FBI
employs a transformer-based interaction module to fuse flow-derived tactile
features with visual inputs, training a one-step diffusion policy for real-time
execution. Extensive experiments demonstrate that the proposed method
outperforms the baseline methods in both simulation and the real world on two
customized in-hand manipulation tasks and three standard dexterous manipulation
tasks. Code, models, and more results are available in the website
https://sites.google.com/view/dex-fbi.","Yijin Chen, Wenqiang Xu, Zhenjun Yu, Tutian Tang, Yutong Li, Siqiong Yao, Cewu Lu",2025-08-20 05:53:05+00:00,http://arxiv.org/abs/2508.14441v1,http://arxiv.org/pdf/2508.14441v1,cs.RO
Dimension-Decomposed Learning for Quadrotor Geometric Attitude Control with Almost Global Exponential Convergence on SO(3),"This paper introduces a lightweight and interpretable online learning
approach called Dimension-Decomposed Learning (DiD-L) for disturbance
identification in quadrotor geometric attitude control. As a module instance of
DiD-L, we propose the Sliced Adaptive-Neuro Mapping (SANM). Specifically, to
address underlying underfitting problems, the high-dimensional mapping for
online identification is axially ``sliced"" into multiple low-dimensional
submappings (slices). In this way, the complex high-dimensional problem is
decomposed into a set of simple low-dimensional subtasks addressed by shallow
neural networks and adaptive laws. These neural networks and adaptive laws are
updated online via Lyapunov-based adaptation without the persistent excitation
(PE) condition. To enhance the interpretability of the proposed approach, we
prove that the state solution of the rotational error dynamics exponentially
converges into an arbitrarily small ball within an almost global attraction
domain, despite time-varying disturbances and inertia uncertainties. This
result is novel as it demonstrates exponential convergence without requiring
pre-training for unseen disturbances and specific knowledge of the model. To
our knowledge in the quadrotor control field, DiD-L is the first online
learning approach that is lightweight enough to run in real-time at 400 Hz on
microcontroller units (MCUs) such as STM32, and has been validated through
real-world experiments.","Tianhua Gao, Masashi Izumita, Kohji Tomita, Akiya Kamimura",2025-08-20 04:41:42+00:00,http://arxiv.org/abs/2508.14422v2,http://arxiv.org/pdf/2508.14422v2,"eess.SY, cs.RO, cs.SY, math.OC"
DEXTER-LLM: Dynamic and Explainable Coordination of Multi-Robot Systems in Unknown Environments via Large Language Models,"Online coordination of multi-robot systems in open and unknown environments
faces significant challenges, particularly when semantic features detected
during operation dynamically trigger new tasks. Recent large language model
(LLMs)-based approaches for scene reasoning and planning primarily focus on
one-shot, end-to-end solutions in known environments, lacking both dynamic
adaptation capabilities for online operation and explainability in the
processes of planning. To address these issues, a novel framework (DEXTER-LLM)
for dynamic task planning in unknown environments, integrates four modules: (i)
a mission comprehension module that resolves partial ordering of tasks
specified by natural languages or linear temporal logic formulas (LTL); (ii) an
online subtask generator based on LLMs that improves the accuracy and
explainability of task decomposition via multi-stage reasoning; (iii) an
optimal subtask assigner and scheduler that allocates subtasks to robots via
search-based optimization; and (iv) a dynamic adaptation and human-in-the-loop
verification module that implements multi-rate, event-based updates for both
subtasks and their assignments, to cope with new features and tasks detected
online. The framework effectively combines LLMs' open-world reasoning
capabilities with the optimality of model-based assignment methods,
simultaneously addressing the critical issue of online adaptability and
explainability. Experimental evaluations demonstrate exceptional performances,
with 100% success rates across all scenarios, 160 tasks and 480 subtasks
completed on average (3 times the baselines), 62% less queries to LLMs during
adaptation, and superior plan quality (2 times higher) for compound tasks.
Project page at https://tcxm.github.io/DEXTER-LLM/","Yuxiao Zhu, Junfeng Chen, Xintong Zhang, Meng Guo, Zhongkui Li",2025-08-20 03:27:23+00:00,http://arxiv.org/abs/2508.14387v1,http://arxiv.org/pdf/2508.14387v1,cs.RO
Offline Imitation Learning upon Arbitrary Demonstrations by Pre-Training Dynamics Representations,"Limited data has become a major bottleneck in scaling up offline imitation
learning (IL). In this paper, we propose enhancing IL performance under limited
expert data by introducing a pre-training stage that learns dynamics
representations, derived from factorizations of the transition dynamics. We
first theoretically justify that the optimal decision variable of offline IL
lies in the representation space, significantly reducing the parameters to
learn in the downstream IL. Moreover, the dynamics representations can be
learned from arbitrary data collected with the same dynamics, allowing the
reuse of massive non-expert data and mitigating the limited data issues. We
present a tractable loss function inspired by noise contrastive estimation to
learn the dynamics representations at the pre-training stage. Experiments on
MuJoCo demonstrate that our proposed algorithm can mimic expert policies with
as few as a single trajectory. Experiments on real quadrupeds show that we can
leverage pre-trained dynamics representations from simulator data to learn to
walk from a few real-world demonstrations.","Haitong Ma, Bo Dai, Zhaolin Ren, Yebin Wang, Na Li",2025-08-20 03:23:20+00:00,http://arxiv.org/abs/2508.14383v1,http://arxiv.org/pdf/2508.14383v1,"cs.RO, cs.LG"
PRREACH: Probabilistic Risk Assessment Using Reachability for UAV Control,"We present a new approach for designing risk-bounded controllers for Uncrewed
Aerial Vehicles (UAVs). Existing frameworks for assessing risk of UAV
operations rely on knowing the conditional probability of an incident occurring
given different causes. Limited data for computing these probabilities makes
real-world implementation of these frameworks difficult. Furthermore, existing
frameworks do not include control methods for risk mitigation. Our approach
relies on UAV dynamics, and employs reachability analysis for a probabilistic
risk assessment over all feasible UAV trajectories. We use this holistic risk
assessment to formulate a control optimization problem that minimally changes a
UAV's existing control law to be bounded by an accepted risk threshold. We call
our approach PRReach. Public and readily available UAV dynamics models and open
source spatial data for mapping hazard outcomes enables practical
implementation of PRReach for both offline pre-flight and online in-flight risk
assessment and mitigation. We evaluate PRReach through simulation experiments
on real-world data. Results show that PRReach controllers reduce risk by up to
24% offline, and up to 53% online from classical controllers.","Nicole Fronda, Hariharan Narayanan, Sadia Afrin Ananna, Steven Weber, Houssam Abbas",2025-08-20 03:22:09+00:00,http://arxiv.org/abs/2509.04451v1,http://arxiv.org/pdf/2509.04451v1,"eess.SY, cs.RO, cs.SY"
FiReFly: Fair Distributed Receding Horizon Planning for Multiple UAVs,"We propose injecting notions of fairness into multi-robot motion planning.
When robots have competing interests, it is important to optimize for some kind
of fairness in their usage of resources. In this work, we explore how the
robots' energy expenditures might be fairly distributed among them, while
maintaining mission success. We formulate a distributed fair motion planner and
integrate it with safe controllers in a algorithm called FiReFly. For simulated
reach-avoid missions, FiReFly produces fairer trajectories and improves mission
success rates over a non-fair planner. We find that real-time performance is
achievable up to 15 UAVs, and that scaling up to 50 UAVs is possible with
trade-offs between runtime and fairness improvements.","Nicole Fronda, Bardh Hoxha, Houssam Abbas",2025-08-20 03:21:44+00:00,http://arxiv.org/abs/2508.14381v1,http://arxiv.org/pdf/2508.14381v1,cs.RO
Fair-CoPlan: Negotiated Flight Planning with Fair Deconfliction for Urban Air Mobility,"Urban Air Mobility (UAM) is an emerging transportation paradigm in which
Uncrewed Aerial Systems (UAS) autonomously transport passengers and goods in
cities. The UAS have different operators with different, sometimes competing
goals, yet must share the airspace. We propose a negotiated, semi-distributed
flight planner that optimizes UAS' flight lengths {\em in a fair manner}.
Current flight planners might result in some UAS being given disproportionately
shorter flight paths at the expense of others. We introduce Fair-CoPlan, a
planner in which operators and a Provider of Service to the UAM (PSU) together
compute \emph{fair} flight paths. Fair-CoPlan has three steps: First, the PSU
constrains take-off and landing choices for flights based on capacity at and
around vertiports. Then, operators plan independently under these constraints.
Finally, the PSU resolves any conflicting paths, optimizing for path length
fairness. By fairly spreading the cost of deconfliction Fair-CoPlan encourages
wider participation in UAM, ensures safety of the airspace and the areas below
it, and promotes greater operator flexibility. We demonstrate Fair-CoPlan
through simulation experiments and find fairer outcomes than a non-fair planner
with minor delays as a trade-off.","Nicole Fronda, Phil Smith, Bardh Hoxha, Yash Pant, Houssam Abbas",2025-08-20 03:21:19+00:00,http://arxiv.org/abs/2508.14380v1,http://arxiv.org/pdf/2508.14380v1,cs.RO
Action-Constrained Imitation Learning,"Policy learning under action constraints plays a central role in ensuring
safe behaviors in various robot control and resource allocation applications.
In this paper, we study a new problem setting termed Action-Constrained
Imitation Learning (ACIL), where an action-constrained imitator aims to learn
from a demonstrative expert with larger action space. The fundamental challenge
of ACIL lies in the unavoidable mismatch of occupancy measure between the
expert and the imitator caused by the action constraints. We tackle this
mismatch through \textit{trajectory alignment} and propose DTWIL, which
replaces the original expert demonstrations with a surrogate dataset that
follows similar state trajectories while adhering to the action constraints.
Specifically, we recast trajectory alignment as a planning problem and solve it
via Model Predictive Control, which aligns the surrogate trajectories with the
expert trajectories based on the Dynamic Time Warping (DTW) distance. Through
extensive experiments, we demonstrate that learning from the dataset generated
by DTWIL significantly enhances performance across multiple robot control tasks
and outperforms various benchmark imitation learning algorithms in terms of
sample efficiency. Our code is publicly available at
https://github.com/NYCU-RL-Bandits-Lab/ACRL-Baselines.","Chia-Han Yeh, Tse-Sheng Nan, Risto Vuorio, Wei Hung, Hung-Yen Wu, Shao-Hua Sun, Ping-Chun Hsieh",2025-08-20 03:19:07+00:00,http://arxiv.org/abs/2508.14379v1,http://arxiv.org/pdf/2508.14379v1,"cs.RO, cs.LG"
Learning Point Cloud Representations with Pose Continuity for Depth-Based Category-Level 6D Object Pose Estimation,"Category-level object pose estimation aims to predict the 6D pose and 3D size
of objects within given categories. Existing approaches for this task rely
solely on 6D poses as supervisory signals without explicitly capturing the
intrinsic continuity of poses, leading to inconsistencies in predictions and
reduced generalization to unseen poses. To address this limitation, we propose
HRC-Pose, a novel depth-only framework for category-level object pose
estimation, which leverages contrastive learning to learn point cloud
representations that preserve the continuity of 6D poses. HRC-Pose decouples
object pose into rotation and translation components, which are separately
encoded and leveraged throughout the network. Specifically, we introduce a
contrastive learning strategy for multi-task, multi-category scenarios based on
our 6D pose-aware hierarchical ranking scheme, which contrasts point clouds
from multiple categories by considering rotational and translational
differences as well as categorical information. We further design pose
estimation modules that separately process the learned rotation-aware and
translation-aware embeddings. Our experiments demonstrate that HRC-Pose
successfully learns continuous feature spaces. Results on REAL275 and CAMERA25
benchmarks show that our method consistently outperforms existing depth-only
state-of-the-art methods and runs in real-time, demonstrating its effectiveness
and potential for real-world applications. Our code is at
https://github.com/zhujunli1993/HRC-Pose.","Zhujun Li, Shuo Zhang, Ioannis Stamos",2025-08-20 02:09:02+00:00,http://arxiv.org/abs/2508.14358v1,http://arxiv.org/pdf/2508.14358v1,"cs.CV, cs.AI, cs.RO"
D$^2$-LIO: Enhanced Optimization for LiDAR-IMU Odometry Considering Directional Degeneracy,"LiDAR-inertial odometry (LIO) plays a vital role in achieving accurate
localization and mapping, especially in complex environments. However, the
presence of LiDAR feature degeneracy poses a major challenge to reliable state
estimation. To overcome this issue, we propose an enhanced LIO framework that
integrates adaptive outlier-tolerant correspondence with a scan-to-submap
registration strategy. The core contribution lies in an adaptive outlier
removal threshold, which dynamically adjusts based on point-to-sensor distance
and the motion amplitude of platform. This mechanism improves the robustness of
feature matching in varying conditions. Moreover, we introduce a flexible
scan-to-submap registration method that leverages IMU data to refine pose
estimation, particularly in degenerate geometric configurations. To further
enhance localization accuracy, we design a novel weighting matrix that fuses
IMU preintegration covariance with a degeneration metric derived from the
scan-to-submap process. Extensive experiments conducted in both indoor and
outdoor environments-characterized by sparse or degenerate features-demonstrate
that our method consistently outperforms state-of-the-art approaches in terms
of both robustness and accuracy.","Guodong Yao, Hao Wang, Qing Chang",2025-08-20 01:51:53+00:00,http://arxiv.org/abs/2508.14355v1,http://arxiv.org/pdf/2508.14355v1,cs.RO
Adapting Biological Reflexes for Dynamic Reorientation in Space Manipulator Systems,"Robotic arms mounted on spacecraft, known as space manipulator systems
(SMSs), are critical for enabling on-orbit assembly, satellite servicing, and
debris removal. However, controlling these systems in microgravity remains a
significant challenge due to the dynamic coupling between the manipulator and
the spacecraft base. This study explores the potential of using biological
inspiration to address this issue, focusing on animals, particularly lizards,
that exhibit mid-air righting reflexes. Based on similarities between SMSs and
these animals in terms of behavior, morphology, and environment, their
air-righting motion trajectories are extracted from high-speed video recordings
using computer vision techniques. These trajectories are analyzed within a
multi-objective optimization framework to identify the key behavioral goals and
assess their relative importance. The resulting motion profiles are then
applied as reference trajectories for SMS control, with baseline controllers
used to track them. The findings provide a step toward translating evolved
animal behaviors into interpretable, adaptive control strategies for space
robotics, with implications for improving maneuverability and robustness in
future missions.","Daegyun Choi, Alhim Vera, Donghoon Kim",2025-08-19 20:26:55+00:00,http://arxiv.org/abs/2508.14258v1,http://arxiv.org/pdf/2508.14258v1,"cs.RO, physics.bio-ph"
SLAM-based Safe Indoor Exploration Strategy,"This paper suggests a 2D exploration strategy for a planar space cluttered
with obstacles. Rather than using point robots capable of adjusting their
position and altitude instantly, this research is tailored to classical agents
with circular footprints that cannot control instantly their pose. Inhere, a
self-balanced dual-wheeled differential drive system is used to explore the
place. The system is equipped with linear accelerometers and angular
gyroscopes, a 3D-LiDAR, and a forward-facing RGB-D camera. The system performs
RTAB-SLAM using the IMU and the LiDAR, while the camera is used for loop
closures. The mobile agent explores the planar space using a safe skeleton
approach that places the agent as far as possible from the static obstacles.
During the exploration strategy, the heading is towards any offered openings of
the space. This space exploration strategy has as its highest priority the
agent's safety in avoiding the obstacles followed by the exploration of
undetected space. Experimental studies with a ROS-enabled mobile agent are
presented indicating the path planning strategy while exploring the space.","Omar Mostafa, Nikolaos Evangeliou, Anthony Tzes",2025-08-19 19:50:24+00:00,http://arxiv.org/abs/2508.14235v1,http://arxiv.org/pdf/2508.14235v1,cs.RO
Lightweight Tracking Control for Computationally Constrained Aerial Systems with the Newton-Raphson Method,"We investigate the performance of a lightweight tracking controller, based on
a flow version of the Newton-Raphson method, applied to a miniature blimp and a
mid-size quadrotor. This tracking technique has been shown to enjoy theoretical
guarantees of performance and has been applied with success in simulation
studies and on mobile robots with simple motion models. This paper investigates
the technique through real-world flight experiments on aerial hardware
platforms subject to realistic deployment and onboard computational
constraints. The technique's performance is assessed in comparison with the
established control frameworks of feedback linearization for the blimp, and
nonlinear model predictive control for both quadrotor and blimp. The
performance metrics under consideration are (i) root mean square error of
flight trajectories with respect to target trajectories, (ii) algorithms'
computation times, and (iii) CPU energy consumption associated with the control
algorithms. The experimental findings show that the Newton-Raphson flow-based
tracking controller achieves comparable or superior tracking performance to the
baseline methods with substantially reduced computation time and energy
expenditure.","Evanns Morales-Cuadrado, Luke Baird, Yorai Wardi, Samuel Coogan",2025-08-19 18:18:41+00:00,http://arxiv.org/abs/2508.14185v1,http://arxiv.org/pdf/2508.14185v1,"cs.RO, cs.SY, eess.SY, math.OC"
Towards Unified Probabilistic Verification and Validation of Vision-Based Autonomy,"Precise and comprehensive situational awareness is a critical capability of
modern autonomous systems. Deep neural networks that perceive task-critical
details from rich sensory signals have become ubiquitous; however, their
black-box behavior and sensitivity to environmental uncertainty and
distribution shifts make them challenging to verify formally. Abstraction-based
verification techniques for vision-based autonomy produce safety guarantees
contingent on rigid assumptions, such as bounded errors or known unique
distributions. Such overly restrictive and inflexible assumptions limit the
validity of the guarantees, especially in diverse and uncertain test-time
environments. We propose a methodology that unifies the verification models of
perception with their offline validation. Our methodology leverages interval
MDPs and provides a flexible end-to-end guarantee that adapts directly to the
out-of-distribution test-time conditions. We evaluate our methodology on a
synthetic perception Markov chain with well-defined state estimation
distributions and a mountain car benchmark. Our findings reveal that we can
guarantee tight yet rigorous bounds on overall system safety.","Jordan Peper, Yan Miao, Sayan Mitra, Ivan Ruchkin",2025-08-19 18:13:02+00:00,http://arxiv.org/abs/2508.14181v1,http://arxiv.org/pdf/2508.14181v1,"eess.SY, cs.RO, cs.SY"
RynnEC: Bringing MLLMs into Embodied World,"We introduce RynnEC, a video multimodal large language model designed for
embodied cognition. Built upon a general-purpose vision-language foundation
model, RynnEC incorporates a region encoder and a mask decoder, enabling
flexible region-level video interaction. Despite its compact architecture,
RynnEC achieves state-of-the-art performance in object property understanding,
object segmentation, and spatial reasoning. Conceptually, it offers a
region-centric video paradigm for the brain of embodied agents, providing
fine-grained perception of the physical world and enabling more precise
interactions. To mitigate the scarcity of annotated 3D datasets, we propose an
egocentric video based pipeline for generating embodied cognition data.
Furthermore, we introduce RynnEC-Bench, a region-centered benchmark for
evaluating embodied cognitive capabilities. We anticipate that RynnEC will
advance the development of general-purpose cognitive cores for embodied agents
and facilitate generalization across diverse embodied tasks. The code, model
checkpoints, and benchmark are available at:
https://github.com/alibaba-damo-academy/RynnEC","Ronghao Dang, Yuqian Yuan, Yunxuan Mao, Kehan Li, Jiangpin Liu, Zhikai Wang, Xin Li, Fan Wang, Deli Zhao",2025-08-19 18:00:01+00:00,http://arxiv.org/abs/2508.14160v1,http://arxiv.org/pdf/2508.14160v1,"cs.CV, cs.AI, cs.RO"
"Train Once, Deploy Anywhere: Realize Data-Efficient Dynamic Object Manipulation","Realizing generalizable dynamic object manipulation is important for
enhancing manufacturing efficiency, as it eliminates specialized engineering
for various scenarios. To this end, imitation learning emerges as a promising
paradigm, leveraging expert demonstrations to teach a policy manipulation
skills. Although the generalization of an imitation learning policy can be
improved by increasing demonstrations, demonstration collection is
labor-intensive. To address this problem, this paper investigates whether
strong generalization in dynamic object manipulation is achievable with only a
few demonstrations. Specifically, we develop an entropy-based theoretical
framework to quantify the optimization of imitation learning. Based on this
framework, we propose a system named Generalizable Entropy-based Manipulation
(GEM). Extensive experiments in simulated and real tasks demonstrate that GEM
can generalize across diverse environment backgrounds, robot embodiments,
motion dynamics, and object geometries. Notably, GEM has been deployed in a
real canteen for tableware collection. Without any in-scene demonstration, it
achieves a success rate of over 97% across more than 10,000 operations.","Zhuoling Li, Xiaoyang Wu, Zhenhua Xu, Hengshuang Zhao",2025-08-19 17:59:59+00:00,http://arxiv.org/abs/2508.14042v1,http://arxiv.org/pdf/2508.14042v1,cs.RO
"ResPlan: A Large-Scale Vector-Graph Dataset of 17,000 Residential Floor Plans","We introduce ResPlan, a large-scale dataset of 17,000 detailed, structurally
rich, and realistic residential floor plans, created to advance spatial AI
research. Each plan includes precise annotations of architectural elements
(walls, doors, windows, balconies) and functional spaces (such as kitchens,
bedrooms, and bathrooms). ResPlan addresses key limitations of existing
datasets such as RPLAN (Wu et al., 2019) and MSD (van Engelenburg et al., 2024)
by offering enhanced visual fidelity and greater structural diversity,
reflecting realistic and non-idealized residential layouts. Designed as a
versatile, general-purpose resource, ResPlan supports a wide range of
applications including robotics, reinforcement learning, generative AI, virtual
and augmented reality, simulations, and game development. Plans are provided in
both geometric and graph-based formats, enabling direct integration into
simulation engines and fast 3D conversion. A key contribution is an open-source
pipeline for geometry cleaning, alignment, and annotation refinement.
Additionally, ResPlan includes structured representations of room connectivity,
supporting graph-based spatial reasoning tasks. Finally, we present comparative
analyses with existing benchmarks and outline several open benchmark tasks
enabled by ResPlan. Ultimately, ResPlan offers a significant advance in scale,
realism, and usability, providing a robust foundation for developing and
benchmarking next-generation spatial intelligence systems.","Mohamed Abouagour, Eleftherios Garyfallidis",2025-08-19 17:07:47+00:00,http://arxiv.org/abs/2508.14006v1,http://arxiv.org/pdf/2508.14006v1,"cs.CV, cs.RO, 68T45"
Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation,"Generalization in embodied AI is hindered by the ""seeing-to-doing gap,"" which
stems from data scarcity and embodiment heterogeneity. To address this, we
pioneer ""pointing"" as a unified, embodiment-agnostic intermediate
representation, defining four core embodied pointing abilities that bridge
high-level vision-language comprehension with low-level action primitives. We
introduce Embodied-R1, a 3B Vision-Language Model (VLM) specifically designed
for embodied reasoning and pointing. We use a wide range of embodied and
general visual reasoning datasets as sources to construct a large-scale
dataset, Embodied-Points-200K, which supports key embodied pointing
capabilities. We then train Embodied-R1 using a two-stage Reinforced
Fine-tuning (RFT) curriculum with a specialized multi-task reward design.
Embodied-R1 achieves state-of-the-art performance on 11 embodied spatial and
pointing benchmarks. Critically, it demonstrates robust zero-shot
generalization by achieving a 56.2% success rate in the SIMPLEREnv and 87.5%
across 8 real-world XArm tasks without any task-specific fine-tuning,
representing a 62% improvement over strong baselines. Furthermore, the model
exhibits high robustness against diverse visual disturbances. Our work shows
that a pointing-centric representation, combined with an RFT training paradigm,
offers an effective and generalizable pathway to closing the perception-action
gap in robotics.","Yifu Yuan, Haiqin Cui, Yaoting Huang, Yibin Chen, Fei Ni, Zibin Dong, Pengyi Li, Yan Zheng, Jianye Hao",2025-08-19 16:50:01+00:00,http://arxiv.org/abs/2508.13998v1,http://arxiv.org/pdf/2508.13998v1,"cs.RO, cs.AI, cs.LG"
The Social Context of Human-Robot Interactions,"The Human-Robot Interaction (HRI) community often highlights the social
context of an interaction as a key consideration when designing, implementing,
and evaluating robot behavior. Unfortunately, researchers use the term ""social
context"" in varied ways. This can lead to miscommunication, making it
challenging to draw connections between related work on understanding and
modeling the social contexts of human-robot interactions. To address this gap,
we survey the HRI literature for existing definitions and uses of the term
""social context"". Then, we propose a conceptual model for describing the social
context of a human-robot interaction. We apply this model to existing work, and
we discuss a range of attributes of social contexts that can help researchers
plan for interactions, develop behavior models for robots, and gain insights
after interactions have taken place. We conclude with a discussion of open
research questions in relation to understanding and modeling the social
contexts of human-robot interactions.","Sydney Thompson, Kate Candon, Marynel Vázquez",2025-08-19 16:15:58+00:00,http://arxiv.org/abs/2508.13982v1,http://arxiv.org/pdf/2508.13982v1,"cs.RO, cs.AI, cs.HC, cs.MA, I.2.9; I.2"
Toward an Interaction-Centered Approach to Robot Trustworthiness,"As robots get more integrated into human environments, fostering
trustworthiness in embodied robotic agents becomes paramount for an effective
and safe human-robot interaction (HRI). To achieve that, HRI applications must
promote human trust that aligns with robot skills and avoid misplaced trust or
overtrust, which can pose safety risks and ethical concerns. To achieve that,
HRI applications must promote human trust that aligns with robot skills and
avoid misplaced trust or overtrust, which can pose safety risks and ethical
concerns. In this position paper, we outline an interaction-based framework for
building trust through mutual understanding between humans and robots. We
emphasize two main pillars: human awareness and transparency, referring to the
robot ability to interpret human actions accurately and to clearly communicate
its intentions and goals, respectively. By integrating these two pillars,
robots can behave in a manner that aligns with human expectations and needs
while providing their human partners with both comprehension and control over
their actions. We also introduce four components that we think are important
for bridging the gap between a human-perceived sense of trust and a robot true
capabilities.","Carlo Mazzola, Hassan Ali, Kristína Malinovská, Igor Farkaš",2025-08-19 16:13:33+00:00,http://arxiv.org/abs/2508.13976v1,http://arxiv.org/pdf/2508.13976v1,cs.RO
Augmenting cobots for sheet-metal SMEs with 3D object recognition and localisation,"Due to high-mix-low-volume production, sheet-metal workshops today are
challenged by small series and varying orders. As standard automation solutions
tend to fall short, SMEs resort to repetitive manual labour impacting
production costs and leading to tech-skilled workforces not being used to their
full potential. The COOCK+ ROBUST project aims to transform cobots into mobile
and reconfigurable production assistants by integrating existing technologies,
including 3D object recognition and localisation. This article explores both
the opportunities and challenges of enhancing cobotic systems with these
technologies in an industrial setting, outlining the key steps involved in the
process. Additionally, insights from a past project, carried out by the ACRO
research unit in collaboration with an industrial partner, serves as a concrete
implementation example throughout.","Martijn Cramer, Yanming Wu, David De Schepper, Eric Demeester",2025-08-19 15:56:06+00:00,http://arxiv.org/abs/2508.13964v1,http://arxiv.org/pdf/2508.13964v1,"cs.RO, cs.CV"
Multimodal Data Storage and Retrieval for Embodied AI: A Survey,"Embodied AI (EAI) agents continuously interact with the physical world,
generating vast, heterogeneous multimodal data streams that traditional
management systems are ill-equipped to handle. In this survey, we first
systematically evaluate five storage architectures (Graph Databases,
Multi-Model Databases, Data Lakes, Vector Databases, and Time-Series
Databases), focusing on their suitability for addressing EAI's core
requirements, including physical grounding, low-latency access, and dynamic
scalability. We then analyze five retrieval paradigms (Fusion Strategy-Based
Retrieval, Representation Alignment-Based Retrieval, Graph-Structure-Based
Retrieval, Generation Model-Based Retrieval, and Efficient Retrieval-Based
Optimization), revealing a fundamental tension between achieving long-term
semantic coherence and maintaining real-time responsiveness. Based on this
comprehensive analysis, we identify key bottlenecks, spanning from the
foundational Physical Grounding Gap to systemic challenges in cross-modal
integration, dynamic adaptation, and open-world generalization. Finally, we
outline a forward-looking research agenda encompassing physics-aware data
models, adaptive storage-retrieval co-optimization, and standardized
benchmarking, to guide future research toward principled data management
solutions for EAI. Our survey is based on a comprehensive review of more than
180 related studies, providing a rigorous roadmap for designing the robust,
high-performance data management frameworks essential for the next generation
of autonomous embodied systems.","Yihao Lu, Hao Tang",2025-08-19 15:04:02+00:00,http://arxiv.org/abs/2508.13901v1,http://arxiv.org/pdf/2508.13901v1,"cs.RO, cs.CV"
Driving Style Recognition Like an Expert Using Semantic Privileged Information from Large Language Models,"Existing driving style recognition systems largely depend on low-level
sensor-derived features for training, neglecting the rich semantic reasoning
capability inherent to human experts. This discrepancy results in a fundamental
misalignment between algorithmic classifications and expert judgments. To
bridge this gap, we propose a novel framework that integrates Semantic
Privileged Information (SPI) derived from large language models (LLMs) to align
recognition outcomes with human-interpretable reasoning. First, we introduce
DriBehavGPT, an interactive LLM-based module that generates natural-language
descriptions of driving behaviors. These descriptions are then encoded into
machine learning-compatible representations via text embedding and
dimensionality reduction. Finally, we incorporate them as privileged
information into Support Vector Machine Plus (SVM+) for training, enabling the
model to approximate human-like interpretation patterns. Experiments across
diverse real-world driving scenarios demonstrate that our SPI-enhanced
framework outperforms conventional methods, achieving F1-score improvements of
7.6% (car-following) and 7.9% (lane-changing). Importantly, SPI is exclusively
used during training, while inference relies solely on sensor data, ensuring
computational efficiency without sacrificing performance. These results
highlight the pivotal role of semantic behavioral representations in improving
recognition accuracy while advancing interpretable, human-centric driving
systems.","Zhaokun Chen, Chaopeng Zhang, Xiaohan Li, Wenshuo Wang, Gentiane Venture, Junqiang Xi",2025-08-19 14:43:51+00:00,http://arxiv.org/abs/2508.13881v1,http://arxiv.org/pdf/2508.13881v1,cs.RO
Toward Deployable Multi-Robot Collaboration via a Symbolically-Guided Decision Transformer,"Reinforcement learning (RL) has demonstrated great potential in robotic
operations. However, its data-intensive nature and reliance on the Markov
Decision Process (MDP) assumption limit its practical deployment in real-world
scenarios involving complex dynamics and long-term temporal dependencies, such
as multi-robot manipulation. Decision Transformers (DTs) have emerged as a
promising offline alternative by leveraging causal transformers for sequence
modeling in RL tasks. However, their applications to multi-robot manipulations
still remain underexplored. To address this gap, we propose a novel framework,
Symbolically-Guided Decision Transformer (SGDT), which integrates a
neuro-symbolic mechanism with a causal transformer to enable deployable
multi-robot collaboration. In the proposed SGDT framework, a neuro-symbolic
planner generates a high-level task-oriented plan composed of symbolic
subgoals. Guided by these subgoals, a goal-conditioned decision transformer
(GCDT) performs low-level sequential decision-making for multi-robot
manipulation. This hierarchical architecture enables structured, interpretable,
and generalizable decision making in complex multi-robot collaboration tasks.
We evaluate the performance of SGDT across a range of task scenarios, including
zero-shot and few-shot scenarios. To our knowledge, this is the first work to
explore DT-based technology for multi-robot manipulation.","Rathnam Vidushika Rasanji, Jin Wei-Kocsis, Jiansong Zhang, Dongming Gan, Ragu Athinarayanan, Paul Asunda",2025-08-19 14:42:18+00:00,http://arxiv.org/abs/2508.13877v1,http://arxiv.org/pdf/2508.13877v1,"cs.RO, cs.AI"
Learning to Drive Ethically: Embedding Moral Reasoning into Autonomous Driving,"Autonomous vehicles hold great promise for reducing traffic fatalities and
improving transportation efficiency, yet their widespread adoption hinges on
embedding robust ethical reasoning into routine and emergency maneuvers,
particularly to protect vulnerable road users (VRUs) such as pedestrians and
cyclists. Here, we present a hierarchical Safe Reinforcement Learning (Safe RL)
framework that explicitly integrates moral considerations with standard driving
objectives. At the decision level, a Safe RL agent is trained using a composite
ethical risk cost, combining collision probability and harm severity, to
generate high-level motion targets. A dynamic Prioritized Experience Replay
mechanism amplifies learning from rare but critical, high-risk events. At the
execution level, polynomial path planning coupled with
Proportional-Integral-Derivative (PID) and Stanley controllers translates these
targets into smooth, feasible trajectories, ensuring both accuracy and comfort.
We train and validate our approach on rich, real-world traffic datasets
encompassing diverse vehicles, cyclists, and pedestrians, and demonstrate that
it outperforms baseline methods in reducing ethical risk and maintaining
driving performance. To our knowledge, this is the first study of ethical
decision-making for autonomous vehicles via Safe RL evaluated on real-world,
human-mixed traffic scenarios. Our results highlight the potential of combining
formal control theory and data-driven learning to advance ethically accountable
autonomy that explicitly protects those most at risk in urban traffic
environments.","Dianzhao Li, Ostap Okhrin",2025-08-19 14:24:02+00:00,http://arxiv.org/abs/2508.14926v2,http://arxiv.org/pdf/2508.14926v2,"cs.LG, cs.AI, cs.RO"
A Screw Approach to the Approximation of the Local Geometry of the Configuration Space and of the set of Configurations of Certain Rank of Lower Pair Linkages,"A motion of a mechanism is a curve in its configuration space (c-space).
Singularities of the c-space are kinematic singularities of the mechanism. Any
mobility analysis of a particular mechanism amounts to investigating the
c-space geometry at a given configuration. A higher-order analysis is necessary
to determine the finite mobility. To this end, past research lead to approaches
using higher-order time derivatives of loop closure constraints assuming
(implicitly) that all possible motions are smooth. This continuity assumption
limits the generality of these methods. In this paper an approach to the
higher-order local mobility analysis of lower pair multi-loop linkages is
presented. This is based on a higher-order Taylor series expansion of the
geometric constraint mapping, for which a recursive algebraic expression in
terms of joint screws is presented. An exhaustive local analysis includes
analysis of the set of constraint singularities (configurations where the
constraint Jacobian has certain corank). A local approximation of the set of
configurations with certain rank is presented, along with an explicit
expression for the differentials of Jacobian minors in terms of instantaneous
joint screws. The c-space and the set of points of certain corank are therewith
locally approximated by an algebraic variety determined algebraically from the
mechanism's screw system. Results are shown for a simple planar 4-bar linkage,
which exhibits a bifurcation singularity, and for a planar three-loop linkage
exhibiting a cusp in c-space. The latter cannot be treated by the higher-order
local analysis methods proposed in the literature.",Andreas Mueller,2025-08-19 13:04:04+00:00,http://arxiv.org/abs/2508.13802v1,http://arxiv.org/pdf/2508.13802v1,"math.DG, cs.NA, cs.RO, math.NA"
Trajectory Tracking and Stabilization of Quadrotors Using Deep Koopman Model Predictive Control,"This paper presents a data-driven control framework for quadrotor systems
that integrates a deep Koopman operator with model predictive control (DK-MPC).
The deep Koopman operator is trained on sampled flight data to construct a
high-dimensional latent representation in which the nonlinear quadrotor
dynamics are approximated by linear models. This linearization enables the
application of MPC to efficiently optimize control actions over a finite
prediction horizon, ensuring accurate trajectory tracking and stabilization.
The proposed DK-MPC approach is validated through a series of
trajectory-following and point-stabilization numerical experiments, where it
demonstrates superior tracking accuracy and significantly lower computation
time compared to conventional nonlinear MPC. These results highlight the
potential of Koopman-based learning methods to handle complex quadrotor
dynamics while meeting the real-time requirements of embedded flight control.
Future work will focus on extending the framework to more agile flight
scenarios and improving robustness against external disturbances.",Haitham El-Hussieny,2025-08-19 12:54:56+00:00,http://arxiv.org/abs/2508.13795v1,http://arxiv.org/pdf/2508.13795v1,cs.RO
Blast Hole Seeking and Dipping -- The Navigation and Perception Framework in a Mine Site Inspection Robot,"In open-pit mining, holes are drilled into the surface of the excavation site
and detonated with explosives to facilitate digging. These blast holes need to
be inspected internally for investigation of downhole material types and
properties. Knowing these properties can lead to significant savings in
material handling costs in downstream processes. Manual hole inspection is slow
and expensive, with major limitations in revealing the geometric and geological
properties of the holes and their contents. This has been the motivation for
the development of our autonomous mine-site inspection robot - ""DIPPeR"". In
this paper, the automation aspect of the project is explained. We present a
robust blast hole seeking and detection framework that enables target-based
navigation and accurate down-hole sensor positioning. The pipeline first
processes point-cloud data collected by the on-board LiDAR sensors, extracting
the cone-shaped volume of drill-waste above the ground. By projecting the 3D
cone points into a virtual depth image, segmentation is achieved in the 2D
domain, yielding a circular hole at the image centre and a collared cone face.
We then identify the hole centre using a robust detection module while
suppressing non-maximum candidates, ensuring precise sensor placement for
down-hole inspection and avoiding collisions with the cavity wall. To enable
autonomous hole-seeking, the pipeline automatically adjusts its projection
parameters during robot navigation to account for variations in point sparsity
and hole opening size, ensuring a consistent hole appearance in 2D images. This
allows continuous tracking of the target hole as the robot approaches the goal
point. We demonstrate the effectiveness of our navigation and perception system
in both high-fidelity simulation environments and on-site field tests. A
demonstration video is available at
""https://www.youtube.com/watch?v=fRNbcBcaSqE"".","Liyang Liu, Ehsan Mihankhah, Nathan Wallace, Javier Martinez, Andrew J. Hill",2025-08-19 12:40:20+00:00,http://arxiv.org/abs/2508.13785v1,http://arxiv.org/pdf/2508.13785v1,cs.RO
MR6D: Benchmarking 6D Pose Estimation for Mobile Robots,"Existing 6D pose estimation datasets primarily focus on small household
objects typically handled by robot arm manipulators, limiting their relevance
to mobile robotics. Mobile platforms often operate without manipulators,
interact with larger objects, and face challenges such as long-range
perception, heavy self-occlusion, and diverse camera perspectives. While recent
models generalize well to unseen objects, evaluations remain confined to
household-like settings that overlook these factors. We introduce MR6D, a
dataset designed for 6D pose estimation for mobile robots in industrial
environments. It includes 92 real-world scenes featuring 16 unique objects
across static and dynamic interactions. MR6D captures the challenges specific
to mobile platforms, including distant viewpoints, varied object
configurations, larger object sizes, and complex occlusion/self-occlusion
patterns. Initial experiments reveal that current 6D pipelines underperform in
these settings, with 2D segmentation being another hurdle. MR6D establishes a
foundation for developing and evaluating pose estimation methods tailored to
the demands of mobile robotics. The dataset is available at
https://huggingface.co/datasets/anas-gouda/mr6d.","Anas Gouda, Shrutarv Awasthi, Christian Blesing, Lokeshwaran Manohar, Frank Hoffmann, Alice Kirchheim",2025-08-19 12:21:34+00:00,http://arxiv.org/abs/2508.13775v1,http://arxiv.org/pdf/2508.13775v1,"cs.CV, cs.RO"
Assessing Pedestrian Behavior Around Autonomous Cleaning Robots in Public Spaces: Findings from a Field Observation,"As autonomous robots become more common in public spaces, spontaneous
encounters with laypersons are more frequent. For this, robots need to be
equipped with communication strategies that enhance momentary transparency and
reduce the probability of critical situations. Adapting these robotic
strategies requires consideration of robot movements, environmental conditions,
and user characteristics and states. While numerous studies have investigated
the impact of distraction on pedestrians' movement behavior, limited research
has examined this behavior in the presence of autonomous robots. This research
addresses the impact of robot type and robot movement pattern on distracted and
undistracted pedestrians' movement behavior. In a field setting, unaware
pedestrians were videotaped while moving past two working, autonomous cleaning
robots. Out of N=498 observed pedestrians, approximately 8% were distracted by
smartphones. Distracted and undistracted pedestrians did not exhibit
significant differences in their movement behaviors around the robots. Instead,
both the larger sweeping robot and the offset rectangular movement pattern
significantly increased the number of lateral adaptations compared to the
smaller cleaning robot and the circular movement pattern. The offset
rectangular movement pattern also led to significantly more close lateral
adaptations. Depending on the robot type, the movement patterns led to
differences in the distances of lateral adaptations. The study provides initial
insights into pedestrian movement behavior around an autonomous cleaning robot
in public spaces, contributing to the growing field of HRI research.","Maren Raab, Linda Miller, Zhe Zeng, Pascal Jansen, Martin Baumann, Johannes Kraus",2025-08-19 10:05:08+00:00,http://arxiv.org/abs/2508.13699v1,http://arxiv.org/pdf/2508.13699v1,cs.RO
AutoMPC: A Code Generator for MPC-based Automated Driving,"Model Predictive Control (MPC) is a powerful technique to control nonlinear,
multi-input multi-output systems subject to input and state constraints. It is
now a standard tool for trajectory tracking control of automated vehicles. As
such it has been used in many research and development projects. However, MPC
faces several challenges to be integrated into industrial production vehicles.
The most important ones are its high computational demands and the complexity
of implementation. The software packages AutoMPC aims to address both of these
challenges. It builds on a robustified version of an active set algorithm for
Nonlinear MPC. The algorithm is embedded into a framework for vehicle
trajectory tracking, which makes it easy to used, yet highly customizable.
Automatic code generation transforms the selections into a standalone,
computationally efficient C-code file with static memory allocation. As such it
can be readily deployed on a wide range of embedded platforms, e.g., based on
Matlab/Simulink or Robot Operating System (ROS). Compared to a previous version
of the code, the vehicle model and the numerical integration method can be
manually specified, besides basic algorithm parameters. All of this information
and all specifications are directly baked into the generated C-code. The
algorithm is suitable driving scenarios at low or high speeds, even drifting,
and supports direction changes. Multiple simulation scenarios show the
versatility and effectiveness of the AutoMPC code, with the guarantee of a
feasible solution, a high degree of robustness, and computational efficiency.","Georg Schildbach, Jasper Pflughaupt",2025-08-19 09:04:43+00:00,http://arxiv.org/abs/2508.13656v1,http://arxiv.org/pdf/2508.13656v1,"eess.SY, cs.MS, cs.RO, cs.SY, 93-04"
The 9th AI City Challenge,"The ninth AI City Challenge continues to advance real-world applications of
computer vision and AI in transportation, industrial automation, and public
safety. The 2025 edition featured four tracks and saw a 17% increase in
participation, with 245 teams from 15 countries registered on the evaluation
server. Public release of challenge datasets led to over 30,000 downloads to
date. Track 1 focused on multi-class 3D multi-camera tracking, involving
people, humanoids, autonomous mobile robots, and forklifts, using detailed
calibration and 3D bounding box annotations. Track 2 tackled video question
answering in traffic safety, with multi-camera incident understanding enriched
by 3D gaze labels. Track 3 addressed fine-grained spatial reasoning in dynamic
warehouse environments, requiring AI systems to interpret RGB-D inputs and
answer spatial questions that combine perception, geometry, and language. Both
Track 1 and Track 3 datasets were generated in NVIDIA Omniverse. Track 4
emphasized efficient road object detection from fisheye cameras, supporting
lightweight, real-time deployment on edge devices. The evaluation framework
enforced submission limits and used a partially held-out test set to ensure
fair benchmarking. Final rankings were revealed after the competition
concluded, fostering reproducibility and mitigating overfitting. Several teams
achieved top-tier results, setting new benchmarks in multiple tasks.","Zheng Tang, Shuo Wang, David C. Anastasiu, Ming-Ching Chang, Anuj Sharma, Quan Kong, Norimasa Kobori, Munkhjargal Gochoo, Ganzorig Batnasan, Munkh-Erdene Otgonbold, Fady Alnajjar, Jun-Wei Hsieh, Tomasz Kornuta, Xiaolong Li, Yilin Zhao, Han Zhang, Subhashree Radhakrishnan, Arihant Jain, Ratnesh Kumar, Vidya N. Murali, Yuxing Wang, Sameer Satish Pusegaonkar, Yizhou Wang, Sujit Biswas, Xunlei Wu, Zhedong Zheng, Pranamesh Chakraborty, Rama Chellappa",2025-08-19 06:55:06+00:00,http://arxiv.org/abs/2508.13564v1,http://arxiv.org/pdf/2508.13564v1,"cs.CV, cs.AI, cs.LG, cs.RO"
MimicFunc: Imitating Tool Manipulation from a Single Human Video via Functional Correspondence,"Imitating tool manipulation from human videos offers an intuitive approach to
teaching robots, while also providing a promising and scalable alternative to
labor-intensive teleoperation data collection for visuomotor policy learning.
While humans can mimic tool manipulation behavior by observing others perform a
task just once and effortlessly transfer the skill to diverse tools for
functionally equivalent tasks, current robots struggle to achieve this level of
generalization. A key challenge lies in establishing function-level
correspondences, considering the significant geometric variations among
functionally similar tools, referred to as intra-function variations. To
address this challenge, we propose MimicFunc, a framework that establishes
functional correspondences with function frame, a function-centric local
coordinate frame constructed with keypoint-based abstraction, for imitating
tool manipulation skills. Experiments demonstrate that MimicFunc effectively
enables the robot to generalize the skill from a single RGB-D human video to
manipulating novel tools for functionally equivalent tasks. Furthermore,
leveraging MimicFunc's one-shot generalization capability, the generated
rollouts can be used to train visuomotor policies without requiring
labor-intensive teleoperation data collection for novel objects. Our code and
video are available at https://sites.google.com/view/mimicfunc.","Chao Tang, Anxing Xiao, Yuhong Deng, Tianrun Hu, Wenlong Dong, Hanbo Zhang, David Hsu, Hong Zhang",2025-08-19 05:49:47+00:00,http://arxiv.org/abs/2508.13534v1,http://arxiv.org/pdf/2508.13534v1,"cs.RO, cs.AI, cs.CV"
A Three-Level Whole-Body Disturbance Rejection Control Framework for Dynamic Motions in Legged Robots,"This paper presents a control framework designed to enhance the stability and
robustness of legged robots in the presence of uncertainties, including model
uncertainties, external disturbances, and faults. The framework enables the
full-state feedback estimator to estimate and compensate for uncertainties in
whole-body dynamics of the legged robots. First, we propose a novel moving
horizon extended state observer (MH-ESO) to estimate uncertainties and mitigate
noise in legged systems, which can be integrated into the framework for
disturbance compensation. Second, we introduce a three-level whole-body
disturbance rejection control framework (T-WB-DRC). Unlike the previous
two-level approach, this three-level framework considers both the plan based on
whole-body dynamics without uncertainties and the plan based on dynamics with
uncertainties, significantly improving payload transportation, external
disturbance rejection, and fault tolerance. Third, simulations of both humanoid
and quadruped robots in the Gazebo simulator demonstrate the effectiveness and
versatility of T-WB-DRC. Finally, extensive experimental trials on a quadruped
robot validate the robustness and stability of the system when using T-WB-DRC
under various disturbance conditions.","Bolin Li, Gewei Zuo, Zhixiang Wang, Xiaotian Ke, Lijun Zhu, Han Ding",2025-08-19 05:43:54+00:00,http://arxiv.org/abs/2508.13531v2,http://arxiv.org/pdf/2508.13531v2,cs.RO
Unified Hierarchical MPC in Task Executing for Modular Manipulators across Diverse Morphologies,"This work proposes a unified Hierarchical Model Predictive Control (H-MPC)
for modular manipulators across various morphologies, as the controller can
adapt to different configurations to execute the given task without extensive
parameter tuning in the controller. The H-MPC divides the control process into
two levels: a high-level MPC and a low-level MPC. The high-level MPC predicts
future states and provides trajectory information, while the low-level MPC
refines control actions by updating the predictive model based on this
high-level information. This hierarchical structure allows for the integration
of kinematic constraints and ensures smooth joint-space trajectories, even near
singular configurations. Moreover, the low-level MPC incorporates secondary
linearization by leveraging predictive information from the high-level MPC,
effectively capturing the second-order Taylor expansion information of the
kinematic model while still maintaining a linearized model formulation. This
approach not only preserves the simplicity of a linear control model but also
enhances the accuracy of the kinematic representation, thereby improving
overall control precision and reliability. To validate the effectiveness of the
control policy, we conduct extensive evaluations across different manipulator
morphologies and demonstrate the execution of pick-and-place tasks in
real-world scenarios.","Maolin Lei, Edoardo Romiti, Arturo Laurenzi, Cheng Zhou, Wanli Xing, Liang Lu, Nikos G. Tsagarakis",2025-08-19 04:59:54+00:00,http://arxiv.org/abs/2508.13513v1,http://arxiv.org/pdf/2508.13513v1,cs.RO
ROVER: Robust Loop Closure Verification with Trajectory Prior in Repetitive Environments,"Loop closure detection is important for simultaneous localization and mapping
(SLAM), which associates current observations with historical keyframes,
achieving drift correction and global relocalization. However, a falsely
detected loop can be fatal, and this is especially difficult in repetitive
environments where appearance-based features fail due to the high similarity.
Therefore, verification of a loop closure is a critical step in avoiding false
positive detections. Existing works in loop closure verification predominantly
focus on learning invariant appearance features, neglecting the prior knowledge
of the robot's spatial-temporal motion cue, i.e., trajectory. In this letter,
we propose ROVER, a loop closure verification method that leverages the
historical trajectory as a prior constraint to reject false loops in
challenging repetitive environments. For each loop candidate, it is first used
to estimate the robot trajectory with pose-graph optimization. This trajectory
is then submitted to a scoring scheme that assesses its compliance with the
trajectory without the loop, which we refer to as the trajectory prior, to
determine if the loop candidate should be accepted. Benchmark comparisons and
real-world experiments demonstrate the effectiveness of the proposed method.
Furthermore, we integrate ROVER into state-of-the-art SLAM systems to verify
its robustness and efficiency. Our source code and self-collected dataset are
available at https://github.com/jarvisyjw/ROVER.","Jingwen Yu, Jiayi Yang, Anjun Hu, Jiankun Wang, Ping Tan, Hong Zhang",2025-08-19 03:34:08+00:00,http://arxiv.org/abs/2508.13488v1,http://arxiv.org/pdf/2508.13488v1,"cs.RO, cs.CV"
"Multi-Robot Navigation in Social Mini-Games: Definitions, Taxonomy, and Algorithms","The ``Last Mile Challenge'' has long been considered an important, yet
unsolved, challenge for autonomous vehicles, public service robots, and
delivery robots. A central issue in this challenge is the ability of robots to
navigate constrained and cluttered environments that have high agency (e.g.,
doorways, hallways, corridor intersections), often while competing for space
with other robots and humans. We refer to these environments as ``Social
Mini-Games'' (SMGs). Traditional navigation approaches designed for MRN do not
perform well in SMGs, which has led to focused research on dedicated SMG
solvers. However, publications on SMG navigation research make different
assumptions (on centralized versus decentralized, observability, communication,
cooperation, etc.), and have different objective functions (safety versus
liveness). These assumptions and objectives are sometimes implicitly assumed or
described informally. This makes it difficult to establish appropriate
baselines for comparison in research papers, as well as making it difficult for
practitioners to find the papers relevant to their concrete application. Such
ad-hoc representation of the field also presents a barrier to new researchers
wanting to start research in this area. SMG navigation research requires its
own taxonomy, definitions, and evaluation protocols to guide effective research
moving forward. This survey is the first to catalog SMG solvers using a
well-defined and unified taxonomy and to classify existing methods accordingly.
It also discusses the essential properties of SMG solvers, defines what SMGs
are and how they appear in practice, outlines how to evaluate SMG solvers, and
highlights the differences between SMG solvers and general navigation systems.
The survey concludes with an overview of future directions and open challenges
in the field.","Rohan Chandra, Shubham Singh, Abhishek Jha, Dannon Andrade, Hriday Sainathuni, Katia Sycara",2025-08-19 02:33:15+00:00,http://arxiv.org/abs/2508.13459v2,http://arxiv.org/pdf/2508.13459v2,"cs.RO, cs.MA"
