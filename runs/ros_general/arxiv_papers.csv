title,summary,authors,published,updated,url,pdf_url,matched_query,year
"A Standing Support Mobility Robot for Enhancing Independence in Elderly
  Daily Living","This paper presents a standing support mobility robot ""Moby"" developed to
enhance independence and safety for elderly individuals during daily activities
such as toilet transfers. Unlike conventional seated mobility aids, the robot
maintains users in an upright posture, reducing physical strain, supporting
natural social interaction at eye level, and fostering a greater sense of
self-efficacy. Moby offers a novel alternative by functioning both passively
and with mobility support, enabling users to perform daily tasks more
independently. Its main advantages include ease of use, lightweight design,
comfort, versatility, and effective sit-to-stand assistance. The robot
leverages the Robot Operating System (ROS) for seamless control, featuring
manual and autonomous operation modes. A custom control system enables safe and
intuitive interaction, while the integration with NAV2 and LiDAR allows for
robust navigation capabilities. This paper reviews existing mobility solutions
and compares them to Moby, details the robot's design, and presents objective
and subjective experimental results using the NASA-TLX method and time
comparisons to other methods to validate our design criteria and demonstrate
the advantages of our contribution.","Ricardo J. Manríquez-Cisterna, Ankit A. Ravankar, Jose V. Salazar Luces, Takuro Hatsukari, Yasuhisa Hirata",2025-08-27T12:02:02Z,2025-08-27T12:02:02Z,http://arxiv.org/abs/2508.19816v1,http://arxiv.org/pdf/2508.19816v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2025
"SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and
  Manipulation","Event cameras offer microsecond latency, high dynamic range, and low power
consumption, making them ideal for real-time robotic perception under
challenging conditions such as motion blur, occlusion, and illumination
changes. However, despite their advantages, synthetic event-based vision
remains largely unexplored in mainstream robotics simulators. This lack of
simulation setup hinders the evaluation of event-driven approaches for robotic
manipulation and navigation tasks. This work presents an open-source,
user-friendly v2e robotics operating system (ROS) package for Gazebo simulation
that enables seamless event stream generation from RGB camera feeds. The
package is used to investigate event-based robotic policies (ERP) for real-time
navigation and manipulation. Two representative scenarios are evaluated: (1)
object following with a mobile robot and (2) object detection and grasping with
a robotic manipulator. Transformer-based ERPs are trained by behavior cloning
and compared to RGB-based counterparts under various operating conditions.
Experimental results show that event-guided policies consistently deliver
competitive advantages. The results highlight the potential of event-driven
perception to improve real-time robotic navigation and manipulation, providing
a foundation for broader integration of event cameras into robotic policy
learning. The GitHub repo for the dataset and code:
https://eventbasedvision.github.io/SEBVS/","Krishna Vinod, Prithvi Jai Ramesh, Pavan Kumar B N, Bharatesh Chakravarthi",2025-08-25T04:14:04Z,2025-08-25T04:14:04Z,http://arxiv.org/abs/2508.17643v1,http://arxiv.org/pdf/2508.17643v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2025
SLAM-based Safe Indoor Exploration Strategy,"This paper suggests a 2D exploration strategy for a planar space cluttered
with obstacles. Rather than using point robots capable of adjusting their
position and altitude instantly, this research is tailored to classical agents
with circular footprints that cannot control instantly their pose. Inhere, a
self-balanced dual-wheeled differential drive system is used to explore the
place. The system is equipped with linear accelerometers and angular
gyroscopes, a 3D-LiDAR, and a forward-facing RGB-D camera. The system performs
RTAB-SLAM using the IMU and the LiDAR, while the camera is used for loop
closures. The mobile agent explores the planar space using a safe skeleton
approach that places the agent as far as possible from the static obstacles.
During the exploration strategy, the heading is towards any offered openings of
the space. This space exploration strategy has as its highest priority the
agent's safety in avoiding the obstacles followed by the exploration of
undetected space. Experimental studies with a ROS-enabled mobile agent are
presented indicating the path planning strategy while exploring the space.","Omar Mostafa, Nikolaos Evangeliou, Anthony Tzes",2025-08-19T19:50:24Z,2025-08-19T19:50:24Z,http://arxiv.org/abs/2508.14235v1,http://arxiv.org/pdf/2508.14235v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2025
"Hierarchical Deep Deterministic Policy Gradient for Autonomous Maze
  Navigation of Mobile Robots","Maze navigation is a fundamental challenge in robotics, requiring agents to
traverse complex environments efficiently. While the Deep Deterministic Policy
Gradient (DDPG) algorithm excels in control tasks, its performance in maze
navigation suffers from sparse rewards, inefficient exploration, and
long-horizon planning difficulties, often leading to low success rates and
average rewards, sometimes even failing to achieve effective navigation. To
address these limitations, this paper proposes an efficient Hierarchical DDPG
(HDDPG) algorithm, which includes high-level and low-level policies. The
high-level policy employs an advanced DDPG framework to generate intermediate
subgoals from a long-term perspective and on a higher temporal scale. The
low-level policy, also powered by the improved DDPG algorithm, generates
primitive actions by observing current states and following the subgoal
assigned by the high-level policy. The proposed method enhances stability with
off-policy correction, refining subgoal assignments by relabeling historical
experiences. Additionally, adaptive parameter space noise is utilized to
improve exploration, and a reshaped intrinsic-extrinsic reward function is
employed to boost learning efficiency. Further optimizations, including
gradient clipping and Xavier initialization, are employed to improve
robustness. The proposed algorithm is rigorously evaluated through numerical
simulation experiments executed using the Robot Operating System (ROS) and
Gazebo. Regarding the three distinct final targets in autonomous maze
navigation tasks, HDDPG significantly overcomes the limitations of standard
DDPG and its variants, improving the success rate by at least 56.59% and
boosting the average reward by a minimum of 519.03 compared to baseline
algorithms.","Wenjie Hu, Ye Zhou, Hann Woei Ho",2025-08-07T03:06:22Z,2025-08-07T03:06:22Z,http://arxiv.org/abs/2508.04994v1,http://arxiv.org/pdf/2508.04994v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2025
"HuNavSim 2.0: An Enhanced Human Navigation Simulator for Human-Aware
  Robot Navigation","This work presents a new iteration of the Human Navigation Simulator
(HuNavSim), a novel open-source tool for the simulation of different
human-agent navigation behaviors in scenarios with mobile robots. The tool,
programmed under the ROS 2 framework, can be used together with different
well-known robotics simulators such as Gazebo or NVidia Isaac Sim. The main
goal is to facilitate the development and evaluation of human-aware robot
navigation systems in simulation. In this new version, several features have
been improved and new ones added, such as the extended set of actions and
conditions that can be combined in Behavior Trees to compound complex and
realistic human behaviors.","Miguel Escudero-Jiménez, Noé Pérez-Higueras, Andrés Martínez-Silva, Fernando Caballero, Luis Merino",2025-07-23T08:31:35Z,2025-07-25T07:41:24Z,http://arxiv.org/abs/2507.17317v2,http://arxiv.org/pdf/2507.17317v2.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2025
"CoNav Chair: Development and Evaluation of a Shared Control based
  Wheelchair for the Built Environment","As the global population of people with disabilities (PWD) continues to grow,
so will the need for mobility solutions that promote independent living and
social integration. Wheelchairs are vital for the mobility of PWD in both
indoor and outdoor environments. The current SOTA in powered wheelchairs is
based on either manually controlled or fully autonomous modes of operation,
offering limited flexibility and often proving difficult to navigate in
spatially constrained environments. Moreover, research on robotic wheelchairs
has focused predominantly on complete autonomy or improved manual control;
approaches that can compromise efficiency and user trust. To overcome these
challenges, this paper introduces the CoNav Chair, a smart wheelchair based on
the Robot Operating System (ROS) and featuring shared control navigation and
obstacle avoidance capabilities that are intended to enhance navigational
efficiency, safety, and ease of use for the user. The paper outlines the CoNav
Chair's design and presents a preliminary usability evaluation comparing three
distinct navigation modes, namely, manual, shared, and fully autonomous,
conducted with 21 healthy, unimpaired participants traversing an indoor
building environment. Study findings indicated that the shared control
navigation framework had significantly fewer collisions and performed
comparably, if not superior to the autonomous and manual modes, on task
completion time, trajectory length, and smoothness; and was perceived as being
safer and more efficient based on user reported subjective assessments of
usability. Overall, the CoNav system demonstrated acceptable safety and
performance, laying the foundation for subsequent usability testing with end
users, namely, PWDs who rely on a powered wheelchair for mobility.","Yifan Xu, Qianwei Wang, Jordan Lillie, Vineet Kamat, Carol Menassa, Clive D'Souza",2025-07-15T20:34:42Z,2025-07-15T20:34:42Z,http://arxiv.org/abs/2507.11716v1,http://arxiv.org/pdf/2507.11716v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2025
"Imitation Learning for Obstacle Avoidance Using End-to-End CNN-Based
  Sensor Fusion","Obstacle avoidance is crucial for mobile robots' navigation in both known and
unknown environments. This research designs, trains, and tests two custom
Convolutional Neural Networks (CNNs), using color and depth images from a depth
camera as inputs. Both networks adopt sensor fusion to produce an output: the
mobile robot's angular velocity, which serves as the robot's steering command.
A newly obtained visual dataset for navigation was collected in diverse
environments with varying lighting conditions and dynamic obstacles. During
data collection, a communication link was established over Wi-Fi between a
remote server and the robot, using Robot Operating System (ROS) topics.
Velocity commands were transmitted from the server to the robot, enabling
synchronized recording of visual data and the corresponding steering commands.
Various evaluation metrics, such as Mean Squared Error, Variance Score, and
Feed-Forward time, provided a clear comparison between the two networks and
clarified which one to use for the application.","Lamiaa H. Zain, Hossam H. Ammar, Raafat E. Shalaby",2025-07-10T18:53:44Z,2025-07-10T18:53:44Z,http://arxiv.org/abs/2507.08112v1,http://arxiv.org/pdf/2507.08112v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2025
Ark: An Open-source Python-based Framework for Robot Learning,"Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics
Challenges to the first humanoid-robot kickboxing tournament-yet commercial
autonomy still lags behind progress in machine learning. A major bottleneck is
software: current robot stacks demand steep learning curves, low-level C/C++
expertise, fragmented tooling, and intricate hardware integration, in stark
contrast to the Python-centric, well-documented ecosystems that propelled
modern AI. We introduce ARK, an open-source, Python-first robotics framework
designed to close that gap. ARK presents a Gym-style environment interface that
allows users to collect data, preprocess it, and train policies using
state-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy)
while seamlessly toggling between high-fidelity simulation and physical robots.
A lightweight client-server architecture provides networked
publisher-subscriber communication, and optional C/C++ bindings ensure
real-time performance when needed. ARK ships with reusable modules for control,
SLAM, motion planning, system identification, and visualization, along with
native ROS interoperability. Comprehensive documentation and case studies-from
manipulation to mobile navigation-demonstrate rapid prototyping, effortless
hardware swapping, and end-to-end pipelines that rival the convenience of
mainstream machine-learning workflows. By unifying robotics and AI practices
under a common Python umbrella, ARK lowers entry barriers and accelerates
research and commercial deployment of autonomous robots.","Magnus Dierking, Christopher E. Mower, Sarthak Das, Huang Helong, Jiacheng Qiu, Cody Reading, Wei Chen, Huidong Liang, Huang Guowei, Jan Peters, Quan Xingyue, Jun Wang, Haitham Bou-Ammar",2025-06-24T20:23:39Z,2025-07-14T17:46:29Z,http://arxiv.org/abs/2506.21628v2,http://arxiv.org/pdf/2506.21628v2.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2025
Real-Time Initialization of Unknown Anchors for UWB-aided Navigation,"This paper presents a framework for the real-time initialization of unknown
Ultra-Wideband (UWB) anchors in UWB-aided navigation systems. The method is
designed for localization solutions where UWB modules act as supplementary
sensors. Our approach enables the automatic detection and calibration of
previously unknown anchors during operation, removing the need for manual
setup. By combining an online Positional Dilution of Precision (PDOP)
estimation, a lightweight outlier detection method, and an adaptive robust
kernel for non-linear optimization, our approach significantly improves
robustness and suitability for real-world applications compared to
state-of-the-art. In particular, we show that our metric which triggers an
initialization decision is more conservative than current ones commonly based
on initial linear or non-linear initialization guesses. This allows for better
initialization geometry and subsequently lower initialization errors. We
demonstrate the proposed approach on two different mobile robots: an autonomous
forklift and a quadcopter equipped with a UWB-aided Visual-Inertial Odometry
(VIO) framework. The results highlight the effectiveness of the proposed method
with robust initialization and low positioning error. We open-source our code
in a C++ library including a ROS wrapper.","Giulio Delama, Igor Borowski, Roland Jung, Stephan Weiss",2025-06-18T14:54:09Z,2025-06-18T14:54:09Z,http://arxiv.org/abs/2506.15518v1,http://arxiv.org/pdf/2506.15518v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2025
VAULT: A Mobile Mapping System for ROS 2-based Autonomous Robots,"Localization plays a crucial role in the navigation capabilities of
autonomous robots, and while indoor environments can rely on wheel odometry and
2D LiDAR-based mapping, outdoor settings such as agriculture and forestry,
present unique challenges that necessitate real-time localization and
consistent mapping. Addressing this need, this paper introduces the VAULT
prototype, a ROS 2-based mobile mapping system (MMS) that combines various
sensors to enable robust outdoor and indoor localization. The proposed solution
harnesses the power of Global Navigation Satellite System (GNSS) data,
visual-inertial odometry (VIO), inertial measurement unit (IMU) data, and the
Extended Kalman Filter (EKF) to generate reliable 3D odometry. To further
enhance the localization accuracy, Visual SLAM (VSLAM) is employed, resulting
in the creation of a comprehensive 3D point cloud map. By leveraging these
sensor technologies and advanced algorithms, the prototype offers a
comprehensive solution for outdoor localization in autonomous mobile robots,
enabling them to navigate and map their surroundings with confidence and
precision.","Miguel Á. González-Santamarta, Francisco J. Rodríguez-Lera, Vicente Matellán-Olivera",2025-06-11T10:26:32Z,2025-06-11T10:26:32Z,http://arxiv.org/abs/2506.09583v1,http://arxiv.org/pdf/2506.09583v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2025
"ROS-related Robotic Systems Development with V-model-based Application
  of MeROS Metamodel","Systems built on the Robot Operating System (ROS) are increasingly easy to
assemble, yet hard to govern and reliably coordinate. Beyond the sheer number
of subsystems involved, the difficulty stems from their diversity and
interaction depth. In this paper, we use a compact heterogeneous robotic system
(HeROS), combining mobile and manipulation capabilities, as a demonstration
vehicle under dynamically changing tasks. Notably, all its subsystems are
powered by ROS.
  The use of compatible interfaces and other ROS integration capabilities
simplifies the construction of such systems. However, this only addresses part
of the complexity: the semantic coherence and structural traceability are even
more important for precise coordination and call for deliberate engineering
methods. The Model-Based Systems Engineering (MBSE) discipline, which emerged
from the experience of complexity management in large-scale engineering
domains, offers the methodological foundations needed.
  Despite their strengths in complementary aspects of robotics systems
engineering, the lack of a unified approach to integrate ROS and MBSE hinders
the full potential of these tools. Motivated by the anticipated impact of such
a synergy in robotics practice, we propose a structured methodology based on
MeROS - a SysML metamodel created specifically to put the ROS-based systems
into the focus of the MBSE workflow. As its methodological backbone, we adapt
the well-known V-model to this context, illustrating how complex robotic
systems can be designed with traceability and validation capabilities embedded
into their lifecycle using practices familiar to engineering teams.","Tomasz Winiarski, Jan Kaniuka, Daniel Giełdowski, Jakub Ostrysz, Krystian Radlak, Dmytro Kushnir",2025-06-10T11:44:00Z,2025-08-22T17:50:24Z,http://arxiv.org/abs/2506.08706v2,http://arxiv.org/pdf/2506.08706v2.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2025
"Reducing Latency in LLM-Based Natural Language Commands Processing for
  Robot Navigation","The integration of Large Language Models (LLMs), such as GPT, in industrial
robotics enhances operational efficiency and human-robot collaboration.
However, the computational complexity and size of these models often provide
latency problems in request and response times. This study explores the
integration of the ChatGPT natural language model with the Robot Operating
System 2 (ROS 2) to mitigate interaction latency and improve robotic system
control within a simulated Gazebo environment. We present an architecture that
integrates these technologies without requiring a middleware transport
platform, detailing how a simulated mobile robot responds to text and voice
commands. Experimental results demonstrate that this integration improves
execution speed, usability, and accessibility of the human-robot interaction by
decreasing the communication latency by 7.01\% on average. Such improvements
facilitate smoother, real-time robot operations, which are crucial for
industrial automation and precision tasks.","Diego Pollini, Bruna V. Guterres, Rodrigo S. Guerra, Ricardo B. Grando",2025-05-29T21:16:14Z,2025-05-29T21:16:14Z,http://arxiv.org/abs/2506.00075v1,http://arxiv.org/pdf/2506.00075v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2025
"Capability-Driven Skill Generation with LLMs: A RAG-Based Approach for
  Reusing Existing Libraries and Interfaces","Modern automation systems increasingly rely on modular architectures, with
capabilities and skills as one solution approach. Capabilities define the
functions of resources in a machine-readable form and skills provide the
concrete implementations that realize those capabilities. However, the
development of a skill implementation conforming to a corresponding capability
remains a time-consuming and challenging task. In this paper, we present a
method that treats capabilities as contracts for skill implementations and
leverages large language models to generate executable code based on natural
language user input. A key feature of our approach is the integration of
existing software libraries and interface technologies, enabling the generation
of skill implementations across different target languages. We introduce a
framework that allows users to incorporate their own libraries and resource
interfaces into the code generation process through a retrieval-augmented
generation architecture. The proposed method is evaluated using an autonomous
mobile robot controlled via Python and ROS 2, demonstrating the feasibility and
flexibility of the approach.","Luis Miguel Vieira da Silva, Aljosha Köcher, Nicolas König, Felix Gehlhoff, Alexander Fay",2025-05-06T08:27:04Z,2025-05-06T08:27:04Z,http://arxiv.org/abs/2505.03295v1,http://arxiv.org/pdf/2505.03295v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2025
"SimPRIVE: a Simulation framework for Physical Robot Interaction with
  Virtual Environments","The use of machine learning in cyber-physical systems has attracted the
interest of both industry and academia. However, no general solution has yet
been found against the unpredictable behavior of neural networks and
reinforcement learning agents. Nevertheless, the improvements of
photo-realistic simulators have paved the way towards extensive testing of
complex algorithms in different virtual scenarios, which would be expensive and
dangerous to implement in the real world.
  This paper presents SimPRIVE, a simulation framework for physical robot
interaction with virtual environments, which operates as a vehicle-in-the-loop
platform, rendering a virtual world while operating the vehicle in the real
world.
  Using SimPRIVE, any physical mobile robot running on ROS 2 can easily be
configured to move its digital twin in a virtual world built with the Unreal
Engine 5 graphic engine, which can be populated with objects, people, or other
vehicles with programmable behavior.
  SimPRIVE has been designed to accommodate custom or pre-built virtual worlds
while being light-weight to contain execution times and allow fast rendering.
Its main advantage lies in the possibility of testing complex algorithms on the
full software and hardware stack while minimizing the risks and costs of a test
campaign. The framework has been validated by testing a reinforcement learning
agent trained for obstacle avoidance on an AgileX Scout Mini rover that
navigates a virtual office environment where everyday objects and people are
placed as obstacles. The physical rover moves with no collision in an indoor
limited space, thanks to a LiDAR-based heuristic.","Federico Nesti, Gianluca D'Amico, Mauro Marinoni, Giorgio Buttazzo",2025-04-30T09:22:55Z,2025-04-30T09:22:55Z,http://arxiv.org/abs/2504.21454v1,http://arxiv.org/pdf/2504.21454v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2025
"Lightweight LiDAR-Camera 3D Dynamic Object Detection and Multi-Class
  Trajectory Prediction","Service mobile robots are often required to avoid dynamic objects while
performing their tasks, but they usually have only limited computational
resources. So we present a lightweight multi-modal framework for 3D object
detection and trajectory prediction. Our system synergistically integrates
LiDAR and camera inputs to achieve real-time perception of pedestrians,
vehicles, and riders in 3D space. The framework proposes two novel modules: 1)
a Cross-Modal Deformable Transformer (CMDT) for object detection with high
accuracy and acceptable amount of computation, and 2) a Reference
Trajectory-based Multi-Class Transformer (RTMCT) for efficient and diverse
trajectory prediction of mult-class objects with flexible trajectory lengths.
Evaluations on the CODa benchmark demonstrate superior performance over
existing methods across detection (+2.03% in mAP) and trajectory prediction
(-0.408m in minADE5 of pedestrians) metrics. Remarkably, the system exhibits
exceptional deployability - when implemented on a wheelchair robot with an
entry-level NVIDIA 3060 GPU, it achieves real-time inference at 13.2 fps. To
facilitate reproducibility and practical deployment, we release the related
code of the method at https://github.com/TossherO/3D_Perception and its ROS
inference version at https://github.com/TossherO/ros_packages.","Yushen He, Lei Zhao, Tianchen Deng, Zipeng Fang, Weidong Chen",2025-04-18T11:59:34Z,2025-04-18T11:59:34Z,http://arxiv.org/abs/2504.13647v1,http://arxiv.org/pdf/2504.13647v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2025
"Strengthening Multi-Robot Systems for SAR: Co-Designing Robotics and
  Communication Towards 6G","This paper presents field-tested use cases from Search and Rescue (SAR)
missions, highlighting the co-design of mobile robots and communication systems
to support Edge-Cloud architectures based on 5G Standalone (SA). The main goal
is to contribute to the effective cooperation of multiple robots and first
responders. Our field experience includes the development of Hybrid Wireless
Sensor Networks (H-WSNs) for risk and victim detection, smartphones integrated
into the Robot Operating System (ROS) as Edge devices for mission requests and
path planning, real-time Simultaneous Localization and Mapping (SLAM) via
Multi-Access Edge Computing (MEC), and implementation of Uncrewed Ground
Vehicles (UGVs) for victim evacuation in different navigation modes. These
experiments, conducted in collaboration with actual first responders,
underscore the need for intelligent network resource management, balancing
low-latency and high-bandwidth demands. Network slicing is key to ensuring
critical emergency services are performed despite challenging communication
conditions. The paper identifies architectural needs, lessons learned, and
challenges to be addressed by 6G technologies to enhance emergency response
capabilities.","Juan Bravo-Arrabal, Ricardo Vázquez-Martín, J. J. Fernández-Lozano, Alfonso García-Cerezo",2025-04-02T17:47:11Z,2025-04-02T17:47:11Z,http://arxiv.org/abs/2504.01940v1,http://arxiv.org/pdf/2504.01940v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2025
"Energy-aware Joint Orchestration of 5G and Robots: Experimental Testbed
  and Field Validation","5G mobile networks introduce a new dimension for connecting and operating
mobile robots in outdoor environments, leveraging cloud-native and offloading
features of 5G networks to enable fully flexible and collaborative cloud robot
operations. However, the limited battery life of robots remains a significant
obstacle to their effective adoption in real-world exploration scenarios. This
paper explores, via field experiments, the potential energy-saving gains of
OROS, a joint orchestration of 5G and Robot Operating System (ROS) that
coordinates multiple 5G-connected robots both in terms of navigation and
sensing, as well as optimizes their cloud-native service resource utilization
while minimizing total resource and energy consumption on the robots based on
real-time feedback. We designed, implemented and evaluated our proposed OROS in
an experimental testbed composed of commercial off-the-shelf robots and a local
5G infrastructure deployed on a campus. The experimental results demonstrated
that OROS significantly outperforms state-of-the-art approaches in terms of
energy savings by offloading demanding computational tasks to the 5G edge
infrastructure and dynamic energy management of on-board sensors (e.g.,
switching them off when they are not needed). This strategy achieves
approximately 15% energy savings on the robots, thereby extending battery life,
which in turn allows for longer operating times and better resource
utilization.","Milan Groshev, Lanfranco Zanzi, Carmen Delgado, Xi Li, Antonio de la Oliva, Xavier Costa-Perez",2025-03-25T12:54:25Z,2025-03-25T12:54:25Z,http://arxiv.org/abs/2503.19613v1,http://arxiv.org/pdf/2503.19613v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2025
Planning and Control for Deformable Linear Object Manipulation,"Manipulating a deformable linear object (DLO) such as wire, cable, and rope
is a common yet challenging task due to their high degrees of freedom and
complex deformation behaviors, especially in an environment with obstacles.
Existing local control methods are efficient but prone to failure in complex
scenarios, while precise global planners are computationally intensive and
difficult to deploy. This paper presents an efficient, easy-to-deploy framework
for collision-free DLO manipulation using mobile manipulators. We demonstrate
the effectiveness of leveraging standard planning tools for high-dimensional
DLO manipulation without requiring custom planners or extensive data-driven
models. Our approach combines an off-the-shelf global planner with a real-time
local controller. The global planner approximates the DLO as a series of rigid
links connected by spherical joints, enabling rapid path planning without the
need for problem-specific planners or large datasets. The local controller
employs control barrier functions (CBFs) to enforce safety constraints,
maintain the DLO integrity, prevent overstress, and handle obstacle avoidance.
It compensates for modeling inaccuracies by using a state-of-the-art
position-based dynamics technique that approximates physical properties like
Young's and shear moduli. We validate our framework through extensive
simulations and real-world demonstrations. In complex obstacle
scenarios-including tent pole transport, corridor navigation, and tasks
requiring varied stiffness-our method achieves a 100% success rate over
thousands of trials, with significantly reduced planning times compared to
state-of-the-art techniques. Real-world experiments include transportation of a
tent pole and a rope using mobile manipulators. We share our ROS-based
implementation to facilitate adoption in various applications.","Burak Aksoy, John Wen",2025-03-06T01:44:36Z,2025-03-06T01:44:36Z,http://arxiv.org/abs/2503.04007v1,http://arxiv.org/pdf/2503.04007v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2025
Equivariant Filter Design for Range-only SLAM,"Range-only Simultaneous Localisation and Mapping (RO-SLAM) is of interest due
to its practical applications in ultra-wideband (UWB) and Bluetooth Low Energy
(BLE) localisation in terrestrial and aerial applications and acoustic beacon
localisation in submarine applications. In this work, we consider a mobile
robot equipped with an inertial measurement unit (IMU) and a range sensor that
measures distances to a collection of fixed landmarks. We derive an equivariant
filter (EqF) for the RO-SLAM problem based on a symmetry Lie group that is
compatible with the range measurements. The proposed filter does not require
bootstrapping or initialisation of landmark positions, and demonstrates
robustness to the no-prior situation. The filter is demonstrated on a
real-world dataset, and it is shown to significantly outperform a
state-of-the-art EKF alternative in terms of both accuracy and robustness.","Yixiao Ge, Arthur Pearce, Pieter van Goor, Robert Mahony",2025-03-05T23:48:32Z,2025-03-05T23:48:32Z,http://arxiv.org/abs/2503.03973v1,http://arxiv.org/pdf/2503.03973v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2025
"RobotIQ: Empowering Mobile Robots with Human-Level Planning for
  Real-World Execution","This paper introduces RobotIQ, a framework that empowers mobile robots with
human-level planning capabilities, enabling seamless communication via natural
language instructions through any Large Language Model. The proposed framework
is designed in the ROS architecture and aims to bridge the gap between humans
and robots, enabling robots to comprehend and execute user-expressed text or
voice commands. Our research encompasses a wide spectrum of robotic tasks,
ranging from fundamental logical, mathematical, and learning reasoning for
transferring knowledge in domains like navigation, manipulation, and object
localization, enabling the application of learned behaviors from simulated
environments to real-world operations. All encapsulated within a modular
crafted robot library suite of API-wise control functions, RobotIQ offers a
fully functional AI-ROS-based toolset that allows researchers to design and
develop their own robotic actions tailored to specific applications and robot
configurations. The effectiveness of the proposed system was tested and
validated both in simulated and real-world experiments focusing on a home
service scenario that included an assistive application designed for elderly
people. RobotIQ with an open-source, easy-to-use, and adaptable robotic library
suite for any robot can be found at https://github.com/emmarapt/RobotIQ.","Emmanuel K. Raptis, Athanasios Ch. Kapoutsis, Elias B. Kosmatopoulos",2025-02-18T13:49:28Z,2025-02-18T13:49:28Z,http://arxiv.org/abs/2502.12862v1,http://arxiv.org/pdf/2502.12862v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2025
"CoNav Chair: Design of a ROS-based Smart Wheelchair for Shared Control
  Navigation in the Built Environment","With the number of people with disabilities (PWD) increasing worldwide each
year, the demand for mobility support to enable independent living and social
integration is also growing. Wheelchairs commonly support the mobility of PWD
in both indoor and outdoor environments. However, current powered wheelchairs
(PWC) often fail to meet the needs of PWD, who may find it difficult to operate
them. Furthermore, existing research on robotic wheelchairs typically focuses
either on full autonomy or enhanced manual control, which can lead to reduced
efficiency and user trust. To address these issues, this paper proposes a Robot
Operating System (ROS)-based smart wheelchair, called CoNav Chair, that
incorporates a shared control navigation algorithm and obstacle avoidance to
support PWD while fostering efficiency and trust between the robot and the
user. Our design consists of hardware and software components. Experimental
results conducted in a typical indoor social environment demonstrate the
performance and effectiveness of the smart wheelchair hardware and software
design. This integrated design promotes trust and autonomy, which are crucial
for the acceptance of assistive mobility technologies in the built environment.","Yifan Xu, Qianwei Wang, Jordan Lillie, Vineet Kamat, Carol Menassa",2025-01-16T17:31:27Z,2025-01-16T17:31:27Z,http://arxiv.org/abs/2501.09680v1,http://arxiv.org/pdf/2501.09680v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2025
"Comparison of Various SLAM Systems for Mobile Robot in an Indoor
  Environment","This article presents a comparative analysis of a mobile robot trajectories
computed by various ROS-based SLAM systems. For this reason we developed a
prototype of a mobile robot with common sensors: 2D lidar, a monocular and ZED
stereo cameras. Then we conducted experiments in a typical office environment
and collected data from all sensors, running all tested SLAM systems based on
the acquired dataset. We studied the following SLAM systems: (a) 2D
lidar-based: GMapping, Hector SLAM, Cartographer; (b) monocular camera-based:
Large Scale Direct monocular SLAM (LSD SLAM), ORB SLAM, Direct Sparse Odometry
(DSO); and (c) stereo camera-based: ZEDfu, Real-Time Appearance-Based Mapping
(RTAB map), ORB SLAM, Stereo Parallel Tracking and Mapping (S-PTAM). Since all
SLAM methods were tested on the same dataset we compared results for different
SLAM systems with appropriate metrics, demonstrating encouraging results for
lidar-based Cartographer SLAM, Monocular ORB SLAM and Stereo RTAB Map methods.","Maksim Filipenko, Ilya Afanasyev",2025-01-16T12:01:44Z,2025-01-16T12:01:44Z,http://arxiv.org/abs/2501.09490v1,http://arxiv.org/pdf/2501.09490v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2025
"Towards Probabilistic Inference of Human Motor Intentions by Assistive
  Mobile Robots Controlled via a Brain-Computer Interface","Assistive mobile robots are a transformative technology that helps persons
with disabilities regain the ability to move freely. Although autonomous
wheelchairs significantly reduce user effort, they still require human input to
allow users to maintain control and adapt to changing environments. Brain
Computer Interface (BCI) stands out as a highly user-friendly option that does
not require physical movement. Current BCI systems can understand whether users
want to accelerate or decelerate, but they implement these changes in discrete
speed steps rather than allowing for smooth, continuous velocity adjustments.
This limitation prevents the systems from mimicking the natural, fluid speed
changes seen in human self-paced motion. The authors aim to address this
limitation by redesigning the perception-action cycle in a BCI controlled
robotic system: improving how the robotic agent interprets the user's motion
intentions (world state) and implementing these actions in a way that better
reflects natural physical properties of motion, such as inertia and damping.
The scope of this paper focuses on the perception aspect. We asked and answered
a normative question ""what computation should the robotic agent carry out to
optimally perceive incomplete or noisy sensory observations?"" Empirical EEG
data were collected, and probabilistic representation that served as world
state distributions were learned and evaluated in a Generative Adversarial
Network framework. The ROS framework was established that connected with a
Gazebo environment containing a digital twin of an indoor space and a virtual
model of a robotic wheelchair. Signal processing and statistical analyses were
implemented to identity the most discriminative features in the
spatial-spectral-temporal dimensions, which are then used to construct the
world model for the robotic agent to interpret user motion intentions as a
Bayesian observer.","Xiaoshan Zhou, Carol M. Menassa, Vineet R. Kamat",2025-01-09T23:18:38Z,2025-01-09T23:18:38Z,http://arxiv.org/abs/2501.05610v1,http://arxiv.org/pdf/2501.05610v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2025
"Sim-to-Real Transfer for Mobile Robots with Reinforcement Learning: from
  NVIDIA Isaac Sim to Gazebo and Real ROS 2 Robots","Unprecedented agility and dexterous manipulation have been demonstrated with
controllers based on deep reinforcement learning (RL), with a significant
impact on legged and humanoid robots. Modern tooling and simulation platforms,
such as NVIDIA Isaac Sim, have been enabling such advances. This article
focuses on demonstrating the applications of Isaac in local planning and
obstacle avoidance as one of the most fundamental ways in which a mobile robot
interacts with its environments. Although there is extensive research on
proprioception-based RL policies, the article highlights less standardized and
reproducible approaches to exteroception. At the same time, the article aims to
provide a base framework for end-to-end local navigation policies and how a
custom robot can be trained in such simulation environment. We benchmark
end-to-end policies with the state-of-the-art Nav2, navigation stack in Robot
Operating System (ROS). We also cover the sim-to-real transfer process by
demonstrating zero-shot transferability of policies trained in the Isaac
simulator to real-world robots. This is further evidenced by the tests with
different simulated robots, which show the generalization of the learned
policy. Finally, the benchmarks demonstrate comparable performance to Nav2,
opening the door to quick deployment of state-of-the-art end-to-end local
planners for custom robot platforms, but importantly furthering the
possibilities by expanding the state and action spaces or task definitions for
more complex missions. Overall, with this article we introduce the most
important steps, and aspects to consider, in deploying RL policies for local
path planning and obstacle avoidance with Isaac Sim training, Gazebo testing,
and ROS 2 for real-time inference in real robots. The code is available at
https://github.com/sahars93/RL-Navigation.","Sahar Salimpour, Jorge Peña-Queralta, Diego Paez-Granados, Jukka Heikkonen, Tomi Westerlund",2025-01-06T10:26:16Z,2025-01-06T10:26:16Z,http://arxiv.org/abs/2501.02902v1,http://arxiv.org/pdf/2501.02902v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2025
"Toward an Automated, Proactive Safety Warning System Development for
  Truck Mounted Attenuators in Mobile Work Zones","Even though Truck Mounted Attenuators (TMA)/Autonomous Truck Mounted
Attenuators (ATMA) and traffic control devices are increasingly used in mobile
work zones to enhance safety, work zone collisions remain a significant safety
concern in the United States. In Missouri, there were 63 TMA-related crashes in
2023, a 27% increase compared to 2022. Currently, all the signs in the mobile
work zones are passive safety measures, relying on drivers' recognition and
attention. Some distracted drivers may ignore these signs and warnings, raising
safety concerns. In this study, we proposed an additional proactive warning
system that could be applied to the TMA/ATMA to improve overall safety. A
feasible solution has been demonstrated by integrating a Panoptic Driving
Perception algorithm into the Robot Operating System (ROS) and applying it to
the TMA/ATMA systems. This enables us to alert vehicles on a collision course
with the TMA. Our experimental setup, currently conducted in a laboratory
environment with two ROS robots and a desktop GPU, demonstrates the system's
capability to calculate real-time distance and speed and activate warning
signals. Leveraging ROS's distributed computing capabilities allows for
flexible system deployment and cost reduction. In future field tests, by
combining the stopping sight distance (SSD) standards from the AASHTO Green
Book, the system enables real-time monitoring of oncoming vehicles and provides
additional proactive warnings to enhance the safety of mobile work zones.","Xiang Yu, Linlin Zhang,  Yaw,  Adu-Gyamfi",2024-12-24T05:50:45Z,2024-12-24T05:50:45Z,http://arxiv.org/abs/2412.18189v1,http://arxiv.org/pdf/2412.18189v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
"Scalable and low-cost remote lab platforms: Teaching industrial robotics
  using open-source tools and understanding its social implications","With recent advancements in industrial robots, educating students in new
technologies and preparing them for the future is imperative. However, access
to industrial robots for teaching poses challenges, such as the high cost of
acquiring these robots, the safety of the operator and the robot, and
complicated training material. This paper proposes two low-cost platforms built
using open-source tools like Robot Operating System (ROS) and its latest
version ROS 2 to help students learn and test algorithms on remotely connected
industrial robots. Universal Robotics (UR5) arm and a custom mobile rover were
deployed in different life-size testbeds, a greenhouse, and a warehouse to
create an Autonomous Agricultural Harvester System (AAHS) and an Autonomous
Warehouse Management System (AWMS). These platforms were deployed for a period
of 7 months and were tested for their efficacy with 1,433 and 1,312 students,
respectively. The hardware used in AAHS and AWMS was controlled remotely for
160 and 355 hours, respectively, by students over a period of 3 months.","Amit Kumar, Jaison Jose, Archit Jain, Siddharth Kulkarni, Kavi Arya",2024-12-19T20:03:13Z,2024-12-19T20:03:13Z,http://arxiv.org/abs/2412.15369v1,http://arxiv.org/pdf/2412.15369v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
"VLsI: Verbalized Layers-to-Interactions from Large to Small Vision
  Language Models","The recent surge in high-quality visual instruction tuning samples from
closed-source vision-language models (VLMs) such as GPT-4V has accelerated the
release of open-source VLMs across various model sizes. However, scaling VLMs
to improve performance using larger models brings significant computational
challenges, especially for deployment on resource-constrained devices like
mobile platforms and robots. To address this, we propose VLsI: Verbalized
Layers-to-Interactions, a new VLM family in 2B and 7B model sizes, which
prioritizes efficiency without compromising accuracy. VLsI leverages a unique,
layer-wise distillation process, introducing intermediate ""verbalizers"" that
map features from each layer to natural language space, allowing smaller VLMs
to flexibly align with the reasoning processes of larger VLMs. This approach
mitigates the training instability often encountered in output imitation and
goes beyond typical final-layer tuning by aligning the small VLMs' layer-wise
progression with that of the large ones. We validate VLsI across ten
challenging vision-language benchmarks, achieving notable performance gains
(11.0% for 2B and 17.4% for 7B) over GPT-4V without the need for model scaling,
merging, or architectural changes.","Byung-Kwan Lee, Ryo Hachiuma, Yu-Chiang Frank Wang, Yong Man Ro, Yueh-Hua Wu",2024-12-02T18:58:25Z,2024-12-02T18:58:25Z,http://arxiv.org/abs/2412.01822v1,http://arxiv.org/pdf/2412.01822v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
A ROS~2-based Navigation and Simulation Stack for the Robotino,"The Robotino, developed by Festo Didactic, serves as a versatile platform in
education and research for mobile robotics tasks. However, there currently is
no ROS2 integration for the Robotino available. In this paper, we describe our
work on a Webots simulation environment for a Robotino platform extended by
LIDAR sensors. A ROS2 integration and a pre-configured setup for localization
and navigation using existing ROS packages from the Nav2 suite are provided. We
validate our setup by comparing simulations with real-world experiments
conducted by three Robotinos in a logistics environment in our lab.
Additionally, we tested the setup using a ROS 2 hardware driver for the
Robotino developed by team GRIPS of the RoboCup Logistics League. The results
demonstrate the feasibility of using ROS2 and Nav2 for navigation tasks on the
Robotino platform showing great consistency between simulation and real-world
performance.","Saurabh Borse, Tarik Viehmann, Alexander Ferrein, Gerhard Lakemeyer",2024-11-14T13:40:41Z,2024-11-14T13:40:41Z,http://arxiv.org/abs/2411.09441v1,http://arxiv.org/pdf/2411.09441v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
"Development of an indoor localization and navigation system based on
  monocular SLAM for mobile robots","Localization and navigation are two crucial issues for mobile robots. In this
paper, we propose an approach for localization and navigation systems for a
differential-drive robot based on monocular SLAM. The system is implemented on
the Robot Operating System (ROS). The hardware includes a differential-drive
robot with an embedded computing platform (Jetson Xavier AGX), a 2D camera, and
a LiDAR sensor for collecting external environmental information. The A*
algorithm and Dynamic Window Approach (DWA) are used for path planning based on
a 2D grid map. The ORB_SLAM3 algorithm is utilized to extract environmental
features, providing the robot's pose for the localization and navigation
processes. Finally, the system is tested in the Gazebo simulation environment
and visualized through Rviz, demonstrating the efficiency and potential of the
system for indoor localization and navigation of mobile robots.","Thanh Nguyen Canh, Duc Manh Do, Xiem HoangVan",2024-11-08T05:26:10Z,2024-11-08T05:26:10Z,http://arxiv.org/abs/2411.05337v1,http://arxiv.org/pdf/2411.05337v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
SODA: a Soft Origami Dynamic utensil for Assisted feeding,"SODA aims to revolutionize assistive feeding systems by designing a
multi-purpose utensil using origami-inspired artificial muscles. Traditional
utensils, such as forks and spoons,are hard and stiff, causing discomfort and
fear among users, especially when operated by autonomous robotic arms.
Additionally, these systems require frequent utensil changes to handle
different food types. Our innovative utensil design addresses these issues by
offering a versatile, adaptive solution that can seamlessly transition between
gripping and scooping various foods without the need for manual intervention.
Utilizing the flexibility and strength of origami-inspired artificial muscles,
the utensil ensures safe and comfortable interactions, enhancing user
experience and efficiency. This approach not only simplifies the feeding
process but also promotes greater independence for individuals with limited
mobility, contributing to the advancement of soft robotics in healthcare
applications.","Yuxin Ray Song, Shufan Wang",2024-10-25T13:42:03Z,2024-10-25T13:42:03Z,http://arxiv.org/abs/2410.19558v1,http://arxiv.org/pdf/2410.19558v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
Finite-Time Trajectory Tracking of a Four wheeled Mecanum Mobile Robot,"Four Wheeled Mecanum Robot (FWMR) possess the capability to move in any
direction on a plane making it a cornerstone system in modern industrial
operations. Despite the extreme maneuverability offered by FWMR, the practical
implementation or real-time simulation of Mecanum wheel robots encounters
substantial challenges in trajectory tracking control. In this research work,
we present a finite-time control law using backstepping technique to perform
stabilization and trajectory tracking objectives for a FWMR system. A rigorous
stability proof is presented and explicit computation of the finite-time is
provided. For tracking objective, we demonstrate the results taking an S-shaped
trajectory inclined towards collision avoidance applications. Simulation
validation in real time using Gazebo-ROS on a Mecanum robot model is carried
out which complies with the theoretical results.","Anil B, Mayank Pandey, Sneha Gajbhiye",2024-10-09T10:49:16Z,2024-10-09T10:49:16Z,http://arxiv.org/abs/2410.06762v1,http://arxiv.org/pdf/2410.06762v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
"Key-Scan-Based Mobile Robot Navigation: Integrated Mapping, Planning,
  and Control using Graphs of Scan Regions","Safe autonomous navigation in a priori unknown environments is an essential
skill for mobile robots to reliably and adaptively perform diverse tasks (e.g.,
delivery, inspection, and interaction) in unstructured cluttered environments.
Hybrid metric-topological maps, constructed as a pose graph of local submaps,
offer a computationally efficient world representation for adaptive mapping,
planning, and control at the regional level. In this paper, we consider a pose
graph of locally sensed star-convex scan regions as a metric-topological map,
with star convexity enabling simple yet effective local navigation strategies.
We design a new family of safe local scan navigation policies and present a
perception-driven feedback motion planning method through the sequential
composition of local scan navigation policies, enabling provably correct and
safe robot navigation over the union of local scan regions. We introduce a new
concept of bridging and frontier scans for automated key scan selection and
exploration for integrated mapping and navigation in unknown environments. We
demonstrate the effectiveness of our key-scan-based navigation and mapping
framework using a mobile robot equipped with a 360$^{\circ}$ laser range
scanner in 2D cluttered environments through numerical ROS-Gazebo simulations
and real hardware~experiments.","Dharshan Bashkaran Latha, Ömür Arslan",2024-09-20T18:25:09Z,2024-09-20T18:25:09Z,http://arxiv.org/abs/2409.13838v1,http://arxiv.org/pdf/2409.13838v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
A Soft Robotic Exosuit For Knee Extension Using Hyper-Bending Actuators,"Movement disorders impact muscle strength and mobility, and despite
therapeutic efforts, many people with movement disorders have challenges
functioning independently. Soft wearable robots, or exosuits, offer a promising
solution for continuous daily support, however, commercially viable devices are
not widely available. Here, we introduce a design framework for lower limb
exosuits centered on a soft pneumatically driven fabric-based actuator. Our
design consists of a novel multi-material textile sleeve that incorporates
braided mesh and knit-elastic materials to realize hyper-bending actuators. The
actuators incorporate 3D-printed self-sealing end caps that are attached to a
semi-rigid human-robot interface to secure them to the body. We will
demonstrate the effectiveness of our exosuit in generating enough force to
assist during sit-to-stand transitions.","Tuo Liu, Jonathan Realmuto",2024-09-19T00:12:29Z,2024-09-19T00:12:29Z,http://arxiv.org/abs/2410.02802v1,http://arxiv.org/pdf/2410.02802v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
"An Open-Source Soft Robotic Platform for Autonomous Aerial Manipulation
  in the Wild","Aerial manipulation combines the versatility and speed of flying platforms
with the functional capabilities of mobile manipulation, which presents
significant challenges due to the need for precise localization and control.
Traditionally, researchers have relied on offboard perception systems, which
are limited to expensive and impractical specially equipped indoor
environments. In this work, we introduce a novel platform for autonomous aerial
manipulation that exclusively utilizes onboard perception systems. Our platform
can perform aerial manipulation in various indoor and outdoor environments
without depending on external perception systems. Our experimental results
demonstrate the platform's ability to autonomously grasp various objects in
diverse settings. This advancement significantly improves the scalability and
practicality of aerial manipulation applications by eliminating the need for
costly tracking solutions. To accelerate future research, we open source our
ROS 2 software stack and custom hardware design, making our contributions
accessible to the broader research community.","Erik Bauer, Marc Blöchlinger, Pascal Strauch, Arman Raayatsanati, Curdin Cavelti, Robert K. Katzschmann",2024-09-11T23:31:58Z,2024-09-11T23:31:58Z,http://arxiv.org/abs/2409.07662v1,http://arxiv.org/pdf/2409.07662v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
"Investigating Mixed Reality for Communication Between Humans and Mobile
  Manipulators","This article investigates mixed reality (MR) to enhance human-robot
collaboration (HRC). The proposed solution adopts MR as a communication layer
to convey a mobile manipulator's intentions and upcoming actions to the humans
with whom it interacts, thus improving their collaboration. A user study
involving 20 participants demonstrated the effectiveness of this MR-focused
approach in facilitating collaborative tasks, with a positive effect on overall
collaboration performances and human satisfaction.","Mohamad Shaaban, Simone Macci`o, Alessandro Carf`ı, Fulvio Mastrogiovanni",2024-09-03T21:50:55Z,2024-09-03T21:50:55Z,http://arxiv.org/abs/2409.02312v1,http://arxiv.org/pdf/2409.02312v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
"Benchmarking ML Approaches to UWB-Based Range-Only Posture Recognition
  for Human Robot-Interaction","Human pose estimation involves detecting and tracking the positions of
various body parts using input data from sources such as images, videos, or
motion and inertial sensors. This paper presents a novel approach to human pose
estimation using machine learning algorithms to predict human posture and
translate them into robot motion commands using ultra-wideband (UWB) nodes, as
an alternative to motion sensors. The study utilizes five UWB sensors
implemented on the human body to enable the classification of still poses and
more robust posture recognition. This approach ensures effective posture
recognition across a variety of subjects. These range measurements serve as
input features for posture prediction models, which are implemented and
compared for accuracy. For this purpose, machine learning algorithms including
K-Nearest Neighbors (KNN), Support Vector Machine (SVM), and deep Multi-Layer
Perceptron (MLP) neural network are employed and compared in predicting
corresponding postures. We demonstrate the proposed approach for real-time
control of different mobile/aerial robots with inference implemented in a ROS 2
node. Experimental results demonstrate the efficacy of the approach, showcasing
successful prediction of human posture and corresponding robot movements with
high accuracy.","Salma Salimi, Sahar Salimpour, Jorge Peña Queralta, Wallace Moreira Bessa, Tomi Westerlund",2024-08-28T11:24:17Z,2024-08-28T11:24:17Z,http://arxiv.org/abs/2408.15717v1,http://arxiv.org/pdf/2408.15717v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
A Mini-Review on Mobile Manipulators with Variable Autonomy,"This paper presents a mini-review of the current state of research in mobile
manipulators with variable levels of autonomy, emphasizing their associated
challenges and application environments. The need for mobile manipulators in
different environments is evident due to the unique challenges and risks each
presents. Many systems deployed in these environments are not fully autonomous,
requiring human-robot teaming to ensure safe and reliable operations under
uncertainties. Through this analysis, we identify gaps and challenges in the
literature on Variable Autonomy, including cognitive workload and communication
delays, and propose future directions, including whole-body Variable Autonomy
for mobile manipulators, virtual reality frameworks, and large language models
to reduce operators' complexity and cognitive load in some challenging and
uncertain scenarios.","Cesar Alan Contreras, Alireza Rastegarpanah, Rustam Stolkin, Manolis Chiou",2024-08-20T14:18:35Z,2024-08-20T14:18:35Z,http://arxiv.org/abs/2408.10887v1,http://arxiv.org/pdf/2408.10887v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
"Micro-integrated crossed-beam optical dipole trap system with long-term
  alignment stability for mobile atomic quantum technologies","Quantum technologies extensively use laser light for state preparation,
manipulation, and readout. For field applications, these systems must be robust
and compact, driving the need for miniaturized and highly stable optical setups
and system integration. In this work, we present a micro-integrated
crossed-beam optical dipole trap setup, the $\mu$XODT, designed for trapping
and cooling $^{87}\text{Rb}$. This fiber-coupled setup operates at
$1064\,\text{nm}$ wavelength with up to $2.5\,\text{W}$ optical power and
realizes a free-space crossed beam geometry. The $\mu$XODT precisely overlaps
two focused beams ($w_0 \approx 33\,\mu\text{m}$) at their waists in a
$45^\circ$ crossing angle, achieving a position difference $\leq
3.4\,\mu\text{m}$ and 0.998 power ratio between both beams with long-term
stability. We describe the design and assembly process in detail, along with
optical and thermal tests with temperatures of up to $65\,^\circ C$. The
system's volume of $25\,\text{ml}$ represents a reduction of more than two
orders of magnitude compared to typically used macroscopic setups, while
demonstrating exceptional mechanical robustness and thermal stability. The
$\mu$XODT is integrated with a $^{87}\text{Rb}$ 3D MOT setup, trapping $3
\times 10^5$ atoms from a laser-cooled atomic cloud, and has shown no signs of
degradation after two years of operation.","Marc Christ, Oliver Anton, Conrad Zimmermann, Victoria A Henderson, Elisa Da Ros, Markus Krutzik",2024-08-13T19:56:45Z,2024-08-13T19:56:45Z,http://arxiv.org/abs/2408.07187v1,http://arxiv.org/pdf/2408.07187v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
"neuROSym: Deployment and Evaluation of a ROS-based Neuro-Symbolic Model
  for Human Motion Prediction","Autonomous mobile robots can rely on several human motion detection and
prediction systems for safe and efficient navigation in human environments, but
the underline model architectures can have different impacts on the
trustworthiness of the robot in the real world. Among existing solutions for
context-aware human motion prediction, some approaches have shown the benefit
of integrating symbolic knowledge with state-of-the-art neural networks. In
particular, a recent neuro-symbolic architecture (NeuroSyM) has successfully
embedded context with a Qualitative Trajectory Calculus (QTC) for spatial
interactions representation. This work achieved better performance than
neural-only baseline architectures on offline datasets. In this paper, we
extend the original architecture to provide neuROSym, a ROS package for robot
deployment in real-world scenarios, which can run, visualise, and evaluate
previous neural-only and neuro-symbolic models for motion prediction online. We
evaluated these models, NeuroSyM and a baseline SGAN, on a TIAGo robot in two
scenarios with different human motion patterns. We assessed accuracy and
runtime performance of the prediction models, showing a general improvement in
case our neuro-symbolic architecture is used. We make the neuROSym package1
publicly available to the robotics community.","Sariah Mghames, Luca Castri, Marc Hanheide, Nicola Bellotto",2024-06-24T11:13:06Z,2024-06-24T11:13:06Z,http://arxiv.org/abs/2407.01593v1,http://arxiv.org/pdf/2407.01593v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
"Tactile Aware Dynamic Obstacle Avoidance in Crowded Environment with
  Deep Reinforcement Learning","Mobile robots operating in crowded environments require the ability to
navigate among humans and surrounding obstacles efficiently while adhering to
safety standards and socially compliant mannerisms. This scale of the robot
navigation problem may be classified as both a local path planning and
trajectory optimization problem. This work presents an array of force sensors
that act as a tactile layer to complement the use of a LiDAR for the purpose of
inducing awareness of contact with any surrounding objects within immediate
vicinity of a mobile robot undetected by LiDARs. By incorporating the tactile
layer, the robot can take more risks in its movements and possibly go right up
to an obstacle or wall, and gently squeeze past it. In addition, we built up a
simulation platform via Pybullet which integrates Robot Operating System (ROS)
and reinforcement learning (RL) together. A touch-aware neural network model
was trained on it to create an RL-based local path planner for dynamic obstacle
avoidance. Our proposed method was demonstrated successfully on an
omni-directional mobile robot who was able to navigate in a crowded environment
with high agility and versatility in movement, while not being overly sensitive
to nearby obstacles-not-in-contact.","Yung Chuen Ng, Qi Wen Shervina Lim, Chun Ye Tan, Zhen Hao Gan, Meng Yee Michael Chuah",2024-06-19T10:50:04Z,2025-08-14T07:47:04Z,http://arxiv.org/abs/2406.13434v2,http://arxiv.org/pdf/2406.13434v2.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
"Person Transfer in the Field: Examining Real World Sequential
  Human-Robot Interaction Between Two Robots","With more robots being deployed in the world, users will likely interact with
multiple robots sequentially when receiving services. In this paper, we
describe an exploratory field study in which unsuspecting participants
experienced a ``person transfer'' -- a scenario in which they first interacted
with one stationary robot before another mobile robot joined to complete the
interaction. In our 7-hour study spanning 4 days, we recorded 18 instances of
person transfers with 40+ individuals. We also interviewed 11 participants
after the interaction to further understand their experience. We used the
recorded video and interview data to extract interesting insights about
in-the-field sequential human-robot interaction, such as mobile robot
handovers, trust in person transfer, and the importance of the robots'
positions. Our findings expose pitfalls and present important factors to
consider when designing sequential human-robot interaction.","Xiang Zhi Tan, Elizabeth J. Carter, Aaron Steinfeld",2024-06-11T02:59:34Z,2024-06-11T02:59:34Z,http://arxiv.org/abs/2406.06904v1,http://arxiv.org/pdf/2406.06904v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
"Adaptive Control in Assistive Application -- A Study Evaluating Shared
  Control by Users with Limited Upper Limb Mobility","Shared control in assistive robotics blends human autonomy with computer
assistance, thus simplifying complex tasks for individuals with physical
impairments. This study assesses an adaptive Degrees of Freedom control method
specifically tailored for individuals with upper limb impairments. It employs a
between-subjects analysis with 24 participants, conducting 81 trials across
three distinct input devices in a realistic everyday-task setting. Given the
diverse capabilities of the vulnerable target demographic and the known
challenges in statistical comparisons due to individual differences, the study
focuses primarily on subjective qualitative data. The results reveal
consistently high success rates in trial completions, irrespective of the input
device used. Participants appreciated their involvement in the research
process, displayed a positive outlook, and quick adaptability to the control
system. Notably, each participant effectively managed the given task within a
short time frame.","Felix Ferdinand Goldau, Max Pascher, Annalies Baumeister, Patrizia Tolle, Jens Gerken, Udo Frese",2024-06-10T08:36:55Z,2024-06-10T08:36:55Z,http://arxiv.org/abs/2406.06103v1,http://arxiv.org/pdf/2406.06103v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
"Advancing Behavior Generation in Mobile Robotics through High-Fidelity
  Procedural Simulations","This paper introduces YamaS, a simulator integrating Unity3D Engine with
Robotic Operating System for robot navigation research and aims to facilitate
the development of both Deep Reinforcement Learning (Deep-RL) and Natural
Language Processing (NLP). It supports single and multi-agent configurations
with features like procedural environment generation, RGB vision, and dynamic
obstacle navigation. Unique to YamaS is its ability to construct single and
multi-agent environments, as well as generating agent's behaviour through
textual descriptions. The simulator's fidelity is underscored by comparisons
with the real-world Yamabiko Beego robot, demonstrating high accuracy in sensor
simulations and spatial reasoning. Moreover, YamaS integrates Virtual Reality
(VR) to augment Human-Robot Interaction (HRI) studies, providing an immersive
platform for developers and researchers. This fusion establishes YamaS as a
versatile and valuable tool for the development and testing of autonomous
systems, contributing to the fields of robot simulation and AI-driven training
methodologies.","Victor A. Kich, Jair A. Bottega, Raul Steinmetz, Ricardo B. Grando, Ayanori Yorozu, Akihisa Ohya",2024-05-27T04:31:55Z,2024-05-27T04:31:55Z,http://arxiv.org/abs/2405.16818v1,http://arxiv.org/pdf/2405.16818v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
"Optimal Whole Body Trajectory Planning for Mobile Manipulators in
  Planetary Exploration and Construction","Space robotics poses unique challenges arising from the limitation of energy
and computational resources, and the complexity of the environment and employed
platforms. At the control center, offline motion planning is fundamental in the
computation of optimized trajectories accounting for the system's constraints.
Smooth movements, collision and forbidden areas avoidance, target visibility
and energy consumption are all important factors to consider to be able to
generate feasible and optimal plans. When mobile manipulators (terrestrial,
aerial) are employed, the base and the arm movements are often separately
planned, ultimately resulting in sub-optimal solutions. We propose an Optimal
Whole Body Planner (OptiWB) based on Discrete Dynamic Programming (DDP) and
optimal interpolation. Kinematic redundancy is exploited for collision and
forbidden areas avoidance, and to improve target illumination and visibility
from onboard cameras. The planner, implemented in ROS (Robot Operating System),
interfaces 3DROCS, a mission planner used in several programs of the European
Space Agency (ESA) to support planetary exploration surface missions and part
of the ExoMars Rover's planning software. The proposed approach is exercised on
a simplified version of the Analog-1 Interact rover by ESA, a 7-DOFs robotic
arm mounted on a four wheels non-holonomic platform.","Federica Storiale, Enrico Ferrentino, Federico Salvioli, Konstantinos Kapellos, Pasquale Chiacchio",2024-05-23T09:39:23Z,2024-05-23T09:39:23Z,http://arxiv.org/abs/2405.14363v1,http://arxiv.org/pdf/2405.14363v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
"BSL: Navigation Method Considering Blind Spots Based on ROS Navigation
  Stack and Blind Spots Layer for Mobile Robot","This paper proposes a navigation method considering blind spots based on the
robot operating system (ROS) navigation stack and blind spots layer (BSL) for a
wheeled mobile robot. In this paper, environmental information is recognized
using a laser range finder (LRF) and RGB-D cameras. Blind spots occur when
corners or obstacles are present in the environment, and may lead to collisions
if a human or object moves toward the robot from these blind spots. To prevent
such collisions, this paper proposes a navigation method considering blind
spots based on the local cost map layer of the BSL for the wheeled mobile
robot. Blind spots are estimated by utilizing environmental data collected
through RGB-D cameras. The navigation method that takes these blind spots into
account is achieved through the implementation of the BSL and a local path
planning method that employs an enhanced cost function of dynamic window
approach. The effectiveness of the proposed method was further demonstrated
through simulations and experiments.","Masato Kobayashi, Naoki Motoi",2024-05-09T00:37:38Z,2024-05-09T00:37:38Z,http://arxiv.org/abs/2405.05479v1,http://arxiv.org/pdf/2405.05479v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
ROS2swarm - A ROS 2 Package for Swarm Robot Behaviors,"Developing reusable software for mobile robots is still challenging. Even
more so for swarm robots, despite the desired simplicity of the robot
controllers. Prototyping and experimenting are difficult due to the multi-robot
setting and often require robot-robot communication. Also, the diversity of
swarm robot hardware platforms increases the need for hardware-independent
software concepts. The main advantages of the commonly used robot software
architecture ROS 2 are modularity and platform independence. We propose a new
ROS 2 package, ROS2swarm, for applications of swarm robotics that provides a
library of ready-to-use swarm behavioral primitives. We show the successful
application of our approach on three different platforms, the TurtleBot3
Burger, the TurtleBot3 Waffle Pi, and the Jackal UGV, and with a set of
different behavioral primitives, such as aggregation, dispersion, and
collective decision-making. The proposed approach is easy to maintain,
extendable, and has good potential for simplifying swarm robotics experiments
in future applications.","Tanja Katharina Kaiser, Marian Johannes Begemann, Tavia Plattenteich, Lars Schilling, Georg Schildbach, Heiko Hamann",2024-05-03T19:05:14Z,2024-05-03T19:05:14Z,http://arxiv.org/abs/2405.02438v1,http://arxiv.org/pdf/2405.02438v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
"Accurate Pose Prediction on Signed Distance Fields for Mobile Ground
  Robots in Rough Terrain","Autonomous locomotion for mobile ground robots in unstructured environments
such as waypoint navigation or flipper control requires a sufficiently accurate
prediction of the robot-terrain interaction. Heuristics like occupancy grids or
traversability maps are widely used but limit actions available to robots with
active flippers as joint positions are not taken into account. We present a
novel iterative geometric method to predict the 3D pose of mobile ground robots
with active flippers on uneven ground with high accuracy and online planning
capabilities. This is achieved by utilizing the ability of signed distance
fields to represent surfaces with sub-voxel accuracy. The effectiveness of the
presented approach is demonstrated on two different tracked robots in
simulation and on a real platform. Compared to a tracking system as ground
truth, our method predicts the robot position and orientation with an average
accuracy of 3.11 cm and 3.91{\deg}, outperforming a recent heightmap-based
approach. The implementation is made available as an open-source ROS package.","Martin Oehler, Oskar von Stryk",2024-05-03T14:24:27Z,2024-05-03T14:24:27Z,http://arxiv.org/abs/2405.02121v1,http://arxiv.org/pdf/2405.02121v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
Autonomous Robot for Disaster Mapping and Victim Localization,"In response to the critical need for effective reconnaissance in disaster
scenarios, this research article presents the design and implementation of a
complete autonomous robot system using the Turtlebot3 with Robotic Operating
System (ROS) Noetic. Upon deployment in closed, initially unknown environments,
the system aims to generate a comprehensive map and identify any present
'victims' using AprilTags as stand-ins. We discuss our solution for search and
rescue missions, while additionally exploring more advanced algorithms to
improve search and rescue functionalities. We introduce a Cubature Kalman
Filter to help reduce the mean squared error [m] for AprilTag localization and
an information-theoretic exploration algorithm to expedite exploration in
unknown environments. Just like turtles, our system takes it slow and steady,
but when it's time to save the day, it moves at ninja-like speed! Despite
Donatello's shell, he's no slowpoke - he zips through obstacles with the
agility of a teenage mutant ninja turtle. So, hang on tight to your shells and
get ready for a whirlwind of reconnaissance!
  Full pipeline code https://github.com/rzhao5659/MRProject/tree/main
  Exploration code https://github.com/rzhao5659/MRProject/tree/main","Michael Potter, Rahil Bhowal, Richard Zhao, Anuj Patel, Jingming Cheng",2024-04-21T20:32:02Z,2024-04-21T20:32:02Z,http://arxiv.org/abs/2404.13767v1,http://arxiv.org/pdf/2404.13767v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
CBFKIT: A Control Barrier Function Toolbox for Robotics Applications,"This paper introduces CBFKit, a Python/ROS toolbox for safe robotics planning
and control under uncertainty. The toolbox provides a general framework for
designing control barrier functions for mobility systems within both
deterministic and stochastic environments. It can be connected to the ROS
open-source robotics middleware, allowing for the setup of multi-robot
applications, encoding of environments and maps, and integrations with
predictive motion planning algorithms. Additionally, it offers multiple CBF
variations and algorithms for robot control. The CBFKit is demonstrated on the
Toyota Human Support Robot (HSR) in both simulation and in physical
experiments.","Mitchell Black, Georgios Fainekos, Bardh Hoxha, Hideki Okamoto, Danil Prokhorov",2024-04-10T16:49:39Z,2024-04-10T16:49:39Z,http://arxiv.org/abs/2404.07158v1,http://arxiv.org/pdf/2404.07158v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
"OtterROS: Picking and Programming an Uncrewed Surface Vessel for
  Experimental Field Robotics Research with ROS 2","There exist a wide range of options for field robotics research using ground
and aerial mobile robots, but there are comparatively few robust and
research-ready uncrewed surface vessels (USVs). This workshop paper starts with
a snapshot of USVs currently available to the research community and then
describes ""OtterROS"", an open source ROS 2 solution for the Otter USV. Field
experiments using OtterROS are described, which highlight the utility of the
Otter USV and the benefits of using ROS 2 in aquatic robotics research. For
those interested in USV research, the paper details recommended hardware to run
OtterROS and includes an example ROS 2 package using OtterROS, removing
unnecessary non-recurring engineering from field robotics research activities.","Thomas M. C. Sears, M. Riley Cooper, Sabrina R. Button, Joshua A. Marshall",2024-04-08T15:59:47Z,2024-04-23T00:50:41Z,http://arxiv.org/abs/2404.05627v2,http://arxiv.org/pdf/2404.05627v2.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
"Localization and Perception for Control of a Low Speed Autonomous
  Shuttle in a Campus Pilot Deployment","Future SAE Level 4 and Level 5 autonomous vehicles will require novel
applications of localization, perception, control and artificial intelligence
technology in order to offer innovative and disruptive solutions to current
mobility problems. Accurate localization is essential for self driving vehicle
navigation in GPS inaccessible environments. This thesis concentrates on low
speed autonomous shuttles that are mainly utilized for university campus
intelligent transportation systems and presents initial results of ongoing work
on developing solutions to the localization and perception challenges of a
university planned pilot deployment orientated application. The paper treats
autonomous driving with real time kinematics GPS (Global Positioning Systems)
with an inertial measurement unit (IMU), combined with simultaneous
localization and mapping (SLAM) with threedimensional light detection and
ranging (LIDAR) sensor, which provides solutions to scenarios where GPS is not
available or a lower cost and hence lower accuracy GPS is desirable. The
in-house automated low speed electric vehicle from the Automated Driving Lab is
used in experimental evaluation and verification. An improved version of Hector
SLAM was implemented on ROS and compared with high resolution GPS aided
localization framework in the same hardware architecture. The overall
configuration that combines ROS with DSpace controller can be easily
transplantable prototype in other hardware architectures for future similar
research. Real-world experiments that are reported here have been conducted in
a small test area close to the Ohio State University AV pilot test route. are
used for demonstrating the feasibility and robustness of this approach to
developing and evaluating low speed autonomous shuttle localization and
perception algorithms for control and decision making.",Bowen Wen,2024-04-02T21:38:39Z,2024-04-02T21:38:39Z,http://arxiv.org/abs/2407.00820v1,http://arxiv.org/pdf/2407.00820v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
"CARLOS: An Open, Modular, and Scalable Simulation Framework for the
  Development and Testing of Software for C-ITS","Future mobility systems and their components are increasingly defined by
their software. The complexity of these cooperative intelligent transport
systems (C-ITS) and the everchanging requirements posed at the software require
continual software updates. The dynamic nature of the system and the
practically innumerable scenarios in which different software components work
together necessitate efficient and automated development and testing procedures
that use simulations as one core methodology. The availability of such
simulation architectures is a common interest among many stakeholders,
especially in the field of automated driving. That is why we propose CARLOS -
an open, modular, and scalable simulation framework for the development and
testing of software in C-ITS that leverages the rich CARLA and ROS ecosystems.
We provide core building blocks for this framework and explain how it can be
used and extended by the community. Its architecture builds upon modern
microservice and DevOps principles such as containerization and continuous
integration. In our paper, we motivate the architecture by describing important
design principles and showcasing three major use cases - software prototyping,
data-driven development, and automated testing. We make CARLOS and example
implementations of the three use cases publicly available at
github.com/ika-rwth-aachen/carlos","Christian Geller, Benedikt Haas, Amarin Kloeker, Jona Hermens, Bastian Lampe, Till Beemelmanns, Lutz Eckstein",2024-04-02T10:48:36Z,2024-04-19T13:48:59Z,http://arxiv.org/abs/2404.01836v3,http://arxiv.org/pdf/2404.01836v3.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
"Efficient Multi-Band Temporal Video Filter for Reducing Human-Robot
  Interaction","Although mobile robots have on-board sensors to perform navigation, their
efficiency in completing paths can be enhanced by planning to avoid human
interaction. Infrastructure cameras can capture human activity continuously for
the purpose of compiling activity analytics to choose efficient times and
routes. We describe a cascade temporal filtering method to efficiently extract
short- and long-term activity in two time dimensions, isochronal and
chronological, for use in global path planning and local navigation
respectively. The temporal filter has application either independently, or, if
object recognition is also required, it can be used as a pre-filter to perform
activity-gating of the more computationally expensive neural network
processing. For a testbed 32-camera network, we show how this hybrid approach
can achieve over 8 times improvement in frames per second throughput and 6.5
times reduction of system power use. We also show how the cost map of static
objects in the ROS robot software development framework is augmented with
dynamic regions determined from the temporal filter.",Lawrence O'Gorman,2024-03-26T20:41:35Z,2024-03-26T20:41:35Z,http://arxiv.org/abs/2403.18096v1,http://arxiv.org/pdf/2403.18096v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
"RHINO-VR Experience: Teaching Mobile Robotics Concepts in an Interactive
  Museum Exhibit","In 1997, the very first tour guide robot RHINO was deployed in a museum in
Germany. With the ability to navigate autonomously through the environment, the
robot gave tours to over 2,000 visitors. Today, RHINO itself has become an
exhibit and is no longer operational. In this paper, we present RHINO-VR, an
interactive museum exhibit using virtual reality (VR) that allows museum
visitors to experience the historical robot RHINO in operation in a virtual
museum. RHINO-VR, unlike static exhibits, enables users to familiarize
themselves with basic mobile robotics concepts without the fear of damaging the
exhibit. In the virtual environment, the user is able to interact with RHINO in
VR by pointing to a location to which the robot should navigate and observing
the corresponding actions of the robot. To include other visitors who cannot
use the VR, we provide an external observation view to make RHINO visible to
them. We evaluated our system by measuring the frame rate of the VR simulation,
comparing the generated virtual 3D models with the originals, and conducting a
user study. The user-study showed that RHINO-VR improved the visitors'
understanding of the robot's functionality and that they would recommend
experiencing the VR exhibit to others.","Erik Schlachhoff, Nils Dengler, Leif Van Holland, Patrick Stotko, Jorge de Heuvel, Reinhard Klein, Maren Bennewitz",2024-03-22T12:07:03Z,2024-06-10T13:22:41Z,http://arxiv.org/abs/2403.15151v2,http://arxiv.org/pdf/2403.15151v2.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
"Frontier-Based Exploration for Multi-Robot Rendezvous in
  Communication-Restricted Unknown Environments","Multi-robot rendezvous and exploration are fundamental challenges in the
domain of mobile robotic systems. This paper addresses multi-robot rendezvous
within an initially unknown environment where communication is only possible
after the rendezvous. Traditionally, exploration has been focused on rapidly
mapping the environment, often leading to suboptimal rendezvous performance in
later stages. We adapt a standard frontier-based exploration technique to
integrate exploration and rendezvous into a unified strategy, with a mechanism
that allows robots to re-visit previously explored regions thus enhancing
rendezvous opportunities. We validate our approach in 3D realistic simulations
using ROS, showcasing its effectiveness in achieving faster rendezvous times
compared to exploration strategies.","Mauro Tellaroli, Matteo Luperto, Michele Antonazzi, Nicola Basilico",2024-03-18T09:50:05Z,2024-07-19T13:01:51Z,http://arxiv.org/abs/2403.11617v3,http://arxiv.org/pdf/2403.11617v3.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
"V2AIX: A Multi-Modal Real-World Dataset of ETSI ITS V2X Messages in
  Public Road Traffic","Connectivity is a main driver for the ongoing megatrend of automated
mobility: future Cooperative Intelligent Transport Systems (C-ITS) will connect
road vehicles, traffic signals, roadside infrastructure, and even vulnerable
road users, sharing data and compute for safer, more efficient, and more
comfortable mobility. In terms of communication technology for realizing such
vehicle-to-everything (V2X) communication, the WLAN-based peer-to-peer approach
(IEEE 802.11p, ITS-G5 in Europe) competes with C-V2X based on cellular
technologies (4G and beyond). Irrespective of the underlying communication
standard, common message interfaces are crucial for a common understanding
between vehicles, especially from different manufacturers. Targeting this
issue, the European Telecommunications Standards Institute (ETSI) has been
standardizing V2X message formats such as the Cooperative Awareness Message
(CAM). In this work, we present V2AIX, a multi-modal real-world dataset of ETSI
ITS messages gathered in public road traffic, the first of its kind. Collected
in measurement drives and with stationary infrastructure, we have recorded more
than 285 000 V2X messages from more than 2380 vehicles and roadside units in
public road traffic. Alongside a first analysis of the dataset, we present a
way of integrating ETSI ITS V2X messages into the Robot Operating System (ROS).
This enables researchers to not only thoroughly analyze real-world V2X data,
but to also study and implement standardized V2X messages in ROS-based
automated driving applications. The full dataset is publicly available for
non-commercial use at v2aix.ika.rwth-aachen.de.","Guido Kueppers, Jean-Pierre Busch, Lennart Reiher, Lutz Eckstein",2024-03-15T11:41:14Z,2024-08-02T22:07:34Z,http://arxiv.org/abs/2403.10221v2,http://arxiv.org/pdf/2403.10221v2.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
"Enhancing Trust in Autonomous Agents: An Architecture for Accountability
  and Explainability through Blockchain and Large Language Models","The deployment of autonomous agents in environments involving human
interaction has increasingly raised security concerns. Consequently,
understanding the circumstances behind an event becomes critical, requiring the
development of capabilities to justify their behaviors to non-expert users.
Such explanations are essential in enhancing trustworthiness and safety, acting
as a preventive measure against failures, errors, and misunderstandings.
Additionally, they contribute to improving communication, bridging the gap
between the agent and the user, thereby improving the effectiveness of their
interactions. This work presents an accountability and explainability
architecture implemented for ROS-based mobile robots. The proposed solution
consists of two main components. Firstly, a black box-like element to provide
accountability, featuring anti-tampering properties achieved through blockchain
technology. Secondly, a component in charge of generating natural language
explanations by harnessing the capabilities of Large Language Models (LLMs)
over the data contained within the previously mentioned black box. The study
evaluates the performance of our solution in three different scenarios, each
involving autonomous agent navigation functionalities. This evaluation includes
a thorough examination of accountability and explainability metrics,
demonstrating the effectiveness of our approach in using accountable data from
robot actions to obtain coherent, accurate and understandable explanations,
even when facing challenges inherent in the use of autonomous agents in
real-world scenarios.","Laura Fernández-Becerra, Miguel Ángel González-Santamarta, Ángel Manuel Guerrero-Higueras, Francisco Javier Rodríguez-Lera, Vicente Matellán Olivera",2024-03-14T16:57:18Z,2025-07-15T18:49:29Z,http://arxiv.org/abs/2403.09567v4,http://arxiv.org/pdf/2403.09567v4.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
"A Framework for Controlling Multiple Industrial Robots using Mobile
  Applications","Purpose: Over the last few decades, the development of the hardware and
software has enabled the application of advanced systems. In the robotics
field, the UI design is an intriguing area to be explored due to the creation
of devices with a wide range of functionalities in a reduced size. Moreover,
the idea of using the same UI to control several systems arouses a great
interest considering that this involves less learning effort and time for the
users. Therefore, this paper will present a mobile application to control two
industrial robots with four modes of operation. Design/methodology/approach:
The smartphone was selected to be the interface due to its wide range of
capabilities and the MIT Inventor App was used to create the application, whose
environment is supported by Android smartphones. For the validation, ROS was
used since it is a fundamental framework utilised in industrial robotics and
the Arduino Uno was used to establish the data transmission between the
smartphone and the board NVIDIA Jetson TX2. In MIT Inventor App, the graphical
interface was created to visualize the options available in the app whereas two
scripts in python were programmed to perform the simulations in ROS and carry
out the tests. Findings: The results indicated that the use of the sliders to
control the robots is more favourable than the Orientation Sensor due to the
sensibility of the sensor and human limitations to hold the smartphone
perfectly still. Another important finding was the limitations of the
autonomous mode, in which the robot grabs an object. In this case, the
configuration of the Kinect camera and the controllers has a significant impact
on the success of the simulation. Finally, it was observed that the delay was
appropriate despite the use of the Arduino UNO to transfer the data between the
Smartphone and the Nvidia Jetson TX2.","Daniela Alvarado, Seemal Asif",2024-03-12T13:23:40Z,2024-03-12T13:23:40Z,http://arxiv.org/abs/2403.07639v1,http://arxiv.org/pdf/2403.07639v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
"Magnetic polarons beyond linear spin-wave theory: Mesons dressed by
  magnons","When a mobile hole is doped into an antiferromagnet, its movement will
distort the surrounding magnetic order and yield a magnetic polaron. The
resulting complex interplay of spin and charge degrees of freedom gives rise to
very rich physics and is widely believed to be at the heart of high-temperature
superconductivity in cuprates. In this paper, we develop a quantitative
theoretical formalism, based on the phenomenological parton description, to
describe magnetic polarons in the strong coupling regime. We construct an
effective Hamiltonian with weak coupling to the spin-wave excitations in the
background, making the use of standard polaronic methods possible. Our starting
point is a single hole doped into an AFM described by a 'geometric string'
capturing the strongly correlated hopping processes of charge and spin degrees
of freedom, beyond linear spin-wave approximation. Subsequently, we introduce
magnon excitations through a generalized 1/S expansion and derive an effective
coupling of these spin-waves to the hole plus the string (the meson) to arrive
at an effective polaron Hamiltonian with density-density type interactions.
After making a Born-Oppenheimer-type approximation, this system is solved using
the self-consistent Born approximation to extract the renormalized polaron
properties. We apply our formalism (i) to calculate beyond linear spin-wave
ARPES spectra, (ii) to reveal the interplay of ro-vibrational meson
excitations, and (ii) to analyze the pseudogap expected at low doping.
Moreover, our work paves the way for exploring magnetic polarons out-of
equilibrium or in frustrated systems, where weak-coupling approaches are
desirable and going beyond linear spin-wave theory becomes necessary.","Pit Bermes, Annabelle Bohrdt, Fabian Grusdt",2024-01-31T19:14:17Z,2024-01-31T19:14:17Z,http://arxiv.org/abs/2402.00130v1,http://arxiv.org/pdf/2402.00130v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
SHViT: Single-Head Vision Transformer with Memory Efficient Macro Design,"Recently, efficient Vision Transformers have shown great performance with low
latency on resource-constrained devices. Conventionally, they use 4x4 patch
embeddings and a 4-stage structure at the macro level, while utilizing
sophisticated attention with multi-head configuration at the micro level. This
paper aims to address computational redundancy at all design levels in a
memory-efficient manner. We discover that using larger-stride patchify stem not
only reduces memory access costs but also achieves competitive performance by
leveraging token representations with reduced spatial redundancy from the early
stages. Furthermore, our preliminary analyses suggest that attention layers in
the early stages can be substituted with convolutions, and several attention
heads in the latter stages are computationally redundant. To handle this, we
introduce a single-head attention module that inherently prevents head
redundancy and simultaneously boosts accuracy by parallelly combining global
and local information. Building upon our solutions, we introduce SHViT, a
Single-Head Vision Transformer that obtains the state-of-the-art speed-accuracy
tradeoff. For example, on ImageNet-1k, our SHViT-S4 is 3.3x, 8.1x, and 2.4x
faster than MobileViTv2 x1.0 on GPU, CPU, and iPhone12 mobile device,
respectively, while being 1.3% more accurate. For object detection and instance
segmentation on MS COCO using Mask-RCNN head, our model achieves performance
comparable to FastViT-SA12 while exhibiting 3.8x and 2.0x lower backbone
latency on GPU and mobile device, respectively.","Seokju Yun, Youngmin Ro",2024-01-29T09:12:23Z,2024-03-27T04:14:59Z,http://arxiv.org/abs/2401.16456v2,http://arxiv.org/pdf/2401.16456v2.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
"Open-Source, Cost-Aware Kinematically Feasible Planning for Mobile and
  Surface Robotics","We present Smac Planner, an openly available, search-based planning framework
that addresses the critical need for kinematically feasible path planning
across diverse robot platforms. Smac Planner provides high-performance
implementations of Cost-Aware A*, Hybrid-A*, and State Lattice planners that
can be deployed for Ackermann, legged, and other large non-circular robots. Our
framework introduces novel ""Cost-Aware"" variations that significantly improve
performance in complex environments common to mobile robotics while maintaining
kinematic feasibility constraints. Integrated as the standard planning system
within the popular ROS 2 Navigation stack, Nav2, Smac Planner now powers
thousands of robots worldwide across academic research, commercial
applications, and field deployments.","Steve Macenski, Matthew Booker, Joshua Wallace, Tobias Fischer",2024-01-23T20:08:41Z,2025-05-08T23:31:39Z,http://arxiv.org/abs/2401.13078v2,http://arxiv.org/pdf/2401.13078v2.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2024
"Modular Customizable ROS-Based Framework for Rapid Development of Social
  Robots","Developing socially competent robots requires tight integration of robotics,
computer vision, speech processing, and web technologies. We present the
Socially-interactive Robot Software platform (SROS), an open-source framework
addressing this need through a modular layered architecture. SROS bridges the
Robot Operating System (ROS) layer for mobility with web and Android interface
layers using standard messaging and APIs. Specialized perceptual and
interactive skills are implemented as ROS services for reusable deployment on
any robot. This facilitates rapid prototyping of collaborative behaviors that
synchronize perception with physical actuation. We experimentally validated
core SROS technologies including computer vision, speech processing, and GPT2
autocomplete speech implemented as plug-and-play ROS services. Modularity is
demonstrated through the successful integration of an additional ROS package,
without changes to hardware or software platforms. The capabilities enabled
confirm SROS's effectiveness in developing socially interactive robots through
synchronized cross-domain interaction. Through demonstrations showing
synchronized multimodal behaviors on an example platform, we illustrate how the
SROS architectural approach addresses shortcomings of previous work by lowering
barriers for researchers to advance the state-of-the-art in adaptive,
collaborative customizable human-robot systems through novel applications
integrating perceptual and social abilities.","Mahta Akhyani, Hadi Moradi",2023-11-27T12:54:20Z,2023-11-27T12:54:20Z,http://arxiv.org/abs/2311.15780v1,http://arxiv.org/pdf/2311.15780v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2023
"Racing With ROS 2 A Navigation System for an Autonomous Formula Student
  Race Car","The advent of autonomous vehicle technologies has significantly impacted
various sectors, including motorsport, where Formula Student and Formula:
Society of Automotive Engineers introduced autonomous racing classes. These
offer new challenges to aspiring engineers, including the team at QUT
Motorsport, but also raise the entry barrier due to the complexity of
high-speed navigation and control. This paper presents an open-source solution
using the Robot Operating System 2, specifically its open-source navigation
stack, to address these challenges in autonomous Formula Student race cars. We
compare off-the-shelf navigation libraries that this stack comprises of against
traditional custom-made programs developed by QUT Motorsport to evaluate their
applicability in autonomous racing scenarios and integrate them onto an
autonomous race car. Our contributions include quantitative and qualitative
comparisons of these packages against traditional navigation solutions, aiming
to lower the entry barrier for autonomous racing. This paper also serves as a
comprehensive tutorial for teams participating in similar racing disciplines
and other autonomous mobile robot applications.","Alastair Bradford, Grant van Breda, Tobias Fischer",2023-11-24T04:40:26Z,2023-11-24T04:40:26Z,http://arxiv.org/abs/2311.14276v1,http://arxiv.org/pdf/2311.14276v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2023
"The Otbot project: Dynamic modelling, parameter identification, and
  motion control of an omnidirectional tire-wheeled robot","In recent years, autonomous mobile platforms are finding an increasing range
of applications in inspection or surveillance tasks, or to the transport of
objects, in places such as smart warehouses, factories or hospitals. In these
environments it is useful for the robot to have omnidirectional capability in
the plane, so it can navigate through narrow or cluttered areas, or make
position and orientation changes without having to maneuver. While this
capability is usually achieved with directional sliding wheels, this work
studies a particular robot that achieves omnidirectionality using conventional
wheels, which are easier to manufacture and maintain, and support larger loads
in general. This robot, which we call ``Otbot'' (for omnidirectional
tire-wheeled robot), was already conceived in the late 1990s, but all the
controllers that have been proposed for it are based on purely kinematic models
so far. These controllers may be sufficient if the robot is light, or if its
motors are powerful, but on platforms that have to carry large loads, or that
have more limited motors, it is necessary to resort to control laws based on
dynamic models if the full acceleration capacities are to be exploited. This
work develops a dynamic model of Otbot, proposes a plausible methodology to
identify its parameters, and designs a control law that, using this model, is
able to track prescribed trajectories in an accurate and robust manner.","Pere Giró, Enric Celaya, Lluís Ros",2023-11-17T19:31:32Z,2023-11-17T19:31:32Z,http://arxiv.org/abs/2311.10834v1,http://arxiv.org/pdf/2311.10834v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2023
"Collaborative Goal Tracking of Multiple Mobile Robots Based on Geometric
  Graph Neural Network","Multiple mobile robots play a significant role in various spatially
distributed tasks.In unfamiliar and non-repetitive scenarios, reconstructing
the global map is time-inefficient and sometimes unrealistic. Hence, research
has focused on achieving real-time collaborative planning by utilizing sensor
data from multiple robots located at different positions, all without relying
on a global map.This paper introduces a Multi-Robot collaborative Path Planning
method based on Geometric Graph Neural Network (MRPP-GeoGNN). We extract the
features of each neighboring robot's sensory data and integrate the relative
positions of neighboring robots into each interaction layer to incorporate
obstacle information along with location details using geometric feature
encoders. After that, a MLP layer is used to map the amalgamated local features
to multiple forward directions for the robot's actual movement. We generated
expert data in ROS to train the network and carried out both simulations and
physical experiments to validate the effectiveness of the proposed method.
Simulation results demonstrate an approximate 5% improvement in accuracy
compared to the model based solely on CNN on expert datasets. The success rate
is enhanced by about 4% compared to CNN, and the flowtime increase is reduced
by approximately 18% in the ROS test, surpassing other GNN models. Besides, the
proposed method is able to leverage neighbor's information and greatly improves
path efficiency in real-world scenarios.","Weining Lu, Qingquan Lin, Litong Meng, Chenxi Li, Bin Liang",2023-11-13T06:40:31Z,2024-11-17T15:07:34Z,http://arxiv.org/abs/2311.07105v3,http://arxiv.org/pdf/2311.07105v3.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2023
"AdaptiX -- A Transitional XR Framework for Development and Evaluation of
  Shared Control Applications in Assistive Robotics","With the ongoing efforts to empower people with mobility impairments and the
increase in technological acceptance by the general public, assistive
technologies, such as collaborative robotic arms, are gaining popularity. Yet,
their widespread success is limited by usability issues, specifically the
disparity between user input and software control along the autonomy continuum.
To address this, shared control concepts provide opportunities to combine the
targeted increase of user autonomy with a certain level of computer assistance.
This paper presents the free and open-source AdaptiX XR framework for
developing and evaluating shared control applications in a high-resolution
simulation environment. The initial framework consists of a simulated robotic
arm with an example scenario in Virtual Reality (VR), multiple standard control
interfaces, and a specialized recording/replay system. AdaptiX can easily be
extended for specific research needs, allowing Human-Robot Interaction (HRI)
researchers to rapidly design and test novel interaction methods, intervention
strategies, and multi-modal feedback techniques, without requiring an actual
physical robotic arm during the early phases of ideation, prototyping, and
evaluation. Also, a Robot Operating System (ROS) integration enables the
controlling of a real robotic arm in a PhysicalTwin approach without any
simulation-reality gap. Here, we review the capabilities and limitations of
AdaptiX in detail and present three bodies of research based on the framework.
AdaptiX can be accessed at https://adaptix.robot-research.de.","Max Pascher, Felix Ferdinand Goldau, Kirill Kronhardt, Udo Frese, Jens Gerken",2023-10-24T14:44:41Z,2024-05-17T16:43:15Z,http://arxiv.org/abs/2310.15887v3,http://arxiv.org/pdf/2310.15887v3.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2023
"Socially reactive navigation models for mobile robots in dynamic
  environments","The objective of this work is to expand upon previous works, considering
socially acceptable behaviours within robot navigation and interaction, and
allow a robot to closely approach static and dynamic individuals or groups. The
space models developed in this dissertation are adaptive, that is, capable of
changing over time to accommodate the changing circumstances often existent
within a social environment. The space model's parameters' adaptation occurs
with the end goal of enabling a close interaction between humans and robots and
is thus capable of taking into account not only the arrangement of the groups,
but also the basic characteristics of the robot itself. This work also further
develops a preexisting approach pose estimation algorithm in order to better
guarantee the safety and comfort of the humans involved in the interaction, by
taking into account basic human sensibilities. The algorithms are integrated
into ROS's navigation system through the use of the $costmap2d$ and the
$move\_base$ packages. The space model adaptation is tested via comparative
evaluation against previous algorithms through the use of datasets. The entire
navigation system is then evaluated through both simulations (static and
dynamic) and real life situations (static). These experiments demonstrate that
the developed space model and approach pose estimation algorithms are capable
of enabling a robot to closely approach individual humans and groups, while
maintaining considerations for their comfort and sensibilities.","Ricarte Ribeiro, Plinio Moreno",2023-10-15T18:55:55Z,2023-10-15T18:55:55Z,http://arxiv.org/abs/2310.09916v1,http://arxiv.org/pdf/2310.09916v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2023
"An MCTS-DRL Based Obstacle and Occlusion Avoidance Methodology in
  Robotic Follow-Ahead Applications","We propose a novel methodology for robotic follow-ahead applications that
address the critical challenge of obstacle and occlusion avoidance. Our
approach effectively navigates the robot while ensuring avoidance of collisions
and occlusions caused by surrounding objects. To achieve this, we developed a
high-level decision-making algorithm that generates short-term navigational
goals for the mobile robot. Monte Carlo Tree Search is integrated with a Deep
Reinforcement Learning method to enhance the performance of the decision-making
process and generate more reliable navigational goals. Through extensive
experimentation and analysis, we demonstrate the effectiveness and superiority
of our proposed approach in comparison to the existing follow-ahead
human-following robotic methods. Our code is available at
https://github.com/saharLeisiazar/follow-ahead-ros.","Sahar Leisiazar, Edward J. Park, Angelica Lim, Mo Chen",2023-09-28T22:42:37Z,2023-09-28T22:42:37Z,http://arxiv.org/abs/2309.16884v1,http://arxiv.org/pdf/2309.16884v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2023
"Mapping Pipelines and Simultaneous Localization for Petrochemical
  Industry Robots","Inspecting petrochemical pipelines is challenging due to hazardous materials,
narrow diameters, and inaccessible locations. Mobile robots are promising for
autonomous pipeline inspection and mapping. This project aimed to simulate and
implement a robot capable of simultaneous localization and mapping (SLAM) in an
indoor maze-like environment representing simplified pipelines. The approach
involved simulating a differential drive robot in Gazebo/ROS, equipping it with
sensors, implementing SLAM using mapping, and path planning with move_base. A
physical robot was then built and tested by manually driving it in a
constructed maze while collecting sensor data and mapping. Sensor fusion of
wheel encoders, Kinect camera, and inertial measurement unit (IMU) data was
explored to improve odometry and mapping accuracy without encoders. The final
map had reasonable correspondence to the true maze despite lacking wheel
encoders. In summary, results show the feasibility of using ROS-based SLAM for
pipeline inspection if accounting for real-world complexities.",Mahta Akhyani,2023-09-28T21:46:56Z,2023-09-28T21:46:56Z,http://arxiv.org/abs/2311.11948v1,http://arxiv.org/pdf/2311.11948v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2023
"PRIEST: Projection Guided Sampling-Based Optimization For Autonomous
  Navigation","Efficient navigation in unknown and dynamic environments is crucial for
expanding the application domain of mobile robots. The core challenge stems
from the nonavailability of a feasible global path for guiding
optimization-based local planners. As a result, existing local planners often
get trapped in poor local minima. In this paper, we present a novel optimizer
that can explore multiple homotopies to plan high-quality trajectories over
long horizons while still being fast enough for real-time applications. We
build on the gradient-free paradigm by augmenting the trajectory sampling
strategy with a projection optimization that guides the samples toward a
feasible region. As a result, our approach can recover from the frequently
encountered pathological cases wherein all the sampled trajectories lie in the
high-cost region. Furthermore, we also show that our projection optimization
has a highly parallelizable structure that can be easily accelerated over GPUs.
We push the state-of-the-art in the following respects. Over the navigation
stack of the Robot Operating System (ROS), we show an improvement of 7-13% in
success rate and up to two times in total travel time metric. On the same
benchmarks and metrics, our approach achieves up to 44% improvement over MPPI
and its recent variants. On simple point-to-point navigation tasks, our
optimizer is up to two times more reliable than SOTA gradient-based solvers, as
well as sampling-based approaches such as the Cross-Entropy Method (CEM) and
VPSTO. Codes: https://github.com/fatemeh-rastgar/PRIEST","Fatemeh Rastgar, Houman Masnavi, Basant Sharma, Alvo Aabloo, Jan Swevers, Arun Kumar Singh",2023-09-15T08:12:48Z,2023-09-15T08:12:48Z,http://arxiv.org/abs/2309.08235v1,http://arxiv.org/pdf/2309.08235v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2023
"Robot-assisted Soil Apparent Electrical Conductivity Measurements in
  Orchards","Soil apparent electrical conductivity (ECa) is a vital metric in Precision
Agriculture and Smart Farming, as it is used for optimal water content
management, geological mapping, and yield prediction. Several existing methods
seeking to estimate soil electrical conductivity are available, including
physical soil sampling, ground sensor installation and monitoring, and the use
of sensors that can obtain proximal ECa estimates. However, such methods can be
either very laborious and/or too costly for practical use over larger field
canopies. Robot-assisted ECa measurements, in contrast, may offer a scalable
and cost-effective solution. In this work, we present one such solution that
involves a ground mobile robot equipped with a customized and adjustable
platform to hold an Electromagnetic Induction (EMI) sensor to perform
semi-autonomous and on-demand ECa measurements under various field conditions.
The platform is designed to be easily re-configurable in terms of sensor
placement; results from testing for traversability and robot-to-sensor
interference across multiple case studies help establish appropriate tradeoffs
for sensor placement. Further, a developed simulation software package enables
rapid and accessible estimation of terrain traversability in relation to
desired EMI sensor placement. Extensive experimental evaluation across
different fields demonstrates that the obtained robot-assisted ECa measurements
are of high linearity compared with the ground truth (data collected manually
by a handheld EMI sensor) by scoring more than $90\%$ in Pearson correlation
coefficient in both plot measurements and estimated ECa maps generated by
kriging interpolation. The proposed robotic solution supports autonomous
behavior development in the field since it utilizes the ROS navigation stack
along with the RTK GNSS positioning data and features various ranging sensors.","Dimitrios Chatziparaschis, Elia Scudiero, Konstantinos Karydis",2023-09-10T20:23:00Z,2023-09-10T20:23:00Z,http://arxiv.org/abs/2309.05128v1,http://arxiv.org/pdf/2309.05128v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2023
"osmAG: Hierarchical Semantic Topometric Area Graph Maps in the OSM
  Format for Mobile Robotics","Maps are essential to mobile robotics tasks like localization and planning.
We propose the open street map (osm) XML based Area Graph file format to store
hierarchical, topometric semantic multi-floor maps of indoor and outdoor
environments, since currently no such format is popular within the robotics
community. Building on-top of osm we leverage the available open source editing
tools and libraries of osm, while adding the needed mobile robotics aspect with
building-level obstacle representation yet very compact, topometric data that
facilitates planning algorithms. Through the use of common osm keys as well as
custom ones we leverage the power of semantic annotation to enable various
applications. For example, we support planning based on robot capabilities, to
take the locomotion mode and attributes in conjunction with the environment
information into account. The provided C++ library is integrated into ROS. We
evaluate the performance of osmAG using real data in a global path planning
application on a very big osmAG map, demonstrating its convenience and
effectiveness for mobile robots.","Delin Feng, Chengqian Li, Yongqi Zhang, Chen Yu, Soeren Schwertfeger",2023-09-09T13:36:24Z,2023-09-09T13:36:24Z,http://arxiv.org/abs/2309.04791v1,http://arxiv.org/pdf/2309.04791v1.pdf,all:ROS AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2023
"Privacy Perceptions in Robot-Assisted Well-Being Coaching: Examining the
  Roles of Information Transparency, User Control, and Proactivity","Social robots are increasingly recognized as valuable supporters in the field
of well-being coaching. They can function as independent coaches or provide
support alongside human coaches, and healthcare professionals. In coaching
interactions, these robots often handle sensitive information shared by users,
making privacy a relevant issue. Despite this, little is known about the
factors that shape users' privacy perceptions. This research aims to examine
three key factors systematically: (1) the transparency about information usage,
(2) the level of specific user control over how the robot uses their
information, and (3) the robot's behavioral approach - whether it acts
proactively or only responds on demand. Our results from an online study (N =
200) show that even when users grant the robot general access to personal data,
they additionally expect the ability to explicitly control how that information
is interpreted and shared during sessions. Experimental conditions that
provided such control received significantly higher ratings for perceived
privacy appropriateness and trust. Compared to user control, the effects of
transparency and proactivity on privacy appropriateness perception were low,
and we found no significant impact. The results suggest that merely informing
users or proactive sharing is insufficient without accompanying user control.
These insights underscore the need for further research on mechanisms that
allow users to manage robots' information processing and sharing, especially
when social robots take on more proactive roles alongside humans.","Atikkhan Faridkhan Nilgar, Manuel Dietrich, Kristof Van Laerhoven",2025-09-04T16:19:24Z,2025-09-04T16:19:24Z,http://arxiv.org/abs/2509.04358v1,http://arxiv.org/pdf/2509.04358v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Dependency Chain Analysis of ROS 2 DDS QoS Policies: From Lifecycle
  Tutorial to Static Verification","Robot Operating System 2 (ROS 2) relies on the Data Distribution Service
(DDS), which offers more than 20 Quality of Service (QoS) policies governing
availability, reliability, and resource usage. Yet ROS 2 users lack clear
guidance on safe policy combinations and validation processes prior to
deployment, which often leads to trial-and-error tuning and unexpected runtime
failures. To address these challenges, we analyze DDS Publisher-Subscriber
communication over a life cycle divided into Discovery, Data Exchange, and
Disassociation, and provide a user oriented tutorial explaining how 16 QoS
policies operate in each phase. Building on this analysis, we derive a QoS
dependency chain that formalizes inter-policy relationships and classifies 41
dependency violation rules, capturing constraints that commonly cause
communication failures in practice. Finally, we introduce QoS Guard, a ROS 2
package that statically validates DDS XML profiles offline, flags conflicts,
and enables safe, predeployment tuning without establishing a live ROS 2
session. Together, these contributions give ROS 2 users both conceptual insight
and a concrete tool that enables early detection of misconfigurations,
improving the reliability and resource efficiency of ROS 2 based robotic
systems.","Sanghoon Lee, Junha Kang, Kyung-Joon Park",2025-09-03T14:57:21Z,2025-09-03T14:57:21Z,http://arxiv.org/abs/2509.03381v1,http://arxiv.org/pdf/2509.03381v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Analyzing Reluctance to Ask for Help When Cooperating With Robots:
  Insights to Integrate Artificial Agents in HRC","As robot technology advances, collaboration between humans and robots will
become more prevalent in industrial tasks. When humans run into issues in such
scenarios, a likely future involves relying on artificial agents or robots for
aid. This study identifies key aspects for the design of future user-assisting
agents. We analyze quantitative and qualitative data from a user study
examining the impact of on-demand assistance received from a remote human in a
human-robot collaboration (HRC) assembly task. We study scenarios in which
users require help and we assess their experiences in requesting and receiving
assistance. Additionally, we investigate participants' perceptions of future
non-human assisting agents and whether assistance should be on-demand or
unsolicited. Through a user study, we analyze the impact that such design
decisions (human or artificial assistant, on-demand or unsolicited help) can
have on elicited emotional responses, productivity, and preferences of humans
engaged in HRC tasks.","Ane San Martin, Michael Hagenow, Julie Shah, Johan Kildal, Elena Lazkano",2025-09-01T13:08:09Z,2025-09-01T13:08:09Z,http://arxiv.org/abs/2509.01450v1,http://arxiv.org/pdf/2509.01450v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Autonomous Aggregate Sorting in Construction and Mining via Computer
  Vision-Aided Robotic Arm Systems","Traditional aggregate sorting methods, whether manual or mechanical, often
suffer from low precision, limited flexibility, and poor adaptability to
diverse material properties such as size, shape, and lithology. To address
these limitations, this study presents a computer vision-aided robotic arm
system designed for autonomous aggregate sorting in construction and mining
applications. The system integrates a six-degree-of-freedom robotic arm, a
binocular stereo camera for 3D perception, and a ROS-based control framework.
Core techniques include an attention-augmented YOLOv8 model for aggregate
detection, stereo matching for 3D localization, Denavit-Hartenberg kinematic
modeling for arm motion control, minimum enclosing rectangle analysis for size
estimation, and hand-eye calibration for precise coordinate alignment.
Experimental validation with four aggregate types achieved an average grasping
and sorting success rate of 97.5%, with comparable classification accuracy.
Remaining challenges include the reliable handling of small aggregates and
texture-based misclassification. Overall, the proposed system demonstrates
significant potential to enhance productivity, reduce operational costs, and
improve safety in aggregate handling, while providing a scalable framework for
advancing smart automation in construction, mining, and recycling industries.","Md. Taherul Islam Shawon, Yuan Li, Yincai Cai, Junjie Niu, Ting Peng",2025-08-30T03:44:11Z,2025-08-30T03:44:11Z,http://arxiv.org/abs/2509.00339v1,http://arxiv.org/pdf/2509.00339v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Context-Aware Risk Estimation in Home Environments: A Probabilistic
  Framework for Service Robots","We present a novel framework for estimating accident-prone regions in
everyday indoor scenes, aimed at improving real-time risk awareness in service
robots operating in human-centric environments. As robots become integrated
into daily life, particularly in homes, the ability to anticipate and respond
to environmental hazards is crucial for ensuring user safety, trust, and
effective human-robot interaction. Our approach models object-level risk and
context through a semantic graph-based propagation algorithm. Each object is
represented as a node with an associated risk score, and risk propagates
asymmetrically from high-risk to low-risk objects based on spatial proximity
and accident relationship. This enables the robot to infer potential hazards
even when they are not explicitly visible or labeled. Designed for
interpretability and lightweight onboard deployment, our method is validated on
a dataset with human-annotated risk regions, achieving a binary risk detection
accuracy of 75%. The system demonstrates strong alignment with human
perception, particularly in scenes involving sharp or unstable objects. These
results underline the potential of context-aware risk reasoning to enhance
robotic scene understanding and proactive safety behaviors in shared
human-robot spaces. This framework could serve as a foundation for future
systems that make context-driven safety decisions, provide real-time alerts, or
autonomously assist users in avoiding or mitigating hazards within home
environments.","Sena Ishii, Akash Chikhalikar, Ankit A. Ravankar, Jose Victorio Salazar Luces, Yasuhisa Hirata",2025-08-27T11:14:05Z,2025-08-27T11:14:05Z,http://arxiv.org/abs/2508.19788v1,http://arxiv.org/pdf/2508.19788v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"AS2FM: Enabling Statistical Model Checking of ROS 2 Systems for Robust
  Autonomy","Designing robotic systems to act autonomously in unforeseen environments is a
challenging task. This work presents a novel approach to use formal
verification, specifically Statistical Model Checking (SMC), to verify system
properties of autonomous robots at design-time. We introduce an extension of
the SCXML format, designed to model system components including both Robot
Operating System 2 (ROS 2) and Behavior Tree (BT) features. Further, we
contribute Autonomous Systems to Formal Models (AS2FM), a tool to translate the
full system model into JANI. The use of JANI, a standard format for
quantitative model checking, enables verification of system properties with
off-the-shelf SMC tools. We demonstrate the practical usability of AS2FM both
in terms of applicability to real-world autonomous robotic control systems, and
in terms of verification runtime scaling. We provide a case study, where we
successfully identify problems in a ROS 2-based robotic manipulation use case
that is verifiable in less than one second using consumer hardware.
Additionally, we compare to the state of the art and demonstrate that our
method is more comprehensive in system feature support, and that the
verification runtime scales linearly with the size of the model, instead of
exponentially.","Christian Henkel, Marco Lampacrescia, Michaela Klauck, Matteo Morelli",2025-08-26T09:00:18Z,2025-08-26T09:00:18Z,http://arxiv.org/abs/2508.18820v1,http://arxiv.org/pdf/2508.18820v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Integration of Computer Vision with Adaptive Control for Autonomous
  Driving Using ADORE","Ensuring safety in autonomous driving requires a seamless integration of
perception and decision making under uncertain conditions. Although computer
vision (CV) models such as YOLO achieve high accuracy in detecting traffic
signs and obstacles, their performance degrades in drift scenarios caused by
weather variations or unseen objects. This work presents a simulated autonomous
driving system that combines a context aware CV model with adaptive control
using the ADORE framework. The CARLA simulator was integrated with ADORE via
the ROS bridge, allowing real-time communication between perception, decision,
and control modules. A simulated test case was designed in both clear and drift
weather conditions to demonstrate the robust detection performance of the
perception model while ADORE successfully adapted vehicle behavior to speed
limits and obstacles with low response latency. The findings highlight the
potential of coupling deep learning-based perception with rule-based adaptive
decision making to improve automotive safety critical system.","Abu Shad Ahammed, Md Shahi Amran Hossain, Sayeri Mukherjee, Roman Obermaisser, Md. Ziaur Rahman",2025-08-25T12:55:30Z,2025-09-02T19:01:33Z,http://arxiv.org/abs/2508.17985v2,http://arxiv.org/pdf/2508.17985v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
SoK: Cybersecurity Assessment of Humanoid Ecosystem,"Humanoids are progressing toward practical deployment across healthcare,
industrial, defense, and service sectors. While typically considered
cyber-physical systems (CPSs), their dependence on traditional networked
software stacks (e.g., Linux operating systems), robot operating system (ROS)
middleware, and over-the-air update channels, creates a distinct security
profile that exposes them to vulnerabilities conventional CPS models do not
fully address. Prior studies have mainly examined specific threats, such as
LiDAR spoofing or adversarial machine learning (AML). This narrow focus
overlooks how an attack targeting one component can cascade harm throughout the
robot's interconnected systems. We address this gap through a systematization
of knowledge (SoK) that takes a comprehensive approach, consolidating
fragmented research from robotics, CPS, and network security domains. We
introduce a seven-layer security model for humanoid robots, organizing 39 known
attacks and 35 defenses across the humanoid ecosystem-from hardware to
human-robot interaction. Building on this security model, we develop a
quantitative 39x35 attack-defense matrix with risk-weighted scoring, validated
through Monte Carlo analysis. We demonstrate our method by evaluating three
real-world robots: Pepper, G1 EDU, and Digit. The scoring analysis revealed
varying security maturity levels, with scores ranging from 39.9% to 79.5%
across the platforms. This work introduces a structured, evidence-based
assessment method that enables systematic security evaluation, supports
cross-platform benchmarking, and guides prioritization of security investments
in humanoid robotics.","Priyanka Prakash Surve, Asaf Shabtai, Yuval Elovici",2025-08-24T18:13:33Z,2025-09-01T16:04:07Z,http://arxiv.org/abs/2508.17481v2,http://arxiv.org/pdf/2508.17481v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"A Rapid Iterative Trajectory Planning Method for Automated Parking
  through Differential Flatness","As autonomous driving continues to advance, automated parking is becoming
increasingly essential. However, significant challenges arise when implementing
path velocity decomposition (PVD) trajectory planning for automated parking.
The primary challenge is ensuring rapid and precise collision-free trajectory
planning, which is often in conflict. The secondary challenge involves
maintaining sufficient control feasibility of the planned trajectory,
particularly at gear shifting points (GSP). This paper proposes a PVD-based
rapid iterative trajectory planning (RITP) method to solve the above
challenges. The proposed method effectively balances the necessity for time
efficiency and precise collision avoidance through a novel collision avoidance
framework. Moreover, it enhances the overall control feasibility of the planned
trajectory by incorporating the vehicle kinematics model and including terminal
smoothing constraints (TSC) at GSP during path planning. Specifically, the
proposed method leverages differential flatness to ensure the planned path
adheres to the vehicle kinematic model. Additionally, it utilizes TSC to
maintain curvature continuity at GSP, thereby enhancing the control feasibility
of the overall trajectory. The simulation results demonstrate superior time
efficiency and tracking errors compared to model-integrated and other
iteration-based trajectory planning methods. In the real-world experiment, the
proposed method was implemented and validated on a ROS-based vehicle,
demonstrating the applicability of the RITP method for real vehicles.","Zhouheng Li, Lei Xie, Cheng Hu, Hongye Su",2025-08-23T14:36:48Z,2025-08-23T14:36:48Z,http://arxiv.org/abs/2508.17038v1,http://arxiv.org/pdf/2508.17038v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"RoboBuddy in the Classroom: Exploring LLM-Powered Social Robots for
  Storytelling in Learning and Integration Activities","Creating and improvising scenarios for content approaching is an enriching
technique in education. However, it comes with a significant increase in the
time spent on its planning, which intensifies when using complex technologies,
such as social robots. Furthermore, addressing multicultural integration is
commonly embedded in regular activities due to the already tight curriculum.
Addressing these issues with a single solution, we implemented an intuitive
interface that allows teachers to create scenario-based activities from their
regular curriculum using LLMs and social robots. We co-designed different
frameworks of activities with 4 teachers and deployed it in a study with 27
students for 1 week. Beyond validating the system's efficacy, our findings
highlight the positive impact of integration policies perceived by the children
and demonstrate the importance of scenario-based activities in students'
enjoyment, observed to be significantly higher when applying storytelling.
Additionally, several implications of using LLMs and social robots in long-term
classroom activities are discussed.","Daniel Tozadore, Nur Ertug, Yasmine Chaker, Mortadha Abderrahim",2025-08-22T13:14:09Z,2025-08-22T13:14:09Z,http://arxiv.org/abs/2508.16706v1,http://arxiv.org/pdf/2508.16706v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Take That for Me: Multimodal Exophora Resolution with Interactive
  Questioning for Ambiguous Out-of-View Instructions","Daily life support robots must interpret ambiguous verbal instructions
involving demonstratives such as ``Bring me that cup,'' even when objects or
users are out of the robot's view. Existing approaches to exophora resolution
primarily rely on visual data and thus fail in real-world scenarios where the
object or user is not visible. We propose Multimodal Interactive Exophora
resolution with user Localization (MIEL), which is a multimodal exophora
resolution framework leveraging sound source localization (SSL), semantic
mapping, visual-language models (VLMs), and interactive questioning with
GPT-4o. Our approach first constructs a semantic map of the environment and
estimates candidate objects from a linguistic query with the user's skeletal
data. SSL is utilized to orient the robot toward users who are initially
outside its visual field, enabling accurate identification of user gestures and
pointing directions. When ambiguities remain, the robot proactively interacts
with the user, employing GPT-4o to formulate clarifying questions. Experiments
in a real-world environment showed results that were approximately 1.3 times
better when the user was visible to the robot and 2.0 times better when the
user was not visible to the robot, compared to the methods without SSL and
interactive questioning. The project website is
https://emergentsystemlabstudent.github.io/MIEL/.","Akira Oyama, Shoichi Hasegawa, Akira Taniguchi, Yoshinobu Hagiwara, Tadahiro Taniguchi",2025-08-22T07:09:06Z,2025-08-22T07:09:06Z,http://arxiv.org/abs/2508.16143v1,http://arxiv.org/pdf/2508.16143v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Securing Swarms: Cross-Domain Adaptation for ROS2-based CPS Anomaly
  Detection","Cyber-physical systems (CPS) are being increasingly utilized for critical
applications. CPS combines sensing and computing elements, often having
multi-layer designs with networking, computational, and physical interfaces,
which provide them with enhanced capabilities for a variety of application
scenarios. However, the combination of physical and computational elements also
makes CPS more vulnerable to attacks compared to network-only systems, and the
resulting impacts of CPS attacks can be substantial. Intelligent intrusion
detection systems (IDS) are an effective mechanism by which CPS can be secured,
but the majority of current solutions often train and validate on network
traffic-only datasets, ignoring the distinct attacks that may occur on other
system layers. In order to address this, we develop an adaptable CPS anomaly
detection model that can detect attacks within CPS without the need for
previously labeled data. To achieve this, we utilize domain adaptation
techniques that allow us to transfer known attack knowledge from a network
traffic-only environment to a CPS environment. We validate our approach using a
state-of-the-art CPS intrusion dataset that combines network, operating system
(OS), and Robot Operating System (ROS) data. Through this dataset, we are able
to demonstrate the effectiveness of our model across network traffic-only and
CPS environments with distinct attack types and its ability to outperform other
anomaly detection methods.","Julia Boone, Fatemeh Afghah",2025-08-20T20:02:28Z,2025-08-20T20:02:28Z,http://arxiv.org/abs/2508.15865v1,http://arxiv.org/pdf/2508.15865v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
Toward an Interaction-Centered Approach to Robot Trustworthiness,"As robots get more integrated into human environments, fostering
trustworthiness in embodied robotic agents becomes paramount for an effective
and safe human-robot interaction (HRI). To achieve that, HRI applications must
promote human trust that aligns with robot skills and avoid misplaced trust or
overtrust, which can pose safety risks and ethical concerns. To achieve that,
HRI applications must promote human trust that aligns with robot skills and
avoid misplaced trust or overtrust, which can pose safety risks and ethical
concerns. In this position paper, we outline an interaction-based framework for
building trust through mutual understanding between humans and robots. We
emphasize two main pillars: human awareness and transparency, referring to the
robot ability to interpret human actions accurately and to clearly communicate
its intentions and goals, respectively. By integrating these two pillars,
robots can behave in a manner that aligns with human expectations and needs
while providing their human partners with both comprehension and control over
their actions. We also introduce four components that we think are important
for bridging the gap between a human-perceived sense of trust and a robot true
capabilities.","Carlo Mazzola, Hassan Ali, Kristína Malinovská, Igor Farkaš",2025-08-19T16:13:33Z,2025-08-19T16:13:33Z,http://arxiv.org/abs/2508.13976v1,http://arxiv.org/pdf/2508.13976v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
AutoMPC: A Code Generator for MPC-based Automated Driving,"Model Predictive Control (MPC) is a powerful technique to control nonlinear,
multi-input multi-output systems subject to input and state constraints. It is
now a standard tool for trajectory tracking control of automated vehicles. As
such it has been used in many research and development projects. However, MPC
faces several challenges to be integrated into industrial production vehicles.
The most important ones are its high computational demands and the complexity
of implementation. The software packages AutoMPC aims to address both of these
challenges. It builds on a robustified version of an active set algorithm for
Nonlinear MPC. The algorithm is embedded into a framework for vehicle
trajectory tracking, which makes it easy to used, yet highly customizable.
Automatic code generation transforms the selections into a standalone,
computationally efficient C-code file with static memory allocation. As such it
can be readily deployed on a wide range of embedded platforms, e.g., based on
Matlab/Simulink or Robot Operating System (ROS). Compared to a previous version
of the code, the vehicle model and the numerical integration method can be
manually specified, besides basic algorithm parameters. All of this information
and all specifications are directly baked into the generated C-code. The
algorithm is suitable driving scenarios at low or high speeds, even drifting,
and supports direction changes. Multiple simulation scenarios show the
versatility and effectiveness of the AutoMPC code, with the guarantee of a
feasible solution, a high degree of robustness, and computational efficiency.","Georg Schildbach, Jasper Pflughaupt",2025-08-19T09:04:43Z,2025-08-19T09:04:43Z,http://arxiv.org/abs/2508.13656v1,http://arxiv.org/pdf/2508.13656v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Insights from Interviews with Teachers and Students on the Use of a
  Social Robot in Computer Science Class in Sixth Grade","In this paper we report on first insights from interviews with teachers and
students on using social robots in computer science class in sixth grade. Our
focus is on learning about requirements and potential applications. We are
particularly interested in getting both perspectives, the teachers' and the
learners' view on how robots could be used and what features they should or
should not have. Results show that teachers as well as students are very open
to robots in the classroom. However, requirements are partially quite
heterogeneous among the groups. This leads to complex design challenges which
we discuss at the end of this paper.","Ann-Sophie L. Schenk, Stefan Schiffer, Heqiu Song",2025-08-18T14:22:28Z,2025-08-19T11:08:27Z,http://arxiv.org/abs/2508.12946v2,http://arxiv.org/pdf/2508.12946v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Implementation and evaluation of a prediction algorithm for an
  autonomous vehicle","This paper presents a prediction algorithm that estimates the vehicle
trajectory every five milliseconds for an autonomous vehicle. A kinematic and a
dynamic bicycle model are compared, with the dynamic model exhibiting superior
accuracy at higher speeds. Vehicle parameters such as mass, center of gravity,
moment of inertia, and cornering stiffness are determined experimentally. For
cornering stiffness, a novel measurement procedure using optical position
tracking is introduced. The model is incorporated into an extended Kalman
filter and implemented in a ROS node in C++. The algorithm achieves a
positional deviation of only 1.25 cm per meter over the entire test drive and
is up to 82.6% more precise than the kinematic model.",Marco Leon Rapp,2025-08-17T10:02:14Z,2025-08-17T10:02:14Z,http://arxiv.org/abs/2508.12312v1,http://arxiv.org/pdf/2508.12312v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
Into the Wild: When Robots Are Not Welcome,"Social robots are increasingly being deployed in public spaces, where they
face not only technological difficulties and unexpected user utterances, but
also objections from stakeholders who may not be comfortable with introducing a
robot into those spaces. We describe our difficulties with deploying a social
robot in two different public settings: 1) Student services center; 2) Refugees
and asylum seekers drop-in service. Although this is a failure report, in each
use case we eventually managed to earn the trust of the staff and form a
relationship with them, allowing us to deploy our robot and conduct our
studies.","Shaul Ashkenazi, Gabriel Skantze, Jane Stuart-Smith, Mary Ellen Foster",2025-08-16T15:18:17Z,2025-08-20T13:32:18Z,http://arxiv.org/abs/2508.12075v2,http://arxiv.org/pdf/2508.12075v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Swarm-in-Blocks: Simplifying Drone Swarm Programming with Block-Based
  Language","Swarm in Blocks, originally developed for CopterHack 2022, is a high-level
interface that simplifies drone swarm programming using a block-based language.
Building on the Clover platform, this tool enables users to create
functionalities like loops and conditional structures by assembling code
blocks. In 2023, we introduced Swarm in Blocks 2.0, further refining the
platform to address the complexities of swarm management in a user-friendly
way. As drone swarm applications grow in areas like delivery, agriculture, and
surveillance, the challenge of managing them, especially for beginners, has
also increased. The Atena team developed this interface to make swarm handling
accessible without requiring extensive knowledge of ROS or programming. The
block-based approach not only simplifies swarm control but also expands
educational opportunities in programming.","Agnes Bressan de Almeida, Joao Aires Correa Fernandes Marsicano",2025-08-15T14:20:09Z,2025-08-15T14:20:09Z,http://arxiv.org/abs/2508.11498v1,http://arxiv.org/pdf/2508.11498v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
Optimizing ROS 2 Communication for Wireless Robotic Systems,"Wireless transmission of large payloads, such as high-resolution images and
LiDAR point clouds, is a major bottleneck in ROS 2, the leading open-source
robotics middleware. The default Data Distribution Service (DDS) communication
stack in ROS 2 exhibits significant performance degradation over lossy wireless
links. Despite the widespread use of ROS 2, the underlying causes of these
wireless communication challenges remain unexplored. In this paper, we present
the first in-depth network-layer analysis of ROS 2's DDS stack under wireless
conditions with large payloads. We identify the following three key issues:
excessive IP fragmentation, inefficient retransmission timing, and congestive
buffer bursts. To address these issues, we propose a lightweight and fully
compatible DDS optimization framework that tunes communication parameters based
on link and payload characteristics. Our solution can be seamlessly applied
through the standard ROS 2 application interface via simple XML-based QoS
configuration, requiring no protocol modifications, no additional components,
and virtually no integration efforts. Extensive experiments across various
wireless scenarios demonstrate that our framework successfully delivers large
payloads in conditions where existing DDS modes fail, while maintaining low
end-to-end latency.","Sanghoon Lee, Taehun Kim, Jiyeong Chae, Kyung-Joon Park",2025-08-15T10:00:01Z,2025-08-15T10:00:01Z,http://arxiv.org/abs/2508.11366v1,http://arxiv.org/pdf/2508.11366v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Utilizing Vision-Language Models as Action Models for Intent Recognition
  and Assistance","Human-robot collaboration requires robots to quickly infer user intent,
provide transparent reasoning, and assist users in achieving their goals. Our
recent work introduced GUIDER, our framework for inferring navigation and
manipulation intents. We propose augmenting GUIDER with a vision-language model
(VLM) and a text-only language model (LLM) to form a semantic prior that
filters objects and locations based on the mission prompt. A vision pipeline
(YOLO for object detection and the Segment Anything Model for instance
segmentation) feeds candidate object crops into the VLM, which scores their
relevance given an operator prompt; in addition, the list of detected object
labels is ranked by a text-only LLM. These scores weight the existing
navigation and manipulation layers of GUIDER, selecting context-relevant
targets while suppressing unrelated objects. Once the combined belief exceeds a
threshold, autonomy changes occur, enabling the robot to navigate to the
desired area and retrieve the desired object, while adapting to any changes in
the operator's intent. Future work will evaluate the system on Isaac Sim using
a Franka Emika arm on a Ridgeback base, with a focus on real-time assistance.","Cesar Alan Contreras, Manolis Chiou, Alireza Rastegarpanah, Michal Szulik, Rustam Stolkin",2025-08-14T22:19:09Z,2025-08-14T22:19:09Z,http://arxiv.org/abs/2508.11093v1,http://arxiv.org/pdf/2508.11093v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Why Report Failed Interactions With Robots?! Towards Vignette-based
  Interaction Quality","Although the quality of human-robot interactions has improved with the advent
of LLMs, there are still various factors that cause systems to be sub-optimal
when compared to human-human interactions. The nature and criticality of
failures are often dependent on the context of the interaction and so cannot be
generalized across the wide range of scenarios and experiments which have been
implemented in HRI research. In this work we propose the use of a technique
overlooked in the field of HRI, ethnographic vignettes, to clearly highlight
these failures, particularly those that are rarely documented. We describe the
methodology behind the process of writing vignettes and create our own based on
our personal experiences with failures in HRI systems. We emphasize the
strength of vignettes as the ability to communicate failures from a
multi-disciplinary perspective, promote transparency about the capabilities of
robots, and document unexpected behaviours which would otherwise be omitted
from research reports. We encourage the use of vignettes to augment existing
interaction evaluation methods.","Agnes Axelsson, Merle Reimann, Ronald Cumbal, Hannah Pelikan, Divesh Lala",2025-08-14T12:44:39Z,2025-08-15T16:07:03Z,http://arxiv.org/abs/2508.10603v2,http://arxiv.org/pdf/2508.10603v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
Probabilistic Latency Analysis of the Data Distribution Service in ROS 2,"Robot Operating System 2 (ROS 2) is now the de facto standard for robotic
communication, pairing UDP transport with the Data Distribution Service (DDS)
publish-subscribe middleware. DDS achieves reliability through periodic
heartbeats that solicit acknowledgments for missing samples and trigger
selective retransmissions. In lossy wireless networks, the tight coupling among
heartbeat period, IP fragmentation, and retransmission interval obscures end to
end latency behavior and leaves practitioners with little guidance on how to
tune these parameters. To address these challenges, we propose a probabilistic
latency analysis (PLA) that analytically models the reliable transmission
process of ROS 2 DDS communication using a discrete state approach. By
systematically analyzing both middleware level and transport level events, PLA
computes the steady state probability distribution of unacknowledged messages
and the retransmission latency. We validate our PLA across 270 scenarios,
exploring variations in packet delivery ratios, message sizes, and both
publishing and retransmission intervals, demonstrating a close alignment
between analytical predictions and experimental results. Our findings establish
a theoretical basis to systematically optimize reliability, latency, and
performance in wireless industrial robotics.","Sanghoon Lee, Hyung-Seok Park, Jiyeong Chae, Kyung-Joon Park",2025-08-14T07:36:51Z,2025-08-14T07:36:51Z,http://arxiv.org/abs/2508.10413v1,http://arxiv.org/pdf/2508.10413v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Observations of atypical users from a pilot deployment of a public-space
  social robot in a church","Though a goal of HRI is the natural integration of social robots into
everyday public spaces, real-world studies still occur mostly within controlled
environments with predetermined participants. True public spaces present an
environment which is largely unconstrained and unpredictable, frequented by a
diverse range of people whose goals can often conflict with those of the robot.
When combined with the general unfamiliarity most people have with social
robots, this leads to unexpected human-robot interactions in these public
spaces that are rarely discussed or detected in other contexts. In this paper,
we describe atypical users we observed interacting with our robot, and those
who did not, during a three-day pilot deployment within a large working church
and visitor attraction. We then discuss theoretical future advances in the
field that could address these challenges, as well as immediate practical
mitigations and strategies to help improve public space human-robot
interactions in the present. This work contributes empirical insights into the
dynamics of human-robot interaction in public environments and offers
actionable guidance for more effective future deployments for social robot
designers.","Andrew Blair, Peggy Gregory, Mary Ellen Foster",2025-08-14T07:35:11Z,2025-08-14T07:35:11Z,http://arxiv.org/abs/2508.16622v1,http://arxiv.org/pdf/2508.16622v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"AZRA: Extending the Affective Capabilities of Zoomorphic Robots using
  Augmented Reality","Zoomorphic robots could serve as accessible and practical alternatives for
users unable or unwilling to keep pets. However, their affective interactions
are often simplistic and short-lived, limiting their potential for domestic
adoption. In order to facilitate more dynamic and nuanced affective
interactions and relationships between users and zoomorphic robots we present
AZRA, a novel augmented reality (AR) framework that extends the affective
capabilities of these robots without physical modifications. To demonstrate
AZRA, we augment a zoomorphic robot, Petit Qoobo, with novel emotional displays
(face, light, sound, thought bubbles) and interaction modalities (voice, touch,
proximity, gaze). Additionally, AZRA features a computational model of emotion
to calculate the robot's emotional responses, daily moods, evolving personality
and needs. We highlight how AZRA can be used for rapid participatory
prototyping and enhancing existing robots, then discuss implications on future
zoomorphic robot development.","Shaun Macdonald, Salma ElSayed, Mark McGill",2025-08-11T22:39:13Z,2025-08-11T22:39:13Z,http://arxiv.org/abs/2508.08507v1,http://arxiv.org/pdf/2508.08507v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"MonoMPC: Monocular Vision Based Navigation with Learned Collision Model
  and Risk-Aware Model Predictive Control","Navigating unknown environments with a single RGB camera is challenging, as
the lack of depth information prevents reliable collision-checking. While some
methods use estimated depth to build collision maps, we found that depth
estimates from vision foundation models are too noisy for zero-shot navigation
in cluttered environments.
  We propose an alternative approach: instead of using noisy estimated depth
for direct collision-checking, we use it as a rich context input to a learned
collision model. This model predicts the distribution of minimum obstacle
clearance that the robot can expect for a given control sequence. At inference,
these predictions inform a risk-aware MPC planner that minimizes estimated
collision risk. Our joint learning pipeline co-trains the collision model and
risk metric using both safe and unsafe trajectories. Crucially, our
joint-training ensures optimal variance in our collision model that improves
navigation in highly cluttered environments. Consequently, real-world
experiments show 9x and 7x improvements in success rates over NoMaD and the ROS
stack, respectively. Ablation studies further validate the effectiveness of our
design choices.","Basant Sharma, Prajyot Jadhav, Pranjal Paul, K. Madhava Krishna, Arun Kumar Singh",2025-08-10T15:27:23Z,2025-08-10T15:27:23Z,http://arxiv.org/abs/2508.07387v1,http://arxiv.org/pdf/2508.07387v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
Manipulator for people with limited abilities,"The topic of this final qualification work was chosen due to the importance
of developing robotic systems designed to assist people with disabilities.
Advances in robotics and automation technologies have opened up new prospects
for creating devices that can significantly improve the quality of life for
these people. In this context, designing a robotic hand with a control system
adapted to the needs of people with disabilities is a major scientific and
practical challenge. This work addresses the problem of developing and
manufacturing a four-degree-of-freedom robotic hand suitable for practical
manipulation. Addressing this issue requires a comprehensive approach,
encompassing the design of the hand's mechanical structure, the development of
its control system, and its integration with a technical vision system and
software based on the Robot Operating System (ROS).","Bingkun Huang, Evgeniy Kotov, Arkady Yuschenko",2025-08-09T12:35:31Z,2025-08-09T12:35:31Z,http://arxiv.org/abs/2508.06969v1,http://arxiv.org/pdf/2508.06969v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
Robots can defuse high-intensity conflict situations,"This paper investigates the specific scenario of high-intensity
confrontations between humans and robots, to understand how robots can defuse
the conflict. It focuses on the effectiveness of using five different affective
expression modalities as main drivers for defusing the conflict. The aim is to
discover any strengths or weaknesses in using each modality to mitigate the
hostility that people feel towards a poorly performing robot. The defusing of
the situation is accomplished by making the robot better at acknowledging the
conflict and by letting it express remorse. To facilitate the tests, we used a
custom affective robot in a simulated conflict situation with 105 test
participants. The results show that all tested expression modalities can
successfully be used to defuse the situation and convey an acknowledgment of
the confrontation. The ratings were remarkably similar, but the movement
modality was different (ANON p$<$.05) than the other modalities. The test
participants also had similar affective interpretations on how impacted the
robot was of the confrontation across all expression modalities. This indicates
that defusing a high-intensity interaction may not demand special attention to
the expression abilities of the robot, but rather require attention to the
abilities of being socially aware of the situation and reacting in accordance
with it.","Morten Roed Frederiksen, Kasper Støy",2025-08-07T13:16:46Z,2025-08-07T13:16:46Z,http://arxiv.org/abs/2508.05373v1,http://arxiv.org/pdf/2508.05373v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
Affecta-Context: The Context-Guided Behavior Adaptation Framework,"This paper presents Affecta-context, a general framework to facilitate
behavior adaptation for social robots. The framework uses information about the
physical context to guide its behaviors in human-robot interactions. It
consists of two parts: one that represents encountered contexts and one that
learns to prioritize between behaviors through human-robot interactions. As
physical contexts are encountered the framework clusters them by their measured
physical properties. In each context, the framework learns to prioritize
between behaviors to optimize the physical attributes of the robot's behavior
in line with its current environment and the preferences of the users it
interacts with. This paper illlustrates the abilities of the Affecta-context
framework by enabling a robot to autonomously learn the prioritization of
discrete behaviors. This was achieved by training across 72 interactions in two
different physical contexts with 6 different human test participants. The paper
demonstrates the trained Affecta-context framework by verifying the robot's
ability to generalize over the input and to match its behaviors to a previously
unvisited physical context.","Morten Roed Frederiksen, Kasper Støy",2025-08-07T13:07:44Z,2025-08-07T13:07:44Z,http://arxiv.org/abs/2508.05359v1,http://arxiv.org/pdf/2508.05359v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Towards Embodied Agentic AI: Review and Classification of LLM- and
  VLM-Driven Robot Autonomy and Interaction","Foundation models, including large language models (LLMs) and vision-language
models (VLMs), have recently enabled novel approaches to robot autonomy and
human-robot interfaces. In parallel, vision-language-action models (VLAs) or
large behavior models (LBMs) are increasing the dexterity and capabilities of
robotic systems. This survey paper focuses on those works advancing towards
agentic applications and architectures. This includes initial efforts exploring
GPT-style interfaces to tooling, as well as more complex system where AI agents
are coordinators, planners, perception actors, or generalist interfaces. Such
agentic architectures allow robots to reason over natural language
instructions, invoke APIs, plan task sequences, or assist in operations and
diagnostics. In addition to peer-reviewed research, due to the fast-evolving
nature of the field, we highlight and include community-driven projects, ROS
packages, and industrial frameworks that show emerging trends. We propose a
taxonomy for classifying model integration approaches and present a comparative
analysis of the role that agents play in different solutions in today's
literature.","Sahar Salimpour, Lei Fu, Farhad Keramat, Leonardo Militano, Giovanni Toffetti, Harry Edelman, Jorge Peña Queralta",2025-08-07T11:48:03Z,2025-08-14T12:55:31Z,http://arxiv.org/abs/2508.05294v2,http://arxiv.org/pdf/2508.05294v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"From Canada to Japan: How 10,000 km Affect User Perception in Robot
  Teleoperation","Robot teleoperation (RTo) has emerged as a viable alternative to local
control, particularly when human intervention is still necessary. This research
aims to study the distance effect on user perception in RTo, exploring the
potential of teleoperated robots for older adult care. We propose an evaluation
of non-expert users' perception of long-distance RTo, examining how their
perception changes before and after interaction, as well as comparing it to
that of locally operated robots. We have designed a specific protocol
consisting of multiple questionnaires, along with a dedicated software
architecture using the Robotics Operating System (ROS) and Unity. The results
revealed no statistically significant differences between the local and remote
robot conditions, suggesting that robots may be a viable alternative to
traditional local control.","Siméon Capy, Thomas M. Kwok, Kevin Joseph, Yuichiro Kawasumi, Koichi Nagashima, Tomoya Sasaki, Yue Hu, Eiichi Yoshida",2025-08-07T08:25:20Z,2025-08-07T08:25:20Z,http://arxiv.org/abs/2508.05143v1,http://arxiv.org/pdf/2508.05143v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"On the causality between affective impact and coordinated human-robot
  reactions","In an effort to improve how robots function in social contexts, this paper
investigates if a robot that actively shares a reaction to an event with a
human alters how the human perceives the robot's affective impact. To verify
this, we created two different test setups. One to highlight and isolate the
reaction element of affective robot expressions, and one to investigate the
effects of applying specific timing delays to a robot reacting to a physical
encounter with a human. The first test was conducted with two different groups
(n=84) of human observers, a test group and a control group both interacting
with the robot. The second test was performed with 110 participants using
increasingly longer reaction delays for the robot with every ten participants.
The results show a statistically significant change (p$<$.05) in perceived
affective impact for the robots when they react to an event shared with a human
observer rather than reacting at random. The result also shows for shared
physical interaction, the near-human reaction times from the robot are most
appropriate for the scenario. The paper concludes that a delay time around
200ms may render the biggest impact on human observers for small-sized
non-humanoid robots. It further concludes that a slightly shorter reaction time
around 100ms is most effective when the goal is to make the human observers
feel they made the biggest impact on the robot.","Morten Roed Frederiksen, Kasper Støy",2025-08-06T19:24:04Z,2025-08-06T19:24:04Z,http://arxiv.org/abs/2508.04834v1,http://arxiv.org/pdf/2508.04834v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
Tactile Comfort: Lowering Heart Rate Through Interactions,"Children diagnosed with anxiety disorders are taught a range of strategies to
navigate situations of heightened anxiety. Techniques such as deep breathing
and repetition of mantras are commonly employed, as they are known to be
calming and reduce elevated heart rates. Although these strategies are often
effective, their successful application relies on prior training of the
children for successful use when faced with challenging situations. This paper
investigates a pocket-sized companion robot designed to offer a relaxation
technique requiring no prior training, with a focus on immediate impact on the
user's heart rate. The robot utilizes a tactile game to divert the user's
attention, thereby promoting relaxation. We conducted two studies with children
who were not diagnosed with anxiety: a 14-day pilot study with two children
(age 8) and a main study with 18 children (ages 7-8). Both studies employed a
within-subjects design and focused on measuring heart rate during tactile
interaction with the robot and during non-use. Interacting with the robot was
found to significantly lower the study participants' heart rate (p$<$0.01)
compared to the non-use condition, indicating a consistent calming effect
across all participants. These results suggest that tactile companion robots
have the potential to enhance the therapeutic value of relaxation techniques.","Morten Roed Frederiksen, Kasper Støy, Maja Matarić",2025-08-06T12:06:35Z,2025-08-06T12:06:35Z,http://arxiv.org/abs/2508.04372v1,http://arxiv.org/pdf/2508.04372v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Stinger Robot: A Self-Bracing Robotic Platform for Autonomous Drilling
  in Confined Underground Environments","The increasing demand for critical raw materials has revitalized interest in
abandoned underground mines, which pose extreme challenges for conventional
drilling machinery due to confined, unstructured, and infrastructure-less
environments. This paper presents the Stinger Robot, a novel compact robotic
platform specifically designed for autonomous high-force drilling in such
settings. The robot features a mechanically self-locking tri-leg bracing
mechanism that enables stable anchoring to irregular tunnel surfaces. A key
innovation lies in its force-aware, closed-loop control strategy, which enables
force interaction with unstructured environments during bracing and drilling.
Implemented as a finite-state machine in ROS 2, the control policy dynamically
adapts leg deployment based on real-time contact feedback and load thresholds,
ensuring stability without external supports. We demonstrate, through
simulation and preliminary hardware tests, that the Stinger Robot can
autonomously stabilize and drill in conditions previously inaccessible to
nowadays mining machines. This work constitutes the first validated robotic
architecture to integrate distributed force-bracing and autonomous drilling in
underground environments, laying the groundwork for future collaborative mining
operations using modular robot systems.","H. Liu, L. S. Moreu, T. S. Andersen, V. V. Puche, M. Fumagalli",2025-07-31T20:05:14Z,2025-07-31T20:05:14Z,http://arxiv.org/abs/2508.06521v1,http://arxiv.org/pdf/2508.06521v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
Distributed AI Agents for Cognitive Underwater Robot Autonomy,"Achieving robust cognitive autonomy in robots navigating complex,
unpredictable environments remains a fundamental challenge in robotics. This
paper presents Underwater Robot Self-Organizing Autonomy (UROSA), a
groundbreaking architecture leveraging distributed Large Language Model AI
agents integrated within the Robot Operating System 2 (ROS 2) framework to
enable advanced cognitive capabilities in Autonomous Underwater Vehicles. UROSA
decentralises cognition into specialised AI agents responsible for multimodal
perception, adaptive reasoning, dynamic mission planning, and real-time
decision-making. Central innovations include flexible agents dynamically
adapting their roles, retrieval-augmented generation utilising vector databases
for efficient knowledge management, reinforcement learning-driven behavioural
optimisation, and autonomous on-the-fly ROS 2 node generation for runtime
functional extensibility. Extensive empirical validation demonstrates UROSA's
promising adaptability and reliability through realistic underwater missions in
simulation and real-world deployments, showing significant advantages over
traditional rule-based architectures in handling unforeseen scenarios,
environmental uncertainties, and novel mission objectives. This work not only
advances underwater autonomy but also establishes a scalable, safe, and
versatile cognitive robotics framework capable of generalising to a diverse
array of real-world applications.","Markus Buchholz, Ignacio Carlucho, Michele Grimaldi, Yvan R. Petillot",2025-07-31T17:18:55Z,2025-08-04T08:56:21Z,http://arxiv.org/abs/2507.23735v2,http://arxiv.org/pdf/2507.23735v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"RobEthiChor: Automated Context-aware Ethics-based Negotiation for
  Autonomous Robots","The presence of autonomous systems is growing at a fast pace and it is
impacting many aspects of our lives. Designed to learn and act independently,
these systems operate and perform decision-making without human intervention.
However, they lack the ability to incorporate users' ethical preferences, which
are unique for each individual in society and are required to personalize the
decision-making processes. This reduces user trust and prevents autonomous
systems from behaving according to the moral beliefs of their end-users. When
multiple systems interact with differing ethical preferences, they must
negotiate to reach an agreement that satisfies the ethical beliefs of all the
parties involved and adjust their behavior consequently. To address this
challenge, this paper proposes RobEthiChor, an approach that enables autonomous
systems to incorporate user ethical preferences and contextual factors into
their decision-making through ethics-based negotiation. RobEthiChor features a
domain-agnostic reference architecture for designing autonomous systems capable
of ethic-based negotiating. The paper also presents RobEthiChor-Ros, an
implementation of RobEthiChor within the Robot Operating System (ROS), which
can be deployed on robots to provide them with ethics-based negotiation
capabilities. To evaluate our approach, we deployed RobEthiChor-Ros on real
robots and ran scenarios where a pair of robots negotiate upon resource
contention. Experimental results demonstrate the feasibility and effectiveness
of the system in realizing ethics-based negotiation. RobEthiChor allowed robots
to reach an agreement in more than 73\% of the scenarios with an acceptable
negotiation time (0.67s on average). Experiments also demonstrate that the
negotiation approach implemented in RobEthiChor is scalable.","Mashal Afzal Memon, Gianluca Filippone, Gian Luca Scoccia, Marco Autili, Paola Inverardi",2025-07-30T13:21:38Z,2025-07-30T13:21:38Z,http://arxiv.org/abs/2507.22664v1,http://arxiv.org/pdf/2507.22664v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"DISCOVERSE: Efficient Robot Simulation in Complex High-Fidelity
  Environments","We present the first unified, modular, open-source 3DGS-based simulation
framework for Real2Sim2Real robot learning. It features a holistic Real2Sim
pipeline that synthesizes hyper-realistic geometry and appearance of complex
real-world scenarios, paving the way for analyzing and bridging the Sim2Real
gap. Powered by Gaussian Splatting and MuJoCo, Discoverse enables massively
parallel simulation of multiple sensor modalities and accurate physics, with
inclusive supports for existing 3D assets, robot models, and ROS plugins,
empowering large-scale robot learning and complex robotic benchmarks. Through
extensive experiments on imitation learning, Discoverse demonstrates
state-of-the-art zero-shot Sim2Real transfer performance compared to existing
simulators. For code and demos: https://air-discoverse.github.io/.","Yufei Jia, Guangyu Wang, Yuhang Dong, Junzhe Wu, Yupei Zeng, Haonan Lin, Zifan Wang, Haizhou Ge, Weibin Gu, Kairui Ding, Zike Yan, Yunjie Cheng, Yue Li, Ziming Wang, Chuxuan Li, Wei Sui, Lu Shi, Guanzhong Tian, Ruqi Huang, Guyue Zhou",2025-07-29T16:33:32Z,2025-07-29T16:33:32Z,http://arxiv.org/abs/2507.21981v1,http://arxiv.org/pdf/2507.21981v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"NMPCM: Nonlinear Model Predictive Control on Resource-Constrained
  Microcontrollers","Nonlinear Model Predictive Control (NMPC) is a powerful approach for
controlling highly dynamic robotic systems, as it accounts for system dynamics
and optimizes control inputs at each step. However, its high computational
complexity makes implementation on resource-constrained microcontrollers
impractical. While recent studies have demonstrated the feasibility of Model
Predictive Control (MPC) with linearized dynamics on microcontrollers, applying
full NMPC remains a significant challenge. This work presents an efficient
solution for generating and deploying NMPC on microcontrollers (NMPCM) to
control quadrotor UAVs. The proposed method optimizes computational efficiency
while maintaining high control accuracy. Simulations in Gazebo/ROS and
real-world experiments validate the effectiveness of the approach,
demonstrating its capability to achieve high-frequency NMPC execution in
real-time systems. The code is available at:
https://github.com/aralab-unr/NMPCM.","Van Chung Nguyen, Pratik Walunj, Chuong Le, An Duy Nguyen, Hung Manh La",2025-07-28T18:25:05Z,2025-07-28T18:25:05Z,http://arxiv.org/abs/2507.21259v1,http://arxiv.org/pdf/2507.21259v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"A real-time full-chain wearable sensor-based musculoskeletal simulation:
  an OpenSim-ROS Integration","Musculoskeletal modeling and simulations enable the accurate description and
analysis of the movement of biological systems with applications such as
rehabilitation assessment, prosthesis, and exoskeleton design. However, the
widespread usage of these techniques is limited by costly sensors,
laboratory-based setups, computationally demanding processes, and the use of
diverse software tools that often lack seamless integration. In this work, we
address these limitations by proposing an integrated, real-time framework for
musculoskeletal modeling and simulations that leverages OpenSimRT, the robotics
operating system (ROS), and wearable sensors. As a proof-of-concept, we
demonstrate that this framework can reasonably well describe inverse kinematics
of both lower and upper body using either inertial measurement units or
fiducial markers. Additionally, we show that it can effectively estimate
inverse dynamics of the ankle joint and muscle activations of major lower limb
muscles during daily activities, including walking, squatting and sit to stand,
stand to sit when combined with pressure insoles. We believe this work lays the
groundwork for further studies with more complex real-time and wearable
sensor-based human movement analysis systems and holds potential to advance
technologies in rehabilitation, robotics and exoskeleton designs.","Frederico Belmonte Klein, Zhaoyuan Wan, Huawei Wang, Ruoli Wang",2025-07-26T20:04:26Z,2025-07-26T20:04:26Z,http://arxiv.org/abs/2507.20049v1,http://arxiv.org/pdf/2507.20049v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Towards Multimodal Social Conversations with Robots: Using
  Vision-Language Models","Large language models have given social robots the ability to autonomously
engage in open-domain conversations. However, they are still missing a
fundamental social skill: making use of the multiple modalities that carry
social interactions. While previous work has focused on task-oriented
interactions that require referencing the environment or specific phenomena in
social interactions such as dialogue breakdowns, we outline the overall needs
of a multimodal system for social conversations with robots. We then argue that
vision-language models are able to process this wide range of visual
information in a sufficiently general manner for autonomous social robots. We
describe how to adapt them to this setting, which technical challenges remain,
and briefly discuss evaluation practices.","Ruben Janssens, Tony Belpaeme",2025-07-25T12:06:53Z,2025-08-18T16:27:19Z,http://arxiv.org/abs/2507.19196v2,http://arxiv.org/pdf/2507.19196v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Gaze-supported Large Language Model Framework for Bi-directional
  Human-Robot Interaction","The rapid development of Large Language Models (LLMs) creates an exciting
potential for flexible, general knowledge-driven Human-Robot Interaction (HRI)
systems for assistive robots. Existing HRI systems demonstrate great progress
in interpreting and following user instructions, action generation, and robot
task solving. On the other hand, bi-directional, multi-modal, and context-aware
support of the user in collaborative tasks still remains an open challenge. In
this paper, we present a gaze- and speech-informed interface to the assistive
robot, which is able to perceive the working environment from multiple vision
inputs and support the dynamic user in their tasks. Our system is designed to
be modular and transferable to adapt to diverse tasks and robots, and it is
capable of real-time use of language-based interaction state representation and
fast on board perception modules. Its development was supported by multiple
public dissemination events, contributing important considerations for improved
robustness and user experience. Furthermore, in two lab studies, we compare the
performance and user ratings of our system with those of a traditional scripted
HRI pipeline. Our findings indicate that an LLM-based approach enhances
adaptability and marginally improves user engagement and task execution metrics
but may produce redundant output, while a scripted pipeline is well suited for
more straightforward tasks.","Jens V. Rüppel, Andrey Rudenko, Tim Schreiter, Martin Magnusson, Achim J. Lilienthal",2025-07-21T15:38:25Z,2025-07-21T15:38:25Z,http://arxiv.org/abs/2507.15729v1,http://arxiv.org/pdf/2507.15729v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Digital twin and extended reality for teleoperation of the electric
  vehicle battery disassembly","Disassembling and sorting Electric Vehicle Batteries (EVBs) supports a
sustainable transition to electric vehicles by enabling a closed-loop supply
chain. Currently, the manual disassembly process exposes workers to hazards,
including electrocution and toxic chemicals. We propose a teleoperated system
for the safe disassembly and sorting of EVBs. A human-in-the-loop can create
and save disassembly sequences for unknown EVB types, enabling future
automation. An RGB camera aligns the physical and digital twins of the EVB, and
the digital twin of the robot is based on the Robot Operating System (ROS)
middleware. This hybrid approach combines teleoperation and automation to
improve safety, adaptability, and efficiency in EVB disassembly and sorting.
The economic contribution is realized by reducing labor dependency and
increasing throughput in battery recycling. An online pilot study was set up to
evaluate the usability of the presented approach, and the results demonstrate
the potential as a user-friendly solution.","Tero Kaarlela, Sami Salo, Jose Outeiro",2025-07-20T11:56:59Z,2025-07-20T11:56:59Z,http://arxiv.org/abs/2507.14929v1,http://arxiv.org/pdf/2507.14929v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"In-Home Social Robots Design for Cognitive Stimulation Therapy in
  Dementia Care","Individual cognitive stimulation therapy (iCST) is a non-pharmacological
intervention for improving the cognition and quality of life of persons with
dementia (PwDs); however, its effectiveness is limited by low adherence to
delivery by their family members. In this work, we present the user-centered
design and evaluation of a novel socially assistive robotic system to provide
iCST therapy to PwDs in their homes for long-term use. We consulted with 16
dementia caregivers and professionals. Through these consultations, we gathered
design guidelines and developed the prototype. The prototype was validated by
testing it with three dementia professionals and five PwDs. The evaluation
revealed PwDs enjoyed using the system and are willing to adopt its use over
the long term. One shortcoming was the system's speech-to-text capabilities,
where it frequently failed to understand the PwDs.","Emmanuel Akinrintoyo, Nicole Salomons",2025-07-17T23:48:09Z,2025-07-17T23:48:09Z,http://arxiv.org/abs/2507.13578v1,http://arxiv.org/pdf/2507.13578v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
An Empirical Study of Interaction Bugs in ROS-based Software,"Modern robotic systems integrate multiple independent software and hardware
components, each responsible for distinct functionalities such as perception,
decision-making, and execution. These components interact extensively to
accomplish complex end-to-end tasks. As a result, the overall system
reliability depends not only on the correctness of individual components, but
also on the correctness of their interactions. Failures often manifest at the
boundaries between components, yet interaction-related reliability issues in
robotics--referred to here as interaction bugs (iBugs)--remain underexplored.
  This work presents an empirical study of iBugs within robotic systems built
using the Robot Operating System (ROS), a widely adopted open-source robotics
framework. A total of 121 iBugs were analyzed across ten actively maintained
and representative ROS projects. The identified iBugs are categorized into
three major types: intra-system iBugs, hardware iBugs, and environmental iBugs,
covering a broad range of interaction scenarios in robotics. The analysis
includes an examination of root causes, fixing strategies, and the impact of
these bugs. Several findingsa are derived that shed light on the nature of
iBugs and suggest directions for improving their prevention and detection.
These insights aim to inform the design of more robust and safer robotic
systems.","Zhixiang Chen, Zhuangbin Chen, Xingjie Cai, Wei Li, Zibin Zheng",2025-07-14T12:56:24Z,2025-07-14T12:56:24Z,http://arxiv.org/abs/2507.10235v1,http://arxiv.org/pdf/2507.10235v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"ROS Help Desk: GenAI Powered, User-Centric Framework for ROS Error
  Diagnosis and Debugging","As the robotics systems increasingly integrate into daily life, from smart
home assistants to the new-wave of industrial automation systems (Industry
4.0), there's an increasing need to bridge the gap between complex robotic
systems and everyday users. The Robot Operating System (ROS) is a flexible
framework often utilised in writing robot software, providing tools and
libraries for building complex robotic systems. However, ROS's distributed
architecture and technical messaging system create barriers for understanding
robot status and diagnosing errors. This gap can lead to extended maintenance
downtimes, as users with limited ROS knowledge may struggle to quickly diagnose
and resolve system issues. Moreover, this deficit in expertise often delays
proactive maintenance and troubleshooting, further increasing the frequency and
duration of system interruptions. ROS Help Desk provides intuitive error
explanations and debugging support, dynamically customized to users of varying
expertise levels. It features user-centric debugging tools that simplify error
diagnosis, implements proactive error detection capabilities to reduce
downtime, and integrates multimodal data processing for comprehensive system
state understanding across multi-sensor data (e.g., lidar, RGB). Testing
qualitatively and quantitatively with artificially induced errors demonstrates
the system's ability to proactively and accurately diagnose problems,
ultimately reducing maintenance time and fostering more effective human-robot
collaboration.","Kavindie Katuwandeniya, Samith Rajapaksha Jayasekara Widhanapathirana",2025-07-10T15:24:31Z,2025-07-10T15:24:31Z,http://arxiv.org/abs/2507.07846v1,http://arxiv.org/pdf/2507.07846v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Toward a Full-Stack Co-Simulation Platform for Testing of Automated
  Driving Systems","Virtual testing has emerged as an effective approach to accelerate the
deployment of automated driving systems. Nevertheless, existing simulation
toolchains encounter difficulties in integrating rapid, automated scenario
generation with simulation environments supporting advanced automated driving
capabilities. To address this limitation, a full-stack toolchain is presented,
enabling automatic scenario generation from real-world datasets and efficient
validation through a co-simulation platform based on CarMaker, ROS, and Apollo.
The simulation results demonstrate the effectiveness of the proposed toolchain.
A demonstration video showcasing the toolchain is available at the provided
link: https://youtu.be/taJw_-CmSiY.","Dong Bi, Yongqi Zhao, Zhengguo Gu, Tomislav Mihalj, Jia Hu, Arno Eichberger",2025-07-09T14:19:58Z,2025-07-09T14:19:58Z,http://arxiv.org/abs/2507.06884v1,http://arxiv.org/pdf/2507.06884v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Integrating Perceptions: A Human-Centered Physical Safety Model for
  Human-Robot Interaction","Ensuring safety in human-robot interaction (HRI) is essential to foster user
trust and enable the broader adoption of robotic systems. Traditional safety
models primarily rely on sensor-based measures, such as relative distance and
velocity, to assess physical safety. However, these models often fail to
capture subjective safety perceptions, which are shaped by individual traits
and contextual factors. In this paper, we introduce and analyze a parameterized
general safety model that bridges the gap between physical and perceived safety
by incorporating a personalization parameter, $\rho$, into the safety
measurement framework to account for individual differences in safety
perception. Through a series of hypothesis-driven human-subject studies in a
simulated rescue scenario, we investigate how emotional state, trust, and robot
behavior influence perceived safety. Our results show that $\rho$ effectively
captures meaningful individual differences, driven by affective responses,
trust in task consistency, and clustering into distinct user types.
Specifically, our findings confirm that predictable and consistent robot
behavior as well as the elicitation of positive emotional states, significantly
enhance perceived safety. Moreover, responses cluster into a small number of
user types, supporting adaptive personalization based on shared safety models.
Notably, participant role significantly shapes safety perception, and repeated
exposure reduces perceived safety for participants in the casualty role,
emphasizing the impact of physical interaction and experiential change. These
findings highlight the importance of adaptive, human-centered safety models
that integrate both psychological and behavioral dimensions, offering a pathway
toward more trustworthy and effective HRI in safety-critical domains.","Pranav Pandey, Ramviyas Parasuraman, Prashant Doshi",2025-07-09T09:47:05Z,2025-07-09T09:47:05Z,http://arxiv.org/abs/2507.06700v1,http://arxiv.org/pdf/2507.06700v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
OTAS: Open-vocabulary Token Alignment for Outdoor Segmentation,"Understanding open-world semantics is critical for robotic planning and
control, particularly in unstructured outdoor environments. Current
vision-language mapping approaches rely on object-centric segmentation priors,
which often fail outdoors due to semantic ambiguities and indistinct semantic
class boundaries. We propose OTAS - an Open-vocabulary Token Alignment method
for Outdoor Segmentation. OTAS overcomes the limitations of open-vocabulary
segmentation models by extracting semantic structure directly from the output
tokens of pretrained vision models. By clustering semantically similar
structures across single and multiple views and grounding them in language,
OTAS reconstructs a geometrically consistent feature field that supports
open-vocabulary segmentation queries. Our method operates zero-shot, without
scene-specific fine-tuning, and runs at up to ~17 fps. OTAS provides a minor
IoU improvement over fine-tuned and open-vocabulary 2D segmentation methods on
the Off-Road Freespace Detection dataset. Our model achieves up to a 151% IoU
improvement over open-vocabulary mapping methods in 3D segmentation on
TartanAir. Real-world reconstructions demonstrate OTAS' applicability to
robotic applications. The code and ROS node will be made publicly available
upon paper acceptance.","Simon Schwaiger, Stefan Thalhammer, Wilfried Wöber, Gerald Steinbauer-Wagner",2025-07-08T22:49:03Z,2025-07-08T22:49:03Z,http://arxiv.org/abs/2507.08851v1,http://arxiv.org/pdf/2507.08851v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Comparative Evaluation of VR-Enabled Robots and Human Operators for
  Targeted Disease Management in Vineyards","This study explores the use of immersive virtual reality (VR) as a control
interface for agricultural robots in vineyard disease detection and treatment.
Using a Unity-ROS simulation, it compares three agents: a human operator, an
immersive VR-controlled robot, and a non-immersive VR-controlled robot. During
the scanning phase, humans perform best due to agility and control speed.
However, in the treatment phase, immersive VR robots outperform others,
completing tasks up to 65% faster by using stored infection data and optimized
path planning. In yield-map-based navigation, immersive robots are also 38%
faster than humans. Despite slower performance in manual scanning tasks,
immersive VR excels in memory-guided, repetitive operations. The study
highlights the role of interface design and path optimization, noting
limitations in simulation fidelity and generalizability. It concludes that
immersive VR has strong potential to enhance efficiency and precision in
precision agriculture.","Hasan Seyyedhasani, Daniel Udekwe, Muhammad Ali Qadri",2025-07-05T21:36:02Z,2025-07-05T21:36:02Z,http://arxiv.org/abs/2507.04167v1,http://arxiv.org/pdf/2507.04167v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
Image-driven Robot Drawing with Rapid Lognormal Movements,"Large image generation and vision models, combined with differentiable
rendering technologies, have become powerful tools for generating paths that
can be drawn or painted by a robot. However, these tools often overlook the
intrinsic physicality of the human drawing/writing act, which is usually
executed with skillful hand/arm gestures. Taking this into account is important
for the visual aesthetics of the results and for the development of closer and
more intuitive artist-robot collaboration scenarios. We present a method that
bridges this gap by enabling gradient-based optimization of natural human-like
motions guided by cost functions defined in image space. To this end, we use
the sigma-lognormal model of human hand/arm movements, with an adaptation that
enables its use in conjunction with a differentiable vector graphics (DiffVG)
renderer. We demonstrate how this pipeline can be used to generate feasible
trajectories for a robot by combining image-driven objectives with a
minimum-time smoothing criterion. We demonstrate applications with generation
and robotic reproduction of synthetic graffiti as well as image abstraction.","Daniel Berio, Guillaume Clivaz, Michael Stroh, Oliver Deussen, Réjean Plamondon, Sylvain Calinon, Frederic Fol Leymarie",2025-07-03T20:51:27Z,2025-07-03T20:51:27Z,http://arxiv.org/abs/2507.03166v1,http://arxiv.org/pdf/2507.03166v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
Personalised Explanations in Long-term Human-Robot Interactions,"In the field of Human-Robot Interaction (HRI), a fundamental challenge is to
facilitate human understanding of robots. The emerging domain of eXplainable
HRI (XHRI) investigates methods to generate explanations and evaluate their
impact on human-robot interactions. Previous works have highlighted the need to
personalise the level of detail of these explanations to enhance usability and
comprehension. Our paper presents a framework designed to update and retrieve
user knowledge-memory models, allowing for adapting the explanations' level of
detail while referencing previously acquired concepts. Three architectures
based on our proposed framework that use Large Language Models (LLMs) are
evaluated in two distinct scenarios: a hospital patrolling robot and a kitchen
assistant robot. Experimental results demonstrate that a two-stage
architecture, which first generates an explanation and then personalises it, is
the framework architecture that effectively reduces the level of detail only
when there is related user knowledge.","Ferran Gebellí, Anaís Garrell, Jan-Gerrit Habekost, Séverin Lemaignan, Stefan Wermter, Raquel Ros",2025-07-03T10:40:39Z,2025-07-03T10:40:39Z,http://arxiv.org/abs/2507.03049v1,http://arxiv.org/pdf/2507.03049v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Effective Explanations for Belief-Desire-Intention Robots: When and What
  to Explain","When robots perform complex and context-dependent tasks in our daily lives,
deviations from expectations can confuse users. Explanations of the robot's
reasoning process can help users to understand the robot intentions. However,
when to provide explanations and what they contain are important to avoid user
annoyance. We have investigated user preferences for explanation demand and
content for a robot that helps with daily cleaning tasks in a kitchen. Our
results show that users want explanations in surprising situations and prefer
concise explanations that clearly state the intention behind the confusing
action and the contextual factors that were relevant to this decision. Based on
these findings, we propose two algorithms to identify surprising actions and to
construct effective explanations for Belief-Desire-Intention (BDI) robots. Our
algorithms can be easily integrated in the BDI reasoning process and pave the
way for better human-robot interaction with context- and user-specific
explanations.","Cong Wang, Roberto Calandra, Verena Klös",2025-07-02T12:02:07Z,2025-07-02T12:02:07Z,http://arxiv.org/abs/2507.02016v1,http://arxiv.org/pdf/2507.02016v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
Online Human Action Detection during Escorting,"The deployment of robot assistants in large indoor spaces has seen
significant growth, with escorting tasks becoming a key application. However,
most current escorting robots primarily rely on navigation-focused strategies,
assuming that the person being escorted will follow without issue. In crowded
environments, this assumption often falls short, as individuals may struggle to
keep pace, become obstructed, get distracted, or need to stop unexpectedly. As
a result, conventional robotic systems are often unable to provide effective
escorting services due to their limited understanding of human movement
dynamics. To address these challenges, an effective escorting robot must
continuously detect and interpret human actions during the escorting process
and adjust its movement accordingly. However, there is currently no existing
dataset designed specifically for human action detection in the context of
escorting. Given that escorting often occurs in crowded environments, where
other individuals may enter the robot's camera view, the robot also needs to
identify the specific human it is escorting (the subject) before predicting
their actions. Since no existing model performs both person re-identification
and action prediction in real-time, we propose a novel neural network
architecture that can accomplish both tasks. This enables the robot to adjust
its speed dynamically based on the escortee's movements and seamlessly resume
escorting after any disruption. In comparative evaluations against strong
baselines, our system demonstrates superior efficiency and effectiveness,
showcasing its potential to significantly improve robotic escorting services in
complex, real-world scenarios.","Siddhartha Mondal, Avik Mitra, Chayan Sarkar",2025-06-30T07:25:31Z,2025-06-30T07:25:31Z,http://arxiv.org/abs/2506.23573v1,http://arxiv.org/pdf/2506.23573v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"MGPRL: Distributed Multi-Gaussian Processes for Wi-Fi-based Multi-Robot
  Relative Localization in Large Indoor Environments","Relative localization is a crucial capability for multi-robot systems
operating in GPS-denied environments. Existing approaches for multi-robot
relative localization often depend on costly or short-range sensors like
cameras and LiDARs. Consequently, these approaches face challenges such as high
computational overhead (e.g., map merging) and difficulties in disjoint
environments. To address this limitation, this paper introduces MGPRL, a novel
distributed framework for multi-robot relative localization using convex-hull
of multiple Wi-Fi access points (AP). To accomplish this, we employ
co-regionalized multi-output Gaussian Processes for efficient Radio Signal
Strength Indicator (RSSI) field prediction and perform uncertainty-aware
multi-AP localization, which is further coupled with weighted convex hull-based
alignment for robust relative pose estimation. Each robot predicts the RSSI
field of the environment by an online scan of APs in its environment, which are
utilized for position estimation of multiple APs. To perform relative
localization, each robot aligns the convex hull of its predicted AP locations
with that of the neighbor robots. This approach is well-suited for devices with
limited computational resources and operates solely on widely available Wi-Fi
RSSI measurements without necessitating any dedicated pre-calibration or
offline fingerprinting. We rigorously evaluate the performance of the proposed
MGPRL in ROS simulations and demonstrate it with real-world experiments,
comparing it against multiple state-of-the-art approaches. The results showcase
that MGPRL outperforms existing methods in terms of localization accuracy and
computational efficiency. Finally, we open source MGPRL as a ROS package
https://github.com/herolab-uga/MGPRL.","Sai Krishna Ghanta, Ramviyas Parasuraman",2025-06-30T04:35:00Z,2025-06-30T04:35:00Z,http://arxiv.org/abs/2506.23514v1,http://arxiv.org/pdf/2506.23514v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
Bootstrapping Human-Like Planning via LLMs,"Robot end users increasingly require accessible means of specifying tasks for
robots to perform. Two common end-user programming paradigms include
drag-and-drop interfaces and natural language programming. Although natural
language interfaces harness an intuitive form of human communication,
drag-and-drop interfaces enable users to meticulously and precisely dictate the
key actions of the robot's task. In this paper, we investigate the degree to
which both approaches can be combined. Specifically, we construct a large
language model (LLM)-based pipeline that accepts natural language as input and
produces human-like action sequences as output, specified at a level of
granularity that a human would produce. We then compare these generated action
sequences to another dataset of hand-specified action sequences. Although our
results reveal that larger models tend to outperform smaller ones in the
production of human-like action sequences, smaller models nonetheless achieve
satisfactory performance.","David Porfirio, Vincent Hsiao, Morgan Fine-Morris, Leslie Smith, Laura M. Hiatt",2025-06-27T20:00:51Z,2025-06-27T20:00:51Z,http://arxiv.org/abs/2506.22604v1,http://arxiv.org/pdf/2506.22604v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Evaluating Pointing Gestures for Target Selection in Human-Robot
  Collaboration","Pointing gestures are a common interaction method used in Human-Robot
Collaboration for various tasks, ranging from selecting targets to guiding
industrial processes. This study introduces a method for localizing pointed
targets within a planar workspace. The approach employs pose estimation, and a
simple geometric model based on shoulder-wrist extension to extract gesturing
data from an RGB-D stream. The study proposes a rigorous methodology and
comprehensive analysis for evaluating pointing gestures and target selection in
typical robotic tasks. In addition to evaluating tool accuracy, the tool is
integrated into a proof-of-concept robotic system, which includes object
detection, speech transcription, and speech synthesis to demonstrate the
integration of multiple modalities in a collaborative application. Finally, a
discussion over tool limitations and performance is provided to understand its
role in multimodal robotic systems. All developments are available at:
https://github.com/NMKsas/gesture_pointer.git.","Noora Sassali, Roel Pieters",2025-06-27T10:51:31Z,2025-06-27T10:51:31Z,http://arxiv.org/abs/2506.22116v1,http://arxiv.org/pdf/2506.22116v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"LMPVC and Policy Bank: Adaptive voice control for industrial robots with
  code generating LLMs and reusable Pythonic policies","Modern industry is increasingly moving away from mass manufacturing, towards
more specialized and personalized products. As manufacturing tasks become more
complex, full automation is not always an option, human involvement may be
required. This has increased the need for advanced human robot collaboration
(HRC), and with it, improved methods for interaction, such as voice control.
Recent advances in natural language processing, driven by artificial
intelligence (AI), have the potential to answer this demand. Large language
models (LLMs) have rapidly developed very impressive general reasoning
capabilities, and many methods of applying this to robotics have been proposed,
including through the use of code generation. This paper presents Language
Model Program Voice Control (LMPVC), an LLM-based prototype voice control
architecture with integrated policy programming and teaching capabilities,
built for use with Robot Operating System 2 (ROS2) compatible robots. The
architecture builds on prior works using code generation for voice control by
implementing an additional programming and teaching system, the Policy Bank. We
find this system can compensate for the limitations of the underlying LLM, and
allow LMPVC to adapt to different downstream tasks without a slow and costly
training process. The architecture and additional results are released on
GitHub (https://github.com/ozzyuni/LMPVC).","Ossi Parikka, Roel Pieters",2025-06-27T09:14:14Z,2025-06-27T09:14:14Z,http://arxiv.org/abs/2506.22028v1,http://arxiv.org/pdf/2506.22028v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"A Review of Personalisation in Human-Robot Collaboration and Future
  Perspectives Towards Industry 5.0","The shift in research focus from Industry 4.0 to Industry 5.0 (I5.0) promises
a human-centric workplace, with social and well-being values at the centre of
technological implementation. Human-Robot Collaboration (HRC) is a core aspect
of I5.0 development, with an increase in adaptive and personalised interactions
and behaviours. This review investigates recent advancements towards
personalised HRC, where user-centric adaption is key. There is a growing trend
for adaptable HRC research, however there lacks a consistent and unified
approach. The review highlights key research trends on which personal factors
are considered, workcell and interaction design, and adaptive task completion.
This raises various key considerations for future developments, particularly
around the ethical and regulatory development of personalised systems, which
are discussed in detail.","James Fant-Male, Roel Pieters",2025-06-25T13:53:10Z,2025-06-25T13:53:10Z,http://arxiv.org/abs/2506.20447v1,http://arxiv.org/pdf/2506.20447v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Why Robots Are Bad at Detecting Their Mistakes: Limitations of
  Miscommunication Detection in Human-Robot Dialogue","Detecting miscommunication in human-robot interaction is a critical function
for maintaining user engagement and trust. While humans effortlessly detect
communication errors in conversations through both verbal and non-verbal cues,
robots face significant challenges in interpreting non-verbal feedback, despite
advances in computer vision for recognizing affective expressions. This
research evaluates the effectiveness of machine learning models in detecting
miscommunications in robot dialogue. Using a multi-modal dataset of 240
human-robot conversations, where four distinct types of conversational failures
were systematically introduced, we assess the performance of state-of-the-art
computer vision models. After each conversational turn, users provided feedback
on whether they perceived an error, enabling an analysis of the models' ability
to accurately detect robot mistakes. Despite using state-of-the-art models, the
performance barely exceeds random chance in identifying miscommunication, while
on a dataset with more expressive emotional content, they successfully
identified confused states. To explore the underlying cause, we asked human
raters to do the same. They could also only identify around half of the induced
miscommunications, similarly to our model. These results uncover a fundamental
limitation in identifying robot miscommunications in dialogue: even when users
perceive the induced miscommunication as such, they often do not communicate
this to their robotic conversation partner. This knowledge can shape
expectations of the performance of computer vision models and can help
researchers to design better human-robot conversations by deliberately
eliciting feedback where needed.","Ruben Janssens, Jens De Bock, Sofie Labat, Eva Verhelst, Veronique Hoste, Tony Belpaeme",2025-06-25T09:25:04Z,2025-06-25T09:25:04Z,http://arxiv.org/abs/2506.20268v1,http://arxiv.org/pdf/2506.20268v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Preserving Sense of Agency: User Preferences for Robot Autonomy and User
  Control across Household Tasks","Roboticists often design with the assumption that assistive robots should be
fully autonomous. However, it remains unclear whether users prefer highly
autonomous robots, as prior work in assistive robotics suggests otherwise. High
robot autonomy can reduce the user's sense of agency, which represents feeling
in control of one's environment. How much control do users, in fact, want over
the actions of robots used for in-home assistance? We investigate how robot
autonomy levels affect users' sense of agency and the autonomy level they
prefer in contexts with varying risks. Our study asked participants to rate
their sense of agency as robot users across four distinct autonomy levels and
ranked their robot preferences with respect to various household tasks. Our
findings revealed that participants' sense of agency was primarily influenced
by two factors: (1) whether the robot acts autonomously, and (2) whether a
third party is involved in the robot's programming or operation. Notably, an
end-user programmed robot highly preserved users' sense of agency, even though
it acts autonomously. However, in high-risk settings, e.g., preparing a snack
for a child with allergies, they preferred robots that prioritized their
control significantly more. Additional contextual factors, such as trust in a
third party operator, also shaped their preferences.","Claire Yang, Heer Patel, Max Kleiman-Weiner, Maya Cakmak",2025-06-24T00:06:58Z,2025-06-24T00:06:58Z,http://arxiv.org/abs/2506.19202v1,http://arxiv.org/pdf/2506.19202v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
Mirror Eyes: Explainable Human-Robot Interaction at a Glance,"The gaze of a person tends to reflect their interest. This work explores what
happens when this statement is taken literally and applied to robots. Here we
present a robot system that employs a moving robot head with a screen-based eye
model that can direct the robot's gaze to points in physical space and present
a reflection-like mirror image of the attended region on top of each eye. We
conducted a user study with 33 participants, who were asked to instruct the
robot to perform pick-and-place tasks, monitor the robot's task execution, and
interrupt it in case of erroneous actions. Despite a deliberate lack of
instructions about the role of the eyes and a very brief system exposure,
participants felt more aware about the robot's information processing, detected
erroneous actions earlier, and rated the user experience higher when eye-based
mirroring was enabled compared to non-reflective eyes. These results suggest a
beneficial and intuitive utilization of the introduced method in cooperative
human-robot interaction.","Matti Krüger, Daniel Tanneberg, Chao Wang, Stephan Hasler, Michael Gienger",2025-06-23T10:06:26Z,2025-06-23T10:06:26Z,http://arxiv.org/abs/2506.18466v1,http://arxiv.org/pdf/2506.18466v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Optimizing Exploration with a New Uncertainty Framework for Active SLAM
  Systems","Accurate reconstruction of the environment is a central goal of Simultaneous
Localization and Mapping (SLAM) systems. However, the agent's trajectory can
significantly affect estimation accuracy. This paper presents a new method to
model map uncertainty in Active SLAM systems using an Uncertainty Map (UM). The
UM uses probability distributions to capture where the map is uncertain,
allowing Uncertainty Frontiers (UF) to be defined as key
exploration-exploitation objectives and potential stopping criteria. In
addition, the method introduces the Signed Relative Entropy (SiREn), based on
the Kullback-Leibler divergence, to measure both coverage and uncertainty
together. This helps balance exploration and exploitation through an
easy-to-understand parameter. Unlike methods that depend on particular SLAM
setups, the proposed approach is compatible with different types of sensors,
such as cameras, LiDARs, and multi-sensor fusion. It also addresses common
problems in exploration planning and stopping conditions. Furthermore,
integrating this map modeling approach with a UF-based planning system enables
the agent to autonomously explore open spaces, a behavior not previously
observed in the Active SLAM literature. Code and implementation details are
available as a ROS node, and all generated data are openly available for public
use, facilitating broader adoption and validation of the proposed approach.","Sebastian Sansoni, Javier Gimenez, Gastón Castro, Santiago Tosetti, Flavio Craparo",2025-06-21T18:12:41Z,2025-06-21T18:12:41Z,http://arxiv.org/abs/2506.17775v1,http://arxiv.org/pdf/2506.17775v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"ROS 2 Agnocast: Supporting Unsized Message Types for True Zero-Copy
  Publish/Subscribe IPC","Robot applications, comprising independent components that mutually
publish/subscribe messages, are built on inter-process communication (IPC)
middleware such as Robot Operating System 2 (ROS 2). In large-scale ROS 2
systems like autonomous driving platforms, true zero-copy communication --
eliminating serialization and deserialization -- is crucial for efficiency and
real-time performance. However, existing true zero-copy middleware solutions
lack widespread adoption as they fail to meet three essential requirements: 1)
Support for all ROS 2 message types including unsized ones; 2) Minimal
modifications to existing application code; 3) Selective implementation of
zero-copy communication between specific nodes while maintaining conventional
communication mechanisms for other inter-node communications including
inter-host node communications. This first requirement is critical, as
production-grade ROS 2 projects like Autoware rely heavily on unsized message
types throughout their codebase to handle diverse use cases (e.g., various
sensors), and depend on the broader ROS 2 ecosystem, where unsized message
types are pervasive in libraries. The remaining requirements facilitate
seamless integration with existing projects. While IceOryx middleware, a
practical true zero-copy solution, meets all but the first requirement, other
studies achieving the first requirement fail to satisfy the remaining criteria.
This paper presents Agnocast, a true zero-copy IPC framework applicable to ROS
2 C++ on Linux that fulfills all these requirements. Our evaluation
demonstrates that Agnocast maintains constant IPC overhead regardless of
message size, even for unsized message types. In Autoware PointCloud
Preprocessing, Agnocast achieves a 16% improvement in average response time and
a 25% improvement in worst-case response time.","Takahiro Ishikawa-Aso, Shinpei Kato",2025-06-20T10:07:50Z,2025-06-20T10:07:50Z,http://arxiv.org/abs/2506.16882v1,http://arxiv.org/pdf/2506.16882v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Experimental Setup and Software Pipeline to Evaluate Optimization based
  Autonomous Multi-Robot Search Algorithms","Signal source localization has been a problem of interest in the multi-robot
systems domain given its applications in search & rescue and hazard
localization in various industrial and outdoor settings. A variety of
multi-robot search algorithms exist that usually formulate and solve the
associated autonomous motion planning problem as a heuristic model-free or
belief model-based optimization process. Most of these algorithms however
remains tested only in simulation, thereby losing the opportunity to generate
knowledge about how such algorithms would compare/contrast in a real physical
setting in terms of search performance and real-time computing performance. To
address this gap, this paper presents a new lab-scale physical setup and
associated open-source software pipeline to evaluate and benchmark multi-robot
search algorithms. The presented physical setup innovatively uses an acoustic
source (that is safe and inexpensive) and small ground robots (e-pucks)
operating in a standard motion-capture environment. This setup can be easily
recreated and used by most robotics researchers. The acoustic source also
presents interesting uncertainty in terms of its noise-to-signal ratio, which
is useful to assess sim-to-real gaps. The overall software pipeline is designed
to readily interface with any multi-robot search algorithm with minimal effort
and is executable in parallel asynchronous form. This pipeline includes a
framework for distributed implementation of multi-robot or swarm search
algorithms, integrated with a ROS (Robotics Operating System)-based software
stack for motion capture supported localization. The utility of this novel
setup is demonstrated by using it to evaluate two state-of-the-art multi-robot
search algorithms, based on swarm optimization and batch-Bayesian Optimization
(called Bayes-Swarm), as well as a random walk baseline.","Aditya Bhatt, Mary Katherine Corra, Franklin Merlo, Prajit KrisshnaKumar, Souma Chowdhury",2025-06-20T03:06:43Z,2025-07-11T17:47:54Z,http://arxiv.org/abs/2506.16710v3,http://arxiv.org/pdf/2506.16710v3.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
See What I Mean? Expressiveness and Clarity in Robot Display Design,"Nonverbal visual symbols and displays play an important role in communication
when humans and robots work collaboratively. However, few studies have
investigated how different types of non-verbal cues affect objective task
performance, especially in a dynamic environment that requires real time
decision-making. In this work, we designed a collaborative navigation task
where the user and the robot only had partial information about the map on each
end and thus the users were forced to communicate with a robot to complete the
task. We conducted our study in a public space and recruited 37 participants
who randomly passed by our setup. Each participant collaborated with a robot
utilizing either animated anthropomorphic eyes and animated icons, or static
anthropomorphic eyes and static icons. We found that participants that
interacted with a robot with animated displays reported the greatest level of
trust and satisfaction; that participants interpreted static icons the best;
and that participants with a robot with static eyes had the highest completion
success. These results suggest that while animation can foster trust with
robots, human-robot communication can be optimized by the addition of familiar
static icons that may be easier for users to interpret. We published our code,
designed symbols, and collected results online at:
https://github.com/mattufts/huamn_Cozmo_interaction.","Matthew Ebisu, Hang Yu, Reuben Aronson, Elaine Short",2025-06-19T23:02:36Z,2025-06-19T23:02:36Z,http://arxiv.org/abs/2506.16643v1,http://arxiv.org/pdf/2506.16643v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Conversations with Andrea: Visitors' Opinions on Android Robots in a
  Museum","The android robot Andrea was set up at a public museum in Germany for six
consecutive days to have conversations with visitors, fully autonomously. No
specific context was given, so visitors could state their opinions regarding
possible use-cases in structured interviews, without any bias. Additionally the
44 interviewees were asked for their general opinions of the robot, their
reasons (not) to interact with it and necessary improvements for future use.
The android's voice and wig were changed between different days of operation to
give varying cues regarding its gender. This did not have a significant impact
on the positive overall perception of the robot. Most visitors want the robot
to provide information about exhibits in the future, while opinions on other
roles, like a receptionist, were both wanted and explicitly not wanted by
different visitors. Speaking more languages (than only English) and faster
response times were the improvements most desired. These findings from the
interviews are in line with an analysis of the system logs, which revealed,
that after chitchat and personal questions, most of the 4436 collected requests
asked for information related to the museum and to converse in a different
language. The valuable insights gained from these real-world interactions are
now used to improve the system to become a useful real-world application.","Marcel Heisler, Christian Becker-Asano",2025-06-18T15:30:20Z,2025-06-18T15:30:20Z,http://arxiv.org/abs/2506.22466v1,http://arxiv.org/pdf/2506.22466v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
I Know You're Listening: Adaptive Voice for HRI,"While the use of social robots for language teaching has been explored, there
remains limited work on a task-specific synthesized voices for language
teaching robots. Given that language is a verbal task, this gap may have severe
consequences for the effectiveness of robots for language teaching tasks. We
address this lack of L2 teaching robot voices through three contributions: 1.
We address the need for a lightweight and expressive robot voice. Using a
fine-tuned version of Matcha-TTS, we use emoji prompting to create an
expressive voice that shows a range of expressivity over time. The voice can
run in real time with limited compute resources. Through case studies, we found
this voice more expressive, socially appropriate, and suitable for long periods
of expressive speech, such as storytelling. 2. We explore how to adapt a
robot's voice to physical and social ambient environments to deploy our voices
in various locations. We found that increasing pitch and pitch rate in noisy
and high-energy environments makes the robot's voice appear more appropriate
and makes it seem more aware of its current environment. 3. We create an
English TTS system with improved clarity for L2 listeners using known
linguistic properties of vowels that are difficult for these listeners. We used
a data-driven, perception-based approach to understand how L2 speakers use
duration cues to interpret challenging words with minimal tense (long) and lax
(short) vowels in English. We found that the duration of vowels strongly
influences the perception for L2 listeners and created an ""L2 clarity mode"" for
Matcha-TTS that applies a lengthening to tense vowels while leaving lax vowels
unchanged. Our clarity mode was found to be more respectful, intelligible, and
encouraging than base Matcha-TTS while reducing transcription errors in these
challenging tense/lax minimal pairs.",Paige Tuttösí,2025-06-18T03:23:41Z,2025-07-30T07:40:35Z,http://arxiv.org/abs/2506.15107v2,http://arxiv.org/pdf/2506.15107v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
EmojiVoice: Towards long-term controllable expressivity in robot speech,"Humans vary their expressivity when speaking for extended periods to maintain
engagement with their listener. Although social robots tend to be deployed with
``expressive'' joyful voices, they lack this long-term variation found in human
speech. Foundation model text-to-speech systems are beginning to mimic the
expressivity in human speech, but they are difficult to deploy offline on
robots. We present EmojiVoice, a free, customizable text-to-speech (TTS)
toolkit that allows social roboticists to build temporally variable, expressive
speech on social robots. We introduce emoji-prompting to allow fine-grained
control of expressivity on a phase level and use the lightweight Matcha-TTS
backbone to generate speech in real-time. We explore three case studies: (1) a
scripted conversation with a robot assistant, (2) a storytelling robot, and (3)
an autonomous speech-to-speech interactive agent. We found that using varied
emoji prompting improved the perception and expressivity of speech over a long
period in a storytelling task, but expressive voice was not preferred in the
assistant use case.","Paige Tuttösí, Shivam Mehta, Zachary Syvenky, Bermet Burkanova, Gustav Eje Henter, Angelica Lim",2025-06-18T02:49:50Z,2025-07-30T12:12:48Z,http://arxiv.org/abs/2506.15085v2,http://arxiv.org/pdf/2506.15085v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"GAMORA: A Gesture Articulated Meta Operative Robotic Arm for Hazardous
  Material Handling in Containment-Level Environments","The convergence of robotics and virtual reality (VR) has enabled safer and
more efficient workflows in high-risk laboratory settings, particularly
virology labs. As biohazard complexity increases, minimizing direct human
exposure while maintaining precision becomes essential. We propose GAMORA
(Gesture Articulated Meta Operative Robotic Arm), a novel VR-guided robotic
system that enables remote execution of hazardous tasks using natural hand
gestures. Unlike existing scripted automation or traditional teleoperation,
GAMORA integrates the Oculus Quest 2, NVIDIA Jetson Nano, and Robot Operating
System (ROS) to provide real-time immersive control, digital twin simulation,
and inverse kinematics-based articulation. The system supports VR-based
training and simulation while executing precision tasks in physical
environments via a 3D-printed robotic arm. Inverse kinematics ensure accurate
manipulation for delicate operations such as specimen handling and pipetting.
The pipeline includes Unity-based 3D environment construction, real-time motion
planning, and hardware-in-the-loop testing. GAMORA achieved a mean positional
discrepancy of 2.2 mm (improved from 4 mm), pipetting accuracy within 0.2 mL,
and repeatability of 1.2 mm across 50 trials. Integrated object detection via
YOLOv8 enhances spatial awareness, while energy-efficient operation (50%
reduced power output) ensures sustainable deployment. The system's
digital-physical feedback loop enables safe, precise, and repeatable automation
of high-risk lab tasks. GAMORA offers a scalable, immersive solution for
robotic control and biosafety in biomedical research environments.","Farha Abdul Wasay, Mohammed Abdul Rahman, Hania Ghouse",2025-06-17T13:40:16Z,2025-06-17T13:40:16Z,http://arxiv.org/abs/2506.14513v1,http://arxiv.org/pdf/2506.14513v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Demonstration Sidetracks: Categorizing Systematic Non-Optimality in
  Human Demonstrations","Learning from Demonstration (LfD) is a popular approach for robots to acquire
new skills, but most LfD methods suffer from imperfections in human
demonstrations. Prior work typically treats these suboptimalities as random
noise. In this paper we study non-optimal behaviors in non-expert
demonstrations and show that they are systematic, forming what we call
demonstration sidetracks. Using a public space study with 40 participants
performing a long-horizon robot task, we recreated the setup in simulation and
annotated all demonstrations. We identify four types of sidetracks
(Exploration, Mistake, Alignment, Pause) and one control pattern (one-dimension
control). Sidetracks appear frequently across participants, and their temporal
and spatial distribution is tied to task context. We also find that users'
control patterns depend on the control interface. These insights point to the
need for better models of suboptimal demonstrations to improve LfD algorithms
and bridge the gap between lab training and real-world deployment. All
demonstrations, infrastructure, and annotations are available at
https://github.com/AABL-Lab/Human-Demonstration-Sidetracks.","Shijie Fang, Hang Yu, Qidi Fang, Reuben M. Aronson, Elaine S. Short",2025-06-12T20:04:55Z,2025-06-12T20:04:55Z,http://arxiv.org/abs/2506.11262v1,http://arxiv.org/pdf/2506.11262v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Are We Generalizing from the Exception? An In-the-Wild Study on
  Group-Sensitive Conversation Design in Human-Agent Interactions","This paper investigates the impact of a group-adaptive conversation design in
two socially interactive agents (SIAs) through two real-world studies. Both
SIAs - Furhat, a social robot, and MetaHuman, a virtual agent - were equipped
with a conversational artificial intelligence (CAI) backend combining hybrid
retrieval and generative models. The studies were carried out in an in-the-wild
setting with a total of $N = 188$ participants who interacted with the SIAs -
in dyads, triads or larger groups - at a German museum. Although the results
did not reveal a significant effect of the group-sensitive conversation design
on perceived satisfaction, the findings provide valuable insights into the
challenges of adapting CAI for multi-party interactions and across different
embodiments (robot vs.\ virtual agent), highlighting the need for multimodal
strategies beyond linguistic pluralization. These insights contribute to the
fields of Human-Agent Interaction (HAI), Human-Robot Interaction (HRI), and
broader Human-Machine Interaction (HMI), providing insights for future research
on effective dialogue adaptation in group settings.","Ana Müller, Sabina Jeschke, Anja Richert",2025-06-12T08:11:09Z,2025-06-12T08:11:09Z,http://arxiv.org/abs/2506.10462v1,http://arxiv.org/pdf/2506.10462v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
Leveraging LLMs for Mission Planning in Precision Agriculture,"Robotics and artificial intelligence hold significant potential for advancing
precision agriculture. While robotic systems have been successfully deployed
for various tasks, adapting them to perform diverse missions remains
challenging, particularly because end users often lack technical expertise. In
this paper, we present an end-to-end system that leverages large language
models (LLMs), specifically ChatGPT, to enable users to assign complex data
collection tasks to autonomous robots using natural language instructions. To
enhance reusability, mission plans are encoded using an existing IEEE task
specification standard, and are executed on robots via ROS2 nodes that bridge
high-level mission descriptions with existing ROS libraries. Through extensive
experiments, we highlight the strengths and limitations of LLMs in this
context, particularly regarding spatial reasoning and solving complex routing
challenges, and show how our proposed implementation overcomes them.","Marcos Abel Zuzuárregui, Stefano Carpin",2025-06-11T18:25:23Z,2025-06-11T18:25:23Z,http://arxiv.org/abs/2506.10093v1,http://arxiv.org/pdf/2506.10093v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Integrating Quantized LLMs into Robotics Systems as Edge AI to Leverage
  their Natural Language Processing Capabilities","Large Language Models (LLMs) have experienced great advancements in the last
year resulting in an increase of these models in several fields to face natural
language tasks. The integration of these models in robotics can also help to
improve several aspects such as human-robot interaction, navigation, planning
and decision-making. Therefore, this paper introduces llama\_ros, a tool
designed to integrate quantized Large Language Models (LLMs) into robotic
systems using ROS 2. Leveraging llama.cpp, a highly optimized runtime engine,
llama\_ros enables the efficient execution of quantized LLMs as edge artificial
intelligence (AI) in robotics systems with resource-constrained environments,
addressing the challenges of computational efficiency and memory limitations.
By deploying quantized LLMs, llama\_ros empowers robots to leverage the natural
language understanding and generation for enhanced decision-making and
interaction which can be paired with prompt engineering, knowledge graphs,
ontologies or other tools to improve the capabilities of autonomous robots.
Additionally, this paper provides insights into some use cases of using
llama\_ros for planning and explainability in robotics.","Miguel Á. González-Santamarta, Francisco J. Rodríguez-Lera, David Sobrín-Hidalgo, Ángel Manuel Guerrero-Higueras, Vicente MatellÁn-Olivera",2025-06-11T10:19:49Z,2025-06-11T10:19:49Z,http://arxiv.org/abs/2506.09581v1,http://arxiv.org/pdf/2506.09581v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Help or Hindrance: Understanding the Impact of Robot Communication in
  Action Teams","The human-robot interaction (HRI) field has recognized the importance of
enabling robots to interact with teams. Human teams rely on effective
communication for successful collaboration in time-sensitive environments.
Robots can play a role in enhancing team coordination through real-time
assistance. Despite significant progress in human-robot teaming research, there
remains an essential gap in how robots can effectively communicate with action
teams using multimodal interaction cues in time-sensitive environments. This
study addresses this knowledge gap in an experimental in-lab study to
investigate how multimodal robot communication in action teams affects workload
and human perception of robots. We explore team collaboration in a medical
training scenario where a robotic crash cart (RCC) provides verbal and
non-verbal cues to help users remember to perform iterative tasks and search
for supplies. Our findings show that verbal cues for object search tasks and
visual cues for task reminders reduce team workload and increase perceived ease
of use and perceived usefulness more effectively than a robot with no feedback.
Our work contributes to multimodal interaction research in the HRI field,
highlighting the need for more human-robot teaming research to understand best
practices for integrating collaborative robots in time-sensitive environments
such as in hospitals, search and rescue, and manufacturing applications.","Tauhid Tanjim, Jonathan St. George, Kevin Ching, Angelique Taylor",2025-06-10T15:19:26Z,2025-06-24T07:33:18Z,http://arxiv.org/abs/2506.08892v3,http://arxiv.org/pdf/2506.08892v3.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Human-Robot Teaming Field Deployments: A Comparison Between Verbal and
  Non-verbal Communication","Healthcare workers (HCWs) encounter challenges in hospitals, such as
retrieving medical supplies quickly from crash carts, which could potentially
result in medical errors and delays in patient care. Robotic crash carts (RCCs)
have shown promise in assisting healthcare teams during medical tasks through
guided object searches and task reminders. Limited exploration has been done to
determine what communication modalities are most effective and least disruptive
to patient care in real-world settings. To address this gap, we conducted a
between-subjects experiment comparing the RCC's verbal and non-verbal
communication of object search with a standard crash cart in resuscitation
scenarios to understand the impact of robot communication on workload and
attitudes toward using robots in the workplace. Our findings indicate that
verbal communication significantly reduced mental demand and effort compared to
visual cues and with a traditional crash cart. Although frustration levels were
slightly higher during collaborations with the robot compared to a traditional
cart, these research insights provide valuable implications for human-robot
teamwork in high-stakes environments.","Tauhid Tanjim, Promise Ekpo, Huajie Cao, Jonathan St. George, Kevin Ching, Hee Rin Lee, Angelique Taylor",2025-06-10T15:17:33Z,2025-06-24T07:24:29Z,http://arxiv.org/abs/2506.08890v2,http://arxiv.org/pdf/2506.08890v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Blending Participatory Design and Artificial Awareness for Trustworthy
  Autonomous Vehicles","Current robotic agents, such as autonomous vehicles (AVs) and drones, need to
deal with uncertain real-world environments with appropriate situational
awareness (SA), risk awareness, coordination, and decision-making. The SymAware
project strives to address this issue by designing an architecture for
artificial awareness in multi-agent systems, enabling safe collaboration of
autonomous vehicles and drones. However, these agents will also need to
interact with human users (drivers, pedestrians, drone operators), which in
turn requires an understanding of how to model the human in the interaction
scenario, and how to foster trust and transparency between the agent and the
human.
  In this work, we aim to create a data-driven model of a human driver to be
integrated into our SA architecture, grounding our research in the principles
of trustworthy human-agent interaction. To collect the data necessary for
creating the model, we conducted a large-scale user-centered study on human-AV
interaction, in which we investigate the interaction between the AV's
transparency and the users' behavior.
  The contributions of this paper are twofold: First, we illustrate in detail
our human-AV study and its findings, and second we present the resulting Markov
chain models of the human driver computed from the study's data. Our results
show that depending on the AV's transparency, the scenario's environment, and
the users' demographics, we can obtain significant differences in the model's
transitions.","Ana Tanevska, Ananthapathmanabhan Ratheesh Kumar, Arabinda Ghosh, Ernesto Casablanca, Ginevra Castellano, Sadegh Soudjani",2025-06-09T11:00:28Z,2025-06-09T11:00:28Z,http://arxiv.org/abs/2506.07633v1,http://arxiv.org/pdf/2506.07633v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Taking Flight with Dialogue: Enabling Natural Language Control for
  PX4-based Drone Agent","Recent advances in agentic and physical artificial intelligence (AI) have
largely focused on ground-based platforms such as humanoid and wheeled robots,
leaving aerial robots relatively underexplored. Meanwhile, state-of-the-art
unmanned aerial vehicle (UAV) multimodal vision-language systems typically rely
on closed-source models accessible only to well-resourced organizations. To
democratize natural language control of autonomous drones, we present an
open-source agentic framework that integrates PX4-based flight control, Robot
Operating System 2 (ROS 2) middleware, and locally hosted models using Ollama.
We evaluate performance both in simulation and on a custom quadcopter platform,
benchmarking four large language model (LLM) families for command generation
and three vision-language model (VLM) families for scene understanding.","Shoon Kit Lim, Melissa Jia Ying Chong, Jing Huey Khor, Ting Yang Ling",2025-06-09T07:37:45Z,2025-06-09T07:37:45Z,http://arxiv.org/abs/2506.07509v1,http://arxiv.org/pdf/2506.07509v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
Underwater Multi-Robot Simulation and Motion Planning in Angler,"Deploying multi-robot systems in underwater environments is expensive and
lengthy; testing algorithms and software in simulation improves development by
decoupling software and hardware. However, this requires a simulation framework
that closely resembles the real-world. Angler is an open-source framework that
simulates low-level communication protocols for an onboard autopilot, such as
ArduSub, providing a framework that is close to reality, but unfortunately
lacking support for simulating multiple robots. We present an extension to
Angler that supports multi-robot simulation and motion planning. Our extension
has a modular architecture that creates non-conflicting communication channels
between Gazebo, ArduSub Software-in-the-Loop (SITL), and MAVROS to operate
multiple robots simultaneously in the same environment. Our multi-robot motion
planning module interfaces with cascaded controllers via a JointTrajectory
controller in ROS~2. We also provide an integration with the Open Motion
Planning Library (OMPL), a collision avoidance module, and tools for procedural
environment generation. Our work enables the development and benchmarking of
underwater multi-robot motion planning in dynamic environments.","Akshaya Agrawal, Evan Palmer, Zachary Kingston, Geoffrey A. Hollinger",2025-06-07T01:07:05Z,2025-06-07T01:07:05Z,http://arxiv.org/abs/2506.06612v1,http://arxiv.org/pdf/2506.06612v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Real-Time LPV-Based Non-Linear Model Predictive Control for Robust
  Trajectory Tracking in Autonomous Vehicles","This paper presents the development and implementation of a Model Predictive
Control (MPC) framework for trajectory tracking in autonomous vehicles under
diverse driving conditions. The proposed approach incorporates a modular
architecture that integrates state estimation, vehicle dynamics modeling, and
optimization to ensure real-time performance. The state-space equations are
formulated in a Linear Parameter Varying (LPV) form, and a curvature-based
tuning method is introduced to optimize weight matrices for varying
trajectories. The MPC framework is implemented using the Robot Operating System
(ROS) for parallel execution of state estimation and control optimization,
ensuring scalability and minimal latency. Extensive simulations and real-time
experiments were conducted on multiple predefined trajectories, demonstrating
high accuracy with minimal cross-track and orientation errors, even under
aggressive maneuvers and high-speed conditions. The results highlight the
robustness and adaptability of the proposed system, achieving seamless
alignment between simulated and real-world performance. This work lays the
foundation for dynamic weight tuning and integration into cooperative
autonomous navigation systems, paving the way for enhanced safety and
efficiency in autonomous driving applications.","Nitish Kumar, Rajalakshmi Pachamuthu",2025-06-05T07:04:10Z,2025-06-05T07:04:10Z,http://arxiv.org/abs/2506.04684v1,http://arxiv.org/pdf/2506.04684v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"AWML: An Open-Source ML-based Robotics Perception Framework to Deploy
  for ROS-based Autonomous Driving Software","In recent years, machine learning technologies have played an important role
in robotics, particularly in the development of autonomous robots and
self-driving vehicles. As the industry matures, robotics frameworks like ROS 2
have been developed and provides a broad range of applications from research to
production. In this work, we introduce AWML, a framework designed to support
MLOps for robotics. AWML provides a machine learning infrastructure for
autonomous driving, supporting not only the deployment of trained models to
robotic systems, but also an active learning pipeline that incorporates
auto-labeling, semi-auto-labeling, and data mining techniques.","Satoshi Tanaka, Samrat Thapa, Kok Seang Tan, Amadeusz Szymko, Lobos Kenzo, Koji Minoda, Shintaro Tomie, Kotaro Uetake, Guolong Zhang, Isamu Yamashita, Takamasa Horibe",2025-05-31T17:29:32Z,2025-05-31T17:29:32Z,http://arxiv.org/abs/2506.00645v1,http://arxiv.org/pdf/2506.00645v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Long Duration Inspection of GNSS-Denied Environments with a Tethered
  UAV-UGV Marsupial System","Unmanned Aerial Vehicles (UAVs) have become essential tools in inspection and
emergency response operations due to their high maneuverability and ability to
access hard-to-reach areas. However, their limited battery life significantly
restricts their use in long-duration missions. This paper presents a novel
tethered marsupial robotic system composed of a UAV and an Unmanned Ground
Vehicle (UGV), specifically designed for autonomous, long-duration inspection
tasks in Global Navigation Satellite System (GNSS)-denied environments. The
system extends the UAV's operational time by supplying power through a tether
connected to high-capacity battery packs carried by the UGV. We detail the
hardware architecture based on off-the-shelf components to ensure replicability
and describe our full-stack software framework, which is composed of
open-source components and built upon the Robot Operating System (ROS). The
proposed software architecture enables precise localization using a Direct
LiDAR Localization (DLL) method and ensures safe path planning and coordinated
trajectory tracking for the integrated UGV-tether-UAV system. We validate the
system through three field experiments: (1) a manual flight endurance test to
estimate the operational duration, (2) an autonomous navigation test, and (3)
an inspection mission to demonstrate autonomous inspection capabilities.
Experimental results confirm the robustness and autonomy of the system, its
capacity to operate in GNSS-denied environments, and its potential for
long-endurance, autonomous inspection and monitoring tasks.","Simón Martínez-Rozas, David Alejo, José Javier Carpio, Fernando Caballero, Luis Merino",2025-05-29T14:05:25Z,2025-05-29T14:05:25Z,http://arxiv.org/abs/2505.23457v1,http://arxiv.org/pdf/2505.23457v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
A first look at ROS 2 applications written in asynchronous Rust,"The increasing popularity of the Rust programming language in building
robotic applications using the Robot Operating System (ROS 2) raises questions
about its real-time execution capabilities, particularly when employing
asynchronous programming. Existing real-time scheduling and response-time
analysis techniques for ROS 2 focus on applications written in C++ and do not
address the unique execution models and challenges presented by Rust's
asynchronous programming paradigm. In this paper, we analyze the execution
model of R2R -- an asynchronous Rust ROS 2 bindings and various asynchronous
Rust runtimes, comparing them with the execution model of C++ ROS 2
applications. We propose a structured approach for R2R applications aimed at
deterministic real-time operation involving thread prioritization and
callback-to-thread mapping schemes. Our experimental evaluation based on
measuring end-to-end latencies of a synthetic application shows that the
proposed approach is effective and outperforms other evaluated configurations.
A more complex autonomous driving case study demonstrates its practical
applicability. Overall, the experimental results indicate that our proposed
structure achieves bounded response times for time-critical tasks. This paves
the way for future work to adapt existing or develop new response-time analysis
techniques for R2R applications using our structure.","Martin Škoudlil, Michal Sojka, Zdeněk Hanzálek",2025-05-27T15:21:58Z,2025-07-28T15:25:07Z,http://arxiv.org/abs/2505.21323v3,http://arxiv.org/pdf/2505.21323v3.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"A Robot Simulation Environment for Virtual Reality Enhanced Underwater
  Manipulation and Seabed Intervention Tasks","This paper presents the (MARUN)2 underwater robotic simulator. The simulator
architecture enables seamless integration with the ROS-based mission software
and web-based user interface of URSULA, a squid inspired biomimetic robot
designed for dexterous underwater manipulation and seabed intervention tasks.
(MARUN)2 utilizes the Unity game engine for physics-based rigid body dynamic
simulation and underwater environment modeling. Utilizing Unity as the
simulation environment enables the integration of virtual reality and haptic
feedback capabilities for a more immersive and realistic experience for
improved operator dexterity and experience. The utility of the simulator and
improved dexterity provided by the VR module is validated through user
experiments.","Sumey El-Muftu, Berke Gur",2025-05-18T14:42:28Z,2025-05-18T14:42:28Z,http://arxiv.org/abs/2505.12450v1,http://arxiv.org/pdf/2505.12450v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Employing Laban Shape for Generating Emotionally and Functionally
  Expressive Trajectories in Robotic Manipulators","Successful human-robot collaboration depends on cohesive communication and a
precise understanding of the robot's abilities, goals, and constraints. While
robotic manipulators offer high precision, versatility, and productivity, they
exhibit expressionless and monotonous motions that conceal the robot's
intention, resulting in a lack of efficiency and transparency with humans. In
this work, we use Laban notation, a dance annotation language, to enable
robotic manipulators to generate trajectories with functional expressivity,
where the robot uses nonverbal cues to communicate its abilities and the
likelihood of succeeding at its task. We achieve this by introducing two novel
variants of Hesitant expressive motion (Spoke-Like and Arc-Like). We also
enhance the emotional expressivity of four existing emotive trajectories
(Happy, Sad, Shy, and Angry) by augmenting Laban Effort usage with Laban Shape.
The functionally expressive motions are validated via a human-subjects study,
where participants equate both variants of Hesitant motion with reduced robot
competency. The enhanced emotive trajectories are shown to be viewed as
distinct emotions using the Valence-Arousal-Dominance (VAD) spectrum,
corroborating the usage of Laban Shape.","Srikrishna Bangalore Raghu, Clare Lohrmann, Akshay Bakshi, Jennifer Kim, Jose Caraveo Herrera, Bradley Hayes, Alessandro Roncone",2025-05-16T21:58:19Z,2025-06-23T18:03:54Z,http://arxiv.org/abs/2505.11716v2,http://arxiv.org/pdf/2505.11716v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Diffusion-SAFE: Shared Autonomy Framework with Diffusion for Safe
  Human-to-Robot Driving Handover","Safe handover in shared autonomy for vehicle control is well-established in
modern vehicles. However, avoiding accidents often requires action several
seconds in advance. This necessitates understanding human driver behavior and
an expert control strategy for seamless intervention when a collision or unsafe
state is predicted. We propose Diffusion-SAFE, a closed-loop shared autonomy
framework leveraging diffusion models to: (1) predict human driving behavior
for detection of potential risks, (2) generate safe expert trajectories, and
(3) enable smooth handovers by blending human and expert policies over a short
time horizon. Unlike prior works which use engineered score functions to rate
driving performance, our approach enables both performance evaluation and
optimal action sequence generation from demonstrations. By adjusting the
forward and reverse processes of the diffusion-based copilot, our method
ensures a gradual transition of control authority, by mimicking the drivers'
behavior before intervention, which mitigates abrupt takeovers, leading to
smooth transitions. We evaluated Diffusion-SAFE in both simulation
(CarRacing-v0) and real-world (ROS-based race car), measuring human-driving
similarity, safety, and computational efficiency. Results demonstrate a 98.5\%
successful handover rate, highlighting the framework's effectiveness in
progressively correcting human actions and continuously sampling optimal robot
actions.","Yunxin Fan, Monroe Kennedy III",2025-05-15T01:12:48Z,2025-05-15T01:12:48Z,http://arxiv.org/abs/2505.09889v1,http://arxiv.org/pdf/2505.09889v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"MDF: Multi-Modal Data Fusion with CNN-Based Object Detection for
  Enhanced Indoor Localization Using LiDAR-SLAM","Indoor localization faces persistent challenges in achieving high accuracy,
particularly in GPS-deprived environments. This study unveils a cutting-edge
handheld indoor localization system that integrates 2D LiDAR and IMU sensors,
delivering enhanced high-velocity precision mapping, computational efficiency,
and real-time adaptability. Unlike 3D LiDAR systems, it excels with rapid
processing, low-cost scalability, and robust performance, setting new standards
for emergency response, autonomous navigation, and industrial automation.
Enhanced with a CNN-driven object detection framework and optimized through
Cartographer SLAM (simultaneous localization and mapping ) in ROS, the system
significantly reduces Absolute Trajectory Error (ATE) by 21.03%, achieving
exceptional precision compared to state-of-the-art approaches like SC-ALOAM,
with a mean x-position error of -0.884 meters (1.976 meters). The integration
of CNN-based object detection ensures robustness in mapping and localization,
even in cluttered or dynamic environments, outperforming existing methods by
26.09%. These advancements establish the system as a reliable, scalable
solution for high-precision localization in challenging indoor scenarios","Saqi Hussain Kalan, Boon Giin Lee, Wan-Young Chung",2025-05-13T09:34:55Z,2025-05-13T09:34:55Z,http://arxiv.org/abs/2505.08388v1,http://arxiv.org/pdf/2505.08388v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
RAI: Flexible Agent Framework for Embodied AI,"With an increase in the capabilities of generative language models, a growing
interest in embodied AI has followed. This contribution introduces RAI - a
framework for creating embodied Multi Agent Systems for robotics. The proposed
framework implements tools for Agents' integration with robotic stacks, Large
Language Models, and simulations. It provides out-of-the-box integration with
state-of-the-art systems like ROS 2. It also comes with dedicated mechanisms
for the embodiment of Agents. These mechanisms have been tested on a physical
robot, Husarion ROSBot XL, which was coupled with its digital twin, for rapid
prototyping. Furthermore, these mechanisms have been deployed in two
simulations: (1) robot arm manipulator and (2) tractor controller. All of these
deployments have been evaluated in terms of their control capabilities,
effectiveness of embodiment, and perception ability. The proposed framework has
been used successfully to build systems with multiple agents. It has
demonstrated effectiveness in all the aforementioned tasks. It also enabled
identifying and addressing the shortcomings of the generative models used for
embodied AI.","Kajetan Rachwał, Maciej Majek, Bartłomiej Boczek, Kacper Dąbrowski, Paweł Liberadzki, Adam Dąbrowski, Maria Ganzha",2025-05-12T13:13:47Z,2025-05-12T13:13:47Z,http://arxiv.org/abs/2505.07532v1,http://arxiv.org/pdf/2505.07532v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Work-in-Progress: Multi-Deadline DAG Scheduling Model for Autonomous
  Driving Systems","Autoware is an autonomous driving system implemented on Robot Operation
System (ROS) 2, where an end-to-end timing guarantee is crucial to ensure
safety. However, existing ROS 2 cause-effect chain models for analyzing
end-to-end latency struggle to accurately represent the complexities of
Autoware, particularly regarding sync callbacks, queue consumption patterns,
and feedback loops. To address these problems, we propose a new scheduling
model that decomposes the end-to-end timing constraints of Autoware into local
relative deadlines for each sub-DAG. This multi-deadline DAG scheduling model
avoids the need for complex analysis of data flows through queues and loops,
while ensuring that all callbacks receive data within correct intervals.
Furthermore, we extend the Global Earliest Deadline First (GEDF) algorithm for
the proposed model and evaluate its effectiveness using a synthetic workload
derived from Autoware.","Atsushi Yano, Takuya Azumi",2025-05-10T23:29:35Z,2025-05-13T02:40:38Z,http://arxiv.org/abs/2505.06780v2,http://arxiv.org/pdf/2505.06780v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Work in Progress: Middleware-Transparent Callback Enforcement in
  Commoditized Component-Oriented Real-time Systems","Real-time scheduling in commoditized component-oriented real-time systems,
such as ROS 2 systems on Linux, has been studied under nested scheduling: OS
thread scheduling and middleware layer scheduling (e.g., ROS 2 Executor).
However, by establishing a persistent one-to-one correspondence between
callbacks and OS threads, we can ignore the middleware layer and directly apply
OS scheduling parameters (e.g., scheduling policy, priority, and affinity) to
individual callbacks. We propose a middleware model that enables this idea and
implements CallbackIsolatedExecutor as a novel ROS 2 Executor. We demonstrate
that the costs (user-kernel switches, context switches, and memory usage) of
CallbackIsolatedExecutor remain lower than those of the MultiThreadedExecutor,
regardless of the number of callbacks. Additionally, the cost of
CallbackIsolatedExecutor relative to SingleThreadedExecutor stays within a
fixed ratio (1.4x for inter-process and 5x for intra-process communication).
Future ROS 2 real-time scheduling research can avoid nested scheduling,
ignoring the existence of the middleware layer.","Takahiro Ishikawa-Aso, Atsushi Yano, Takuya Azumi, Shinpei Kato",2025-05-10T07:16:10Z,2025-05-10T07:16:10Z,http://arxiv.org/abs/2505.06546v1,http://arxiv.org/pdf/2505.06546v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
LLM-Land: Large Language Models for Context-Aware Drone Landing,"Autonomous landing is essential for drones deployed in emergency deliveries,
post-disaster response, and other large-scale missions. By enabling
self-docking on charging platforms, it facilitates continuous operation and
significantly extends mission endurance. However, traditional approaches often
fall short in dynamic, unstructured environments due to limited semantic
awareness and reliance on fixed, context-insensitive safety margins. To address
these limitations, we propose a hybrid framework that integrates large language
model (LLMs) with model predictive control (MPC). Our approach begins with a
vision-language encoder (VLE) (e.g., BLIP), which transforms real-time images
into concise textual scene descriptions. These descriptions are processed by a
lightweight LLM (e.g., Qwen 2.5 1.5B or LLaMA 3.2 1B) equipped with
retrieval-augmented generation (RAG) to classify scene elements and infer
context-aware safety buffers, such as 3 meters for pedestrians and 5 meters for
vehicles. The resulting semantic flags and unsafe regions are then fed into an
MPC module, enabling real-time trajectory replanning that avoids collisions
while maintaining high landing precision. We validate our framework in the
ROS-Gazebo simulator, where it consistently outperforms conventional
vision-based MPC baselines. Our results show a significant reduction in
near-miss incidents with dynamic obstacles, while preserving accurate landings
in cluttered environments.","Siwei Cai, Yuwei Wu, Lifeng Zhou",2025-05-09T19:55:22Z,2025-05-09T19:55:22Z,http://arxiv.org/abs/2505.06399v1,http://arxiv.org/pdf/2505.06399v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
Oh F**k! How Do People Feel about Robots that Leverage Profanity?,"Profanity is nearly as old as language itself, and cursing has become
particularly ubiquitous within the last century. At the same time, robots in
personal and service applications are often overly polite, even though past
work demonstrates the potential benefits of robot norm-breaking. Thus, we
became curious about robots using curse words in error scenarios as a means for
improving social perceptions by human users. We investigated this idea using
three phases of exploratory work: an online video-based study (N = 76) with a
student pool, an online video-based study (N = 98) in the general U.S.
population, and an in-person proof-of-concept deployment (N = 52) in a campus
space, each of which included the following conditions: no-speech,
non-expletive error response, and expletive error response. A surprising result
in the outcomes for all three studies was that although verbal acknowledgment
of an error was typically beneficial (as expected based on prior work), few
significant differences appeared between the non-expletive and expletive error
acknowledgment conditions (counter to our expectations). Within the cultural
context of our work, the U.S., it seems that many users would likely not mind
if robots curse, and may even find it relatable and humorous. This work signals
a promising and mischievous design space that challenges typical robot
character design.","Madison R. Shippy, Brian J. Zhang, Naomi T. Fitter",2025-05-09T06:58:43Z,2025-05-09T06:58:43Z,http://arxiv.org/abs/2505.05831v1,http://arxiv.org/pdf/2505.05831v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"CottonSim: Development of an autonomous visual-guided robotic
  cotton-picking system in the Gazebo","In this study, an autonomous visual-guided robotic cotton-picking system,
built on a Clearpath's Husky robot platform and the Cotton-Eye perception
system, was developed in the Gazebo robotic simulator. Furthermore, a virtual
cotton farm was designed and developed as a Robot Operating System (ROS 1)
package to deploy the robotic cotton picker in the Gazebo environment for
simulating autonomous field navigation. The navigation was assisted by the map
coordinates and an RGB-depth camera, while the ROS navigation algorithm
utilized a trained YOLOv8n-seg model for instance segmentation. The model
achieved a desired mean Average Precision (mAP) of 85.2%, a recall of 88.9%,
and a precision of 93.0% for scene segmentation. The developed ROS navigation
packages enabled our robotic cotton-picking system to autonomously navigate
through the cotton field using map-based and GPS-based approaches, visually
aided by a deep learning-based perception system. The GPS-based navigation
approach achieved a 100% completion rate (CR) with a threshold of 5 x 10^-6
degrees, while the map-based navigation approach attained a 96.7% CR with a
threshold of 0.25 m. This study establishes a fundamental baseline of
simulation for future agricultural robotics and autonomous vehicles in cotton
farming and beyond. CottonSim code and data are released to the research
community via GitHub: https://github.com/imtheva/CottonSim","Thevathayarajh Thayananthan, Xin Zhang, Yanbo Huang, Jingdao Chen, Nuwan K. Wijewardane, Vitor S. Martins, Gary D. Chesser, Christopher T. Goodin",2025-05-08T15:02:35Z,2025-05-08T15:02:35Z,http://arxiv.org/abs/2505.05317v1,http://arxiv.org/pdf/2505.05317v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"T-REX: Vision-Based System for Autonomous Leaf Detection and Grasp
  Estimation","T-Rex (The Robot for Extracting Leaf Samples) is a gantry-based robotic
system developed for autonomous leaf localization, selection, and grasping in
greenhouse environments. The system integrates a 6-degree-of-freedom
manipulator with a stereo vision pipeline to identify and interact with target
leaves. YOLOv8 is used for real-time leaf segmentation, and RAFT-Stereo
provides dense depth maps, allowing the reconstruction of 3D leaf masks. These
observations are processed through a leaf grasping algorithm that selects the
optimal leaf based on clutter, visibility, and distance, and determines a grasp
point by analyzing local surface flatness, top-down approachability, and margin
from edges. The selected grasp point guides a trajectory executed by ROS-based
motion controllers, driving a custom microneedle-equipped end-effector to clamp
the leaf and simulate tissue sampling. Experiments conducted with artificial
plants under varied poses demonstrate that the T-Rex system can consistently
detect, plan, and perform physical interactions with plant-like targets,
achieving a grasp success rate of 66.6\%. This paper presents the system
architecture, implementation, and testing of T-Rex as a step toward plant
sampling automation in Controlled Environment Agriculture (CEA).","Srecharan Selvam, Abhisesh Silwal, George Kantor",2025-05-03T02:17:45Z,2025-05-03T02:17:45Z,http://arxiv.org/abs/2505.01654v1,http://arxiv.org/pdf/2505.01654v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Comparison of Waymo Rider-Only Crash Rates by Crash Type to Human
  Benchmarks at 56.7 Million Miles","SAE Level 4 Automated Driving Systems (ADSs) are deployed on public roads,
including Waymo's Rider-Only (RO) ride-hailing service (without a driver behind
the steering wheel). The objective of this study was to perform a retrospective
safety assessment of Waymo's RO crash rate compared to human benchmarks,
including disaggregated by crash type.
  Eleven crash type groups were identified from commonly relied upon crash
typologies that are derived from human crash databases. Human benchmarks were
aligned to the same vehicle types, road types, and locations as where the Waymo
Driver operated. Waymo crashes were extracted from the NHTSA Standing General
Order (SGO). RO mileage was provided by the company via a public website.
Any-injury-reported, Airbag Deployment, and Suspected Serious Injury+ crash
outcomes were examined because they represented previously established,
safety-relevant benchmarks where statistical testing could be performed at the
current mileage.
  Data was examined over 56.7 million RO miles through the end of January 2025,
resulting in a statistically significant lower crashed vehicle rate for all
crashes compared to the benchmarks in Any-Injury-Reported and Airbag
Deployment, and Suspected Serious Injury+ crashes. Of the crash types, V2V
Intersection crash events represented the largest total crash reduction, with a
96% reduction in Any-injury-reported (87%-99% CI) and a 91% reduction in Airbag
Deployment (76%-98% CI) events. Cyclist, Motorcycle, Pedestrian, Secondary
Crash, and Single Vehicle crashes were also statistically reduced for the
Any-Injury-Reported outcome. There was no statistically significant disbenefit
found in any of the 11 crash type groups.
  This study represents the first retrospective safety assessment of an RO ADS
that made statistical conclusions about more serious crash outcomes and
analyzed crash rates on a crash type basis.","Kristofer D. Kusano, John M. Scanlon, Yin-Hsiu Chen, Timothy L. McMurry, Tilia Gode, Trent Victor",2025-05-02T18:04:20Z,2025-05-02T18:04:20Z,http://arxiv.org/abs/2505.01515v1,http://arxiv.org/pdf/2505.01515v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
Task and Joint Space Dual-Arm Compliant Control,"Robots that interact with humans or perform delicate manipulation tasks must
exhibit compliance. However, most commercial manipulators are rigid and suffer
from significant friction, limiting end-effector tracking accuracy in
torque-controlled modes. To address this, we present a real-time, open-source
impedance controller that smoothly interpolates between joint-space and
task-space compliance. This hybrid approach ensures safe interaction and
precise task execution, such as sub-centimetre pin insertions. We deploy our
controller on Frank, a dual-arm platform with two Kinova Gen3 arms, and
compensate for modelled friction dynamics using a model-free observer. The
system is real-time capable and integrates with standard ROS tools like
MoveIt!. It also supports high-frequency trajectory streaming, enabling
closed-loop execution of trajectories generated by learning-based methods,
optimal control, or teleoperation. Our results demonstrate robust tracking and
compliant behaviour even under high-friction conditions. The complete system is
available open-source at
https://github.com/applied-ai-lab/compliant_controllers.","Alexander L. Mitchell, Tobit Flatscher, Ingmar Posner",2025-04-29T20:20:41Z,2025-04-29T20:20:41Z,http://arxiv.org/abs/2504.21159v1,http://arxiv.org/pdf/2504.21159v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
ROSA: A Knowledge-based Solution for Robot Self-Adaptation,"Autonomous robots must operate in diverse environments and handle multiple
tasks despite uncertainties. This creates challenges in designing software
architectures and task decision-making algorithms, as different contexts may
require distinct task logic and architectural configurations. To address this,
robotic systems can be designed as self-adaptive systems capable of adapting
their task execution and software architecture at runtime based on their
context.This paper introduces ROSA, a novel knowledge-based framework for RObot
Self-Adaptation, which enables task-and-architecture co-adaptation (TACA) in
robotic systems. ROSA achieves this by providing a knowledge model that
captures all application-specific knowledge required for adaptation and by
reasoning over this knowledge at runtime to determine when and how adaptation
should occur. In addition to a conceptual framework, this work provides an
open-source ROS 2-based reference implementation of ROSA and evaluates its
feasibility and performance in an underwater robotics application. Experimental
results highlight ROSA's advantages in reusability and development effort for
designing self-adaptive robotic systems.","Gustavo Rezende Silva, Juliane Päßler, S. Lizeth Tapia Tarifa, Einar Broch Johnsen, Carlos Hernández Corbato",2025-04-29T12:49:45Z,2025-04-29T12:49:45Z,http://arxiv.org/abs/2505.00733v1,http://arxiv.org/pdf/2505.00733v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
Hydra: Marker-Free RGB-D Hand-Eye Calibration,"This work presents an RGB-D imaging-based approach to marker-free hand-eye
calibration using a novel implementation of the iterative closest point (ICP)
algorithm with a robust point-to-plane (PTP) objective formulated on a Lie
algebra. Its applicability is demonstrated through comprehensive experiments
using three well known serial manipulators and two RGB-D cameras. With only
three randomly chosen robot configurations, our approach achieves approximately
90% successful calibrations, demonstrating 2-3x higher convergence rates to the
global optimum compared to both marker-based and marker-free baselines. We also
report 2 orders of magnitude faster convergence time (0.8 +/- 0.4 s) for 9
robot configurations over other marker-free methods. Our method exhibits
significantly improved accuracy (5 mm in task space) over classical approaches
(7 mm in task space) whilst being marker-free. The benchmarking dataset and
code are open sourced under Apache 2.0 License, and a ROS 2 integration with
robot abstraction is provided to facilitate deployment.","Martin Huber, Huanyu Tian, Christopher E. Mower, Lucas-Raphael Müller, Sébastien Ourselin, Christos Bergeles, Tom Vercauteren",2025-04-29T09:39:59Z,2025-04-29T09:39:59Z,http://arxiv.org/abs/2504.20584v1,http://arxiv.org/pdf/2504.20584v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Combining Quality of Service and System Health Metrics in MAPE-K based
  ROS Systems through Behavior Trees","In recent years, the field of robotics has witnessed a significant shift from
operating in structured environments to handling dynamic and unpredictable
settings. To tackle these challenges, methodologies from the field of
self-adaptive systems enabling these systems to react to unforeseen
circumstances during runtime have been applied. The Monitoring-Analysis-
Planning-Execution over Knowledge (MAPE-K) feedback loop model is a popular
approach, often implemented in a managing subsystem, responsible for monitoring
and adapting a managed subsystem. This work explores the implementation of the
MAPE- K feedback loop based on Behavior Trees (BTs) within the Robot Operating
System 2 (ROS2) framework. By delineating the managed and managing subsystems,
our approach enhances the flexibility and adaptability of ROS-based systems,
ensuring they not only meet Quality-of-Service (QoS), but also system health
metric requirements, namely availability of ROS nodes and communication
channels. Our implementation allows for the application of the method to new
managed subsystems without needing custom BT nodes as the desired behavior can
be configured within a specific rule set. We demonstrate the effectiveness of
our method through various experiments on a system showcasing an aerial
perception use case. By evaluating different failure cases, we show both an
increased perception quality and a higher system availability. Our code is open
source","Andreas Wiedholz, Rafael Paintner, Julian Gleißner, Alwin Hoffmann",2025-04-29T07:19:23Z,2025-04-29T07:19:23Z,http://arxiv.org/abs/2504.20477v1,http://arxiv.org/pdf/2504.20477v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Feelbert: A Feedback Linearization-based Embedded Real-Time Quadrupedal
  Locomotion Framework","Quadruped robots have become quite popular for their ability to adapt their
locomotion to generic uneven terrains. For this reason, over time, several
frameworks for quadrupedal locomotion have been proposed, but with little
attention to ensuring a predictable timing behavior of the controller.
  To address this issue, this work presents Feelbert, a modular control
framework for quadrupedal locomotion suitable for execution on an embedded
system under hard real-time execution constraints. It leverages the feedback
linearization control technique to obtain a closed-form control law for the
body, valid for all configurations of the robot. The control law was derived
after defining an appropriate rigid body model that uses the accelerations of
the feet as control variables, instead of the estimated contact forces. This
work also provides a novel algorithm to compute footholds and gait temporal
parameters using the concept of imaginary wheels, and a heuristic algorithm to
select the best gait schedule for the current velocity commands.
  The proposed framework is developed entirely in C++, with no dependencies on
third-party libraries and no dynamic memory allocation, to ensure
predictability and real-time performance. Its implementation allows Feelbert to
be both compiled and executed on an embedded system for critical applications,
as well as integrated into larger systems such as Robot Operating System 2 (ROS
2). For this reason, Feelbert has been tested in both scenarios, demonstrating
satisfactory results both in terms of reference tracking and temporal
predictability, whether integrated into ROS 2 or compiled as a standalone
application on a Raspberry Pi 5.","Aristide Emanuele Casucci, Federico Nesti, Mauro Marinoni, Giorgio Buttazzo",2025-04-28T16:36:28Z,2025-04-29T09:44:33Z,http://arxiv.org/abs/2504.19965v2,http://arxiv.org/pdf/2504.19965v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Implementation Analysis of Collaborative Robot Digital Twins in Physics
  Engines","This paper presents a Digital Twin (DT) of a 6G communications system testbed
that integrates two robotic manipulators with a high-precision optical infrared
tracking system in Unreal Engine 5. Practical details of the setup and
implementation insights provide valuable guidance for users aiming to replicate
such systems, an endeavor that is crucial to advancing DT applications within
the scientific community. Key topics discussed include video streaming,
integration within the Robot Operating System 2 (ROS 2), and bidirectional
communication. The insights provided are intended to support the development
and deployment of DTs in robotics and automation research.","Christian König, Jan Petershans, Jan Herbst, Matthias Rüb, Dennis Krummacker, Eric Mittag, Hans D. Schotten",2025-04-25T09:30:43Z,2025-04-28T07:59:41Z,http://arxiv.org/abs/2504.18200v2,http://arxiv.org/pdf/2504.18200v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
RoboCup Rescue 2025 Team Description Paper UruBots,"This paper describes the approach used by Team UruBots for participation in
the 2025 RoboCup Rescue Robot League competition. Our team aims to participate
for the first time in this competition at RoboCup, using experience learned
from previous competitions and research. We present our vehicle and our
approach to tackle the task of detecting and finding victims in search and
rescue environments. Our approach contains known topics in robotics, such as
ROS, SLAM, Human Robot Interaction and segmentation and perception. Our
proposed approach is open source, available to the RoboCup Rescue community,
where we aim to learn and contribute to the league.","Kevin Farias, Pablo Moraes, Igor Nunes, Juan Deniz, Sebastian Barcelona, Hiago Sodre, William Moraes, Monica Rodriguez, Ahilen Mazondo, Vincent Sandin, Gabriel da Silva, Victoria Saravia, Vinicio Melgar, Santiago Fernandez, Ricardo Grando",2025-04-14T00:37:50Z,2025-04-14T00:37:50Z,http://arxiv.org/abs/2504.09778v1,http://arxiv.org/pdf/2504.09778v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Ice-Breakers, Turn-Takers and Fun-Makers: Exploring Robots for Groups
  with Teenagers","Successful, enjoyable group interactions are important in public and personal
contexts, especially for teenagers whose peer groups are important for
self-identity and self-esteem. Social robots seemingly have the potential to
positively shape group interactions, but it seems difficult to effect such
impact by designing robot behaviors solely based on related (human interaction)
literature. In this article, we take a user-centered approach to explore how
teenagers envisage a social robot ""group assistant"". We engaged 16 teenagers in
focus groups, interviews, and robot testing to capture their views and
reflections about robots for groups. Over the course of a two-week summer
school, participants co-designed the action space for such a robot and
experienced working with/wizarding it for 10+ hours. This experience further
altered and deepened their insights into using robots as group assistants. We
report results regarding teenagers' views on the applicability and use of a
robot group assistant, how these expectations evolved throughout the study, and
their repeat interactions with the robot. Our results indicate that each group
moves on a spectrum of need for the robot, reflected in use of the robot more
(or less) for ice-breaking, turn-taking, and fun-making as the situation
demanded.","Sarah Gillet, Katie Winkle, Giulia Belgiovine, Iolanda Leite",2025-04-09T09:19:24Z,2025-04-09T09:19:24Z,http://arxiv.org/abs/2504.06718v1,http://arxiv.org/pdf/2504.06718v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
Underwater Robotic Simulators Review for Autonomous System Development,"The increasing complexity of underwater robotic systems has led to a surge in
simulation platforms designed to support perception, planning, and control
tasks in marine environments. However, selecting the most appropriate
underwater robotic simulator (URS) remains a challenge due to wide variations
in fidelity, extensibility, and task suitability. This paper presents a
comprehensive review and comparative analysis of five state-of-the-art,
ROS-compatible, open-source URSs: Stonefish, DAVE, HoloOcean, MARUS, and
UNav-Sim. Each simulator is evaluated across multiple criteria including sensor
fidelity, environmental realism, sim-to-real capabilities, and research impact.
We evaluate them across architectural design, sensor and physics modeling, task
capabilities, and research impact. Additionally, we discuss ongoing challenges
in sim-to-real transfer and highlight the need for standardization and
benchmarking in the field. Our findings aim to guide practitioners in selecting
effective simulation environments and inform future development of more robust
and transferable URSs.","Sara Aldhaheri, Yang Hu, Yongchang Xie, Peng Wu, Dimitrios Kanoulas, Yuanchang Liu",2025-04-08T17:43:48Z,2025-04-08T17:43:48Z,http://arxiv.org/abs/2504.06245v1,http://arxiv.org/pdf/2504.06245v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Accessible and Pedagogically-Grounded Explainability for Human-Robot
  Interaction: A Framework Based on UDL and Symbolic Interfaces","This paper presents a novel framework for accessible and
pedagogically-grounded robot explainability, designed to support human-robot
interaction (HRI) with users who have diverse cognitive, communicative, or
learning needs. We combine principles from Universal Design for Learning (UDL)
and Universal Design (UD) with symbolic communication strategies to facilitate
the alignment of mental models between humans and robots. Our approach employs
Asterics Grid and ARASAAC pictograms as a multimodal, interpretable front-end,
integrated with a lightweight HTTP-to-ROS 2 bridge that enables real-time
interaction and explanation triggering. We emphasize that explainability is not
a one-way function but a bidirectional process, where human understanding and
robot transparency must co-evolve. We further argue that in educational or
assistive contexts, the role of a human mediator (e.g., a teacher) may be
essential to support shared understanding. We validate our framework with
examples of multimodal explanation boards and discuss how it can be extended to
different scenarios in education, assistive robotics, and inclusive AI.","Francisco J. Rodríguez Lera, Raquel Fernández Hernández, Sonia Lopez González, Miguel Angel González-Santamarta, Francisco Jesús Rodríguez Sedano, Camino Fernandez Llamas",2025-04-08T16:33:52Z,2025-04-08T16:33:52Z,http://arxiv.org/abs/2504.06189v1,http://arxiv.org/pdf/2504.06189v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
Public speech recognition transcripts as a configuring parameter,"Displaying a written transcript of what a human said (i.e. producing an
""automatic speech recognition transcript"") is a common feature for smartphone
vocal assistants: the utterance produced by a human speaker (e.g. a question)
is displayed on the screen while it is being verbally responded to by the vocal
assistant. Although very rarely, this feature also exists on some ""social""
robots which transcribe human interactants' speech on a screen or a tablet. We
argue that this informational configuration is pragmatically consequential on
the interaction, both for human participants and for the embodied
conversational agent. Based on a corpus of co-present interactions with a
humanoid robot, we attempt to show that this transcript is a contextual feature
which can heavily impact the actions ascribed by humans to the robot: that is,
the way in which humans respond to the robot's behavior as constituting a
specific type of action (rather than another) and as constituting an adequate
response to their own previous turn.","Damien Rudaz, Christian Licoppe",2025-04-06T13:44:47Z,2025-04-06T13:44:47Z,http://arxiv.org/abs/2504.04488v1,http://arxiv.org/pdf/2504.04488v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Estimating Scene Flow in Robot Surroundings with Distributed
  Miniaturized Time-of-Flight Sensors","Tracking motions of humans or objects in the surroundings of the robot is
essential to improve safe robot motions and reactions. In this work, we present
an approach for scene flow estimation from low-density and noisy point clouds
acquired from miniaturized Time of Flight (ToF) sensors distributed on the
robot body. The proposed method clusters points from consecutive frames and
applies Iterative Closest Point (ICP) to estimate a dense motion flow, with
additional steps introduced to mitigate the impact of sensor noise and
low-density data points. Specifically, we employ a fitness-based classification
to distinguish between stationary and moving points and an inlier removal
strategy to refine geometric correspondences. The proposed approach is
validated in an experimental setup where 24 ToF are used to estimate the
velocity of an object moving at different controlled speeds. Experimental
results show that the method consistently approximates the direction of the
motion and its magnitude with an error which is in line with sensor noise.","Jack Sander, Giammarco Caroleo, Alessandro Albini, Perla Maiolino",2025-04-03T09:57:51Z,2025-07-31T09:50:03Z,http://arxiv.org/abs/2504.02439v2,http://arxiv.org/pdf/2504.02439v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Context-Aware Human Behavior Prediction Using Multimodal Large Language
  Models: Challenges and Insights","Predicting human behavior in shared environments is crucial for safe and
efficient human-robot interaction. Traditional data-driven methods to that end
are pre-trained on domain-specific datasets, activity types, and prediction
horizons. In contrast, the recent breakthroughs in Large Language Models (LLMs)
promise open-ended cross-domain generalization to describe various human
activities and make predictions in any context. In particular, Multimodal LLMs
(MLLMs) are able to integrate information from various sources, achieving more
contextual awareness and improved scene understanding. The difficulty in
applying general-purpose MLLMs directly for prediction stems from their limited
capacity for processing large input sequences, sensitivity to prompt design,
and expensive fine-tuning. In this paper, we present a systematic analysis of
applying pre-trained MLLMs for context-aware human behavior prediction. To this
end, we introduce a modular multimodal human activity prediction framework that
allows us to benchmark various MLLMs, input variations, In-Context Learning
(ICL), and autoregressive techniques. Our evaluation indicates that the
best-performing framework configuration is able to reach 92.8% semantic
similarity and 66.1% exact label accuracy in predicting human behaviors in the
target frame.","Yuchen Liu, Lino Lerch, Luigi Palmieri, Andrey Rudenko, Sebastian Koch, Timo Ropinski, Marco Aiello",2025-04-01T14:28:19Z,2025-06-23T14:43:46Z,http://arxiv.org/abs/2504.00839v2,http://arxiv.org/pdf/2504.00839v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"PneuDrive: An Embedded Pressure Control System and Modeling Toolkit for
  Large-Scale Soft Robots","In this paper, we present a modular pressure control system called PneuDrive
that can be used for large-scale, pneumatically-actuated soft robots. The
design is particularly suited for situations which require distributed pressure
control and high flow rates. Up to four embedded pressure control modules can
be daisy-chained together as peripherals on a robust RS-485 bus, enabling
closed-loop control of up to 16 valves with pressures ranging from 0-100 psig
(0-689 kPa) over distances of more than 10 meters. The system is configured as
a C++ ROS node by default. However, independent of ROS, we provide a Python
interface with a scripting API for added flexibility. We demonstrate our
implementation of PneuDrive through various trajectory tracking experiments for
a three-joint, continuum soft robot with 12 different pressure inputs. Finally,
we present a modeling toolkit with implementations of three dynamic actuation
models, all suitable for real-time simulation and control. We demonstrate the
use of this toolkit in customizing each model with real-world data and
evaluating the performance of each model. The results serve as a reference
guide for choosing between several actuation models in a principled manner. A
video summarizing our results can be found here: https://bit.ly/3QkrEqO.","Curtis C. Johnson, Daniel G. Cheney, Dallin L. Cordon, Marc D. Killpack",2025-03-31T20:50:06Z,2025-03-31T20:50:06Z,http://arxiv.org/abs/2504.00222v1,http://arxiv.org/pdf/2504.00222v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
Toward Anxiety-Reducing Pocket Robots for Children,"A common denominator for most therapy treatments for children who suffer from
an anxiety disorder is daily practice routines to learn techniques needed to
overcome anxiety. However, applying those techniques while experiencing anxiety
can be highly challenging. This paper presents the design, implementation, and
pilot study of a tactile hand-held pocket robot AffectaPocket, designed to work
alongside therapy as a focus object to facilitate coping during an anxiety
attack. The robot does not require daily practice to be used, has a small form
factor, and has been designed for children 7 to 12 years old. The pocket robot
works by sensing when it is being held and attempts to shift the child's focus
by presenting them with a simple three-note rhythm-matching game. We conducted
a pilot study of the pocket robot involving four children aged 7 to 10 years,
and then a main study with 18 children aged 6 to 8 years; neither study
involved children with anxiety. Both studies aimed to assess the reliability of
the robot's sensor configuration, its design, and the effectiveness of the user
tutorial. The results indicate that the morphology and sensor setup performed
adequately and the tutorial process enabled the children to use the robot with
little practice. This work demonstrates that the presented pocket robot could
represent a step toward developing low-cost accessible technologies to help
children suffering from anxiety disorders.","Morten Roed Frederiksen, Kasper Støy, Maja Matarić",2025-03-31T13:05:59Z,2025-03-31T13:05:59Z,http://arxiv.org/abs/2503.24041v1,http://arxiv.org/pdf/2503.24041v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Deep Visual Servoing of an Aerial Robot Using Keypoint Feature
  Extraction","The problem of image-based visual servoing (IBVS) of an aerial robot using
deep-learning-based keypoint detection is addressed in this article. A
monocular RGB camera mounted on the platform is utilized to collect the visual
data. A convolutional neural network (CNN) is then employed to extract the
features serving as the visual data for the servoing task. This paper
contributes to the field by circumventing not only the challenge stemming from
the need for man-made marker detection in conventional visual servoing
techniques, but also enhancing the robustness against undesirable factors
including occlusion, varying illumination, clutter, and background changes,
thereby broadening the applicability of perception-guided motion control tasks
in aerial robots. Additionally, extensive physics-based ROS Gazebo simulations
are conducted to assess the effectiveness of this method, in contrast to many
existing studies that rely solely on physics-less simulations. A demonstration
video is available at https://youtu.be/Dd2Her8Ly-E.","Shayan Sepahvand, Niloufar Amiri, Farrokh Janabi-Sharifi",2025-03-29T18:01:07Z,2025-03-29T18:01:07Z,http://arxiv.org/abs/2503.23171v1,http://arxiv.org/pdf/2503.23171v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Perspective-Shifted Neuro-Symbolic World Models: A Framework for
  Socially-Aware Robot Navigation","Navigating in environments alongside humans requires agents to reason under
uncertainty and account for the beliefs and intentions of those around them.
Under a sequential decision-making framework, egocentric navigation can
naturally be represented as a Markov Decision Process (MDP). However, social
navigation additionally requires reasoning about the hidden beliefs of others,
inherently leading to a Partially Observable Markov Decision Process (POMDP),
where agents lack direct access to others' mental states. Inspired by Theory of
Mind and Epistemic Planning, we propose (1) a neuro-symbolic model-based
reinforcement learning architecture for social navigation, addressing the
challenge of belief tracking in partially observable environments; and (2) a
perspective-shift operator for belief estimation, leveraging recent work on
Influence-based Abstractions (IBA) in structured multi-agent settings.","Kevin Alcedo, Pedro U. Lima, Rachid Alami",2025-03-26T10:59:08Z,2025-09-02T14:25:18Z,http://arxiv.org/abs/2503.20425v3,http://arxiv.org/pdf/2503.20425v3.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"CTSAC: Curriculum-Based Transformer Soft Actor-Critic for Goal-Oriented
  Robot Exploration","With the increasing demand for efficient and flexible robotic exploration
solutions, Reinforcement Learning (RL) is becoming a promising approach in the
field of autonomous robotic exploration. However, current RL-based exploration
algorithms often face limited environmental reasoning capabilities, slow
convergence rates, and substantial challenges in Sim-To-Real (S2R) transfer. To
address these issues, we propose a Curriculum Learning-based Transformer
Reinforcement Learning Algorithm (CTSAC) aimed at improving both exploration
efficiency and transfer performance. To enhance the robot's reasoning ability,
a Transformer is integrated into the perception network of the Soft
Actor-Critic (SAC) framework, leveraging historical information to improve the
farsightedness of the strategy. A periodic review-based curriculum learning is
proposed, which enhances training efficiency while mitigating catastrophic
forgetting during curriculum transitions. Training is conducted on the
ROS-Gazebo continuous robotic simulation platform, with LiDAR clustering
optimization to further reduce the S2R gap. Experimental results demonstrate
the CTSAC algorithm outperforms the state-of-the-art non-learning and
learning-based algorithms in terms of success rate and success rate-weighted
exploration time. Moreover, real-world experiments validate the strong S2R
transfer capabilities of CTSAC.","Chunyu Yang, Shengben Bi, Yihui Xu, Xin Zhang",2025-03-18T13:44:29Z,2025-03-18T13:44:29Z,http://arxiv.org/abs/2503.14254v1,http://arxiv.org/pdf/2503.14254v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
A Modular Edge Device Network for Surgery Digitalization,"Future surgical care demands real-time, integrated data to drive informed
decision-making and improve patient outcomes. The pressing need for seamless
and efficient data capture in the OR motivates our development of a modular
solution that bridges the gap between emerging machine learning techniques and
interventional medicine. We introduce a network of edge devices, called Data
Hubs (DHs), that interconnect diverse medical sensors, imaging systems, and
robotic tools via optical fiber and a centralized network switch. Built on the
NVIDIA Jetson Orin NX, each DH supports multiple interfaces (HDMI, USB-C,
Ethernet) and encapsulates device-specific drivers within Docker containers
using the Isaac ROS framework and ROS2. A centralized user interface enables
straightforward configuration and real-time monitoring, while an Nvidia DGX
computer provides state-of-the-art data processing and storage. We validate our
approach through an ultrasound-based 3D anatomical reconstruction experiment
that combines medical imaging, pose tracking, and RGB-D data acquisition.","Vincent Schorp, Frédéric Giraud, Gianluca Pargätzi, Michael Wäspe, Lorenzo von Ritter-Zahony, Marcel Wegmann, Nicola A. Cavalcanti, John Garcia Henao, Nicholas Bünger, Dominique Cachin, Sebastiano Caprara, Philipp Fürnstahl, Fabio Carrillo",2025-03-18T09:08:19Z,2025-04-11T08:11:52Z,http://arxiv.org/abs/2503.14049v3,http://arxiv.org/pdf/2503.14049v3.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"V2X-ReaLO: An Open Online Framework and Dataset for Cooperative
  Perception in Reality","Cooperative perception enabled by Vehicle-to-Everything (V2X) communication
holds significant promise for enhancing the perception capabilities of
autonomous vehicles, allowing them to overcome occlusions and extend their
field of view. However, existing research predominantly relies on simulated
environments or static datasets, leaving the feasibility and effectiveness of
V2X cooperative perception especially for intermediate fusion in real-world
scenarios largely unexplored. In this work, we introduce V2X-ReaLO, an open
online cooperative perception framework deployed on real vehicles and smart
infrastructure that integrates early, late, and intermediate fusion methods
within a unified pipeline and provides the first practical demonstration of
online intermediate fusion's feasibility and performance under genuine
real-world conditions. Additionally, we present an open benchmark dataset
specifically designed to assess the performance of online cooperative
perception systems. This new dataset extends V2X-Real dataset to dynamic,
synchronized ROS bags and provides 25,028 test frames with 6,850 annotated key
frames in challenging urban scenarios. By enabling real-time assessments of
perception accuracy and communication lantency under dynamic conditions,
V2X-ReaLO sets a new benchmark for advancing and optimizing cooperative
perception systems in real-world applications. The codes and datasets will be
released to further advance the field.","Hao Xiang, Zhaoliang Zheng, Xin Xia, Seth Z. Zhao, Letian Gao, Zewei Zhou, Tianhui Cai, Yun Zhang, Jiaqi Ma",2025-03-13T04:31:20Z,2025-03-13T04:31:20Z,http://arxiv.org/abs/2503.10034v1,http://arxiv.org/pdf/2503.10034v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
AirSwarm: Enabling Cost-Effective Multi-UAV Research with COTS drones,"Traditional unmanned aerial vehicle (UAV) swarm missions rely heavily on
expensive custom-made drones with onboard perception or external positioning
systems, limiting their widespread adoption in research and education. To
address this issue, we propose AirSwarm. AirSwarm democratizes multi-drone
coordination using low-cost commercially available drones such as Tello or
Anafi, enabling affordable swarm aerial robotics research and education. Key
innovations include a hierarchical control architecture for reliable multi-UAV
coordination, an infrastructure-free visual SLAM system for precise
localization without external motion capture, and a ROS-based software
framework for simplified swarm development. Experiments demonstrate cm-level
tracking accuracy, low-latency control, communication failure resistance,
formation flight, and trajectory tracking. By reducing financial and technical
barriers, AirSwarm makes multi-robot education and research more accessible.
The complete instructions and open source code will be available at","Xiaowei Li, Kuan Xu, Fen Liu, Ruofei Bai, Shenghai Yuan, Lihua Xie",2025-03-10T03:35:35Z,2025-03-10T03:35:35Z,http://arxiv.org/abs/2503.06890v1,http://arxiv.org/pdf/2503.06890v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"HIPPO-MAT: Decentralized Task Allocation Using GraphSAGE and Multi-Agent
  Deep Reinforcement Learning","This paper tackles decentralized continuous task allocation in heterogeneous
multi-agent systems. We present a novel framework HIPPO-MAT that integrates
graph neural networks (GNN) employing a GraphSAGE architecture to compute
independent embeddings on each agent with an Independent Proximal Policy
Optimization (IPPO) approach for multi-agent deep reinforcement learning. In
our system, unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs)
share aggregated observation data via communication channels while
independently processing these inputs to generate enriched state embeddings.
This design enables dynamic, cost-optimal, conflict-aware task allocation in a
3D grid environment without the need for centralized coordination. A modified
A* path planner is incorporated for efficient routing and collision avoidance.
Simulation experiments demonstrate scalability with up to 30 agents and
preliminary real-world validation on JetBot ROS AI Robots, each running its
model on a Jetson Nano and communicating through an ESP-NOW protocol using
ESP32-S3, which confirms the practical viability of the approach that
incorporates simultaneous localization and mapping (SLAM). Experimental results
revealed that our method achieves a high 92.5% conflict-free success rate, with
only a 16.49% performance gap compared to the centralized Hungarian method,
while outperforming the heuristic decentralized baseline based on greedy
approach. Additionally, the framework exhibits scalability with up to 30 agents
with allocation processing of 0.32 simulation step time and robustness in
responding to dynamically generated tasks.","Lavanya Ratnabala, Robinroy Peter, Aleksey Fedoseev, Dzmitry Tsetserukou",2025-03-08T10:44:14Z,2025-03-08T10:44:14Z,http://arxiv.org/abs/2503.07662v1,http://arxiv.org/pdf/2503.07662v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"HyperGraph ROS: An Open-Source Robot Operating System for Hybrid
  Parallel Computing based on Computational HyperGraph","This paper presents HyperGraph ROS, an open-source robot operating system
that unifies intra-process, inter-process, and cross-device computation into a
computational hypergraph for efficient message passing and parallel execution.
In order to optimize communication, HyperGraph ROS dynamically selects the
optimal communication mechanism while maintaining a consistent API. For
intra-process messages, Intel-TBB Flow Graph is used with C++ pointer passing,
which ensures zero memory copying and instant delivery. Meanwhile,
inter-process and cross-device communication seamlessly switch to ZeroMQ. When
a node receives a message from any source, it is immediately activated and
scheduled for parallel execution by Intel-TBB. The computational hypergraph
consists of nodes represented by TBB flow graph nodes and edges formed by TBB
pointer-based connections for intra-process communication, as well as ZeroMQ
links for inter-process and cross-device communication. This structure enables
seamless distributed parallelism. Additionally, HyperGraph ROS provides
ROS-like utilities such as a parameter server, a coordinate transformation
tree, and visualization tools. Evaluation in diverse robotic scenarios
demonstrates significantly higher transmission and throughput efficiency
compared to ROS 2. Our work is available at
https://github.com/wujiazheng2020a/hyper_graph_ros.","Shufang Zhang, Jiazheng Wu, Jiacheng He, Kaiyi Wang, Shan An",2025-03-07T03:29:52Z,2025-03-07T03:29:52Z,http://arxiv.org/abs/2503.05117v1,http://arxiv.org/pdf/2503.05117v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
PERCY: Personal Emotional Robotic Conversational System,"Traditional rule-based conversational robots, constrained by predefined
scripts and static response mappings, fundamentally lack adaptability for
personalized, long-term human interaction. While Large Language Models (LLMs)
like GPT-4 have revolutionized conversational AI through open-domain
capabilities, current social robots implementing LLMs still lack emotional
awareness and continuous personalization. This dual limitation hinders their
ability to sustain engagement across multiple interaction sessions. We bridge
this gap with PERCY (Personal Emotional Robotic Conversational sYstem), a
system designed to enable open-domain, multi-turn dialogues by dynamically
analyzing users' real-time facial expressions and vocabulary to tailor
responses based on their emotional state. Built on a ROS-based multimodal
framework, PERCY integrates a fine-tuned GPT-4 reasoning engine, combining
textual sentiment analysis with visual emotional cues to accurately assess and
respond to user emotions. We evaluated PERCY's performance through various
dialogue quality metrics, showing strong coherence, relevance, and diversity.
Human evaluations revealed PERCY's superior personalization and comparable
naturalness to other models. This work highlights the potential for integrating
advanced multimodal perception and personalization in social robot dialogue
systems.","Zhijin Meng, Mohammed Althubyani, Shengyuan Xie, Imran Razzak, Eduardo B. Sandoval, Mahdi Bamdad, Francisco Cruz",2025-03-04T03:11:47Z,2025-03-04T03:11:47Z,http://arxiv.org/abs/2503.16473v1,http://arxiv.org/pdf/2503.16473v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
A Navigation System for ROV's inspection on Fish Net Cage,"Autonomous Remotely Operated Vehicles (ROVs) offer a promising solution for
automating fishnet inspection, reducing labor dependency, and improving
operational efficiency. In this paper, we modify an off-the-shelf ROV, the
BlueROV2, into a ROS-based framework and develop a localization module, a path
planning system, and a control framework. For real-time, local localization, we
employ the open-source TagSLAM library. Additionally, we propose a control
strategy based on a Nominal Feedback Controller (NFC) to achieve precise
trajectory tracking. The proposed system has been implemented and validated
through experiments in a controlled laboratory environment, demonstrating its
effectiveness for real-world applications.","Zhikang Ge, Fang Yang, Wenwu Lu, Peng Wei, Yibin Ying, Chen Peng",2025-03-01T13:17:16Z,2025-03-01T13:17:16Z,http://arxiv.org/abs/2503.00482v1,http://arxiv.org/pdf/2503.00482v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
Ro-To-Go! Robust Reactive Control with Signal Temporal Logic,"Signal Temporal Logic (STL) robustness is a common objective for optimal
robot control, but its dependence on history limits the robot's decision-making
capabilities when used in Model Predictive Control (MPC) approaches. In this
work, we introduce Signal Temporal Logic robustness-to-go (Ro-To-Go), a new
quantitative semantics for the logic that isolates the contributions of suffix
trajectories. We prove its relationship to formula progression for Metric
Temporal Logic, and show that the robustness-to-go depends only on the suffix
trajectory and progressed formula. We implement robustness-to-go as the
objective in an MPC algorithm and use formula progression to efficiently
evaluate it online. We test the algorithm in simulation and compare it to MPC
using traditional STL robustness. Our experiments show that using
robustness-to-go results in a higher success rate.","Roland Ilyes, Lara Brudermüller, Nick Hawes, Bruno Lacerda",2025-02-28T18:00:23Z,2025-03-17T14:25:17Z,http://arxiv.org/abs/2503.05792v2,http://arxiv.org/pdf/2503.05792v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
The NavINST Dataset for Multi-Sensor Autonomous Navigation,"The NavINST Laboratory has developed a comprehensive multisensory dataset
from various road-test trajectories in urban environments, featuring diverse
lighting conditions, including indoor garage scenarios with dense 3D maps. This
dataset includes multiple commercial-grade IMUs and a high-end tactical-grade
IMU. Additionally, it contains a wide array of perception-based sensors, such
as a solid-state LiDAR - making it one of the first datasets to do so - a
mechanical LiDAR, four electronically scanning RADARs, a monocular camera, and
two stereo cameras. The dataset also includes forward speed measurements
derived from the vehicle's odometer, along with accurately post-processed
high-end GNSS/IMU data, providing precise ground truth positioning and
navigation information. The NavINST dataset is designed to support advanced
research in high-precision positioning, navigation, mapping, computer vision,
and multisensory fusion. It offers rich, multi-sensor data ideal for developing
and validating robust algorithms for autonomous vehicles. Finally, it is fully
integrated with the ROS, ensuring ease of use and accessibility for the
research community. The complete dataset and development tools are available at
https://navinst.github.io.","Paulo Ricardo Marques de Araujo, Eslam Mounier, Qamar Bader, Emma Dawson, Shaza I. Kaoud Abdelaziz, Ahmed Zekry, Mohamed Elhabiby, Aboelmagd Noureldin",2025-02-19T16:31:56Z,2025-02-19T16:31:56Z,http://arxiv.org/abs/2502.13863v1,http://arxiv.org/pdf/2502.13863v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"SHIFT: An Interdisciplinary Framework for Scaffolding Human Attention
  and Understanding in Explanatory Tasks","In this work, we present a domain-independent approach for adaptive
scaffolding in robotic explanation generation to guide tasks in human-robot
interaction. We present a method for incorporating interdisciplinary research
results into a computational model as a pre-configured scoring system
implemented in a framework called SHIFT. This involves outlining a procedure
for integrating concepts from disciplines outside traditional computer science
into a robotics computational framework. Our approach allows us to model the
human cognitive state into six observable states within the human partner
model. To study the pre-configuration of the system, we implement a
reinforcement learning approach on top of our model. This approach allows
adaptation to individuals who deviate from the configuration of the scoring
system. Therefore, in our proof-of-concept evaluation, the model's adaptability
on four different user types shows that the models' adaptation performs better,
i.e., recouped faster after exploration and has a higher accumulated reward
with our pre-configured scoring system than without it. We discuss further
strategies of speeding up the learning phase to enable a realistic adaptation
behavior to real users. The system is accessible through docker and supports
querying via ROS.","André Groß, Birte Richter, Britta Wrede",2025-02-17T12:46:31Z,2025-02-17T12:46:31Z,http://arxiv.org/abs/2503.16447v1,http://arxiv.org/pdf/2503.16447v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"A Multi-Simulation Approach with Model Predictive Control for Anafi
  Drones","Simulation frameworks are essential for the safe development of robotic
applications. However, different components of a robotic system are often best
simulated in different environments, making full integration challenging. This
is particularly true for partially-open or closed-source simulators, which
commonly suffer from two limitations: (i) lack of runtime control over scene
actors via interfaces like ROS, and (ii) restricted access to real-time state
data (e.g., pose, velocity) of scene objects. In the first part of this work,
we address these issues by integrating aerial drones simulated in Parrot's
Sphinx environment (used for Anafi drones) into the Gazebo simulator. Our
approach uses a mirrored drone instance embedded within Gazebo environments to
bridge the two simulators. One key application is aerial target tracking, a
common task in multi-robot systems. However, Parrot's default PID-based
controller lacks the agility needed for tracking fast-moving targets. To
overcome this, in the second part of this work we develop a model predictive
controller (MPC) that leverages cumulative error states to improve tracking
accuracy. Our MPC significantly outperforms the built-in PID controller in
dynamic scenarios, increasing the effectiveness of the overall system. We
validate our integrated framework by incorporating the Anafi drone into an
existing Gazebo-based airship simulation and rigorously test the MPC against a
custom PID baseline in both simulated and real-world experiments.","Pascal Goldschmid, Aamir Ahmad",2025-02-14T15:11:21Z,2025-07-12T19:46:58Z,http://arxiv.org/abs/2502.10218v2,http://arxiv.org/pdf/2502.10218v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Energy-Efficient Autonomous Aerial Navigation with Dynamic Vision
  Sensors: A Physics-Guided Neuromorphic Approach","Vision-based object tracking is a critical component for achieving autonomous
aerial navigation, particularly for obstacle avoidance. Neuromorphic Dynamic
Vision Sensors (DVS) or event cameras, inspired by biological vision, offer a
promising alternative to conventional frame-based cameras. These cameras can
detect changes in intensity asynchronously, even in challenging lighting
conditions, with a high dynamic range and resistance to motion blur. Spiking
neural networks (SNNs) are increasingly used to process these event-based
signals efficiently and asynchronously. Meanwhile, physics-based artificial
intelligence (AI) provides a means to incorporate system-level knowledge into
neural networks via physical modeling. This enhances robustness, energy
efficiency, and provides symbolic explainability. In this work, we present a
neuromorphic navigation framework for autonomous drone navigation. The focus is
on detecting and navigating through moving gates while avoiding collisions. We
use event cameras for detecting moving objects through a shallow SNN
architecture in an unsupervised manner. This is combined with a lightweight
energy-aware physics-guided neural network (PgNN) trained with depth inputs to
predict optimal flight times, generating near-minimum energy paths. The system
is implemented in the Gazebo simulator and integrates a sensor-fused
vision-to-planning neuro-symbolic framework built with the Robot Operating
System (ROS) middleware. This work highlights the future potential of
integrating event-based vision with physics-guided planning for
energy-efficient autonomous navigation, particularly for low-latency
decision-making.","Sourav Sanyal, Amogh Joshi, Manish Nagaraj, Rohan Kumar Manna, Kaushik Roy",2025-02-09T15:40:18Z,2025-04-23T17:55:38Z,http://arxiv.org/abs/2502.05938v2,http://arxiv.org/pdf/2502.05938v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Generating Physically Realistic and Directable Human Motions from
  Multi-Modal Inputs","This work focuses on generating realistic, physically-based human behaviors
from multi-modal inputs, which may only partially specify the desired motion.
For example, the input may come from a VR controller providing arm motion and
body velocity, partial key-point animation, computer vision applied to videos,
or even higher-level motion goals. This requires a versatile low-level humanoid
controller that can handle such sparse, under-specified guidance, seamlessly
switch between skills, and recover from failures. Current approaches for
learning humanoid controllers from demonstration data capture some of these
characteristics, but none achieve them all. To this end, we introduce the
Masked Humanoid Controller (MHC), a novel approach that applies multi-objective
imitation learning on augmented and selectively masked motion demonstrations.
The training methodology results in an MHC that exhibits the key capabilities
of catch-up to out-of-sync input commands, combining elements from multiple
motion sequences, and completing unspecified parts of motions from sparse
multimodal input. We demonstrate these key capabilities for an MHC learned over
a dataset of 87 diverse skills and showcase different multi-modal use cases,
including integration with planning frameworks to highlight MHC's ability to
solve new user-defined tasks without any finetuning.","Aayam Shrestha, Pan Liu, German Ros, Kai Yuan, Alan Fern",2025-02-08T17:02:11Z,2025-02-08T17:02:11Z,http://arxiv.org/abs/2502.05641v1,http://arxiv.org/pdf/2502.05641v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Demonstrating CavePI: Autonomous Exploration of Underwater Caves by
  Semantic Guidance","Enabling autonomous robots to safely and efficiently navigate, explore, and
map underwater caves is of significant importance to water resource management,
hydrogeology, archaeology, and marine robotics. In this work, we demonstrate
the system design and algorithmic integration of a visual servoing framework
for semantically guided autonomous underwater cave exploration. We present the
hardware and edge-AI design considerations to deploy this framework on a novel
AUV (Autonomous Underwater Vehicle) named CavePI. The guided navigation is
driven by a computationally light yet robust deep visual perception module,
delivering a rich semantic understanding of the environment. Subsequently, a
robust control mechanism enables CavePI to track the semantic guides and
navigate within complex cave structures. We evaluate the system through field
experiments in natural underwater caves and spring-water sites and further
validate its ROS (Robot Operating System)-based digital twin in a simulation
environment. Our results highlight how these integrated design choices
facilitate reliable navigation under feature-deprived, GPS-denied, and
low-visibility conditions.","Alankrit Gupta, Adnan Abdullah, Xianyao Li, Vaishnav Ramesh, Ioannis Rekleitis, Md Jahidul Islam",2025-02-07T23:43:34Z,2025-04-24T17:16:47Z,http://arxiv.org/abs/2502.05384v4,http://arxiv.org/pdf/2502.05384v4.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Reduce Lap Time for Autonomous Racing with Curvature-Integrated MPCC
  Local Trajectory Planning Method","The widespread application of autonomous driving technology has significantly
advanced the field of autonomous racing. Model Predictive Contouring Control
(MPCC) is a highly effective local trajectory planning method for autonomous
racing. However, the traditional MPCC method struggles with racetracks that
have significant curvature changes, limiting the performance of the vehicle
during autonomous racing. To address this issue, we propose a
curvature-integrated MPCC (CiMPCC) local trajectory planning method for
autonomous racing. This method optimizes the velocity of the local trajectory
based on the curvature of the racetrack centerline. The specific implementation
involves mapping the curvature of the racetrack centerline to a reference
velocity profile, which is then incorporated into the cost function for
optimizing the velocity of the local trajectory. This reference velocity
profile is created by normalizing and mapping the curvature of the racetrack
centerline, thereby ensuring efficient and performance-oriented local
trajectory planning in racetracks with significant curvature. The proposed
CiMPCC method has been experimented on a self-built 1:10 scale F1TENTH racing
vehicle deployed with ROS platform. The experimental results demonstrate that
the proposed method achieves outstanding results on a challenging racetrack
with sharp curvature, improving the overall lap time by 11.4%-12.5% compared to
other autonomous racing trajectory planning methods. Our code is available at
https://github.com/zhouhengli/CiMPCC.","Zhouheng Li, Lei Xie, Cheng Hu, Hongye Su",2025-02-06T01:03:54Z,2025-02-06T01:03:54Z,http://arxiv.org/abs/2502.03695v1,http://arxiv.org/pdf/2502.03695v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Force-Based Robotic Imitation Learning: A Two-Phase Approach for
  Construction Assembly Tasks","The drive for efficiency and safety in construction has boosted the role of
robotics and automation. However, complex tasks like welding and pipe insertion
pose challenges due to their need for precise adaptive force control, which
complicates robotic training. This paper proposes a two-phase system to improve
robot learning, integrating human-derived force feedback. The first phase
captures real-time data from operators using a robot arm linked with a virtual
simulator via ROS-Sharp. In the second phase, this feedback is converted into
robotic motion instructions, using a generative approach to incorporate force
feedback into the learning process. This method's effectiveness is demonstrated
through improved task completion times and success rates. The framework
simulates realistic force-based interactions, enhancing the training data's
quality for precise robotic manipulation in construction tasks.","Hengxu You, Yang Ye, Tianyu Zhou, Jing Du",2025-01-24T22:01:23Z,2025-01-24T22:01:23Z,http://arxiv.org/abs/2501.14942v1,http://arxiv.org/pdf/2501.14942v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"HAMMER: Heterogeneous, Multi-Robot Semantic Gaussian Splatting","3D Gaussian Splatting offers expressive scene reconstruction, modeling a
broad range of visual, geometric, and semantic information. However, efficient
real-time map reconstruction with data streamed from multiple robots and
devices remains a challenge. To that end, we propose HAMMER, a server-based
collaborative Gaussian Splatting method that leverages widely available ROS
communication infrastructure to generate 3D, metric-semantic maps from
asynchronous robot data-streams with no prior knowledge of initial robot
positions and varying on-device pose estimators. HAMMER consists of (i) a frame
alignment module that transforms local SLAM poses and image data into a global
frame and requires no prior relative pose knowledge, and (ii) an online module
for training semantic 3DGS maps from streaming data. HAMMER handles mixed
perception modes, adjusts automatically for variations in image pre-processing
among different devices, and distills CLIP semantic codes into the 3D scene for
open-vocabulary language queries. In our real-world experiments, HAMMER creates
higher-fidelity maps (2x) compared to competing baselines and is useful for
downstream tasks, such as semantic goal-conditioned navigation (e.g., ""go to
the couch""). Accompanying content available at hammer-project.github.io.","Javier Yu, Timothy Chen, Mac Schwager",2025-01-24T00:21:10Z,2025-06-03T16:34:57Z,http://arxiv.org/abs/2501.14147v2,http://arxiv.org/pdf/2501.14147v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"RTAMT -- Runtime Robustness Monitors with Application to CPS and
  Robotics","In this paper, we present Real-Time Analog Monitoring Tool (RTAMT), a tool
for quantitative monitoring of Signal Temporal Logic (STL) specifications. The
library implements a flexible architecture that supports: (1) various
environments connected by an Application Programming Interface (API) in Python,
(2) various flavors of temporal logic specification and robustness notion such
as STL, including an interface-aware variant that distinguishes between input
and output variables, and (3) discrete-time and dense-time interpretation of
STL with generation of online and offline monitors. We specifically focus on
robotics and Cyber-Physical Systems (CPSs) applications, showing how to
integrate RTAMT with (1) the Robot Operating System (ROS) and (2)
MATLAB/Simulink environments. We evaluate the tool by demonstrating several use
scenarios involving service robotic and avionic applications.","Tomoya Yamaguchi, Bardh Hoxha, Dejan Nickovic",2025-01-22T10:38:36Z,2025-01-22T10:38:36Z,http://arxiv.org/abs/2501.18608v1,http://arxiv.org/pdf/2501.18608v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Multi-LiCa: A Motion and Targetless Multi LiDAR-to-LiDAR Calibration
  Framework","Today's autonomous vehicles rely on a multitude of sensors to perceive their
environment. To improve the perception or create redundancy, the sensor's
alignment relative to each other must be known. With Multi-LiCa, we present a
novel approach for the alignment, e.g. calibration. We present an automatic
motion- and targetless approach for the extrinsic multi LiDAR-to-LiDAR
calibration without the need for additional sensor modalities or an initial
transformation input. We propose a two-step process with feature-based matching
for the coarse alignment and a GICP-based fine registration in combination with
a cost-based matching strategy. Our approach can be applied to any number of
sensors and positions if there is a partial overlap between the field of view
of single sensors. We show that our pipeline is better generalized to different
sensor setups and scenarios and is on par or better in calibration accuracy
than existing approaches. The presented framework is integrated in ROS 2 but
can also be used as a standalone application. To build upon our work, our
source code is available at: https://github.com/TUMFTM/Multi_LiCa.","Dominik Kulmer, Ilir Tahiraj, Andrii Chumak, Markus Lienkamp",2025-01-19T15:56:04Z,2025-01-19T15:56:04Z,http://arxiv.org/abs/2501.11088v1,http://arxiv.org/pdf/2501.11088v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"ROSAnnotator: A Web Application for ROSBag Data Analysis in Human-Robot
  Interaction","Human-robot interaction (HRI) is an interdisciplinary field that utilises
both quantitative and qualitative methods. While ROSBags, a file format within
the Robot Operating System (ROS), offer an efficient means of collecting
temporally synched multimodal data in empirical studies with real robots, there
is a lack of tools specifically designed to integrate qualitative coding and
analysis functions with ROSBags. To address this gap, we developed
ROSAnnotator, a web-based application that incorporates a multimodal Large
Language Model (LLM) to support both manual and automated annotation of ROSBag
data. ROSAnnotator currently facilitates video, audio, and transcription
annotations and provides an open interface for custom ROS messages and tools.
By using ROSAnnotator, researchers can streamline the qualitative analysis
process, create a more cohesive analysis pipeline, and quickly access
statistical summaries of annotations, thereby enhancing the overall efficiency
of HRI data analysis. https://github.com/CHRI-Lab/ROSAnnotator","Yan Zhang, Haoqi Li, Ramtin Tabatabaei, Wafa Johal",2025-01-13T04:18:52Z,2025-01-13T04:18:52Z,http://arxiv.org/abs/2501.07051v1,http://arxiv.org/pdf/2501.07051v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Robot localization in a mapped environment using Adaptive Monte Carlo
  algorithm","Localization is the challenge of determining the robot's pose in a mapped
environment. This is done by implementing a probabilistic algorithm to filter
noisy sensor measurements and track the robot's position and orientation. This
paper focuses on localizing a robot in a known mapped environment using
Adaptive Monte Carlo Localization or Particle Filters method and send it to a
goal state. ROS, Gazebo and RViz were used as the tools of the trade to
simulate the environment and programming two robots for performing
localization.",Sagarnil Das,2025-01-02T09:12:36Z,2025-01-02T09:12:36Z,http://arxiv.org/abs/2501.01153v1,http://arxiv.org/pdf/2501.01153v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
EVOLVE: Emotion and Visual Output Learning via LLM Evaluation,"Human acceptance of social robots is greatly effected by empathy and
perceived understanding. This necessitates accurate and flexible responses to
various input data from the user. While systems such as this can become
increasingly complex as more states or response types are included, new
research in the application of large language models towards human-robot
interaction has allowed for more streamlined perception and reaction pipelines.
LLM-selected actions and emotional expressions can help reinforce the realism
of displayed empathy and allow for improved communication between the robot and
user. Beyond portraying empathy in spoken or written responses, this shows the
possibilities of using LLMs in actuated, real world scenarios. In this work we
extend research in LLM-driven nonverbal behavior for social robots by
considering more open-ended emotional response selection leveraging new
advances in vision-language models, along with emotionally aligned motion and
color pattern selections that strengthen conveyance of meaning and empathy.","Jordan Sinclair, Christopher Reardon",2024-12-30T00:43:31Z,2024-12-30T00:43:31Z,http://arxiv.org/abs/2412.20632v1,http://arxiv.org/pdf/2412.20632v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
Implementing a Robot Intrusion Prevention System (RIPS) for ROS 2,"It is imperative to develop an intrusion prevention system (IPS),
specifically designed for autonomous robotic systems. This is due to the unique
nature of these cyber-physical systems (CPS), which are not merely typical
distributed systems. These systems employ their own systems software (i.e.
robotic middleware and frameworks) and execute distinct components to
facilitate interaction with various sensors and actuators, and other robotic
components (e.g. cognitive subsystems). Furthermore, as cyber-physical systems,
they engage in interactions with humans and their physical environment, as
exemplified by social robots. These interactions can potentially lead to
serious consequences, including physical damage. In response to this need, we
have designed and implemented RIPS, an intrusion prevention system tailored for
robotic applications based on ROS 2, the framework that has established itself
as the de facto standard for developing robotic applications. This manuscript
provides a comprehensive exposition of the issue, the security aspects of ROS 2
applications, and the key points of the threat model we created for our robotic
environment. It also describes the architecture and the implementation of our
initial research prototype and a language specifically designed for defining
detection and prevention rules for diverse, real-world robotic scenarios.
Moreover, the manuscript provides a comprehensive evaluation of the approach,
that includes a set of experiments with a real social robot executing a well
known testbed used in international robotic competitions.","Enrique Soriano-Salvador, Francisco Martín-Rico, Gorka Guardiola Múzquiz",2024-12-26T16:25:34Z,2024-12-26T16:25:34Z,http://arxiv.org/abs/2412.19272v1,http://arxiv.org/pdf/2412.19272v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Aerial Assistive Payload Transportation Using Quadrotor UAVs with
  Nonsingular Fast Terminal SMC for Human Physical Interaction","This paper presents a novel approach to utilizing underactuated quadrotor
Unmanned Aerial Vehicles (UAVs) as assistive devices in cooperative payload
transportation task through human guidance and physical interaction. The
proposed system consists of two underactuated UAVs rigidly connected to the
transported payload. This task involves the collaboration between human and
UAVs to transport and manipulate a payload. The goal is to reduce the workload
of the human and enable seamless interaction between the human operator and the
aerial vehicle. An Admittance-Nonsingular Fast Terminal Sliding Mode Control
(NFTSMC) is employed to control and asymptotically stabilize the system while
performing the task, where forces are applied to the payload by the human
operator dictate the aerial vehicle's motion. The stability of the proposed
controller is confirmed using Lyapunov analysis. Extensive simulation studies
were conducted using MATLAB, Robot Operating System (ROS), and Gazebo to
validate robustness and effectiveness of the proposed controller in assisting
with payload transportation tasks. Results demonstrates feasibility and
potential benefits utilizing quadrotor UAVs as assistive devices for payload
transportation through intuitive human-guided control. Keywords Cooperative
payload transportation, Admittance control, Sliding mode control, Quadrotor
control","Hussein Naser, Hashim A. Hashim, Mojtaba Ahmadi",2024-12-23T18:02:48Z,2024-12-23T18:02:48Z,http://arxiv.org/abs/2412.17748v1,http://arxiv.org/pdf/2412.17748v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"UA-MPC: Uncertainty-Aware Model Predictive Control for Motorized LiDAR
  Odometry","Accurate and comprehensive 3D sensing using LiDAR systems is crucial for
various applications in photogrammetry and robotics, including facility
inspection, Building Information Modeling (BIM), and robot navigation.
Motorized LiDAR systems can expand the Field of View (FoV) without adding
multiple scanners, but existing motorized LiDAR systems often rely on
constant-speed motor control, leading to suboptimal performance in complex
environments. To address this, we propose UA-MPC, an uncertainty-aware motor
control strategy that balances scanning accuracy and efficiency. By predicting
discrete observabilities of LiDAR Odometry (LO) through ray tracing and
modeling their distribution with a surrogate function, UA-MPC efficiently
optimizes motor speed control according to different scenes. Additionally, we
develop a ROS-based realistic simulation environment for motorized LiDAR
systems, enabling the evaluation of control strategies across diverse
scenarios. Extensive experiments, conducted on both simulated and real-world
scenarios, demonstrate that our method significantly improves odometry accuracy
while preserving the scanning efficiency of motorized LiDAR systems.
Specifically, it achieves over a 60\% reduction in positioning error with less
than a 2\% decrease in efficiency compared to constant-speed control, offering
a smarter and more effective solution for active 3D sensing tasks. The
simulation environment for control motorized LiDAR is open-sourced at:
\url{https://github.com/kafeiyin00/UA-MPC.git}.","Jianping Li, Xinhang Xu, Jinxin Liu, Kun Cao, Shenghai Yuan, Lihua Xie",2024-12-18T14:12:11Z,2024-12-18T14:12:11Z,http://arxiv.org/abs/2412.13873v1,http://arxiv.org/pdf/2412.13873v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Unified Understanding of Environment, Task, and Human for Human-Robot
  Interaction in Real-World Environments","To facilitate human--robot interaction (HRI) tasks in real-world scenarios,
service robots must adapt to dynamic environments and understand the required
tasks while effectively communicating with humans. To accomplish HRI in
practice, we propose a novel indoor dynamic map, task understanding system, and
response generation system. The indoor dynamic map optimizes robot behavior by
managing an occupancy grid map and dynamic information, such as furniture and
humans, in separate layers. The task understanding system targets tasks that
require multiple actions, such as serving ordered items. Task representations
that predefine the flow of necessary actions are applied to achieve highly
accurate understanding. The response generation system is executed in parallel
with task understanding to facilitate smooth HRI by informing humans of the
subsequent actions of the robot. In this study, we focused on waiter duties in
a restaurant setting as a representative application of HRI in a dynamic
environment. We developed an HRI system that could perform tasks such as
serving food and cleaning up while communicating with customers. In experiments
conducted in a simulated restaurant environment, the proposed HRI system
successfully communicated with customers and served ordered food with 90\%
accuracy. In a questionnaire administered after the experiment, the HRI system
of the robot received 4.2 points out of 5. These outcomes indicated the
effectiveness of the proposed method and HRI system in executing waiter tasks
in real-world environments.","Yuga Yano, Akinobu Mizutani, Yukiya Fukuda, Daiju Kanaoka, Tomohiro Ono, Hakaru Tamukoh",2024-12-18T11:05:56Z,2024-12-18T11:05:56Z,http://arxiv.org/abs/2412.13726v1,http://arxiv.org/pdf/2412.13726v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Formal Modeling and Verification of Publisher-Subscriber Paradigm in ROS
  2","The Robot Operating System (ROS) is one of the most popular middleware for
developing robot applications, but it is subject to major shortcomings when
applied to real-time robotic systems in safety-critical environments. For this
reason, ROS 2 was released in 2017 for implementing real-time capabilities in
distributed robotic systems while supporting the most prominent aspects of the
original ROS. There is still not much work done to provide formal guarantees
and correctness of a ROS program. In this paper, we propose a framework to
address this challenging problem of guaranteeing the correct behaviour of
robotic systems. We propose a formal modelling of a ROS 2 program, and also
describe the program using a network of timed automata. We then prove that the
sets of executions of a ROS program in the model and in the network of timed
automata are the same. Thus to analyze a publisher-subscriber scenario of ROS 2
program, our algorithm first converts the program into the model, and then into
the network of timed automata. The applicability and validity of our approach
are verified by conducting several experiments on a simplified system and an
actual robotic system, and the results and limitations are discussed.","Jahid Chowdhury Choton, Lipsy Gupta, Pavithra Prabhakar",2024-12-12T16:57:49Z,2025-01-08T19:49:53Z,http://arxiv.org/abs/2412.16186v2,http://arxiv.org/pdf/2412.16186v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"SRFS: Parallel Processing Fault-tolerant ROS2-based Flight Software for
  the Space Ranger Cubesat","Traditional real-time operating systems (RTOS) often exhibit poor parallel
performance, while thread monitoring in Linux-based systems presents
significant challenges. To address these issues, this paper proposes a
satellite flight software system design based on the Robot Operating System
(ROS), leveraging ROS's built-in reliable publish-subscribe messaging mechanism
for inter-application communication. Considering the complex functional
requirements of modern small satellites, the design incorporates both hardware
and software architecture, alongside system scheduling and error-correction
mechanisms. This approach ensures efficient parallel data processing and system
reliability, while also reducing the development cycle through code reuse.
Comprehensive testing, including system time delay, system management, fault
tolerance, and system maintenance, was conducted to validate the system's
capabilities in telemetry, remote control, new feature integration, and
autonomous error correction. The results demonstrate the high reliability and
ease of maintenance of the satellite flight software offering a reference
framework for the rapid development of high-performance small satellite
operations systems.","Zebei Zhao, Yinghao Xiang, Ziyu Zhou, Kehan Chong, Haoran Ma, Pei Chen",2024-12-11T07:37:50Z,2024-12-11T07:37:50Z,http://arxiv.org/abs/2412.08164v1,http://arxiv.org/pdf/2412.08164v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Self-Supervised Learning-Based Path Planning and Obstacle Avoidance
  Using PPO and B-Splines in Unknown Environments","This paper introduces SmartBSP, an advanced self-supervised learning
framework for real-time path planning and obstacle avoidance in autonomous
robotics navigating through complex environments. The proposed system
integrates Proximal Policy Optimization (PPO) with Convolutional Neural
Networks (CNN) and Actor-Critic architecture to process limited LIDAR inputs
and compute spatial decision-making probabilities. The robot's perceptual field
is discretized into a grid format, which the CNN analyzes to produce a spatial
probability distribution. During the training process a nuanced cost function
is minimized that accounts for path curvature, endpoint proximity, and obstacle
avoidance. Simulations results in different scenarios validate the algorithm's
resilience and adaptability across diverse operational scenarios. Subsequently,
Real-time experiments, employing the Robot Operating System (ROS), were carried
out to assess the efficacy of the proposed algorithm.","Shahab Shokouhi, Oguzhan Oruc, May-Win Thein",2024-12-03T05:20:29Z,2025-09-01T20:13:35Z,http://arxiv.org/abs/2412.02176v2,http://arxiv.org/pdf/2412.02176v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"HPRM: High-Performance Robotic Middleware for Intelligent Autonomous
  Systems","The rise of intelligent autonomous systems, especially in robotics and
autonomous agents, has created a critical need for robust communication
middleware that can ensure real-time processing of extensive sensor data.
Current robotics middleware like Robot Operating System (ROS) 2 faces
challenges with nondeterminism and high communication latency when dealing with
large data across multiple subscribers on a multi-core compute platform. To
address these issues, we present High-Performance Robotic Middleware (HPRM),
built on top of the deterministic coordination language Lingua Franca (LF).
HPRM employs optimizations including an in-memory object store for efficient
zero-copy transfer of large payloads, adaptive serialization to minimize
serialization overhead, and an eager protocol with real-time sockets to reduce
handshake latency. Benchmarks show HPRM achieves up to 173x lower latency than
ROS2 when broadcasting large messages to multiple nodes. We then demonstrate
the benefits of HPRM by integrating it with the CARLA simulator and running
reinforcement learning agents along with object detection workloads. In the
CARLA autonomous driving application, HPRM attains 91.1% lower latency than
ROS2. The deterministic coordination semantics of HPRM, combined with its
optimized IPC mechanisms, enable efficient and predictable real-time
communication for intelligent autonomous systems.","Jacky Kwok, Shulu Li, Marten Lohstroh, Edward A. Lee",2024-12-02T18:46:29Z,2024-12-02T18:46:29Z,http://arxiv.org/abs/2412.01799v1,http://arxiv.org/pdf/2412.01799v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"SCoTT: Strategic Chain-of-Thought Tasking for Wireless-Aware Robot
  Navigation in Digital Twins","Path planning under wireless performance constraints is a complex challenge
in robot navigation. However, naively incorporating such constraints into
classical planning algorithms often incurs prohibitive search costs. In this
paper, we propose SCoTT, a wireless-aware path planning framework that
leverages vision-language models (VLMs) to co-optimize average path gains and
trajectory length using wireless heatmap images and ray-tracing data from a
digital twin (DT). At the core of our framework is Strategic Chain-of-Thought
Tasking (SCoTT), a novel prompting paradigm that decomposes the exhaustive
search problem into structured subtasks, each solved via chain-of-thought
prompting. To establish strong baselines, we compare classical A* and
wireless-aware extensions of it, and derive DP-WA*, an optimal, iterative
dynamic programming algorithm that incorporates all path gains and distance
metrics from the DT, but at significant computational cost. In extensive
experiments, we show that SCoTT achieves path gains within 2% of DP-WA* while
consistently generating shorter trajectories. Moreover, SCoTT's intermediate
outputs can be used to accelerate DP-WA* by reducing its search space, saving
up to 62% in execution time. We validate our framework using four VLMs,
demonstrating effectiveness across both large and small models, thus making it
applicable to a wide range of compact models at low inference cost. We also
show the practical viability of our approach by deploying SCoTT as a ROS node
within Gazebo simulations. Finally, we discuss data acquisition pipelines,
compute requirements, and deployment considerations for VLMs in 6G-enabled DTs,
underscoring the potential of natural language interfaces for wireless-aware
navigation in real-world applications.","Aladin Djuhera, Amin Seffo, Vlad C. Andrei, Holger Boche, Walid Saad",2024-11-27T10:45:49Z,2025-05-29T13:45:00Z,http://arxiv.org/abs/2411.18212v2,http://arxiv.org/pdf/2411.18212v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"ROSMonitoring 2.0: Extending ROS Runtime Verification to Services and
  Ordered Topics","Formal verification of robotic applications presents challenges due to their
hybrid nature and distributed architecture. This paper introduces ROSMonitoring
2.0, an extension of ROSMonitoring designed to facilitate the monitoring of
both topics and services while considering the order in which messages are
published and received. The framework has been enhanced to support these novel
features for ROS1 -- and partially ROS2 environments -- offering improved
real-time support, security, scalability, and interoperability. We discuss the
modifications made to accommodate these advancements and present results
obtained from a case study involving the runtime monitoring of specific
components of a fire-fighting Uncrewed Aerial Vehicle (UAV).","Maryam Ghaffari Saadat, Angelo Ferrando, Louise A. Dennis, Michael Fisher",2024-11-21T18:07:31Z,2024-11-21T18:07:31Z,http://arxiv.org/abs/2411.14367v1,http://arxiv.org/pdf/2411.14367v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
Shrinking POMCP: A Framework for Real-Time UAV Search and Rescue,"Efficient path optimization for drones in search and rescue operations faces
challenges, including limited visibility, time constraints, and complex
information gathering in urban environments. We present a comprehensive
approach to optimize UAV-based search and rescue operations in neighborhood
areas, utilizing both a 3D AirSim-ROS2 simulator and a 2D simulator. The path
planning problem is formulated as a partially observable Markov decision
process (POMDP), and we propose a novel ``Shrinking POMCP'' approach to address
time constraints. In the AirSim environment, we integrate our approach with a
probabilistic world model for belief maintenance and a neurosymbolic navigator
for obstacle avoidance. The 2D simulator employs surrogate ROS2 nodes with
equivalent functionality. We compare trajectories generated by different
approaches in the 2D simulator and evaluate performance across various belief
types in the 3D AirSim-ROS simulator. Experimental results from both simulators
demonstrate that our proposed shrinking POMCP solution achieves significant
improvements in search times compared to alternative methods, showcasing its
potential for enhancing the efficiency of UAV-assisted search and rescue
operations.","Yunuo Zhang, Baiting Luo, Ayan Mukhopadhyay, Daniel Stojcsics, Daniel Elenius, Anirban Roy, Susmit Jha, Miklos Maroti, Xenofon Koutsoukos, Gabor Karsai, Abhishek Dubey",2024-11-20T01:41:29Z,2024-11-20T01:41:29Z,http://arxiv.org/abs/2411.12967v1,http://arxiv.org/pdf/2411.12967v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Analysing Explanation-Related Interactions in Collaborative
  Perception-Cognition-Communication-Action","Effective communication is essential in collaborative tasks, so AI-equipped
robots working alongside humans need to be able to explain their behaviour in
order to cooperate effectively and earn trust. We analyse and classify
communications among human participants collaborating to complete a simulated
emergency response task. The analysis identifies messages that relate to
various kinds of interactive explanations identified in the explainable AI
literature. This allows us to understand what type of explanations humans
expect from their teammates in such settings, and thus where AI-equipped robots
most need explanation capabilities. We find that most explanation-related
messages seek clarification in the decisions or actions taken. We also confirm
that messages have an impact on the performance of our simulated task.","Marc Roig Vilamala, Jack Furby, Julian de Gortari Briseno, Mani Srivastava, Alun Preece, Carolina Fuentes Toro",2024-11-19T13:07:04Z,2024-11-19T13:07:04Z,http://arxiv.org/abs/2411.12483v1,http://arxiv.org/pdf/2411.12483v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"cHyRRT and cHySST: Two Motion Planning Tools for Hybrid Dynamical
  Systems","This paper presents two implementations of the recently developed motion
planning algorithms HyRRT arXiv:2210.1508(2) and HySST arXiv:2305.1864(9).
Specifically, cHyRRT, an implementation of the HyRRT algorithm, generates
solutions to motion planning problems for hybrid systems with a probabilistic
completeness guarantee, while cHySST, an implementation of the asymptotically
near-optimal HySST algorithm, finds near-optimal trajectories based on a
user-defined cost function. The implementations align with the theoretical
foundations of hybrid system theory and are designed based on OMPL, ensuring
compatibility with ROS while prioritizing computational efficiency. The
structure, components, and usage of both tools are detailed. A modified pinball
game and collision-resilient tensegrity multicopter example are provided to
illustrate the tools' key capabilities.","Beverly Xu, Nan Wang, Ricardo Sanfelice",2024-11-18T18:27:37Z,2025-07-06T03:41:28Z,http://arxiv.org/abs/2411.11812v3,http://arxiv.org/pdf/2411.11812v3.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
Querying Perception Streams with Spatial Regular Expressions,"Perception in fields like robotics, manufacturing, and data analysis
generates large volumes of temporal and spatial data to effectively capture
their environments. However, sorting through this data for specific scenarios
is a meticulous and error-prone process, often dependent on the application,
and lacks generality and reproducibility. In this work, we introduce SpREs as a
novel querying language for pattern matching over perception streams containing
spatial and temporal data derived from multi-modal dynamic environments. To
highlight the capabilities of SpREs, we developed the STREM tool as both an
offline and online pattern matching framework for perception data. We
demonstrate the offline capabilities of STREM through a case study on a
publicly available AV dataset (Woven Planet Perception) and its online
capabilities through a case study integrating STREM in ROS with the CARLA
simulator. We also conduct performance benchmark experiments on various SpRE
queries. Using our matching framework, we are able to find over 20,000 matches
within 296 ms making STREM applicable in runtime monitoring applications.","Jacob Anderson, Georgios Fainekos, Bardh Hoxha, Hideki Okamoto, Danil Prokhorov",2024-11-08T20:15:27Z,2024-11-08T20:15:27Z,http://arxiv.org/abs/2411.05946v1,http://arxiv.org/pdf/2411.05946v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Development of a Human-Robot Interaction Platform for Dual-Arm Robots
  Based on ROS and Multimodal Artificial Intelligence","In this paper, we propose the development of an interactive platform between
humans and a dual-arm robotic system based on the Robot Operating System (ROS)
and a multimodal artificial intelligence model. Our proposed platform consists
of two main components: a dual-arm robotic hardware system and software that
includes image processing tasks and natural language processing using a 3D
camera and embedded computing. First, we designed and developed a dual-arm
robotic system with a positional accuracy of less than 2 cm, capable of
operating independently, performing industrial and service tasks while
simultaneously simulating and modeling the robot in the ROS environment.
Second, artificial intelligence models for image processing are integrated to
execute object picking and classification tasks with an accuracy of over 90%.
Finally, we developed remote control software using voice commands through a
natural language processing model. Experimental results demonstrate the
accuracy of the multimodal artificial intelligence model and the flexibility of
the dual-arm robotic system in interactive human environments.","Thanh Nguyen Canh, Ba Phuong Nguyen, Hong Quan Tran, Xiem HoangVan",2024-11-08T05:48:41Z,2024-11-08T05:48:41Z,http://arxiv.org/abs/2411.05342v1,http://arxiv.org/pdf/2411.05342v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Development of a Service Robot for Hospital Environments in
  Rehabilitation Medicine with LiDAR Based Simultaneous Localization and
  Mapping","This paper presents the development and evaluation of a medical service robot
equipped with 3D LiDAR and advanced localization capabilities for use in
hospital environments. The robot employs LiDAR-based Simultaneous Localization
and Mapping SLAM to navigate autonomously and interact effectively within
complex and dynamic healthcare settings. A comparative analysis with
established 3D SLAM technology in Autoware version 1.14.0, under a Linux ROS
framework, provided a benchmark for evaluating our system performance. The
adaptation of Normal Distribution Transform NDT Matching to indoor navigation
allowed for precise real-time mapping and enhanced obstacle avoidance
capabilities. Empirical validation was conducted through manual maneuvers in
various environments, supplemented by ROS simulations to test the system
response to simulated challenges. The findings demonstrate that the robot
integration of 3D LiDAR and NDT Matching significantly improves navigation
accuracy and operational reliability in a healthcare context. This study
highlights the robot ability to perform essential tasks with high efficiency
and identifies potential areas for further improvement, particularly in sensor
performance under diverse environmental conditions. The successful deployment
of this technology in a hospital setting illustrates its potential to support
medical staff and contribute to patient care, suggesting a promising direction
for future research and development in healthcare robotics.","Sayat Ibrayev, Arman Ibrayeva, Bekzat Amanov, Serik Tolenov",2024-11-07T15:37:08Z,2024-11-07T15:37:08Z,http://arxiv.org/abs/2411.04797v1,http://arxiv.org/pdf/2411.04797v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
Energy Consumption in Robotics: A Simplified Modeling Approach,"The energy use of a robot is trajectory-dependent, and thus can be reduced by
optimization of the trajectory. Current methods for robot trajectory
optimization can reduce energy up to 15\% for fixed start and end points,
however their use in industrial robot planning is still restricted due to model
complexity and lack of integration with planning tools which address other
concerns (e.g. collision avoidance). We propose an approach that uses
differentiable inertial and kinematic models from standard open-source tools,
integrating with standard ROS planning methods. An inverse dynamics-based
energy model is optionally extended with a single-parameter electrical model,
simplifying the model identification process. We compare the inertial and
electrical models on a collaborative robot, showing that simplified models
provide competitive accuracy and are easier to deploy in practice.","Valentyn Petrichenko, Lisa Lokstein, Gregor Thiele, Kevin Haninger",2024-11-05T15:39:36Z,2024-11-05T15:39:36Z,http://arxiv.org/abs/2411.03194v1,http://arxiv.org/pdf/2411.03194v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"SPACE: 3D Spatial Co-operation and Exploration Framework for Robust
  Mapping and Coverage with Multi-Robot Systems","In indoor environments, multi-robot visual (RGB-D) mapping and exploration
hold immense potential for application in domains such as domestic service and
logistics, where deploying multiple robots in the same environment can
significantly enhance efficiency. However, there are two primary challenges:
(1) the ""ghosting trail"" effect, which occurs due to overlapping views of
robots impacting the accuracy and quality of point cloud reconstruction, and
(2) the oversight of visual reconstructions in selecting the most effective
frontiers for exploration. Given these challenges are interrelated, we address
them together by proposing a new semi-distributed framework (SPACE) for spatial
cooperation in indoor environments that enables enhanced coverage and 3D
mapping. SPACE leverages geometric techniques, including ""mutual awareness"" and
a ""dynamic robot filter,"" to overcome spatial mapping constraints.
Additionally, we introduce a novel spatial frontier detection system and map
merger, integrated with an adaptive frontier assigner for optimal coverage
balancing the exploration and reconstruction objectives. In extensive
ROS-Gazebo simulations, SPACE demonstrated superior performance over
state-of-the-art approaches in both exploration and mapping metrics.","Sai Krishna Ghanta, Ramviyas Parasuraman",2024-11-04T19:04:09Z,2024-11-04T19:04:09Z,http://arxiv.org/abs/2411.02524v1,http://arxiv.org/pdf/2411.02524v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Efficient Collaborative Navigation through Perception Fusion for
  Multi-Robots in Unknown Environments","For tasks conducted in unknown environments with efficiency requirements,
real-time navigation of multi-robot systems remains challenging due to
unfamiliarity with surroundings.In this paper, we propose a novel multi-robot
collaborative planning method that leverages the perception of different robots
to intelligently select search directions and improve planning efficiency.
Specifically, a foundational planner is employed to ensure reliable exploration
towards targets in unknown environments and we introduce Graph Attention
Architecture with Information Gain Weight(GIWT) to synthesizes the information
from the target robot and its teammates to facilitate effective navigation
around obstacles.In GIWT, after regionally encoding the relative positions of
the robots along with their perceptual features, we compute the shared
attention scores and incorporate the information gain obtained from neighboring
robots as a supplementary weight. We design a corresponding expert data
generation scheme to simulate real-world decision-making conditions for network
training. Simulation experiments and real robot tests demonstrates that the
proposed method significantly improves efficiency and enables collaborative
planning for multiple robots. Our method achieves approximately 82% accuracy on
the expert dataset and reduces the average path length by about 8% and 6%
across two types of tasks compared to the fundamental planner in ROS tests, and
a path length reduction of over 6% in real-world experiments.","Qingquan Lin, Weining Lu, Litong Meng, Chenxi Li, Bin Liang",2024-11-02T14:53:26Z,2024-11-02T14:53:26Z,http://arxiv.org/abs/2411.01274v1,http://arxiv.org/pdf/2411.01274v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
Sensor Fusion for Autonomous Indoor UAV Navigation in Confined Spaces,"In this paper, we address the challenge of navigating through unknown indoor
environments using autonomous aerial robots within confined spaces. The core of
our system involves the integration of key sensor technologies, including depth
sensing from the ZED 2i camera, IMU data, and LiDAR measurements, facilitated
by the Robot Operating System (ROS) and RTAB-Map. Through custom designed
experiments, we demonstrate the robustness and effectiveness of this approach.
Our results showcase a promising navigation accuracy, with errors as low as 0.4
meters, and mapping quality characterized by a Root Mean Square Error (RMSE) of
just 0.13 m. Notably, this performance is achieved while maintaining energy
efficiency and balanced resource allocation, addressing a crucial concern in
UAV applications. Flight tests further underscore the precision of our system
in maintaining desired flight orientations, with a remarkable error rate of
only 0.1%. This work represents a significant stride in the development of
autonomous indoor UAV navigation systems, with potential applications in search
and rescue, facility inspection, and environmental monitoring within GPS-denied
indoor environments.","Alice James, Avishkar Seth, Endrowednes Kuantama, Subhas Mukhopadhyay, Richard Han",2024-10-27T21:16:39Z,2024-10-27T21:16:39Z,http://arxiv.org/abs/2410.20599v1,http://arxiv.org/pdf/2410.20599v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Implementación de Navegación en Plataforma Robótica Móvil Basada
  en ROS y Gazebo","This research focused on utilizing ROS2 and Gazebo for simulating the
TurtleBot3 robot, with the aim of exploring autonomous navigation capabilities.
While the study did not achieve full autonomous navigation, it successfully
established the connection between ROS2 and Gazebo and enabled manual
simulation of the robot's movements. The primary objective was to understand
how these tools can be integrated to support autonomous functions, providing
valuable insights into the development process. The results of this work lay
the groundwork for future research into autonomous robotics. The topic is
particularly engaging for both teenagers and adults interested in discovering
how robots function independently and the underlying technology involved. This
research highlights the potential for further advancements in autonomous
systems and serves as a stepping stone for more in-depth studies in the field.","Angel Da Silva, Santiago Fernández, Braian Vidal, Hiago Sodre, Pablo Moraes, Christopher Peters, Sebastian Barcelona, Vincent Sandin, William Moraes, Ahilen Mazondo, Brandon Macedo, Nathalie Assunção, Bruna de Vargas, André Kelbouscas, Ricardo Grando",2024-10-25T21:14:34Z,2024-10-25T21:14:34Z,http://arxiv.org/abs/2410.19972v1,http://arxiv.org/pdf/2410.19972v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"VECTOR: Velocity-Enhanced GRU Neural Network for Real-Time 3D UAV
  Trajectory Prediction","This paper tackles the challenge of real-time 3D trajectory prediction for
UAVs, which is critical for applications such as aerial surveillance and
defense. Existing prediction models that rely primarily on position data
struggle with accuracy, especially when UAV movements fall outside the position
domain used in training. Our research identifies a gap in utilizing velocity
estimates, first-order dynamics, to better capture the dynamics and enhance
prediction accuracy and generalizability in any position domain. To bridge this
gap, we propose a new trajectory prediction method using Gated Recurrent Units
(GRUs) within sequence-based neural networks. Unlike traditional methods that
rely on RNNs or transformers, this approach forecasts future velocities and
positions based on historical velocity data instead of positions. This is
designed to enhance prediction accuracy and scalability, overcoming challenges
faced by conventional models in handling complex UAV dynamics. The methodology
employs both synthetic and real-world 3D UAV trajectory data, capturing a wide
range of flight patterns, speeds, and agility. Synthetic data is generated
using the Gazebo simulator and PX4 Autopilot, while real-world data comes from
the UZH-FPV and Mid-Air drone racing datasets. The GRU-based models
significantly outperform state-of-the-art RNN approaches, with a mean square
error (MSE) as low as 2 x 10^-8. Overall, our findings confirm the
effectiveness of incorporating velocity data in improving the accuracy of UAV
trajectory predictions across both synthetic and real-world scenarios, in and
out of position data distributions. Finally, we open-source our 5000
trajectories dataset and a ROS 2 package to facilitate the integration with
existing ROS-based UAV systems.","Omer Nacar, Mohamed Abdelkader, Lahouari Ghouti, Kahled Gabr, Abdulrahman S. Al-Batati, Anis Koubaa",2024-10-24T07:16:42Z,2024-10-24T07:16:42Z,http://arxiv.org/abs/2410.23305v1,http://arxiv.org/pdf/2410.23305v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"SERN: Simulation-Enhanced Realistic Navigation for Multi-Agent Robotic
  Systems in Contested Environments","The increasing deployment of autonomous systems in complex environments
necessitates efficient communication and task completion among multiple agents.
This paper presents SERN (Simulation-Enhanced Realistic Navigation), a novel
framework integrating virtual and physical environments for real-time
collaborative decision-making in multi-robot systems. SERN addresses key
challenges in asset deployment and coordination through our bi-directional SERN
ROS Bridge communication framework. Our approach advances the SOTA through:
accurate real-world representation in virtual environments using Unity
high-fidelity simulator; synchronization of physical and virtual robot
movements; efficient ROS data distribution between remote locations; and
integration of SOTA semantic segmentation for enhanced environmental
perception. Additionally, we introduce a Multi-Metric Cost Function (MMCF) that
dynamically balances latency, reliability, computational overhead, and
bandwidth consumption to optimize system performance in contested environments.
We further provide theoretical justification for synchronization accuracy by
proving that the positional error between physical and virtual robots remains
bounded under varying network conditions. Our evaluations show a 15% to 24%
improvement in latency and up to a 15% increase in processing efficiency
compared to traditional ROS setups. Real-world and virtual simulation
experiments with multiple robots (Clearpath Jackal and Husky) demonstrate
synchronization accuracy, achieving less than $5\text{ cm}$ positional error
and under $2^\circ$ rotational error. These results highlight SERN's potential
to enhance situational awareness and multi-agent coordination in diverse,
contested environments.","Jumman Hossain, Emon Dey, Snehalraj Chugh, Masud Ahmed, MS Anwar, Abu-Zaher Faridee, Jason Hoppes, Theron Trout, Anjon Basak, Rafidh Chowdhury, Rishabh Mistry, Hyun Kim, Jade Freeman, Niranjan Suri, Adrienne Raglin, Carl Busart, Timothy Gregory, Anuradha Ravi, Nirmalya Roy",2024-10-22T04:35:57Z,2025-03-14T01:40:51Z,http://arxiv.org/abs/2410.16686v2,http://arxiv.org/pdf/2410.16686v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Leveraging Augmented Reality for Improved Situational Awareness During
  UAV-Driven Search and Rescue Missions","In the high-stakes domain of search-and-rescue missions, the deployment of
Unmanned Aerial Vehicles (UAVs) has become increasingly pivotal. These missions
require seamless, real-time communication among diverse roles within response
teams, particularly between Remote Operators (ROs) and On-Site Operators
(OSOs). Traditionally, ROs and OSOs have relied on radio communication to
exchange critical information, such as the geolocation of victims, hazardous
areas, and points of interest. However, radio communication lacks information
visualization, suffers from noise, and requires mental effort to interpret
information, leading to miscommunications and misunderstandings. To address
these challenges, this paper presents VizCom-AR, an Augmented Reality system
designed to facilitate visual communication between ROs and OSOs and their
situational awareness during UAV-driven search-and-rescue missions. Our
experiments, focus group sessions with police officers, and field study showed
that VizCom-AR enhances spatial awareness of both ROs and OSOs, facilitate
geolocation information exchange, and effectively complement existing
communication tools in UAV-driven emergency response missions. Overall,
VizCom-AR offers a fundamental framework for designing Augmented Reality
systems for large scale UAV-driven rescue missions.","Rushikesh Nalamothu, Puneet Sontha, Janardhan Karravula, Ankit Agrawal",2024-10-16T13:32:37Z,2024-10-16T13:32:37Z,http://arxiv.org/abs/2410.12556v1,http://arxiv.org/pdf/2410.12556v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
A Framework for Adapting Human-Robot Interaction to Diverse User Groups,"To facilitate natural and intuitive interactions with diverse user groups in
real-world settings, social robots must be capable of addressing the varying
requirements and expectations of these groups while adapting their behavior
based on user feedback. While previous research often focuses on specific
demographics, we present a novel framework for adaptive Human-Robot Interaction
(HRI) that tailors interactions to different user groups and enables individual
users to modulate interactions through both minor and major interruptions. Our
primary contributions include the development of an adaptive, ROS-based HRI
framework with an open-source code base. This framework supports natural
interactions through advanced speech recognition and voice activity detection,
and leverages a large language model (LLM) as a dialogue bridge. We validate
the efficiency of our framework through module tests and system trials,
demonstrating its high accuracy in age recognition and its robustness to
repeated user inputs and plan changes.","Theresa Pekarek Rosin, Vanessa Hassouna, Xiaowen Sun, Luca Krohm, Henri-Leon Kordt, Michael Beetz, Stefan Wermter",2024-10-15T08:16:43Z,2025-04-03T08:22:27Z,http://arxiv.org/abs/2410.11377v2,http://arxiv.org/pdf/2410.11377v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"What Am I? Evaluating the Effect of Language Fluency and Task Competency
  on the Perception of a Social Robot","Recent advancements in robot capabilities have enabled them to interact with
people in various human-social environments (HSEs). In many of these
environments, the perception of the robot often depends on its capabilities,
e.g., task competency, language fluency, etc. To enable fluent human-robot
interaction (HRI) in HSEs, it is crucial to understand the impact of these
capabilities on the perception of the robot. Although many works have
investigated the effects of various robot capabilities on the robot's
perception separately, in this paper, we present a large-scale HRI study (n =
60) to investigate the combined impact of both language fluency and task
competency on the perception of a robot. The results suggest that while
language fluency may play a more significant role than task competency in the
perception of the verbal competency of a robot, both language fluency and task
competency contribute to the perception of the intelligence and reliability of
the robot. The results also indicate that task competency may play a more
significant role than language fluency in the perception of meeting
expectations and being a good teammate. The findings of this study highlight
the relationship between language fluency and task competency in the context of
social HRI and will enable the development of more intelligent robots in the
future.","Shahira Ali, Haley N. Green, Tariq Iqbal",2024-10-14T20:51:16Z,2024-10-14T20:51:16Z,http://arxiv.org/abs/2410.11085v1,http://arxiv.org/pdf/2410.11085v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Towards Design and Development of a Low-Cost Unmanned Surface Vehicle
  for Aquaculture Water Quality Monitoring in Shallow Water Environments","Unmanned surface vessels USVs are typically autonomous or remotely operated
and are specifically designed for environmental monitoring in various aquatic
environments Aquaculture requires constant monitoring and management of water
quality for the health and productivity of aquaculture systems Poor water
quality can lead to disease outbreaks reduced growth rates and even mass
mortality of cultured species Many small aquaculture operations operate on
tight budgets and in shallow water environments such as inland ponds coastal
lagoons estuaries and shallow rivers particularly in developing regions This
leads to the foremost manoeuvrability challenge underscoring the crucial need
for agile cost effective USVs as efficient monitoring systems The paper
proposes a low cost 3D printed twin hull catamaran style platform equipped with
an Inertial Measurement Unit IMU and a Global Navigation Satellite System GNSS
with a two layered control framework and a differential drive configuration
developed using two high efficiency T200 thrusters The design utilizes the
Robot Operating System ROS to create the control framework and incorporates
Extended Kalman Filter EKF based sensor fusion techniques for localisation The
paper evaluates the USVs autonomy through open water captive model experiments
employing remote control methods to assess the vessels manoeuvrability and
overall performance in shallow water conditions","Aiyelari Temilolorun, Yogang Singh",2024-10-12T12:22:33Z,2024-10-12T12:22:33Z,http://arxiv.org/abs/2410.09513v1,http://arxiv.org/pdf/2410.09513v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Dynamic Benchmarks: Spatial and Temporal Alignment for ADS Performance
  Evaluation","Deployed SAE level 4+ Automated Driving Systems (ADS) without a human driver
are currently operational ride-hailing fleets on surface streets in the United
States. This current use case and future applications of this technology will
determine where and when the fleets operate, potentially resulting in a
divergence from the distribution of driving of some human benchmark population
within a given locality. Existing benchmarks for evaluating ADS performance
have only done county-level geographical matching of the ADS and benchmark
driving exposure in crash rates. This study presents a novel methodology for
constructing dynamic human benchmarks that adjust for spatial and temporal
variations in driving distribution between an ADS and the overall human driven
fleet. Dynamic benchmarks were generated using human police-reported crash
data, human vehicle miles traveled (VMT) data, and over 20 million miles of
Waymo's rider-only (RO) operational data accumulated across three US counties.
The spatial adjustment revealed significant differences across various severity
levels in adjusted crash rates compared to unadjusted benchmarks with these
differences ranging from 10% to 47% higher in San Francisco, 12% to 20% higher
in Maricopa, and 7% lower to 34% higher in Los Angeles counties. The
time-of-day adjustment in San Francisco, limited to this region due to data
availability, resulted in adjusted crash rates 2% lower to 16% higher than
unadjusted rates, depending on severity level. The findings underscore the
importance of adjusting for spatial and temporal confounders in benchmarking
analysis, which ultimately contributes to a more equitable benchmark for ADS
performance evaluations.","Yin-Hsiu Chen, John M. Scanlon, Kristofer D. Kusano, Timothy L. McMurry, Trent Victor",2024-10-11T15:23:58Z,2024-10-11T15:23:58Z,http://arxiv.org/abs/2410.08903v1,http://arxiv.org/pdf/2410.08903v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Soothing Sensations: Enhancing Interactions with a Socially Assistive
  Robot through Vibrotactile Heartbeats","Physical interactions with socially assistive robots (SARs) positively affect
user wellbeing. However, haptic experiences when touching a SAR are typically
limited to perceiving the robot's movements or shell texture, while other
modalities that could enhance the touch experience with the robot, such as
vibrotactile stimulation, are under-explored. In this exploratory qualitative
study, we investigate the potential of enhancing human interaction with the
PARO robot through vibrotactile heartbeats, with the goal to regulate
subjective wellbeing during stressful situations. We conducted in-depth
one-on-one interviews with 30 participants, who watched three horror movie
clips alone, with PARO, and with a PARO that displayed a vibrotactile
heartbeat. Our findings show that PARO's presence and its interactive
capabilities can help users regulate emotions through attentional redeployment
from a stressor toward the robot. The vibrotactile heartbeat further reinforced
PARO's physical and social presence, enhancing the socio-emotional support
provided by the robot and its perceived life-likeness. We discuss the impact of
individual differences in user experience and implications for the future
design of life-like vibrotactile stimulation for SARs.","Jacqueline Borgstedt, Shaun Macdonald, Karola Marky, Frank E. Pollick, Stephen A. Brewster",2024-10-10T13:15:06Z,2024-10-10T13:15:06Z,http://arxiv.org/abs/2410.07892v1,http://arxiv.org/pdf/2410.07892v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Overcoming Autoware-Ubuntu Incompatibility in Autonomous Driving
  Systems-Equipped Vehicles: Lessons Learned","Autonomous vehicles have been rapidly developed as demand that provides
safety and efficiency in transportation systems. As autonomous vehicles are
designed based on open-source operating and computing systems, there are
numerous resources aimed at building an operating platform composed of Ubuntu,
Autoware, and Robot Operating System (ROS). However, no explicit guidelines
exist to help scholars perform trouble-shooting due to incompatibility between
the Autoware platform and Ubuntu operating systems installed in autonomous
driving systems-equipped vehicles (i.e., Chrysler Pacifica). The paper presents
an overview of integrating the Autoware platform into the autonomous vehicle's
interface based on lessons learned from trouble-shooting processes for
resolving incompatible issues. The trouble-shooting processes are presented
based on resolving the incompatibility and integration issues of Ubuntu 20.04,
Autoware.AI, and ROS Noetic software installed in an autonomous driving
systems-equipped vehicle. Specifically, the paper focused on common
incompatibility issues and code-solving protocols involving Python
compatibility, Compute Unified Device Architecture (CUDA) installation,
Autoware installation, and simulation in Autoware.AI. The objective of the
paper is to provide an explicit and detail-oriented presentation to showcase
how to address incompatibility issues among an autonomous vehicle's operating
interference. The lessons and experience presented in the paper will be useful
for researchers who encountered similar issues and could follow up by
performing trouble-shooting activities and implementing ADS-related projects in
the Ubuntu, Autoware, and ROS operating systems.","Dada Zhang, Md Ruman Islam, Pei-Chi Huang, Chun-Hsing Ho",2024-10-09T02:35:50Z,2024-10-09T02:35:50Z,http://arxiv.org/abs/2410.06492v1,http://arxiv.org/pdf/2410.06492v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Enabling Novel Mission Operations and Interactions with ROSA: The Robot
  Operating System Agent","The advancement of robotic systems has revolutionized numerous industries,
yet their operation often demands specialized technical knowledge, limiting
accessibility for non-expert users. This paper introduces ROSA (Robot Operating
System Agent), an AI-powered agent that bridges the gap between the Robot
Operating System (ROS) and natural language interfaces. By leveraging
state-of-the-art language models and integrating open-source frameworks, ROSA
enables operators to interact with robots using natural language, translating
commands into actions and interfacing with ROS through well-defined tools.
ROSA's design is modular and extensible, offering seamless integration with
both ROS1 and ROS2, along with safety mechanisms like parameter validation and
constraint enforcement to ensure secure, reliable operations. While ROSA is
originally designed for ROS, it can be extended to work with other robotics
middle-wares to maximize compatibility across missions. ROSA enhances
human-robot interaction by democratizing access to complex robotic systems,
empowering users of all expertise levels with multi-modal capabilities such as
speech integration and visual perception. Ethical considerations are thoroughly
addressed, guided by foundational principles like Asimov's Three Laws of
Robotics, ensuring that AI integration promotes safety, transparency, privacy,
and accountability. By making robotic technology more user-friendly and
accessible, ROSA not only improves operational efficiency but also sets a new
standard for responsible AI use in robotics and potentially future mission
operations. This paper introduces ROSA's architecture and showcases initial
mock-up operations in JPL's Mars Yard, a laboratory, and a simulation using
three different robots. The core ROSA library is available as open-source.","Rob Royce, Marcel Kaufmann, Jonathan Becktor, Sangwoo Moon, Kalind Carpenter, Kai Pak, Amanda Towler, Rohan Thakker, Shehryar Khattak",2024-10-09T01:54:02Z,2025-02-13T00:37:06Z,http://arxiv.org/abs/2410.06472v2,http://arxiv.org/pdf/2410.06472v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Custom Non-Linear Model Predictive Control for Obstacle Avoidance in
  Indoor and Outdoor Environments","Navigating complex environments requires Unmanned Aerial Vehicles (UAVs) and
autonomous systems to perform trajectory tracking and obstacle avoidance in
real-time. While many control strategies have effectively utilized linear
approximations, addressing the non-linear dynamics of UAV, especially in
obstacle-dense environments, remains a key challenge that requires further
research. This paper introduces a Non-linear Model Predictive Control (NMPC)
framework for the DJI Matrice 100, addressing these challenges by using a
dynamic model and B-spline interpolation for smooth reference trajectories,
ensuring minimal deviation while respecting safety constraints. The framework
supports various trajectory types and employs a penalty-based cost function for
control accuracy in tight maneuvers. The framework utilizes CasADi for
efficient real-time optimization, enabling the UAV to maintain robust operation
even under tight computational constraints. Simulation and real-world indoor
and outdoor experiments demonstrated the NMPC ability to adapt to disturbances,
resulting in smooth, collision-free navigation.","Lara Laban, Mariusz Wzorek, Piotr Rudol, Tommy Persson",2024-10-03T17:50:19Z,2024-10-03T17:50:19Z,http://arxiv.org/abs/2410.02732v1,http://arxiv.org/pdf/2410.02732v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot
  Interaction","Real-life robot navigation involves more than just reaching a destination; it
requires optimizing movements while addressing scenario-specific goals. An
intuitive way for humans to express these goals is through abstract cues like
verbal commands or rough sketches. Such human guidance may lack details or be
noisy. Nonetheless, we expect robots to navigate as intended. For robots to
interpret and execute these abstract instructions in line with human
expectations, they must share a common understanding of basic navigation
concepts with humans. To this end, we introduce CANVAS, a novel framework that
combines visual and linguistic instructions for commonsense-aware navigation.
Its success is driven by imitation learning, enabling the robot to learn from
human navigation behavior. We present COMMAND, a comprehensive dataset with
human-annotated navigation results, spanning over 48 hours and 219 km, designed
to train commonsense-aware navigation systems in simulated environments. Our
experiments show that CANVAS outperforms the strong rule-based system ROS
NavStack across all environments, demonstrating superior performance with noisy
instructions. Notably, in the orchard environment, where ROS NavStack records a
0% total success rate, CANVAS achieves a total success rate of 67%. CANVAS also
closely aligns with human demonstrations and commonsense constraints, even in
unseen environments. Furthermore, real-world deployment of CANVAS showcases
impressive Sim2Real transfer with a total success rate of 69%, highlighting the
potential of learning from human demonstrations in simulated environments for
real-world applications.","Suhwan Choi, Yongjun Cho, Minchan Kim, Jaeyoon Jung, Myunchul Joe, Yubeen Park, Minseo Kim, Sungwoong Kim, Sungjae Lee, Hwiseong Park, Jiwan Chung, Youngjae Yu",2024-10-02T06:34:45Z,2025-08-08T04:20:36Z,http://arxiv.org/abs/2410.01273v3,http://arxiv.org/pdf/2410.01273v3.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
Precise Workcell Sketching from Point Clouds Using an AR Toolbox,"Capturing real-world 3D spaces as point clouds is efficient and descriptive,
but it comes with sensor errors and lacks object parametrization. These
limitations render point clouds unsuitable for various real-world applications,
such as robot programming, without extensive post-processing (e.g., outlier
removal, semantic segmentation). On the other hand, CAD modeling provides
high-quality, parametric representations of 3D space with embedded semantic
data, but requires manual component creation that is time-consuming and costly.
To address these challenges, we propose a novel solution that combines the
strengths of both approaches. Our method for 3D workcell sketching from point
clouds allows users to refine raw point clouds using an Augmented Reality (AR)
interface that leverages their knowledge and the real-world 3D environment. By
utilizing a toolbox and an AR-enabled pointing device, users can enhance point
cloud accuracy based on the device's position in 3D space. We validate our
approach by comparing it with ground truth models, demonstrating that it
achieves a mean error within 1cm - significant improvement over standard LiDAR
scanner apps.","Krzysztof Zieliński, Bruce Blumberg, Mikkel Baun Kjærgaard",2024-10-01T08:07:51Z,2024-10-01T08:07:51Z,http://arxiv.org/abs/2410.00479v1,http://arxiv.org/pdf/2410.00479v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Multi-Robot Target Monitoring and Encirclement via Triggered Distributed
  Feedback Optimization","We design a distributed feedback optimization strategy, embedded into a
modular ROS 2 control architecture, which allows a team of heterogeneous robots
to cooperatively monitor and encircle a target while patrolling points of
interest. Relying on the aggregative feedback optimization framework, we handle
multi-robot dynamics while minimizing a global performance index depending on
both microscopic (e.g., the location of single robots) and macroscopic
variables (e.g., the spatial distribution of the team). The proposed
distributed policy allows the robots to cooperatively address the global
problem by employing only local measurements and neighboring data exchanges.
These exchanges are performed through an asynchronous communication protocol
ruled by locally-verifiable triggering conditions. We formally prove that our
strategy steers the robots to a set of configurations representing stationary
points of the considered optimization problem. The effectiveness and
scalability of the overall strategy are tested via Monte Carlo campaigns of
realistic Webots ROS 2 virtual experiments. Finally, the applicability of our
solution is shown with real experiments on ground and aerial robots.","Lorenzo Pichierri, Guido Carnevale, Lorenzo Sforni, Giuseppe Notarstefano",2024-09-30T15:32:17Z,2024-09-30T15:32:17Z,http://arxiv.org/abs/2409.20399v1,http://arxiv.org/pdf/2409.20399v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Enhancing robot reliability for health-care facilities by means of
  Human-Aware Navigation Planning","With the aim of enabling robots to cooperate with humans, carry out
human-like tasks, or navigate among humans, we need to ensure that they are
equipped with the ability to comprehend human behaviors and use the extracted
knowledge for intelligent decision-making. This ability is particularly
important in the safety-critical and human-centred environment of health-care
institutions. In the field of robotic navigation, the most cutting-edge
approaches to enhancing robot reliability in the application domain of
healthcare facilities and in general pertain to augmenting navigation systems
with human-aware properties. To implement this in our work, the Co-operative
Human-Aware Navigation planner has been integrated into the ROS-based
differential-drive robot MARRtina and exhaustively challenged within various
simulated contexts and scenarios (mainly modelling the situations relevant in
the medical domain) to draw attention to the integrated system's benefits and
identify its drawbacks or instances of poor performance while exploring the
scope of system capabilities and creating a full characterization of its
applicability. The simulation results are then presented to medical experts,
and the enhanced robot acceptability within the domain is validated with them
as the robot is further planned for deployment.","Olga E. Sorokoletova, Lucca Iocchi",2024-09-25T17:49:02Z,2024-09-25T17:49:02Z,http://arxiv.org/abs/2409.17131v1,http://arxiv.org/pdf/2409.17131v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"A Model-Agnostic Approach for Semantically Driven Disambiguation in
  Human-Robot Interaction","Ambiguities are inevitable in human-robot interaction, especially when a
robot follows user instructions in a large, shared space. For example, if a
user asks the robot to find an object in a home environment with underspecified
instructions, the object could be in multiple locations depending on missing
factors. For instance, a bowl might be in the kitchen cabinet or on the dining
room table, depending on whether it is clean or dirty, full or empty, and the
presence of other objects around it. Previous works on object search have
assumed that the queried object is immediately visible to the robot or have
predicted object locations using one-shot inferences, which are likely to fail
for ambiguous or partially understood instructions. This paper focuses on these
gaps and presents a novel model-agnostic approach leveraging semantically
driven clarifications to enhance the robot's ability to locate queried objects
in fewer attempts. Specifically, we leverage different knowledge embedding
models, and when ambiguities arise, we propose an informative clarification
method, which follows an iterative prediction process. The user experiment
evaluation of our method shows that our approach is applicable to different
custom semantic encoders as well as LLMs, and informative clarifications
improve performances, enabling the robot to locate objects on its first
attempts. The user experiment data is publicly available at
https://github.com/IrmakDogan/ExpressionDataset.","Fethiye Irmak Dogan, Maithili Patel, Weiyu Liu, Iolanda Leite, Sonia Chernova",2024-09-25T15:07:47Z,2025-04-02T13:51:04Z,http://arxiv.org/abs/2409.17004v2,http://arxiv.org/pdf/2409.17004v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Robotic Backchanneling in Online Conversation Facilitation: A
  Cross-Generational Study","Japan faces many challenges related to its aging society, including
increasing rates of cognitive decline in the population and a shortage of
caregivers. Efforts have begun to explore solutions using artificial
intelligence (AI), especially socially embodied intelligent agents and robots
that can communicate with people. Yet, there has been little research on the
compatibility of these agents with older adults in various everyday situations.
To this end, we conducted a user study to evaluate a robot that functions as a
facilitator for a group conversation protocol designed to prevent cognitive
decline. We modified the robot to use backchannelling, a natural human way of
speaking, to increase receptiveness of the robot and enjoyment of the group
conversation experience. We conducted a cross-generational study with young
adults and older adults. Qualitative analyses indicated that younger adults
perceived the backchannelling version of the robot as kinder, more trustworthy,
and more acceptable than the non-backchannelling robot. Finally, we found that
the robot's backchannelling elicited nonverbal backchanneling in older
participants.","Sota Kobuki, Katie Seaborn, Seiki Tokunaga, Kosuke Fukumori, Shun Hidaka, Kazuhiro Tamura, Koji Inoue, Tatsuya Kawahara, Mihoko Otake-Mastuura",2024-09-25T13:08:43Z,2024-09-25T13:08:43Z,http://arxiv.org/abs/2409.16899v1,http://arxiv.org/pdf/2409.16899v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
Continual Learning for Multimodal Data Fusion of a Soft Gripper,"Continual learning (CL) refers to the ability of an algorithm to continuously
and incrementally acquire new knowledge from its environment while retaining
previously learned information. A model trained on one data modality often
fails when tested with a different modality. A straightforward approach might
be to fuse the two modalities by concatenating their features and training the
model on the fused data. However, this requires retraining the model from
scratch each time it encounters a new domain. In this paper, we introduce a
continual learning algorithm capable of incrementally learning different data
modalities by leveraging both class-incremental and domain-incremental learning
scenarios in an artificial environment where labeled data is scarce, yet
non-iid (independent and identical distribution) unlabeled data from the
environment is plentiful. The proposed algorithm is efficient and only requires
storing prototypes for each class. We evaluate the algorithm's effectiveness on
a challenging custom multimodal dataset comprising of tactile data from a soft
pneumatic gripper, and visual data from non-stationary images of objects
extracted from video sequences. Additionally, we conduct an ablation study on
the custom dataset and the Core50 dataset to highlight the contributions of
different components of the algorithm. To further demonstrate the robustness of
the algorithm, we perform a real-time experiment for object classification
using the soft gripper and an external independent camera setup, all
synchronized with the Robot Operating System (ROS) framework.","Nilay Kushawaha, Egidio Falotico",2024-09-20T09:53:27Z,2025-08-21T07:09:11Z,http://arxiv.org/abs/2409.13792v2,http://arxiv.org/pdf/2409.13792v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Enhancing Agricultural Environment Perception via Active Vision and
  Zero-Shot Learning","Agriculture, fundamental for human sustenance, faces unprecedented
challenges. The need for efficient, human-cooperative, and sustainable farming
methods has never been greater. The core contributions of this work involve
leveraging Active Vision (AV) techniques and Zero-Shot Learning (ZSL) to
improve the robot's ability to perceive and interact with agricultural
environment in the context of fruit harvesting. The AV Pipeline implemented
within ROS 2 integrates the Next-Best View (NBV) Planning for 3D environment
reconstruction through a dynamic 3D Occupancy Map. Our system allows the
robotics arm to dynamically plan and move to the most informative viewpoints
and explore the environment, updating the 3D reconstruction using semantic
information produced through ZSL models. Simulation and real-world experimental
results demonstrate our system's effectiveness in complex visibility
conditions, outperforming traditional and static predefined planning methods.
ZSL segmentation models employed, such as YOLO World + EfficientViT SAM,
exhibit high-speed performance and accurate segmentation, allowing flexibility
when dealing with semantic information in unknown agricultural contexts without
requiring any fine-tuning process.","Michele Carlo La Greca, Mirko Usuelli, Matteo Matteucci",2024-09-19T09:26:23Z,2024-09-19T09:26:23Z,http://arxiv.org/abs/2409.12602v1,http://arxiv.org/pdf/2409.12602v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Arena 4.0: A Comprehensive ROS2 Development and Benchmarking Platform
  for Human-centric Navigation Using Generative-Model-based Environment
  Generation","Building on the foundations of our previous work, this paper introduces Arena
4.0, a significant advancement over Arena 3.0, Arena-Bench, Arena 1.0, and
Arena 2.0. Arena 4.0 offers three key novel contributions: (1) a
generative-model-based world and scenario generation approach that utilizes
large language models (LLMs) and diffusion models to dynamically generate
complex, human-centric environments from text prompts or 2D floorplans, useful
for the development and benchmarking of social navigation strategies; (2) a
comprehensive 3D model database, extendable with additional 3D assets that are
semantically linked and annotated for dynamic spawning and arrangement within
3D worlds; and (3) a complete migration to ROS 2, enabling compatibility with
modern hardware and enhanced functionalities for improved navigation,
usability, and easier deployment on real robots. We evaluated the platform's
performance through a comprehensive user study, demonstrating significant
improvements in usability and efficiency compared to previous versions. Arena
4.0 is openly available at https://github.com/Arena-Rosnav.","Volodymyr Shcherbyna1, Linh Kästner, Diego Diaz, Huu Giang Nguyen, Maximilian Ho-Kyoung Schreff, Tim Lenz, Jonas Kreutz, Ahmed Martban, Huajian Zeng, Harold Soh",2024-09-19T05:20:13Z,2024-09-19T05:20:13Z,http://arxiv.org/abs/2409.12471v1,http://arxiv.org/pdf/2409.12471v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
The 1st InterAI Workshop: Interactive AI for Human-centered Robotics,"The workshop is affiliated with 33nd IEEE International Conference on Robot
and Human Interactive Communication (RO-MAN 2024) August 26~30, 2023 /
Pasadena, CA, USA. It is designed as a half-day event, extending over four
hours from 9:00 to 12:30 PST time. It accommodates both in-person and virtual
attendees (via Zoom), ensuring a flexible participation mode. The agenda is
thoughtfully crafted to include a diverse range of sessions: two keynote
speeches that promise to provide insightful perspectives, two dedicated paper
presentation sessions, an interactive panel discussion to foster dialogue among
experts which facilitates deeper dives into specific topics, and a 15-minute
coffee break. The workshop website:
https://sites.google.com/view/interaiworkshops/home.","Yuchong Zhang, Elmira Yadollahi, Yong Ma, Di Fu, Iolanda Leite, Danica Kragic",2024-09-17T13:03:24Z,2024-10-11T11:26:42Z,http://arxiv.org/abs/2409.11150v2,http://arxiv.org/pdf/2409.11150v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Towards No-Code Programming of Cobots: Experiments with Code Synthesis
  by Large Code Models for Conversational Programming","While there has been a lot of research recently on robots in household
environments, at the present time, most robots in existence can be found on
shop floors, and most interactions between humans and robots happen there.
``Collaborative robots'' (cobots) designed to work alongside humans on assembly
lines traditionally require expert programming, limiting ability to make
changes, or manual guidance, limiting expressivity of the resulting programs.
To address these limitations, we explore using Large Language Models (LLMs),
and in particular, their abilities of doing in-context learning, for
conversational code generation. As a first step, we define RATS, the
``Repetitive Assembly Task'', a 2D building task designed to lay the foundation
for simulating industry assembly scenarios. In this task, a `programmer'
instructs a cobot, using natural language, on how a certain assembly is to be
built; that is, the programmer induces a program, through natural language. We
create a dataset that pairs target structures with various example instructions
(human-authored, template-based, and model-generated) and example code. With
this, we systematically evaluate the capabilities of state-of-the-art LLMs for
synthesising this kind of code, given in-context examples. Evaluating in a
simulated environment, we find that LLMs are capable of generating accurate
`first order code' (instruction sequences), but have problems producing
`higher-order code' (abstractions such as functions, or use of loops).","Chalamalasetti Kranti, Sherzod Hakimov, David Schlangen",2024-09-17T10:04:50Z,2025-08-18T15:35:14Z,http://arxiv.org/abs/2409.11041v3,http://arxiv.org/pdf/2409.11041v3.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"A HeARTfelt Robot: Social Robot-Driven Deep Emotional Art Reflection
  with Children","Social-emotional learning (SEL) skills are essential for children to develop
to provide a foundation for future relational and academic success. Using art
as a medium for creation or as a topic to provoke conversation is a well-known
method of SEL learning. Similarly, social robots have been used to teach SEL
competencies like empathy, but the combination of art and social robotics has
been minimally explored. In this paper, we present a novel child-robot
interaction designed to foster empathy and promote SEL competencies via a
conversation about art scaffolded by a social robot. Participants (N=11, age
range: 7-11) conversed with a social robot about emotional and neutral art.
Analysis of video and speech data demonstrated that this interaction design
successfully engaged children in the practice of SEL skills, like emotion
recognition and self-awareness, and greater rates of empathetic reasoning were
observed when children engaged with the robot about emotional art. This study
demonstrated that art-based reflection with a social robot, particularly on
emotional art, can foster empathy in children, and interactions with a social
robot help alleviate discomfort when sharing deep or vulnerable emotions.","Isabella Pu, Golda Nguyen, Lama Alsultan, Rosalind Picard, Cynthia Breazeal, Sharifa Alghowinem",2024-09-16T20:29:20Z,2024-09-16T20:29:20Z,http://arxiv.org/abs/2409.10710v1,http://arxiv.org/pdf/2409.10710v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Online Diffusion-Based 3D Occupancy Prediction at the Frontier with
  Probabilistic Map Reconciliation","Autonomous navigation and exploration in unmapped environments remains a
significant challenge in robotics due to the difficulty robots face in making
commonsense inference of unobserved geometries. Recent advancements have
demonstrated that generative modeling techniques, particularly diffusion
models, can enable systems to infer these geometries from partial observation.
In this work, we present implementation details and results for real-time,
online occupancy prediction using a modified diffusion model. By removing
attention-based visual conditioning and visual feature extraction components,
we achieve a 73$\%$ reduction in runtime with minimal accuracy reduction. These
modifications enable occupancy prediction across the entire map, rather than
being limited to the area around the robot where camera data can be collected.
We introduce a probabilistic update method for merging predicted occupancy data
into running occupancy maps, resulting in a 71$\%$ improvement in predicting
occupancy at map frontiers compared to previous methods. Finally, we release
our code and a ROS node for on-robot operation <upon publication> at
github.com/arpg/sceneSense_ws.","Alec Reed, Lorin Achey, Brendan Crowe, Bradley Hayes, Christoffer Heckman",2024-09-16T19:24:00Z,2024-09-16T19:24:00Z,http://arxiv.org/abs/2409.10681v1,http://arxiv.org/pdf/2409.10681v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"ASMA: An Adaptive Safety Margin Algorithm for Vision-Language Drone
  Navigation via Scene-Aware Control Barrier Functions","In the rapidly evolving field of vision-language navigation (VLN), ensuring
safety for physical agents remains an open challenge. For a human-in-the-loop
language-operated drone to navigate safely, it must understand natural language
commands, perceive the environment, and simultaneously avoid hazards in real
time. Control Barrier Functions (CBFs) are formal methods that enforce safe
operating conditions. Model Predictive Control (MPC) is an optimization
framework that plans a sequence of future actions over a prediction horizon,
ensuring smooth trajectory tracking while obeying constraints. In this work, we
consider a VLN-operated drone platform and enhance its safety by formulating a
novel scene-aware CBF that leverages ego-centric observations from a camera
which has both Red-Green-Blue as well as Depth (RGB-D) channels. A CBF-less
baseline system uses a Vision-Language Encoder with cross-modal attention to
convert commands into an ordered sequence of landmarks. An object detection
model identifies and verifies these landmarks in the captured images to
generate a planned path. To further enhance safety, an Adaptive Safety Margin
Algorithm (ASMA) is proposed. ASMA tracks moving objects and performs
scene-aware CBF evaluation on-the-fly, which serves as an additional constraint
within the MPC framework. By continuously identifying potentially risky
observations, the system performs prediction in real time about unsafe
conditions and proactively adjusts its control actions to maintain safe
navigation throughout the trajectory. Deployed on a Parrot Bebop2 quadrotor in
the Gazebo environment using the Robot Operating System (ROS), ASMA achieves
64%-67% increase in success rates with only a slight increase (1.4%-5.8%) in
trajectory lengths compared to the baseline CBF-less VLN.","Sourav Sanyal, Kaushik Roy",2024-09-16T13:44:50Z,2025-07-19T18:48:48Z,http://arxiv.org/abs/2409.10283v4,http://arxiv.org/pdf/2409.10283v4.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
Voice control interface for surgical robot assistants,"Traditional control interfaces for robotic-assisted minimally invasive
surgery impose a significant cognitive load on surgeons. To improve surgical
efficiency, surgeon-robot collaboration capabilities, and reduce surgeon
burden, we present a novel voice control interface for surgical robotic
assistants. Our system integrates Whisper, state-of-the-art speech recognition,
within the ROS framework to enable real-time interpretation and execution of
voice commands for surgical manipulator control. The proposed system consists
of a speech recognition module, an action mapping module, and a robot control
module. Experimental results demonstrate the system's high accuracy and
inference speed, and demonstrates its feasibility for surgical applications in
a tissue triangulation task. Future work will focus on further improving its
robustness and clinical applicability.","Ana Davila, Jacinto Colan, Yasuhisa Hasegawa",2024-09-16T12:19:48Z,2024-09-16T12:19:48Z,http://arxiv.org/abs/2409.10225v1,http://arxiv.org/pdf/2409.10225v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
ROS2WASM: Bringing the Robot Operating System to the Web,"The Robot Operating System (ROS) has become the de facto standard middleware
in robotics, widely adopted across domains ranging from education to industrial
applications. The RoboStack distribution, a conda-based packaging system for
ROS, has extended ROS's accessibility by facilitating installation across all
major operating systems and architectures, integrating seamlessly with
scientific tools such as PyTorch and Open3D. This paper presents ROS2WASM, a
novel integration of RoboStack with WebAssembly, enabling the execution of ROS
2 and its associated software directly within web browsers, without requiring
local installations. ROS2WASM significantly enhances the reproducibility and
shareability of research, lowers barriers to robotics education, and leverages
WebAssembly's robust security framework to protect against malicious code. We
detail our methodology for cross-compiling ROS 2 packages into WebAssembly, the
development of a specialized middleware for ROS 2 communication within
browsers, and the implementation of https://www.ros2wasm.dev, a web platform
enabling users to interact with ROS 2 environments. Additionally, we extend
support to the Robotics Toolbox for Python and adapt its Swift simulator for
browser compatibility. Our work paves the way for unprecedented accessibility
in robotics, offering scalable, secure, and reproducible environments that have
the potential to transform educational and research paradigms.","Tobias Fischer, Isabel Paredes, Michael Batchelor, Thorsten Beier, Jesse Haviland, Silvio Traversaro, Wolf Vollprecht, Markus Schmitz, Michael Milford",2024-09-16T02:32:54Z,2025-03-07T06:24:57Z,http://arxiv.org/abs/2409.09941v2,http://arxiv.org/pdf/2409.09941v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"SAFER-Splat: A Control Barrier Function for Safe Navigation with Online
  Gaussian Splatting Maps","SAFER-Splat (Simultaneous Action Filtering and Environment Reconstruction) is
a real-time, scalable, and minimally invasive action filter, based on control
barrier functions, for safe robotic navigation in a detailed map constructed at
runtime using Gaussian Splatting (GSplat). We propose a novel Control Barrier
Function (CBF) that not only induces safety with respect to all Gaussian
primitives in the scene, but when synthesized into a controller, is capable of
processing hundreds of thousands of Gaussians while maintaining a minimal
memory footprint and operating at 15 Hz during online Splat training. Of the
total compute time, a small fraction of it consumes GPU resources, enabling
uninterrupted training. The safety layer is minimally invasive, correcting
robot actions only when they are unsafe. To showcase the safety filter, we also
introduce SplatBridge, an open-source software package built with ROS for
real-time GSplat mapping for robots. We demonstrate the safety and robustness
of our pipeline first in simulation, where our method is 20-50x faster, safer,
and less conservative than competing methods based on neural radiance fields.
Further, we demonstrate simultaneous GSplat mapping and safety filtering on a
drone hardware platform using only on-board perception. We verify that under
teleoperation a human pilot cannot invoke a collision. Our videos and codebase
can be found at https://chengine.github.io/safer-splat.","Timothy Chen, Aiden Swann, Javier Yu, Ola Shorinwa, Riku Murai, Monroe Kennedy III, Mac Schwager",2024-09-15T21:25:18Z,2025-03-17T20:42:50Z,http://arxiv.org/abs/2409.09868v2,http://arxiv.org/pdf/2409.09868v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Intelligent LiDAR Navigation: Leveraging External Information and
  Semantic Maps with LLM as Copilot","Traditional robot navigation systems primarily utilize occupancy grid maps
and laser-based sensing technologies, as demonstrated by the popular move_base
package in ROS. Unlike robots, humans navigate not only through spatial
awareness and physical distances but also by integrating external information,
such as elevator maintenance updates from public notification boards and
experiential knowledge, like the need for special access through certain doors.
With the development of Large Language Models (LLMs), which possesses text
understanding and intelligence close to human performance, there is now an
opportunity to infuse robot navigation systems with a level of understanding
akin to human cognition. In this study, we propose using osmAG (Area Graph in
OpensStreetMap textual format), an innovative semantic topometric hierarchical
map representation, to bridge the gap between the capabilities of ROS move_base
and the contextual understanding offered by LLMs. Our methodology employs LLMs
as an actual copilot in robot navigation, enabling the integration of a broader
range of informational inputs while maintaining the robustness of traditional
robotic navigation systems. Our code, demo, map, experiment results can be
accessed at
https://github.com/xiexiexiaoxiexie/Intelligent-LiDAR-Navigation-LLM-as-Copilot.","Fujing Xie, Jiajie Zhang, Sören Schwertfeger",2024-09-13T02:37:28Z,2025-07-19T03:34:13Z,http://arxiv.org/abs/2409.08493v3,http://arxiv.org/pdf/2409.08493v3.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
Kinesthetic Teaching in Robotics: a Mixed Reality Approach,"As collaborative robots become more common in manufacturing scenarios and
adopted in hybrid human-robot teams, we should develop new interaction and
communication strategies to ensure smooth collaboration between agents. In this
paper, we propose a novel communicative interface that uses Mixed Reality as a
medium to perform Kinesthetic Teaching (KT) on any robotic platform. We
evaluate our proposed approach in a user study involving multiple subjects and
two different robots, comparing traditional physical KT with holographic-based
KT through user experience questionnaires and task-related metrics.","Simone Macci`o, Mohamad Shaaban, Alessandro Carf`ı, Fulvio Mastrogiovanni",2024-09-03T21:32:16Z,2024-09-03T21:32:16Z,http://arxiv.org/abs/2409.02305v1,http://arxiv.org/pdf/2409.02305v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Upgrading Pepper Robot s Social Interaction with Advanced Hardware and
  Perception Enhancements","In this paper, we propose hardware and software enhancements for the Pepper
robot to improve its human-robot interaction capabilities. This includes the
integration of an NVIDIA Jetson GPU to enhance computational capabilities and
execute real time algorithms, and a RealSense D435i camera to capture depth
images, as well as the computer vision algorithms to detect and localize the
humans around the robot and estimate their body orientation and gaze direction.
The new stack is implemented on ROS and is running on the extended Pepper
hardware, and the communication with the robot s firmware is done through the
NAOqi ROS driver API. We have also collected a MoCap dataset of human
activities in a controlled environment, together with the corresponding RGB-D
data, to validate the proposed perception algorithms.","Paolo Magri, Javad Amirian, Mohamed Chetouani",2024-09-02T08:12:59Z,2024-09-02T08:12:59Z,http://arxiv.org/abs/2409.01036v1,http://arxiv.org/pdf/2409.01036v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Do Mistakes Matter? Comparing Trust Responses of Different Age Groups to
  Errors Made by Physically Assistive Robots","Trust is a key factor in ensuring acceptable human-robot interaction,
especially in settings where robots may be assisting with critical activities
of daily living. When practically deployed, robots are bound to make occasional
mistakes, yet the degree to which these errors will impact a care recipient's
trust in the robot, especially in performing physically assistive tasks,
remains an open question. To investigate this, we conducted experiments where
participants interacted with physically assistive robots which would
occasionally make intentional mistakes while performing two different tasks:
bathing and feeding. Our study considered the error response of two
populations: younger adults at a university (median age 26) and older adults at
an independent living facility (median age 83). We observed that the impact of
errors on a users' trust in the robot depends on both their age and the task
that the robot is performing. We also found that older adults tend to evaluate
the robot on factors unrelated to the robot's performance, making their trust
in the system more resilient to errors when compared to younger adults. Code
and supplementary materials are available on our project webpage.","Sasha Wald, Kavya Puthuveetil, Zackory Erickson",2024-08-23T15:23:56Z,2024-08-23T15:23:56Z,http://arxiv.org/abs/2408.13153v1,http://arxiv.org/pdf/2408.13153v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
Control-Theoretic Analysis of Shared Control Systems,"Users of shared control systems change their behavior in the presence of
assistance, which conflicts with assumpts about user behavior that some
assistance methods make. In this paper, we propose an analysis technique to
evaluate the user's experience with the assistive systems that bypasses
required assumptions: we model the assistance as a dynamical system that can be
analyzed using control theory techniques. We analyze the shared autonomy
assistance algorithm and make several observations: we identify a problem with
runaway goal confidence and propose a system adjustment to mitigate it, we
demonstrate that the system inherently limits the possible actions available to
the user, and we show that in a simplified setting, the effect of the
assistance is to drive the system to the convex hull of the goals and, once
there, add a layer of indirection between the user control and the system
behavior. We conclude by discussing the possible uses of this analysis for the
field.","Reuben M. Aronson, Elaine Schaertl Short",2024-08-22T03:34:06Z,2024-08-22T03:34:06Z,http://arxiv.org/abs/2408.12103v1,http://arxiv.org/pdf/2408.12103v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
Bidirectional Intent Communication: A Role for Large Foundation Models,"Integrating multimodal foundation models has significantly enhanced
autonomous agents' language comprehension, perception, and planning
capabilities. However, while existing works adopt a \emph{task-centric}
approach with minimal human interaction, applying these models to developing
assistive \emph{user-centric} robots that can interact and cooperate with
humans remains underexplored. This paper introduces ``Bident'', a framework
designed to integrate robots seamlessly into shared spaces with humans. Bident
enhances the interactive experience by incorporating multimodal inputs like
speech and user gaze dynamics. Furthermore, Bident supports verbal utterances
and physical actions like gestures, making it versatile for bidirectional
human-robot interactions. Potential applications include personalized
education, where robots can adapt to individual learning styles and paces, and
healthcare, where robots can offer personalized support, companionship, and
everyday assistance in the home and workplace environments.","Tim Schreiter, Rishi Hazra, Jens Rüppel, Andrey Rudenko",2024-08-20T06:52:27Z,2024-08-20T06:52:27Z,http://arxiv.org/abs/2408.10589v1,http://arxiv.org/pdf/2408.10589v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Timing Analysis and Priority-driven Enhancements of ROS 2 Multi-threaded
  Executors","The second generation of Robotic Operating System, ROS 2, has gained much
attention for its potential to be used for safety-critical robotic
applications. The need to provide a solid foundation for timing correctness and
scheduling mechanisms is therefore growing rapidly. Although there are some
pioneering studies conducted on formally analyzing the response time of
processing chains in ROS 2, the focus has been limited to single-threaded
executors, and multi-threaded executors, despite their advantages, have not
been studied well. To fill this knowledge gap, in this paper, we propose a
comprehensive response-time analysis framework for chains running on ROS 2
multi-threaded executors. We first analyze the timing behavior of the default
scheduling scheme in ROS 2 multi-threaded executors, and then present
priority-driven scheduling enhancements to address the limitations of the
default scheme. Our framework can analyze chains with both arbitrary and
constrained deadlines and also the effect of mutually-exclusive callback
groups. Evaluation is conducted by a case study on NVIDIA Jetson AGX Xavier and
schedulability experiments using randomly-generated chains. The results
demonstrate that our analysis framework can safely upper-bound response times
under various conditions and the priority-driven scheduling enhancements not
only reduce the response time of critical chains but also improve analytical
bounds.","Hoora Sobhani, Hyunjong Choi, Hyoseung Kim",2024-08-15T22:13:11Z,2024-12-30T06:00:31Z,http://arxiv.org/abs/2408.08440v2,http://arxiv.org/pdf/2408.08440v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Toward a Dialogue System Using a Large Language Model to Recognize User
  Emotions with a Camera","The performance of ChatGPT\copyright{} and other LLMs has improved
tremendously, and in online environments, they are increasingly likely to be
used in a wide variety of situations, such as ChatBot on web pages, call center
operations using voice interaction, and dialogue functions using agents. In the
offline environment, multimodal dialogue functions are also being realized,
such as guidance by Artificial Intelligence agents (AI agents) using tablet
terminals and dialogue systems in the form of LLMs mounted on robots. In this
multimodal dialogue, mutual emotion recognition between the AI and the user
will become important. So far, there have been methods for expressing emotions
on the part of the AI agent or for recognizing them using textual or voice
information of the user's utterances, but methods for AI agents to recognize
emotions from the user's facial expressions have not been studied. In this
study, we examined whether or not LLM-based AI agents can interact with users
according to their emotional states by capturing the user in dialogue with a
camera, recognizing emotions from facial expressions, and adding such emotion
information to prompts. The results confirmed that AI agents can have
conversations according to the emotional state for emotional states with
relatively high scores, such as Happy and Angry.","Hiroki Tanioka, Tetsushi Ueta, Masahiko Sano",2024-08-15T07:03:00Z,2025-02-18T12:48:27Z,http://arxiv.org/abs/2408.07982v2,http://arxiv.org/pdf/2408.07982v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"HADRON: Human-friendly Control and Artificial Intelligence for Military
  Drone Operations","As drones are getting more and more entangled in our society, more untrained
users require the capability to operate them. This scenario is to be achieved
through the development of artificial intelligence capabilities assisting the
human operator in controlling the Unmanned Aerial System (UAS) and processing
the sensor data, thereby alleviating the need for extensive operator training.
This paper presents the HADRON project that seeks to develop and test multiple
novel technologies to enable human-friendly control of drone swarms. This
project is divided into three main parts. The first part consists of the
integration of different technologies for the intuitive control of drones,
focusing on novice or inexperienced pilots and operators. The second part
focuses on the development of a multi-drone system that will be controlled from
a command and control station, in which an expert pilot can supervise the
operations of the multiple drones. The third part of the project will focus on
reducing the cognitive load on human operators, whether they are novice or
expert pilots. For this, we will develop AI tools that will assist drone
operators with semi-automated real-time data processing.","Ana M. Casado Faulí, Mario Malizia, Ken Hasselmann, Emile Le Flécher, Geert De Cubber, Ben Lauwens",2024-08-13T17:56:51Z,2024-08-13T17:56:51Z,http://arxiv.org/abs/2408.07063v1,http://arxiv.org/pdf/2408.07063v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Bridging the Gap between ROS~2 and Classical Real-Time Scheduling for
  Periodic Tasks","The Robot Operating System 2 (ROS~2) is a widely used middleware that
provides software libraries and tools for developing robotic systems. In these
systems, tasks are scheduled by ROS~2 executors. Since the scheduling behavior
of the default ROS~2 executor is inherently different from classical real-time
scheduling theory, dedicated analyses or alternative executors, requiring
substantial changes to ROS~2, have been required. In 2023, the events executor,
which features an events queue and allows the possibility to make scheduling
decisions immediately after a job completes, was introduced into ROS~2. In this
paper, we show that, with only minor modifications of the events executor, a
large body of research results from classical real-time scheduling theory
becomes applicable. Hence, this enables analytical bounds on the worst-case
response time and the end-to-end latency, outperforming bounds for the default
ROS 2 executor in many scenarios. Our solution is easy to integrate into
existing ROS 2 systems since it requires only minor backend modifications of
the events executor, which is natively included in ROS 2. The evaluation
results show that our ROS~2 events executor with minor modifications can have
significant improvement in terms of dropped jobs, worst-case response time,
end-to-end latency, and performance compared to the default ROS~2 executor.","Harun Teper, Oren Bell, Mario Günzel, Chris Gill, Jian-Jia Chen",2024-08-07T11:23:09Z,2024-08-07T11:23:09Z,http://arxiv.org/abs/2408.03696v1,http://arxiv.org/pdf/2408.03696v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"High-Quality, ROS Compatible Video Encoding and Decoding for
  High-Definition Datasets","Robotic datasets are important for scientific benchmarking and developing
algorithms, for example for Simultaneous Localization and Mapping (SLAM).
Modern robotic datasets feature video data of high resolution and high
framerates. Storing and sharing those datasets becomes thus very costly,
especially if more than one camera is used for the datasets. It is thus
essential to store this video data in a compressed format. This paper
investigates the use of modern video encoders for robotic datasets. We provide
a software that can replay mp4 videos within ROS 1 and ROS 2 frameworks,
supporting the synchronized playback in simulated time. Furthermore, the paper
evaluates different encoders and their settings to find optimal configurations
in terms of resulting size, quality and encoding time. Through this work we
show that it is possible to store and share even highest quality video datasets
within reasonable storage constraints.","Jian Li, Bowen Xu, Sören Schwertfeger",2024-08-01T13:21:34Z,2024-10-27T04:10:52Z,http://arxiv.org/abs/2408.00538v2,http://arxiv.org/pdf/2408.00538v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Understanding Misconfigurations in ROS: An Empirical Study and Current
  Approaches","The Robot Operating System (ROS) is a popular framework and ecosystem that
allows developers to build robot software systems from reusable, off-the-shelf
components. Systems are often built by customizing and connecting components
via configuration files. While reusable components theoretically allow rapid
prototyping, ensuring proper configuration and connection is challenging, as
evidenced by numerous questions on developer forums. Developers must abide to
the often unchecked and unstated assumptions of individual components. Failure
to do so can result in misconfigurations that are only discovered during field
deployment, at which point errors may lead to unpredictable and dangerous
behavior. Despite misconfigurations having been studied in the broader context
of software engineering, robotics software (and ROS in particular) poses
domain-specific challenges with potentially disastrous consequences. To
understand and improve the reliability of ROS projects, it is critical to
identify the types of misconfigurations faced by developers. To that end, we
perform a study of ROS Answers, a Q&A platform, to identify and categorize
misconfigurations that occur during ROS development. We then conduct a
literature review to assess the coverage of these misconfigurations by existing
detection techniques. In total, we find 12 high-level categories and 50
sub-categories of misconfigurations. Of these categories, 27 are not covered by
existing techniques. To conclude, we discuss how to tackle those
misconfigurations in future work.","Paulo Canelas, Bradley Schmerl, Alcides Fonseca, Christopher S. Timperley",2024-07-27T16:20:43Z,2024-07-27T16:20:43Z,http://arxiv.org/abs/2407.19292v1,http://arxiv.org/pdf/2407.19292v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Improving the ROS 2 Navigation Stack with Real-Time Local Costmap
  Updates for Agricultural Applications","The ROS 2 Navigation Stack (Nav2) has emerged as a widely used software
component providing the underlying basis to develop a variety of high-level
functionalities. However, when used in outdoor environments such as orchards
and vineyards, its functionality is notably limited by the presence of
obstacles and/or situations not commonly found in indoor settings. One such
example is given by tall grass and weeds that can be safely traversed by a
robot, but that can be perceived as obstacles by LiDAR sensors, and then force
the robot to take longer paths to avoid them, or abort navigation altogether.
To overcome these limitations, domain specific extensions must be developed and
integrated into the software pipeline. This paper presents a new, lightweight
approach to address this challenge and improve outdoor robot navigation.
Leveraging the multi-scale nature of the costmaps supporting Nav2, we developed
a system that using a depth camera performs pixel level classification on the
images, and in real time injects corrections into the local cost map, thus
enabling the robot to traverse areas that would otherwise be avoided by the
Nav2. Our approach has been implemented and validated on a Clearpath Husky and
we demonstrate that with this extension the robot is able to perform navigation
tasks that would be otherwise not practical with the standard components.","Ettore Sani, Antonio Sgorbissa, Stefano Carpin",2024-07-26T06:33:10Z,2024-07-26T06:33:10Z,http://arxiv.org/abs/2407.18535v1,http://arxiv.org/pdf/2407.18535v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
Puppeteer Your Robot: Augmented Reality Leader-Follower Teleoperation,"High-quality demonstrations are necessary when learning complex and
challenging manipulation tasks. In this work, we introduce an approach to
puppeteer a robot by controlling a virtual robot in an augmented reality
setting. Our system allows for retaining the advantages of being intuitive from
a physical leader-follower side while avoiding the unnecessary use of expensive
physical setup. In addition, the user is endowed with additional information
using augmented reality. We validate our system with a pilot study n=10 on a
block stacking and rice scooping tasks where the majority rates the system
favorably. Oculus App and corresponding ROS code are available on the project
website: https://ar-puppeteer.github.io/","Jonne van Haastregt, Michael C. Welle, Yuchong Zhang, Danica Kragic",2024-07-16T14:08:06Z,2024-07-16T14:08:06Z,http://arxiv.org/abs/2407.11741v1,http://arxiv.org/pdf/2407.11741v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Speech-Guided Sequential Planning for Autonomous Navigation using Large
  Language Model Meta AI 3 (Llama3)","In social robotics, a pivotal focus is enabling robots to engage with humans
in a more natural and seamless manner. The emergence of advanced large language
models (LLMs) such as Generative Pre-trained Transformers (GPTs) and
autoregressive models like Large Language Model Meta AI (Llamas) has driven
significant advancements in integrating natural language understanding
capabilities into social robots. This paper presents a system for speech-guided
sequential planning in autonomous navigation, utilizing Llama3 and the Robot
Operating System~(ROS). The proposed system involves using Llama3 to interpret
voice commands, extracting essential details through parsing, and decoding
these commands into sequential actions for tasks. Such sequential planning is
essential in various domains, particularly in the pickup and delivery of an
object. Once a sequential navigation task is evaluated, we employ DRL-VO, a
learning-based control policy that allows a robot to autonomously navigate
through social spaces with static infrastructure and (crowds of) people. We
demonstrate the effectiveness of the system in simulation experiment using
Turtlebot 2 in ROS1 and Turtlebot 3 in ROS2. We conduct hardware trials using a
Clearpath Robotics Jackal UGV, highlighting its potential for real-world
deployment in scenarios requiring flexible and interactive robotic behaviors.","Alkesh K. Srivastava, Philip Dames",2024-07-13T13:43:39Z,2024-09-27T00:01:03Z,http://arxiv.org/abs/2407.09890v2,http://arxiv.org/pdf/2407.09890v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"A Deep Reinforcement Learning Framework and Methodology for Reducing the
  Sim-to-Real Gap in ASV Navigation","Despite the increasing adoption of Deep Reinforcement Learning (DRL) for
Autonomous Surface Vehicles (ASVs), there still remain challenges limiting
real-world deployment. In this paper, we first integrate buoyancy and
hydrodynamics models into a modern Reinforcement Learning framework to reduce
training time. Next, we show how system identification coupled with domain
randomization improves the RL agent performance and narrows the sim-to-real
gap. Real-world experiments for the task of capturing floating waste show that
our approach lowers energy consumption by 13.1\% while reducing task completion
time by 7.4\%. These findings, supported by sharing our open-source
implementation, hold the potential to impact the efficiency and versatility of
ASVs, contributing to environmental conservation efforts.","Luis F W Batista, Junghwan Ro, Antoine Richard, Pete Schroepfer, Seth Hutchinson, Cedric Pradalier",2024-07-11T08:03:34Z,2024-07-11T08:03:34Z,http://arxiv.org/abs/2407.08263v1,http://arxiv.org/pdf/2407.08263v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"How Much Progress Did I Make? An Unexplored Human Feedback Signal for
  Teaching Robots","Enhancing the expressiveness of human teaching is vital for both improving
robots' learning from humans and the human-teaching-robot experience. In this
work, we characterize and test a little-used teaching signal:
\textit{progress}, designed to represent the completion percentage of a task.
We conducted two online studies with 76 crowd-sourced participants and one
public space study with 40 non-expert participants to validate the capability
of this progress signal. We find that progress indicates whether the task is
successfully performed, reflects the degree of task completion, identifies
unproductive but harmless behaviors, and is likely to be more consistent across
participants. Furthermore, our results show that giving progress does not
require extra workload and time. An additional contribution of our work is a
dataset of 40 non-expert demonstrations from the public space study through an
ice cream topping-adding task, which we observe to be multi-policy and
sub-optimal, with sub-optimality not only from teleoperation errors but also
from exploratory actions and attempts. The dataset is available at
\url{https://github.com/TeachingwithProgress/Non-Expert\_Demonstrations}.","Hang Yu, Qidi Fang, Shijie Fang, Reuben M. Aronson, Elaine Schaertl Short",2024-07-08T23:47:13Z,2024-07-08T23:47:13Z,http://arxiv.org/abs/2407.06459v1,http://arxiv.org/pdf/2407.06459v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
Active Collaborative Visual SLAM exploiting ORB Features,"In autonomous robotics, a significant challenge involves devising robust
solutions for Active Collaborative SLAM (AC-SLAM). This process requires
multiple robots to cooperatively explore and map an unknown environment by
intelligently coordinating their movements and sensor data acquisition. In this
article, we present an efficient visual AC-SLAM method using aerial and ground
robots for environment exploration and mapping. We propose an efficient
frontiers filtering method that takes into account the common IoU map frontiers
and reduces the frontiers for each robot. Additionally, we also present an
approach to guide robots to previously visited goal positions to promote loop
closure to reduce SLAM uncertainty. The proposed method is implemented in ROS
and evaluated through simulations on publicly available datasets and similar
methods, achieving an accumulative average of 59% of increase in area coverage.","Muhammad Farhan Ahmed, Vincent Frémont, Isabelle Fantoni",2024-07-07T17:35:25Z,2024-09-09T15:30:17Z,http://arxiv.org/abs/2407.05453v2,http://arxiv.org/pdf/2407.05453v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
A Tree-based Next-best-trajectory Method for 3D UAV Exploration,"This work presents a fully integrated tree-based combined
exploration-planning algorithm: Exploration-RRT (ERRT). The algorithm is
focused on providing real-time solutions for local exploration in a fully
unknown and unstructured environment while directly incorporating exploratory
behavior, robot-safe path planning, and robot actuation into the central
problem. ERRT provides a complete sampling and tree-based solution for
evaluating ""where to go next"" by considering a trade-off between maximizing
information gain, and minimizing the distances travelled and the robot
actuation along the path. The complete scheme is evaluated in extensive
simulations, comparisons, as well as real-world field experiments in
constrained and narrow subterranean and GPS-denied environments. The framework
is fully ROS-integrated, straight-forward to use, and we open-source it at
https://github.com/LTU-RAI/ExplorationRRT.","Björn Lindqvist, Akash Patel, Kalle Löfgren, George Nikolakopoulos",2024-07-05T09:46:17Z,2024-07-05T09:46:17Z,http://arxiv.org/abs/2407.04386v1,http://arxiv.org/pdf/2407.04386v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Eyes on the Game: Deciphering Implicit Human Signals to Infer Human
  Proficiency, Trust, and Intent","Effective collaboration between humans and AIs hinges on transparent
communication and alignment of mental models. However, explicit, verbal
communication is not always feasible. Under such circumstances, human-human
teams often depend on implicit, nonverbal cues to glean important information
about their teammates such as intent and expertise, thereby bolstering team
alignment and adaptability. Among these implicit cues, two of the most salient
and fundamental are a human's actions in the environment and their visual
attention. In this paper, we present a novel method to combine eye gaze data
and behavioral data, and evaluate their respective predictive power for human
proficiency, trust, and intent. We first collect a dataset of paired eye gaze
and gameplay data in the fast-paced collaborative ""Overcooked"" environment. We
then train models on this dataset to compare how the predictive powers differ
between gaze data, gameplay data, and their combination. We additionally
compare our method to prior works that aggregate eye gaze data and demonstrate
how these aggregation methods can substantially reduce the predictive ability
of eye gaze. Our results indicate that, while eye gaze data and gameplay data
excel in different situations, a model that integrates both types consistently
outperforms all baselines. This work paves the way for developing intuitive and
responsive agents that can efficiently adapt to new teammates.","Nikhil Hulle, Stéphane Aroca-Ouellette, Anthony J. Ries, Jake Brawer, Katharina von der Wense, Alessandro Roncone",2024-07-03T17:37:19Z,2024-07-03T17:37:19Z,http://arxiv.org/abs/2407.03298v1,http://arxiv.org/pdf/2407.03298v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Performance Comparison of ROS2 Middlewares for Multi-robot Mesh Networks
  in Planetary Exploration","Recent advancements in Multi-Robot Systems (MRS) and mesh network
technologies pave the way for innovative approaches to explore extreme
environments. The Artemis Accords, a series of international agreements, have
further catalyzed this progress by fostering cooperation in space exploration,
emphasizing the use of cutting-edge technologies. In parallel, the widespread
adoption of the Robot Operating System 2 (ROS 2) by companies across various
sectors underscores its robustness and versatility. This paper evaluates the
performances of available ROS 2 MiddleWare (RMW), such as FastRTPS, CycloneDDS
and Zenoh, over a mesh network with a dynamic topology. The final choice of RMW
is determined by the one that would fit the most the scenario: an exploration
of the extreme extra-terrestrial environment using a MRS. The conducted study
in a real environment highlights Zenoh as a potential solution for future
applications, showing a reduced delay, reachability, and CPU usage while being
competitive on data overhead and RAM usage over a dynamic mesh topology","Loïck Pierre Chovet, Gabriel Manuel Garcia, Abhishek Bera, Antoine Richard, Kazuya Yoshida, Miguel Angel Olivares-Mendez",2024-07-03T13:30:57Z,2024-07-03T13:30:57Z,http://arxiv.org/abs/2407.03091v1,http://arxiv.org/pdf/2407.03091v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
Learning Human-Robot Handshaking Preferences for Quadruped Robots,"Quadruped robots are showing impressive abilities to navigate the real world.
If they are to become more integrated into society, social trust in
interactions with humans will become increasingly important. Additionally,
robots will need to be adaptable to different humans based on individual
preferences. In this work, we study the social interaction task of learning
optimal handshakes for quadruped robots based on user preferences. While
maintaining balance on three legs, we parameterize handshakes with a Central
Pattern Generator consisting of an amplitude, frequency, stiffness, and
duration. Through 10 binary choices between handshakes, we learn a belief model
to fit individual preferences for 25 different subjects. Our results show that
this is an effective strategy, with 76% of users feeling happy with their
identified optimal handshake parameters, and 20% feeling neutral. Moreover,
compared with random and test handshakes, the optimized handshakes have
significantly decreased errors in amplitude and frequency, lower Dynamic Time
Warping scores, and improved energy efficiency, all of which indicate robot
synchronization to the user's preferences. Video results can be found at
https://youtu.be/elvPv8mq1KM .","Alessandra Chappuis, Guillaume Bellegarda, Auke Ijspeert",2024-06-28T13:00:44Z,2024-06-28T13:00:44Z,http://arxiv.org/abs/2406.19893v1,http://arxiv.org/pdf/2406.19893v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"ROS-LLM: A ROS framework for embodied AI with task feedback and
  structured reasoning","We present a framework for intuitive robot programming by non-experts,
leveraging natural language prompts and contextual information from the Robot
Operating System (ROS). Our system integrates large language models (LLMs),
enabling non-experts to articulate task requirements to the system through a
chat interface. Key features of the framework include: integration of ROS with
an AI agent connected to a plethora of open-source and commercial LLMs,
automatic extraction of a behavior from the LLM output and execution of ROS
actions/services, support for three behavior modes (sequence, behavior tree,
state machine), imitation learning for adding new robot actions to the library
of possible actions, and LLM reflection via human and environment feedback.
Extensive experiments validate the framework, showcasing robustness,
scalability, and versatility in diverse scenarios, including long-horizon
tasks, tabletop rearrangements, and remote supervisory control. To facilitate
the adoption of our framework and support the reproduction of our results, we
have made our code open-source. You can access it at:
https://github.com/huawei-noah/HEBO/tree/master/ROSLLM.","Christopher E. Mower, Yuhui Wan, Hongzhan Yu, Antoine Grosnit, Jonas Gonzalez-Billandon, Matthieu Zimmer, Jinlong Wang, Xinyu Zhang, Yao Zhao, Anbang Zhai, Puze Liu, Daniel Palenicek, Davide Tateo, Cesar Cadena, Marco Hutter, Jan Peters, Guangjian Tian, Yuzheng Zhuang, Kun Shao, Xingyue Quan, Jianye Hao, Jun Wang, Haitham Bou-Ammar",2024-06-28T08:28:38Z,2024-07-12T11:44:33Z,http://arxiv.org/abs/2406.19741v3,http://arxiv.org/pdf/2406.19741v3.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
RAVE: A Framework for Radar Ego-Velocity Estimation,"State estimation is an essential component of autonomous systems, usually
relying on sensor fusion that integrates data from cameras, LiDARs and IMUs.
Recently, radars have shown the potential to improve the accuracy and
robustness of state estimation and perception, especially in challenging
environmental conditions such as adverse weather and low-light scenarios. In
this paper, we present a framework for ego-velocity estimation, which we call
RAVE, that relies on 3D automotive radar data and encompasses zero velocity
detection, outlier rejection, and velocity estimation. In addition, we propose
a simple filtering method to discard infeasible ego-velocity estimates. We also
conduct a systematic analysis of how different existing outlier rejection
techniques and optimization loss functions impact estimation accuracy. Our
evaluation on three open-source datasets demonstrates the effectiveness of the
proposed filter and a significant positive impact of RAVE on the odometry
accuracy. Furthermore, we release an open-source implementation of the proposed
framework for radar ego-velocity estimation accompanied with a ROS interface.","Vlaho-Josip Štironja, Luka Petrović, Juraj Peršić, Ivan Marković, Ivan Petrović",2024-06-27T02:41:17Z,2024-06-27T02:41:17Z,http://arxiv.org/abs/2406.18850v1,http://arxiv.org/pdf/2406.18850v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Imagining In-distribution States: How Predictable Robot Behavior Can
  Enable User Control Over Learned Policies","It is crucial that users are empowered to take advantage of the functionality
of a robot and use their understanding of that functionality to perform novel
and creative tasks. Given a robot trained with Reinforcement Learning (RL), a
user may wish to leverage that autonomy along with their familiarity of how
they expect the robot to behave to collaborate with the robot. One technique is
for the user to take control of some of the robot's action space through
teleoperation, allowing the RL policy to simultaneously control the rest. We
formalize this type of shared control as Partitioned Control (PC). However,
this may not be possible using an out-of-the-box RL policy. For example, a
user's control may bring the robot into a failure state from the policy's
perspective, causing it to act unexpectedly and hindering the success of the
user's desired task. In this work, we formalize this problem and present
Imaginary Out-of-Distribution Actions, IODA, an initial algorithm which
empowers users to leverage their expectations of a robot's behavior to
accomplish new tasks. We deploy IODA in a user study with a real robot and find
that IODA leads to both better task performance and a higher degree of
alignment between robot behavior and user expectation. We also show that in PC,
there is a strong and significant correlation between task performance and the
robot's ability to meet user expectations, highlighting the need for approaches
like IODA. Code is available at https://github.com/AABL-Lab/ioda_roman_2024","Isaac Sheidlower, Emma Bethel, Douglas Lilly, Reuben M. Aronson, Elaine Schaertl Short",2024-06-19T17:08:28Z,2024-06-19T17:08:28Z,http://arxiv.org/abs/2406.13711v1,http://arxiv.org/pdf/2406.13711v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
ROSfs: A User-Level File System for ROS,"We present ROSfs, a novel user-level file system for the Robot Operating
System (ROS). ROSfs interprets a robot file as a group of sub-files, with each
having a distinct label. ROSfs applies a time index structure to enhance the
flexible data query while the data file is under modification. It provides
multi-robot systems (MRS) with prompt cross-robot data acquisition and
collaboration. We implemented a ROSfs prototype and integrated it into a
mainstream ROS platform. We then applied and evaluated ROSfs on real-world UAVs
and data servers. Evaluation results show that compared with traditional ROS
storage methods, ROSfs improves the offline query performance by up to 129x and
reduces inter-robot online data query latency under a wireless network by up to
7x.","Zijun Xu, Xuanjun Wen, Yanjie Song, Shu Yin",2024-06-15T13:39:20Z,2024-06-15T13:39:20Z,http://arxiv.org/abs/2406.10635v1,http://arxiv.org/pdf/2406.10635v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Voxel Map to Occupancy Map Conversion Using Free Space Projection for
  Efficient Map Representation for Aerial and Ground Robots","This article introduces a novel method for converting 3D voxel maps, commonly
utilized by robots for localization and navigation, into 2D occupancy maps for
both unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs). The
generated 2D maps can be used for more efficient global navigation for both
UAVs and UGVs, in enabling algorithms developed for 2D maps to be useful in 3D
applications, and allowing for faster transfer of maps between multiple agents
in bandwidth-limited scenarios. The proposed method uses the free space
representation in the UFOMap mapping solution to generate 2D occupancy maps.
During the 3D to 2D map conversion, the method conducts safety checks and
eliminates free spaces in the map with dimensions (in the height axis) lower
than the robot's safety margins. This ensures that an aerial or ground robot
can navigate safely, relying primarily on the 2D map generated by the method.
Additionally, the method extracts the height of navigable free space and a
local estimate of the slope of the floor from the 3D voxel map. The height data
is utilized in converting paths generated using the 2D map into paths in 3D
space for both UAVs and UGVs. The slope data identifies areas too steep for a
ground robot to traverse, marking them as occupied, thus enabling a more
accurate representation of the terrain for ground robots. The effectiveness of
the proposed method in enabling computationally efficient navigation for both
aerial and ground robots is validated in two different environments, over both
static maps and in online implementation in an exploration mission. The methods
proposed within this article have been implemented in the popular robotics
framework ROS and are open-sourced. The code is available at:
https://github.com/LTU-RAI/Map-Conversion-3D-Voxel-Map-to-2D-Occupancy-Map.","Scott Fredriksson, Akshit Saradagi, George Nikolakopoulos",2024-06-11T13:55:37Z,2024-07-21T10:38:22Z,http://arxiv.org/abs/2406.07270v2,http://arxiv.org/pdf/2406.07270v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
I2EDL: Interactive Instruction Error Detection and Localization,"In the Vision-and-Language Navigation in Continuous Environments (VLN-CE)
task, the human user guides an autonomous agent to reach a target goal via a
series of low-level actions following a textual instruction in natural
language. However, most existing methods do not address the likely case where
users may make mistakes when providing such instruction (e.g. ""turn left""
instead of ""turn right""). In this work, we address a novel task of Interactive
VLN in Continuous Environments (IVLN-CE), which allows the agent to interact
with the user during the VLN-CE navigation to verify any doubts regarding the
instruction errors. We propose an Interactive Instruction Error Detector and
Localizer (I2EDL) that triggers the user-agent interaction upon the detection
of instruction errors during the navigation. We leverage a pre-trained module
to detect instruction errors and pinpoint them in the instruction by
cross-referencing the textual input and past observations. In such way, the
agent is able to query the user for a timely correction, without demanding the
user's cognitive load, as we locate the probable errors to a precise part of
the instruction. We evaluate the proposed I2EDL on a dataset of instructions
containing errors, and further devise a novel metric, the Success weighted by
Interaction Number (SIN), to reflect both the navigation performance and the
interaction effectiveness. We show how the proposed method can ask focused
requests for corrections to the user, which in turn increases the navigation
success, while minimizing the interactions.","Francesco Taioli, Stefano Rosa, Alberto Castellini, Lorenzo Natale, Alessio Del Bue, Alessandro Farinelli, Marco Cristani, Yiming Wang",2024-06-07T16:52:57Z,2024-06-23T22:58:46Z,http://arxiv.org/abs/2406.05080v2,http://arxiv.org/pdf/2406.05080v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Experimental Evaluation of ROS-Causal in Real-World Human-Robot Spatial
  Interaction Scenarios","Deploying robots in human-shared environments requires a deep understanding
of how nearby agents and objects interact. Employing causal inference to model
cause-and-effect relationships facilitates the prediction of human behaviours
and enables the anticipation of robot interventions. However, a significant
challenge arises due to the absence of implementation of existing causal
discovery methods within the ROS ecosystem, the standard de-facto framework in
robotics, hindering effective utilisation on real robots. To bridge this gap,
in our previous work we proposed ROS-Causal, a ROS-based framework designed for
onboard data collection and causal discovery in human-robot spatial
interactions. In this work, we present an experimental evaluation of ROS-Causal
both in simulation and on a new dataset of human-robot spatial interactions in
a lab scenario, to assess its performance and effectiveness. Our analysis
demonstrates the efficacy of this approach, showcasing how causal models can be
extracted directly onboard by robots during data collection. The online causal
models generated from the simulation are consistent with those from lab
experiments. These findings can help researchers to enhance the performance of
robotic systems in shared environments, firstly by studying the causal
relations between variables in simulation without real people, and then
facilitating the actual robot deployment in real human environments.
ROS-Causal: https://lcastri.github.io/roscausal","Luca Castri, Gloria Beraldo, Sariah Mghames, Marc Hanheide, Nicola Bellotto",2024-06-07T14:20:30Z,2024-06-07T14:20:30Z,http://arxiv.org/abs/2406.04955v1,http://arxiv.org/pdf/2406.04955v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Advancing The Robotics Software Development Experience: Bridging Julia's
  Performance and Python's Ecosystem","Robotics programming typically involves a trade-off between the ease of use
offered by Python and the run-time performance of C++. While multi-language
architectures address this trade-off by coupling Python's ergonomics with C++'s
speed, they introduce complexity at the language interface. This paper proposes
using Julia for performance-critical tasks within Python ROS 2 applications,
providing an elegant solution that streamlines the development process without
disrupting the existing Python workflow.","Gustavo Nunes Goretkin, Joseph Carpinelli, Andy Park",2024-06-06T01:41:12Z,2024-06-06T01:41:12Z,http://arxiv.org/abs/2406.03677v1,http://arxiv.org/pdf/2406.03677v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"An Open and Reconfigurable User Interface to Manage Complex ROS-based
  Robotic Systems","The Robot Operating System (ROS) has significantly gained popularity among
robotic engineers and researchers over the past five years, primarily due to
its powerful infrastructure for node communication, which enables developers to
build modular and large robotic applications. However, ROS presents a steep
learning curve and lacks the intuitive usability of vendor-specific robotic
Graphical User Interfaces (GUIs). Moreover, its modular and distributed nature
complicates the control and monitoring of extensive systems, even for advanced
users. To address these challenges, this paper proposes a highly adaptable and
reconfigurable web-based GUI for intuitively controlling, monitoring, and
configuring complex ROS-based robotic systems. The GUI leverages ROSBridge and
roslibjs to ensure seamless communication with ROS systems via topics and
services. Designed as a versatile platform, the GUI allows for the selective
incorporation of modular features to accommodate diverse robotic systems and
applications. An initial set of commonly used features in robotic applications
is presented. To demonstrate its reconfigurability, the GUI was customized and
tested for four industrial use cases, receiving positive feedback. The
project's repository has been made publicly available to support the robotics
community and lower the entry barrier for ROS in industrial applications.","Pablo Malvido Fresnillo, Saigopal Vasudevan, Jose A. Perez Garcia, Jose L. Martinez Lastra",2024-06-04T11:07:31Z,2024-06-04T11:07:31Z,http://arxiv.org/abs/2406.02210v1,http://arxiv.org/pdf/2406.02210v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"A Robust Filter for Marker-less Multi-person Tracking in Human-Robot
  Interaction Scenarios","Pursuing natural and marker-less human-robot interaction (HRI) has been a
long-standing robotics research focus, driven by the vision of seamless
collaboration without physical markers. Marker-less approaches promise an
improved user experience, but state-of-the-art struggles with the challenges
posed by intrinsic errors in human pose estimation (HPE) and depth cameras.
These errors can lead to issues such as robot jittering, which can
significantly impact the trust users have in collaborative systems. We propose
a filtering pipeline that refines incomplete 3D human poses from an HPE
backbone and a single RGB-D camera to address these challenges, solving for
occlusions that can degrade the interaction. Experimental results show that
using the proposed filter leads to more consistent and noise-free motion
representation, reducing unexpected robot movements and enabling smoother
interaction.","Enrico Martini, Harshil Parekh, Shaoting Peng, Nicola Bombieri, Nadia Figueroa",2024-06-03T22:59:53Z,2024-06-03T22:59:53Z,http://arxiv.org/abs/2406.01832v1,http://arxiv.org/pdf/2406.01832v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"The use of a humanoid robot for older people with dementia in aged care
  facilities","This paper presents an interdisciplinary PhD project using a humanoid robot
to encourage interactive activities for people with dementia living in two aged
care facilities. The aim of the project was to develop software and use
technologies to achieve successful robot-led engagement with older people with
dementia. This paper outlines the qualitative findings from the project's
feasibility stage. The researcher's observations, the participants' attitudes
and the feedback from carers are presented and discussed.","Dongjun Wu, Lihui Pu, Jun Jo, Rene Hexel, Wendy Moyle",2024-05-30T02:33:09Z,2024-05-30T02:33:09Z,http://arxiv.org/abs/2405.19630v1,http://arxiv.org/pdf/2405.19630v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Photorealistic Robotic Simulation using Unreal Engine 5 for Agricultural
  Applications","This work presents a new robotics simulation environment built upon Unreal
Engine 5 (UE5) for agricultural image data generation. The simulation utilizes
the state-of-the-art real-time rendering engine to provide realistic plant
images which are often used in agricultural applications. This study showcases
the rendering accuracy of UE5 in comparison to existing tools and assesses its
positional accuracy when integrated with Robot Operating Systems (ROS). The
results indicate that UE5 achieves an impressive average distance error of
0.021mm when compared to predetermined setpoints in a multi-robot setup
involving two UR10 arms.","Xingjian Li, Lirong Xiang",2024-05-28T19:40:27Z,2024-05-28T19:40:27Z,http://arxiv.org/abs/2405.18551v1,http://arxiv.org/pdf/2405.18551v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"""Pass the butter"": A study on desktop-classic multitasking robotic arm
  based on advanced YOLOv7 and BERT","In recent years, various intelligent autonomous robots have begun to appear
in daily life and production. Desktop-level robots are characterized by their
flexible deployment, rapid response, and suitability for light workload
environments. In order to meet the current societal demand for service robot
technology, this study proposes using a miniaturized desktop-level robot (by
ROS) as a carrier, locally deploying a natural language model (NLP-BERT), and
integrating visual recognition (CV-YOLO) and speech recognition technology
(ASR-Whisper) as inputs to achieve autonomous decision-making and rational
action by the desktop robot. Three comprehensive experiments were designed to
validate the robotic arm, and the results demonstrate excellent performance
using this approach across all three experiments. In Task 1, the execution
rates for speech recognition and action performance were 92.6% and 84.3%,
respectively. In Task 2, the highest execution rates under the given conditions
reached 92.1% and 84.6%, while in Task 3, the highest execution rates were
95.2% and 80.8%, respectively. Therefore, it can be concluded that the proposed
solution integrating ASR, NLP, and other technologies on edge devices is
feasible and provides a technical and engineering foundation for realizing
multimodal desktop-level robots.","Haohua Que, Wenbin Pan, Jie Xu, Hao Luo, Pei Wang, Li Zhang",2024-05-27T15:06:03Z,2024-05-27T15:06:03Z,http://arxiv.org/abs/2405.17250v1,http://arxiv.org/pdf/2405.17250v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
Automated Parking Planning with Vision-Based BEV Approach,"Automated Valet Parking (AVP) is a crucial component of advanced autonomous
driving systems, focusing on the endpoint task within the ""human-vehicle
interaction"" process to tackle the challenges of the ""last mile"".The perception
module of the automated parking algorithm has evolved from local perception
using ultrasonic radar and global scenario precise map matching for
localization to a high-level map-free Birds Eye View (BEV) perception
solution.The BEV scene places higher demands on the real-time performance and
safety of automated parking planning tasks. This paper proposes an improved
automated parking algorithm based on the A* algorithm, integrating vehicle
kinematic models, heuristic function optimization, bidirectional search, and
Bezier curve optimization to enhance the computational speed and real-time
capabilities of the planning algorithm.Numerical optimization methods are
employed to generate the final parking trajectory, ensuring the safety of the
parking path. The proposed approach is experimentally validated in the commonly
used industrial CARLA-ROS joint simulation environment. Compared to traditional
algorithms, this approach demonstrates reduced computation time with more
challenging collision-risk test cases and improved performance in comfort
metrics.",Yuxuan Zhao,2024-05-24T15:26:09Z,2024-05-24T15:26:09Z,http://arxiv.org/abs/2406.15430v1,http://arxiv.org/pdf/2406.15430v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Towards Privacy-Aware and Personalised Assistive Robots: A User-Centred
  Approach","The global increase in the elderly population necessitates innovative
long-term care solutions to improve the quality of life for vulnerable
individuals while reducing caregiver burdens. Assistive robots, leveraging
advancements in Machine Learning, offer promising personalised support.
However, their integration into daily life raises significant privacy concerns.
Widely used frameworks like the Robot Operating System (ROS) historically lack
inherent privacy mechanisms, complicating data-driven approaches in robotics.
This research pioneers user-centric, privacy-aware technologies such as
Federated Learning (FL) to advance assistive robotics. FL enables collaborative
learning without sharing sensitive data, addressing privacy and scalability
issues. This work includes developing solutions for smart wheelchair
assistance, enhancing user independence and well-being. By tackling challenges
related to non-stationary data and heterogeneous environments, the research
aims to improve personalisation and user experience. Ultimately, it seeks to
lead the responsible integration of assistive robots into society, enhancing
the quality of life for elderly and care-dependent individuals.",Fernando E. Casado,2024-05-23T13:14:08Z,2024-05-23T13:14:08Z,http://arxiv.org/abs/2405.14528v1,http://arxiv.org/pdf/2405.14528v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Towards Using Fast Embedded Model Predictive Control for Human-Aware
  Predictive Robot Navigation","Predictive planning is a key capability for robots to efficiently and safely
navigate populated environments. Particularly in densely crowded scenes, with
uncertain human motion predictions, predictive path planning, and control can
become expensive to compute in real time due to the curse of dimensionality.
With the goal of achieving pro-active and legible robot motion in shared
environments, in this paper we present HuMAN-MPC, a computationally efficient
algorithm for Human Motion Aware Navigation using fast embedded Model
Predictive Control. The approach consists of a novel model predictive control
(MPC) formulation that leverages a fast state-of-the-art optimization backend
based on a sequential quadratic programming real-time iteration scheme while
also providing feasibility monitoring. Our experiments, in simulation and on a
fully integrated ROS-based platform, show that the approach achieves great
scalability with fast computation times without penalizing path quality and
efficiency of the resulting avoidance behavior.","Till Hielscher, Lukas Heuer, Frederik Wulle, Luigi Palmieri",2024-05-21T09:12:16Z,2024-05-21T09:12:16Z,http://arxiv.org/abs/2405.12616v1,http://arxiv.org/pdf/2405.12616v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"EdgeLoc: A Communication-Adaptive Parallel System for Real-Time
  Localization in Infrastructure-Assisted Autonomous Driving","This paper presents EdgeLoc, an infrastructure-assisted, real-time
localization system for autonomous driving that addresses the incompatibility
between traditional localization methods and deep learning approaches. The
system is built on top of the Robot Operating System (ROS) and combines the
real-time performance of traditional methods with the high accuracy of deep
learning approaches. The system leverages edge computing capabilities of
roadside units (RSUs) for precise localization to enhance on-vehicle
localization that is based on the real-time visual odometry. EdgeLoc is a
parallel processing system, utilizing a proposed uncertainty-aware pose fusion
solution. It achieves communication adaptivity through online learning and
addresses fluctuations via window-based detection. Moreover, it achieves
optimal latency and maximum improvement by utilizing auto-splitting
vehicle-infrastructure collaborative inference, as well as online distribution
learning for decision-making. Even with the most basic end-to-end deep neural
network for localization estimation, EdgeLoc realizes a 67.75\% reduction in
the localization error for real-time local visual odometry, a 29.95\% reduction
for non-real-time collaborative inference, and a 30.26\% reduction compared to
Kalman filtering. Finally, accuracy-to-latency conversion was experimentally
validated, and an overall experiment was conducted on a practical cellular
network. The system is open sourced at
https://github.com/LoganCome/EdgeAssistedLocalization.","Boyi Liu, Jingwen Tong, Yufan Zhuang",2024-05-20T15:38:53Z,2024-06-08T06:55:32Z,http://arxiv.org/abs/2405.12120v2,http://arxiv.org/pdf/2405.12120v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
Reward-Punishment Reinforcement Learning with Maximum Entropy,"We introduce the ``soft Deep MaxPain'' (softDMP) algorithm, which integrates
the optimization of long-term policy entropy into reward-punishment
reinforcement learning objectives. Our motivation is to facilitate a smoother
variation of operators utilized in the updating of action values beyond
traditional ``max'' and ``min'' operators, where the goal is enhancing sample
efficiency and robustness. We also address two unresolved issues from the
previous Deep MaxPain method. Firstly, we investigate how the negated
(``flipped'') pain-seeking sub-policy, derived from the punishment action
value, collaborates with the ``min'' operator to effectively learn the
punishment module and how softDMP's smooth learning operator provides insights
into the ``flipping'' trick. Secondly, we tackle the challenge of data
collection for learning the punishment module to mitigate inconsistencies
arising from the involvement of the ``flipped'' sub-policy (pain-avoidance
sub-policy) in the unified behavior policy. We empirically explore the first
issue in two discrete Markov Decision Process (MDP) environments, elucidating
the crucial advancements of the DMP approach and the necessity for soft
treatments on the hard operators. For the second issue, we propose a
probabilistic classifier based on the ratio of the pain-seeking sub-policy to
the sum of the pain-seeking and goal-reaching sub-policies. This classifier
assigns roll-outs to separate replay buffers for updating reward and punishment
action-value functions, respectively. Our framework demonstrates superior
performance in Turtlebot 3's maze navigation tasks under the ROS Gazebo
simulation.","Jiexin Wang, Eiji Uchibe",2024-05-20T05:05:14Z,2024-05-20T05:05:14Z,http://arxiv.org/abs/2405.11784v1,http://arxiv.org/pdf/2405.11784v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Advancements in Gravity Compensation and Control for the da Vinci
  Surgical Robot","This research delves into the enhancement of control mechanisms for the da
Vinci Surgical System, focusing on the implementation of gravity compensation
and refining the modeling of the master and patient side manipulators.
Leveraging the Robot Operating System (ROS) the study aimed to fortify the
precision and stability of the robots movements essential for intricate
surgical procedures. Through rigorous parameter identification and the Euler
Lagrange approach the team successfully derived the necessary torque equations
and established a robust mathematical model. Implementation of the actual robot
and simulation in Gazebo highlighted the efficacy of the developed control
strategies facilitating accurate positioning and minimizing drift.
Additionally, the project extended its contributions by constructing a
comprehensive model for the patient side manipulator laying the groundwork for
future research endeavors. This work signifies a significant advancement in the
pursuit of enhanced precision and user control in robotic assisted surgeries.
  NOTE - This work has been submitted to the IEEE for publication. Copyright
may be transferred without notice, after which this version may no longer be
accessible. Copyright on this article is reserved by IEEE",Ankit Shaw,2024-05-17T23:02:20Z,2024-08-01T15:40:40Z,http://arxiv.org/abs/2405.11114v2,http://arxiv.org/pdf/2405.11114v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
Fast LiDAR Upsampling using Conditional Diffusion Models,"The search for refining 3D LiDAR data has attracted growing interest
motivated by recent techniques such as supervised learning or generative
model-based methods. Existing approaches have shown the possibilities for using
diffusion models to generate refined LiDAR data with high fidelity, although
the performance and speed of such methods have been limited. These limitations
make it difficult to execute in real-time, causing the approaches to struggle
in real-world tasks such as autonomous navigation and human-robot interaction.
In this work, we introduce a novel approach based on conditional diffusion
models for fast and high-quality sparse-to-dense upsampling of 3D scene point
clouds through an image representation. Our method employs denoising diffusion
probabilistic models trained with conditional inpainting masks, which have been
shown to give high performance on image completion tasks. We introduce a series
of experiments, including multiple datasets, sampling steps, and conditional
masks. This paper illustrates that our method outperforms the baselines in
sampling speed and quality on upsampling tasks using the KITTI-360 dataset.
Furthermore, we illustrate the generalization ability of our approach by
simultaneously training on real-world and synthetic datasets, introducing
variance in quality and environments.","Sander Elias Magnussen Helgesen, Kazuto Nakashima, Jim Tørresen, Ryo Kurazume",2024-05-08T08:38:28Z,2024-07-23T06:51:06Z,http://arxiv.org/abs/2405.04889v2,http://arxiv.org/pdf/2405.04889v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
Semi-autonomous Robotic Disassembly Enhanced by Mixed Reality,"In this study, we introduce ""SARDiM,"" a modular semi-autonomous platform
enhanced with mixed reality for industrial disassembly tasks. Through a case
study focused on EV battery disassembly, SARDiM integrates Mixed Reality,
object segmentation, teleoperation, force feedback, and variable autonomy.
Utilising the ROS, Unity, and MATLAB platforms, alongside a joint impedance
controller, SARDiM facilitates teleoperated disassembly. The approach combines
FastSAM for real-time object segmentation, generating data which is
subsequently processed through a cluster analysis algorithm to determine the
centroid and orientation of the components, categorizing them by size and
disassembly priority. This data guides the MoveIt platform in trajectory
planning for the Franka Robot arm. SARDiM provides the capability to switch
between two teleoperation modes: manual and semi-autonomous with variable
autonomy. Each was evaluated using four different Interface Methods (IM):
direct view, monitor feed, mixed reality with monitor feed, and point cloud
mixed reality. Evaluations across the eight IMs demonstrated a 40.61% decrease
in joint limit violations using Mode 2. Moreover, Mode 2-IM4 outperformed Mode
1-IM1 by achieving a 2.33%-time reduction while considerably increasing safety,
making it optimal for operating in hazardous environments at a safe distance,
with the same ease of use as teleoperation with a direct view of the
environment.","Alireza Rastegarpanah, Cesar Alan Contreras, Rustam Stolkin",2024-05-06T14:47:40Z,2024-05-06T14:47:40Z,http://arxiv.org/abs/2405.03530v1,http://arxiv.org/pdf/2405.03530v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"ROS 2 on a Chip, Achieving Brain-Like Speeds and Efficiency in Robotic
  Networking","The Robot Operating System (ROS) pubsub model played a pivotal role in
developing sophisticated robotic applications. However, the complexities and
real-time demands of modern robotics necessitate more efficient communication
solutions that are deterministic and isochronous. This article introduces a
groundbreaking approach: embedding ROS 2 message-passing infrastructure
directly onto a specialized hardware chip, significantly enhancing speed and
efficiency in robotic communications. Our FPGA prototypes of the chip design
can send or receive packages in less than 2.5 microseconds, accelerating
networking communications by more than 62x on average and improving energy
consumption by more than 500x when compared to traditional ROS 2 software
implementations on modern CPUs. Additionally, it dramatically reduces maximum
latency in ROS 2 networking communication by more than 30,000x. In situations
of peak latency, our design guarantees an isochronous response within 11
microseconds, a stark improvement over the potential hundreds of milliseconds
reported by modern CPU systems under similar conditions.","Víctor Mayoral-Vilches, Juan Manuel Reina-Muñoz, Martiño Crespo-Álvarez, David Mayoral-Vilches",2024-04-28T15:03:49Z,2024-04-28T15:03:49Z,http://arxiv.org/abs/2404.18208v1,http://arxiv.org/pdf/2404.18208v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
Angle-Aware Coverage with Camera Rotational Motion Control,"This paper presents a novel control strategy for drone networks to improve
the quality of 3D structures reconstructed from aerial images by drones. Unlike
the existing coverage control strategies for this purpose, our proposed
approach simultaneously controls both the camera orientation and drone
translational motion, enabling more comprehensive perspectives and enhancing
the map's overall quality. Subsequently, we present a novel problem
formulation, including a new performance function to evaluate the drone
positions and camera orientations. We then design a QP-based controller with a
control barrier-like function for a constraint on the decay rate of the
objective function. The present problem formulation poses a new challenge,
requiring significantly greater computational efforts than the case involving
only translational motion control. We approach this issue technologically,
namely by introducing JAX, utilizing just-in-time (JIT) compilation and
Graphical Processing Unit (GPU) acceleration. We finally conduct extensive
verifications through simulation in ROS (Robot Operating System) and show the
real-time feasibility of the controller and the superiority of the present
controller to the conventional method.","Zhiyuan Lu, Muhammad Hanif, Takumi Shimizu, Takeshi Hatanaka",2024-04-22T06:53:13Z,2024-04-22T06:53:13Z,http://arxiv.org/abs/2404.13915v1,http://arxiv.org/pdf/2404.13915v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
Cloud-based Digital Twin for Cognitive Robotics,"The paper presents a novel cloud-based digital twin learning platform for
teaching and training concepts of cognitive robotics. Instead of forcing
interested learners or students to install a new operating system and bulky,
fragile software onto their personal laptops just to solve tutorials or coding
assignments of a single lecture on robotics, it would be beneficial to avoid
technical setups and directly dive into the content of cognitive robotics. To
achieve this, the authors utilize containerization technologies and Kubernetes
to deploy and operate containerized applications, including robotics simulation
environments and software collections based on the Robot operating System
(ROS). The web-based Integrated Development Environment JupyterLab is
integrated with RvizWeb and XPRA to provide real-time visualization of sensor
data and robot behavior in a user-friendly environment for interacting with
robotics software. The paper also discusses the application of the platform in
teaching Knowledge Representation, Reasoning, Acquisition and Retrieval, and
Task-Executives. The authors conclude that the proposed platform is a valuable
tool for education and research in cognitive robotics, and that it has the
potential to democratize access to these fields. The platform has already been
successfully employed in various academic courses, demonstrating its
effectiveness in fostering knowledge and skill development.","Arthur Niedźwiecki, Sascha Jongebloed, Yanxiang Zhan, Michaela Kümpel, Jörn Syrbe, Michael Beetz",2024-04-19T14:31:01Z,2024-04-19T14:31:01Z,http://arxiv.org/abs/2404.12909v1,http://arxiv.org/pdf/2404.12909v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
AutoInspect: Towards Long-Term Autonomous Industrial Inspection,"We give an overview of AutoInspect, a ROS-based software system for robust
and extensible mission-level autonomy. Over the past three years AutoInspect
has been deployed in a variety of environments, including at a mine, a chemical
plant, a mock oil rig, decommissioned nuclear power plants, and a fusion
reactor for durations ranging from hours to weeks. The system combines robust
mapping and localisation with graph-based autonomous navigation, mission
execution, and scheduling to achieve a complete autonomous inspection system.
The time from arrival at a new site to autonomous mission execution can be
under an hour. It is deployed on a Boston Dynamics Spot robot using a custom
sensing and compute payload called Frontier. In this work we go into detail of
the system's performance in two long-term deployments of 49 days at a robotics
test facility, and 35 days at the Joint European Torus (JET) fusion reactor in
Oxfordshire, UK.","Michal Staniaszek, Tobit Flatscher, Joseph Rowell, Hanlin Niu, Wenxing Liu, Yang You, Robert Skilton, Maurice Fallon, Nick Hawes",2024-04-19T10:49:33Z,2024-04-23T12:45:15Z,http://arxiv.org/abs/2404.12785v3,http://arxiv.org/pdf/2404.12785v3.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"A Containerized Microservice Architecture for a ROS 2 Autonomous Driving
  Software: An End-to-End Latency Evaluation","The automotive industry is transitioning from traditional ECU-based systems
to software-defined vehicles. A central role of this revolution is played by
containers, lightweight virtualization technologies that enable the flexible
consolidation of complex software applications on a common hardware platform.
Despite their widespread adoption, the impact of containerization on
fundamental real-time metrics such as end-to-end latency, communication jitter,
as well as memory and CPU utilization has remained virtually unexplored. This
paper presents a microservice architecture for a real-world autonomous driving
application where containers isolate each service. Our comprehensive evaluation
shows the benefits in terms of end-to-end latency of such a solution even over
standard bare-Linux deployments. Specifically, in the case of the presented
microservice architecture, the mean end-to-end latency can be improved by 5-8
%. Also, the maximum latencies were significantly reduced using container
deployment.","Tobias Betz, Long Wen, Fengjunjie Pan, Gemb Kaljavesi, Alexander Zuepke, Andrea Bastoni, Marco Caccamo, Alois Knoll, Johannes Betz",2024-04-19T07:33:45Z,2024-04-19T07:33:45Z,http://arxiv.org/abs/2404.12683v1,http://arxiv.org/pdf/2404.12683v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Runtime Verification and Field-based Testing for ROS-based Robotic
  Systems","Robotic systems are becoming pervasive and adopted in increasingly many
domains, such as manufacturing, healthcare, and space exploration. To this end,
engineering software has emerged as a crucial discipline for building
maintainable and reusable robotic systems. The robotics software engineering
research field has received increasing attention, fostering autonomy as a
fundamental goal. However, robotics developers are still challenged to achieve
this goal because simulation cannot realistically deliver solutions to emulate
real-world phenomena. Robots also need to operate in unpredictable and
uncontrollable environments, which require safe and trustworthy self-adaptation
capabilities implemented in software. Typical techniques to address the
challenges are runtime verification, field-based testing, and mitigation
techniques that enable fail-safe solutions. However, no clear guidance exists
for architecting ROS-based systems to enable and facilitate runtime
verification and field-based testing. This paper aims to fill this gap by
providing guidelines to help developers and quality assurance (QA) teams
develop, verify, or test their robots in the field. These guidelines are
carefully tailored to address the challenges and requirements of testing
robotics systems in real-world scenarios. We conducted (i) a literature review
on studies addressing runtime verification and field-based testing for robotic
systems, (ii) mined ROS-based applications repositories, and (iii) validated
the applicability, clarity, and usefulness via two questionnaires with 55
answers overall. We contribute 20 guidelines: 8 for developers and 12 for QA
teams formulated for researchers and practitioners in robotic software
engineering. Finally, we map our guidelines to open challenges in runtime
verification and field-based testing for ROS-based systems, and we outline
promising research directions in the field.","Ricardo Caldas, Juan Antonio Pinera Garcia, Matei Schiopu, Patrizio Pelliccione, Genaina Rodrigues, Thorsten Berger",2024-04-17T15:52:29Z,2024-08-21T06:21:06Z,http://arxiv.org/abs/2404.11498v3,http://arxiv.org/pdf/2404.11498v3.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
Learning Social Navigation from Demonstrations with Deep Neural Networks,"Traditional path-planning techniques treat humans as obstacles. This has
changed since robots started to enter human environments. On modern robots,
social navigation has become an important aspect of navigation systems. To use
learning-based techniques to achieve social navigation, a powerful framework
that is capable of representing complex functions with as few data as possible
is required. In this study, we benefited from recent advances in deep learning
at both global and local planning levels to achieve human-aware navigation on a
simulated robot. Two distinct deep models are trained with respective
objectives: one for global planning and one for local planning. These models
are then employed in the simulated robot. In the end, it has been shown that
our model can successfully carry out both global and local planning tasks. We
have shown that our system could generate paths that successfully reach targets
while avoiding obstacles with better performance compared to feed-forward
neural networks.","Yigit Yildirim, Emre Ugur",2024-04-17T10:51:36Z,2024-04-17T10:51:36Z,http://arxiv.org/abs/2404.11246v1,http://arxiv.org/pdf/2404.11246v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"PAAM: A Framework for Coordinated and Priority-Driven Accelerator
  Management in ROS 2","This paper proposes a Priority-driven Accelerator Access Management (PAAM)
framework for multi-process robotic applications built on top of the Robot
Operating System (ROS) 2 middleware platform. The framework addresses the issue
of predictable execution of time- and safety-critical callback chains that
require hardware accelerators such as GPUs and TPUs. PAAM provides a standalone
ROS executor that acts as an accelerator resource server, arbitrating
accelerator access requests from all other callbacks at the application layer.
This approach enables coordinated and priority-driven accelerator access
management in multi-process robotic systems. The framework design is directly
applicable to all types of accelerators and enables granular control over how
specific chains access accelerators, making it possible to achieve predictable
real-time support for accelerators used by safety-critical callback chains
without making changes to underlying accelerator device drivers. The paper
shows that PAAM also offers a theoretical analysis that can upper bound the
worst-case response time of safety-critical callback chains that necessitate
accelerator access. This paper also demonstrates that complex robotic systems
with extensive accelerator usage that are integrated with PAAM may achieve up
to a 91\% reduction in end-to-end response time of their critical callback
chains.","Daniel Enright, Yecheng Xiang, Hyunjong Choi, Hyoseung Kim",2024-04-09T16:53:52Z,2024-04-09T16:53:52Z,http://arxiv.org/abs/2404.06452v1,http://arxiv.org/pdf/2404.06452v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
ARS548_ros. An ARS 548 RDI radar driver for ROS,"The ARS 548 RDI Radar is a premium model of the fifth generation of 77 GHz
long range radar sensors with new RF antenna arrays, which offer digital beam
forming. This radar measures independently the distance, speed and angle of
objects without any reflectors in one measurement cycle based on Pulse
Compression with New Frequency Modulation. Unfortunately, to the best of our
knowledge, there are no open source drivers available for Linux systems to
enable users to analyze the data acquired by the sensor. In this paper, we
present a driver that can interpret the data from the ARS 548 RDI sensor and
make it available over the Robot Operating System versions 1 and 2 (ROS and
ROS2). Thus, these data can be stored, represented, and analyzed using the
powerful tools offered by ROS. Besides, our driver offers advanced object
features provided by the sensor, such as relative estimated velocity and
acceleration of each object, its orientation and angular velocity. We focus on
the configuration of the sensor and the use of our driver including its
filtering and representation tools. Besides, we offer a video tutorial to help
in its configuration process. Finally, a dataset acquired with this sensor and
an Ouster OS1-32 LiDAR sensor, to have baseline measurements, is available, so
that the user can check the correctness of our driver.","Fernando Fernández-Calatayud, Lucía Coto-Elena, David Alejo, José J. Carpio-Jiménez, Fernando Caballero, Luis Merino",2024-04-06T10:57:57Z,2025-03-05T10:53:24Z,http://arxiv.org/abs/2404.04589v4,http://arxiv.org/pdf/2404.04589v4.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
Modeling social interaction dynamics using temporal graph networks,"Integrating intelligent systems, such as robots, into dynamic group settings
poses challenges due to the mutual influence of human behaviors and internal
states. A robust representation of social interaction dynamics is essential for
effective human-robot collaboration. Existing approaches often narrow their
focus to facial expressions or speech, overlooking the broader context. We
propose employing an adapted Temporal Graph Networks to comprehensively
represent social interaction dynamics while enabling its practical
implementation. Our method incorporates temporal multi-modal behavioral data
including gaze interaction, voice activity and environmental context. This
representation of social interaction dynamics is trained as a link prediction
problem using annotated gaze interaction data. The F1-score outperformed the
baseline model by 37.0%. This improvement is consistent for a secondary task of
next speaker prediction which achieves an improvement of 29.0%. Our
contributions are two-fold, including a model to representing social
interaction dynamics which can be used for many downstream human-robot
interaction tasks like human state inference and next speaker prediction. More
importantly, this is achieved using a more concise yet efficient message
passing method, significantly reducing it from 768 to 14 elements, while
outperforming the baseline model.","J. Taery Kim, Archit Naik, Isuru Jayarathne, Sehoon Ha, Jouh Yeong Chew",2024-04-05T10:00:16Z,2024-04-05T10:00:16Z,http://arxiv.org/abs/2404.06611v1,http://arxiv.org/pdf/2404.06611v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
ROBUST: 221 Bugs in the Robot Operating System,"As robotic systems such as autonomous cars and delivery drones assume greater
roles and responsibilities within society, the likelihood and impact of
catastrophic software failure within those systems is increased.To aid
researchers in the development of new methods to measure and assure the safety
and quality of robotics software, we systematically curated a dataset of 221
bugs across 7 popular and diverse software systems implemented via the Robot
Operating System (ROS). We produce historically accurate recreations of each of
the 221 defective software versions in the form of Docker images, and use a
grounded theory approach to examine and categorize their corresponding faults,
failures, and fixes. Finally, we reflect on the implications of our findings
and outline future research directions for the community.","Christopher S. Timperley, Gijs van der Hoorn, André Santos, Harshavardhan Deshpande, Andrzej Wąsowski",2024-04-04T17:49:38Z,2024-04-04T17:49:38Z,http://arxiv.org/abs/2404.03629v1,http://arxiv.org/pdf/2404.03629v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Integrating Large Language Models with Multimodal Virtual Reality
  Interfaces to Support Collaborative Human-Robot Construction Work","In the construction industry, where work environments are complex,
unstructured and often dangerous, the implementation of Human-Robot
Collaboration (HRC) is emerging as a promising advancement. This underlines the
critical need for intuitive communication interfaces that enable construction
workers to collaborate seamlessly with robotic assistants. This study
introduces a conversational Virtual Reality (VR) interface integrating
multimodal interaction to enhance intuitive communication between construction
workers and robots. By integrating voice and controller inputs with the Robot
Operating System (ROS), Building Information Modeling (BIM), and a game engine
featuring a chat interface powered by a Large Language Model (LLM), the
proposed system enables intuitive and precise interaction within a VR setting.
Evaluated by twelve construction workers through a drywall installation case
study, the proposed system demonstrated its low workload and high usability
with succinct command inputs. The proposed multimodal interaction system
suggests that such technological integration can substantially advance the
integration of robotic assistants in the construction industry.","Somin Park, Carol C. Menassa, Vineet R. Kamat",2024-04-04T14:56:41Z,2024-04-04T14:56:41Z,http://arxiv.org/abs/2404.03498v1,http://arxiv.org/pdf/2404.03498v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
Coordinated Multi-Robot Navigation with Formation Adaptation,"Coordinated multi-robot navigation is an essential ability for a team of
robots operating in diverse environments. Robot teams often need to maintain
specific formations, such as wedge formations, to enhance visibility,
positioning, and efficiency during fast movement. However, complex environments
such as narrow corridors challenge rigid team formations, which makes effective
formation control difficult in real-world environments. To address this
challenge, we introduce a novel Adaptive Formation with Oscillation Reduction
(AFOR) approach to improve coordinated multi-robot navigation. We develop AFOR
under the theoretical framework of hierarchical learning and integrate a
spring-damper model with hierarchical learning to enable both team coordination
and individual robot control. At the upper level, a graph neural network
facilitates formation adaptation and information sharing among the robots. At
the lower level, reinforcement learning enables each robot to navigate and
avoid obstacles while maintaining the formations. We conducted extensive
experiments using Gazebo in the Robot Operating System (ROS), a high-fidelity
Unity3D simulator with ROS, and real robot teams. Results demonstrate that AFOR
enables smooth navigation with formation adaptation in complex scenarios and
outperforms previous methods. More details of this work are provided on the
project website: https://hcrlab.gitlab.io/project/afor.","Zihao Deng, Peng Gao, Williard Joshua Jose, Christopher Reardon, Maggie Wigness, John Rogers, Hao Zhang",2024-04-02T03:44:25Z,2025-03-03T21:48:03Z,http://arxiv.org/abs/2404.01618v2,http://arxiv.org/pdf/2404.01618v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"PhysicsAssistant: An LLM-Powered Interactive Learning Robot for Physics
  Lab Investigations","Robot systems in education can leverage Large language models' (LLMs) natural
language understanding capabilities to provide assistance and facilitate
learning. This paper proposes a multimodal interactive robot (PhysicsAssistant)
built on YOLOv8 object detection, cameras, speech recognition, and chatbot
using LLM to provide assistance to students' physics labs. We conduct a user
study on ten 8th-grade students to empirically evaluate the performance of
PhysicsAssistant with a human expert. The Expert rates the assistants'
responses to student queries on a 0-4 scale based on Bloom's taxonomy to
provide educational support. We have compared the performance of
PhysicsAssistant (YOLOv8+GPT-3.5-turbo) with GPT-4 and found that the human
expert rating of both systems for factual understanding is the same. However,
the rating of GPT-4 for conceptual and procedural knowledge (3 and 3.2 vs 2.2
and 2.6, respectively) is significantly higher than PhysicsAssistant (p <
0.05). However, the response time of GPT-4 is significantly higher than
PhysicsAssistant (3.54 vs 1.64 sec, p < 0.05). Hence, despite the relatively
lower response quality of PhysicsAssistant than GPT-4, it has shown potential
for being used as a real-time lab assistant to provide timely responses and can
offload teachers' labor to assist with repetitive tasks. To the best of our
knowledge, this is the first attempt to build such an interactive multimodal
robotic assistant for K-12 science (physics) education.","Ehsan Latif, Ramviyas Parasuraman, Xiaoming Zhai",2024-03-27T16:11:49Z,2024-06-04T01:41:12Z,http://arxiv.org/abs/2403.18721v2,http://arxiv.org/pdf/2403.18721v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
ROXIE: Defining a Robotic eXplanation and Interpretability Engine,"In an era where autonomous robots increasingly inhabit public spaces, the
imperative for transparency and interpretability in their decision-making
processes becomes paramount. This paper presents the overview of a Robotic
eXplanation and Interpretability Engine (ROXIE), which addresses this critical
need, aiming to demystify the opaque nature of complex robotic behaviors. This
paper elucidates the key features and requirements needed for providing
information and explanations about robot decision-making processes. It also
overviews the suite of software components and libraries available for
deployment with ROS 2, empowering users to provide comprehensive explanations
and interpretations of robot processes and behaviors, thereby fostering trust
and collaboration in human-robot interactions.","Francisco J. Rodríguez-Lera, Miguel A. González-Santamarta, Alejandro González-Cantón, Laura Fernández-Becerra, David Sobrín-Hidalgo, Angel Manuel Guerrero-Higueras",2024-03-25T10:33:20Z,2024-03-25T10:33:20Z,http://arxiv.org/abs/2403.16606v1,http://arxiv.org/pdf/2403.16606v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Extended Reality for Enhanced Human-Robot Collaboration: a
  Human-in-the-Loop Approach","The rise of automation has provided an opportunity to achieve higher
efficiency in manufacturing processes, yet it often compromises the flexibility
required to promptly respond to evolving market needs and meet the demand for
customization. Human-robot collaboration attempts to tackle these challenges by
combining the strength and precision of machines with human ingenuity and
perceptual understanding. In this paper, we conceptualize and propose an
implementation framework for an autonomous, machine learning-based manipulator
that incorporates human-in-the-loop principles and leverages Extended Reality
(XR) to facilitate intuitive communication and programming between humans and
robots. Furthermore, the conceptual framework foresees human involvement
directly in the robot learning process, resulting in higher adaptability and
task generalization. The paper highlights key technologies enabling the
proposed framework, emphasizing the importance of developing the digital
ecosystem as a whole. Additionally, we review the existent implementation
approaches of XR in human-robot collaboration, showcasing diverse perspectives
and methodologies. The challenges and future outlooks are discussed, delving
into the major obstacles and potential research avenues of XR for more natural
human-robot interaction and integration in the industrial landscape.","Yehor Karpichev, Todd Charter, Jayden Hong, Amir M. Soufi Enayati, Homayoun Honari, Mehran Ghafarian Tamizi, Homayoun Najjaran",2024-03-21T17:50:22Z,2024-10-31T21:33:32Z,http://arxiv.org/abs/2403.14597v3,http://arxiv.org/pdf/2403.14597v3.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Changing human's impression of empathy from agent by verbalizing agent's
  position","As anthropomorphic agents (AI and robots) are increasingly used in society,
empathy and trust between people and agents are becoming increasingly
important. A better understanding of agents by people will help to improve the
problems caused by the future use of agents in society. In the past, there has
been a focus on the importance of self-disclosure and the relationship between
agents and humans in their interactions. In this study, we focused on the
attributes of self-disclosure and the relationship between agents and people.
An experiment was conducted to investigate hypotheses on trust and empathy with
agents through six attributes of self-disclosure (opinions and attitudes,
hobbies, work, money, personality, and body) and through competitive and
cooperative relationships before a robotic agent performs a joint task. The
experiment consisted of two between-participant factors: six levels of
self-disclosure attributes and two levels of relationship with the agent. The
results showed that the two factors had no effect on trust in the agent, but
there was statistical significance for the attribute of self-disclosure
regarding a person's empathy toward the agent. In addition, statistical
significance was found regarding the agent's ability to empathize with a person
as perceived by the person only in the case where the type of relationship,
competitive or cooperative, was presented. The results of this study could lead
to an effective method for building relationships with agents, which are
increasingly used in society.","Takahiro Tsumura, Seiji Yamada",2024-03-21T16:56:46Z,2024-03-21T16:56:46Z,http://arxiv.org/abs/2403.14557v1,http://arxiv.org/pdf/2403.14557v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Open Access NAO (OAN): a ROS2-based software framework for HRI
  applications with the NAO robot","This paper presents a new software framework for HRI experimentation with the
sixth version of the common NAO robot produced by the United Robotics Group.
Embracing the common demand of researchers for better performance and new
features for NAO, the authors took advantage of the ability to run ROS2 onboard
on the NAO to develop a framework independent of the APIs provided by the
manufacturer. Such a system provides NAO with not only the basic skills of a
humanoid robot such as walking and reproducing movements of interest but also
features often used in HRI such as: speech recognition/synthesis, face and
object detention, and the use of Generative Pre-trained Transformer (GPT)
models for conversation. The developed code is therefore configured as a
ready-to-use but also highly expandable and improvable tool thanks to the
possibilities provided by the ROS community.","Antonio Bono, Kenji Brameld, Luigi D'Alfonso, Giuseppe Fedele",2024-03-20T20:13:39Z,2024-03-20T20:13:39Z,http://arxiv.org/abs/2403.13960v1,http://arxiv.org/pdf/2403.13960v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Graph Neural Network-based Multi-agent Reinforcement Learning for
  Resilient Distributed Coordination of Multi-Robot Systems","Existing multi-agent coordination techniques are often fragile and vulnerable
to anomalies such as agent attrition and communication disturbances, which are
quite common in the real-world deployment of systems like field robotics. To
better prepare these systems for the real world, we present a graph neural
network (GNN)-based multi-agent reinforcement learning (MARL) method for
resilient distributed coordination of a multi-robot system. Our method,
Multi-Agent Graph Embedding-based Coordination (MAGEC), is trained using
multi-agent proximal policy optimization (PPO) and enables distributed
coordination around global objectives under agent attrition, partial
observability, and limited or disturbed communications. We use a multi-robot
patrolling scenario to demonstrate our MAGEC method in a ROS 2-based simulator
and then compare its performance with prior coordination approaches. Results
demonstrate that MAGEC outperforms existing methods in several experiments
involving agent attrition and communication disturbance, and provides
competitive results in scenarios without such anomalies.","Anthony Goeckner, Yueyuan Sui, Nicolas Martinet, Xinliang Li, Qi Zhu",2024-03-19T18:42:22Z,2024-03-19T18:42:22Z,http://arxiv.org/abs/2403.13093v1,http://arxiv.org/pdf/2403.13093v1.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
Online Concurrent Multi-Robot Coverage Path Planning,"Recently, centralized receding horizon online multi-robot coverage path
planning algorithms have shown remarkable scalability in thoroughly exploring
large, complex, unknown workspaces with many robots. In a horizon, the path
planning and the path execution interleave, meaning when the path planning
occurs for robots with no paths, the robots with outstanding paths do not
execute, and subsequently, when the robots with new or outstanding paths
execute to reach respective goals, path planning does not occur for those
robots yet to get new paths, leading to wastage of both the robotic and the
computation resources. As a remedy, we propose a centralized algorithm that is
not horizon-based. It plans paths at any time for a subset of robots with no
paths, i.e., who have reached their previously assigned goals, while the rest
execute their outstanding paths, thereby enabling concurrent planning and
execution. We formally prove that the proposed algorithm ensures complete
coverage of an unknown workspace and analyze its time complexity. To
demonstrate scalability, we evaluate our algorithm to cover eight large $2$D
grid benchmark workspaces with up to 512 aerial and ground robots,
respectively. A comparison with a state-of-the-art horizon-based algorithm
shows its superiority in completing the coverage with up to 1.6x speedup. For
validation, we perform ROS + Gazebo simulations in six 2D grid benchmark
workspaces with 10 quadcopters and TurtleBots, respectively. We also
successfully conducted one outdoor experiment with three quadcopters and one
indoor with two TurtleBots.","Ratijit Mitra, Indranil Saha",2024-03-15T16:51:30Z,2025-07-28T20:06:51Z,http://arxiv.org/abs/2403.10460v2,http://arxiv.org/pdf/2403.10460v2.pdf,all:ROS AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
OpenScout v1.1 mobile robot: a case study on open hardware continuation,"OpenScout is an Open Source Hardware (OSH) mobile robot for research and
industry. It is extended to v1.1 which includes simplified, cheaper and more
powerful onboard compute hardware; a simulated ROS2 interface; and a Gazebo
simulation. Changes, their rationale, project methodology, and results are
reported as an OSH case study.","Bartosz Krawczyk, Ahmed Elbary, Robbie Cato, Jagdish Patil, Kaung Myat, Anyeh Ndi-Tah, Nivetha Sakthivel, Mark Crampton, Gautham Das, Charles Fox",2025-08-01T13:36:00Z,2025-08-01T13:36:00Z,http://arxiv.org/abs/2508.00625v1,http://arxiv.org/pdf/2508.00625v1.pdf,all:ROS2 AND all:mobile AND submittedDate:[202309062229 TO 202509052229],2025
"DELIVER: A System for LLM-Guided Coordinated Multi-Robot Pickup and
  Delivery using Voronoi-Based Relay Planning","We present DELIVER (Directed Execution of Language-instructed Item Via
Engineered Relay), a fully integrated framework for cooperative multi-robot
pickup and delivery driven by natural language commands. DELIVER unifies
natural language understanding, spatial decomposition, relay planning, and
motion execution to enable scalable, collision-free coordination in real-world
settings. Given a spoken or written instruction, a lightweight instance of
LLaMA3 interprets the command to extract pickup and delivery locations. The
environment is partitioned using a Voronoi tessellation to define
robot-specific operating regions. Robots then compute optimal relay points
along shared boundaries and coordinate handoffs. A finite-state machine governs
each robot's behavior, enabling robust execution. We implement DELIVER on the
MultiTRAIL simulation platform and validate it in both ROS2-based Gazebo
simulations and real-world hardware using TurtleBot3 robots. Empirical results
show that DELIVER maintains consistent mission cost across varying team sizes
while reducing per-agent workload by up to 55% compared to a single-agent
system. Moreover, the number of active relay agents remains low even as team
size increases, demonstrating the system's scalability and efficient agent
utilization. These findings underscore DELIVER's modular and extensible
architecture for language-guided multi-robot coordination, advancing the
frontiers of cyber-physical system integration.","Alkesh K. Srivastava, Jared Michael Levin, Alexander Derrico, Philip Dames",2025-08-26T15:17:08Z,2025-08-26T15:17:08Z,http://arxiv.org/abs/2508.19114v1,http://arxiv.org/pdf/2508.19114v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Visual Perception Engine: Fast and Flexible Multi-Head Inference for
  Robotic Vision Tasks","Deploying multiple machine learning models on resource-constrained robotic
platforms for different perception tasks often results in redundant
computations, large memory footprints, and complex integration challenges. In
response, this work presents Visual Perception Engine (VPEngine), a modular
framework designed to enable efficient GPU usage for visual multitasking while
maintaining extensibility and developer accessibility. Our framework
architecture leverages a shared foundation model backbone that extracts image
representations, which are efficiently shared, without any unnecessary GPU-CPU
memory transfers, across multiple specialized task-specific model heads running
in parallel. This design eliminates the computational redundancy inherent in
feature extraction component when deploying traditional sequential models while
enabling dynamic task prioritization based on application demands. We
demonstrate our framework's capabilities through an example implementation
using DINOv2 as the foundation model with multiple task (depth, object
detection and semantic segmentation) heads, achieving up to 3x speedup compared
to sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngine
offers efficient GPU utilization and maintains a constant memory footprint
while allowing per-task inference frequencies to be adjusted dynamically during
runtime. The framework is written in Python and is open source with ROS2 C++
(Humble) bindings for ease of use by the robotics community across diverse
robotic platforms. Our example implementation demonstrates end-to-end real-time
performance at $\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimized
models.","Jakub Łucki, Jonathan Becktor, Georgios Georgakis, Rob Royce, Shehryar Khattak",2025-08-15T16:42:23Z,2025-08-18T05:11:18Z,http://arxiv.org/abs/2508.11584v2,http://arxiv.org/pdf/2508.11584v2.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Navigation and Exploration with Active Inference: from Biology to
  Industry","By building and updating internal cognitive maps, animals exhibit
extraordinary navigation abilities in complex, dynamic environments. Inspired
by these biological mechanisms, we present a real time robotic navigation
system grounded in the Active Inference Framework (AIF). Our model
incrementally constructs a topological map, infers the agent's location, and
plans actions by minimising expected uncertainty and fulfilling perceptual
goals without any prior training. Integrated into the ROS2 ecosystem, we
validate its adaptability and efficiency across both 2D and 3D environments
(simulated and real world), demonstrating competitive performance with
traditional and state of the art exploration approaches while offering a
biologically inspired navigation approach.","Daria de Tinguy, Tim Verbelen, Bart Dhoedt",2025-08-10T09:51:27Z,2025-08-10T09:51:27Z,http://arxiv.org/abs/2508.07269v1,http://arxiv.org/pdf/2508.07269v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Bio-Inspired Topological Autonomous Navigation with Active Inference in
  Robotics","Achieving fully autonomous exploration and navigation remains a critical
challenge in robotics, requiring integrated solutions for localisation,
mapping, decision-making and motion planning. Existing approaches either rely
on strict navigation rules lacking adaptability or on pre-training, which
requires large datasets. These AI methods are often computationally intensive
or based on static assumptions, limiting their adaptability in dynamic or
unknown environments. This paper introduces a bio-inspired agent based on the
Active Inference Framework (AIF), which unifies mapping, localisation, and
adaptive decision-making for autonomous navigation, including exploration and
goal-reaching. Our model creates and updates a topological map of the
environment in real-time, planning goal-directed trajectories to explore or
reach objectives without requiring pre-training. Key contributions include a
probabilistic reasoning framework for interpretable navigation, robust
adaptability to dynamic changes, and a modular ROS2 architecture compatible
with existing navigation systems. Our method was tested in simulated and
real-world environments. The agent successfully explores large-scale simulated
environments and adapts to dynamic obstacles and drift, proving to be
comparable to other exploration strategies such as Gbplanner, FAEL and
Frontiers. This approach offers a scalable and transparent approach for
navigating complex, unstructured environments.","Daria de Tinguy, Tim Verbelen, Emilio Gamba, Bart Dhoedt",2025-08-10T09:42:13Z,2025-08-10T09:42:13Z,http://arxiv.org/abs/2508.07267v1,http://arxiv.org/pdf/2508.07267v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"MUTE-DSS: A Digital-Twin-Based Decision Support System for Minimizing
  Underwater Radiated Noise in Ship Voyage Planning","We present a novel MUTE-DSS, a digital-twin-based decision support system for
minimizing underwater radiated noise (URN) during ship voyage planning. It is a
ROS2-centric framework that integrates state-of-the-art acoustic models
combining a semi-empirical reference spectrum for near-field modeling with 3D
ray tracing for propagation losses for far-field modeling, offering real-time
computation of the ship noise signature, alongside a data-driven Southern
resident killer whale distribution model. The proposed DSS performs a two-stage
optimization pipeline: Batch Informed Trees for collision-free ship routing and
a genetic algorithm for adaptive ship speed profiling under voyage constraints
that minimizes cumulative URN exposure to marine mammals. The effectiveness of
MUTE-DSS is demonstrated through case studies of ships operating between the
Strait of Georgia and the Strait of Juan de Fuca, comparing optimized voyages
against baseline trajectories derived from automatic identification system
data. Results show substantial reductions in noise exposure level, up to 7.14
dB, corresponding to approximately an 80.68% reduction in a simplified
scenario, and an average 4.90 dB reduction, corresponding to approximately a
67.6% reduction in a more realistic dynamic setting. These results illustrate
the adaptability and practical utility of the proposed decision support system.","Akash Venkateshwaran, Indu Kant Deo, Rajeev K. Jaiman",2025-08-03T20:02:56Z,2025-08-03T20:02:56Z,http://arxiv.org/abs/2508.01907v1,http://arxiv.org/pdf/2508.01907v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Autonomous UAV Navigation for Search and Rescue Missions Using Computer
  Vision and Convolutional Neural Networks","In this paper, we present a subsystem, using Unmanned Aerial Vehicles (UAV),
for search and rescue missions, focusing on people detection, face recognition
and tracking of identified individuals. The proposed solution integrates a UAV
with ROS2 framework, that utilizes multiple convolutional neural networks (CNN)
for search missions. System identification and PD controller deployment are
performed for autonomous UAV navigation. The ROS2 environment utilizes the
YOLOv11 and YOLOv11-pose CNNs for tracking purposes, and the dlib library CNN
for face recognition. The system detects a specific individual, performs face
recognition and starts tracking. If the individual is not yet known, the UAV
operator can manually locate the person, save their facial image and
immediately initiate the tracking process. The tracking process relies on
specific keypoints identified on the human body using the YOLOv11-pose CNN
model. These keypoints are used to track a specific individual and maintain a
safe distance. To enhance accurate tracking, system identification is
performed, based on measurement data from the UAVs IMU. The identified system
parameters are used to design PD controllers that utilize YOLOv11-pose to
estimate the distance between the UAVs camera and the identified individual.
The initial experiments, conducted on 14 known individuals, demonstrated that
the proposed subsystem can be successfully used in real time. The next step
involves implementing the system on a large experimental UAV for field use and
integrating autonomous navigation with GPS-guided control for rescue operations
planning.","Luka Šiktar, Branimir Ćaran, Bojan Šekoranja, Marko Švaco",2025-07-24T07:54:45Z,2025-07-24T07:54:45Z,http://arxiv.org/abs/2507.18160v1,http://arxiv.org/pdf/2507.18160v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Comparison of Localization Algorithms between Reduced-Scale and
  Real-Sized Vehicles Using Visual and Inertial Sensors","Physically reduced-scale vehicles are emerging to accelerate the development
of advanced automated driving functions. In this paper, we investigate the
effects of scaling on self-localization accuracy with visual and
visual-inertial algorithms using cameras and an inertial measurement unit
(IMU). For this purpose, ROS2-compatible visual and visual-inertial algorithms
are selected, and datasets are chosen as a baseline for real-sized vehicles. A
test drive is conducted to record data of reduced-scale vehicles. We compare
the selected localization algorithms, OpenVINS, VINS-Fusion, and RTAB-Map, in
terms of their pose accuracy against the ground-truth and against data from
real-sized vehicles. When comparing the implementation of the selected
localization algorithms to real-sized vehicles, OpenVINS has the lowest average
localization error. Although all selected localization algorithms have
overlapping error ranges, OpenVINS also performs best when applied to a
reduced-scale vehicle. When reduced-scale vehicles were compared to real-sized
vehicles, minor differences were found in translational vehicle motion
estimation accuracy. However, no significant differences were found when
comparing the estimation accuracy of rotational vehicle motion, allowing RSVRs
to be used as testing platforms for self-localization algorithms.","Tobias Kern, Leon Tolksdorf, Christian Birkner",2025-07-15T12:14:57Z,2025-07-15T12:14:57Z,http://arxiv.org/abs/2507.11241v1,http://arxiv.org/pdf/2507.11241v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Hand Gesture Recognition for Collaborative Robots Using Lightweight Deep
  Learning in Real-Time Robotic Systems","Direct and natural interaction is essential for intuitive human-robot
collaboration, eliminating the need for additional devices such as joysticks,
tablets, or wearable sensors. In this paper, we present a lightweight deep
learning-based hand gesture recognition system that enables humans to control
collaborative robots naturally and efficiently. This model recognizes eight
distinct hand gestures with only 1,103 parameters and a compact size of 22 KB,
achieving an accuracy of 93.5%. To further optimize the model for real-world
deployment on edge devices, we applied quantization and pruning using
TensorFlow Lite, reducing the final model size to just 7 KB. The system was
successfully implemented and tested on a Universal Robot UR5 collaborative
robot within a real-time robotic framework based on ROS2. The results
demonstrate that even extremely lightweight models can deliver accurate and
responsive hand gesture-based control for collaborative robots, opening new
possibilities for natural human-robot interaction in constrained environments."," Muhtadin, I Wayan Agus Darmawan, Muhammad Hilmi Rusydiansyah, I Ketut Eddy Purnama, Chastine Fatichah, Mauridhi Hery Purnomo",2025-07-14T08:40:24Z,2025-07-24T04:25:41Z,http://arxiv.org/abs/2507.10055v2,http://arxiv.org/pdf/2507.10055v2.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
Multi-agent Auditory Scene Analysis,"Auditory scene analysis (ASA) aims to retrieve information from the acoustic
environment, by carrying out three main tasks: sound source location,
separation, and classification. These tasks are traditionally executed with a
linear data flow, where the sound sources are first located; then, using their
location, each source is separated into its own audio stream; from each of
which, information is extracted that is relevant to the application scenario
(audio event detection, speaker identification, emotion classification, etc.).
However, running these tasks linearly increases the overall response time,
while making the last tasks (separation and classification) highly sensitive to
errors of the first task (location). A considerable amount of effort and
computational complexity has been employed in the state-of-the-art to develop
techniques that are the least error-prone possible. However, doing so gives
rise to an ASA system that is non-viable in many applications that require a
small computational footprint and a low response time, such as bioacoustics,
hearing-aid design, search and rescue, human-robot interaction, etc. To this
effect, in this work, a multi-agent approach is proposed to carry out ASA where
the tasks are run in parallel, with feedback loops between them to compensate
for local errors, such as: using the quality of the separation output to
correct the location error; and using the classification result to reduce the
localization's sensitivity towards interferences. The result is a multi-agent
auditory scene analysis (MASA) system that is robust against local errors,
without a considerable increase in complexity, and with a low response time.
The complete proposed MASA system is provided as a publicly available framework
that uses open-source tools for sound acquisition and reproduction (JACK) and
inter-agent communication (ROS2), allowing users to add their own agents.","Caleb Rascon, Luis Gato-Diaz, Eduardo García-Alarcón",2025-07-03T16:16:46Z,2025-08-20T14:18:03Z,http://arxiv.org/abs/2507.02755v3,http://arxiv.org/pdf/2507.02755v3.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"ros2 fanuc interface: Design and Evaluation of a Fanuc CRX Hardware
  Interface in ROS2","This paper introduces the ROS2 control and the Hardware Interface (HW)
integration for the Fanuc CRX- robot family. It explains basic implementation
details and communication protocols, and its integration with the Moveit2
motion planning library. We conducted a series of experiments to evaluate
relevant performances in the robotics field. We tested the developed
ros2_fanuc_interface for four relevant robotics cases: step response,
trajectory tracking, collision avoidance integrated with Moveit2, and dynamic
velocity scaling, respectively. Results show that, despite a non-negligible
delay between command and feedback, the robot can track the defined path with
negligible errors (if it complies with joint velocity limits), ensuring
collision avoidance. Full code is open source and available at
https://github.com/paolofrance/ros2_fanuc_interface.","Paolo Franceschi, Marco Faroni, Stefano Baraldo, Anna Valente",2025-06-17T13:08:32Z,2025-06-24T13:40:12Z,http://arxiv.org/abs/2506.14487v2,http://arxiv.org/pdf/2506.14487v2.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"RF-Source Seeking with Obstacle Avoidance using Real-time Modified
  Artificial Potential Fields in Unknown Environments","Navigation of UAVs in unknown environments with obstacles is essential for
applications in disaster response and infrastructure monitoring. However,
existing obstacle avoidance algorithms, such as Artificial Potential Field
(APF) are unable to generalize across environments with different obstacle
configurations. Furthermore, the precise location of the final target may not
be available in applications such as search and rescue, in which case
approaches such as RF source seeking can be used to align towards the target
location. This paper proposes a real-time trajectory planning method, which
involves real-time adaptation of APF through a sampling-based approach. The
proposed approach utilizes only the bearing angle of the target without its
precise location, and adjusts the potential field parameters according to the
environment with new obstacle configurations in real time. The main
contributions of the article are i) an RF source seeking algorithm to provide a
bearing angle estimate using RF signal calculations based on antenna placement,
and ii) a modified APF for adaptable collision avoidance in changing
environments, which are evaluated separately in the simulation software Gazebo,
using ROS2 for communication. Simulation results show that the RF
source-seeking algorithm achieves high accuracy, with an average angular error
of just 1.48 degrees, and with this estimate, the proposed navigation algorithm
improves the success rate of reaching the target by 46% and reduces the
trajectory length by 1.2% compared to standard potential fields.","Shahid Mohammad Mulla, Aryan Kanakapudi, Lakshmi Narasimhan, Anuj Tiwari",2025-06-07T14:20:58Z,2025-06-07T14:20:58Z,http://arxiv.org/abs/2506.06811v1,http://arxiv.org/pdf/2506.06811v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
ROSGuard: A Bandwidth Regulation Mechanism for ROS2-based Applications,"Multicore timing interference, arising when multiple requests contend for the
same shared hardware resources, is a primary concern for timing verification
and validation of time-critical applications. Bandwidth control and regulation
approaches have been proposed in the literature as an effective method to
monitor and limit the impact of timing interference at run time. These
approaches seek for fine-grained control of the bandwidth consumption (at the
microsecond level) to meet stringent timing requirements on embedded critical
systems. Such granularity and configurations, while effective, can become an
entry barrier for the application of bandwidth control to a wide class of
productized, modular ROS2 applications. This is so because those applications
have less stringent timing requirements but would still benefit from bandwidth
regulation, though under less restrictive, and therefore more portable,
granularity and configurations.
  In this work, we provide ROSGuard, a highly-portable, modular implementation
of a timing interference monitoring and control mechanism that builds on the
abstractions available on top of a generic and portable Linux-based software
stack with the Robotic Operating System 2 (ROS2) layer, a widespreadedly
adopted middleware for a wide class of industrial applications, far beyond the
robotic domain. We deploy ROSGuard on an NVIDIA AGX Orin platform as a
representative target for functionally rich distributed AI-based applications
and a set of synthetic and real-world benchmarks. We apply an effective
bandwidth regulation scheme on ROS2-based applications and achieve comparable
effectiveness to specialized, finer-grained state-of-the-art solutions.","Jon Altonaga Puente, Enrico Mezzetti, Irune Agirre Troncoso, Jaume Abella Ferrer, Francisco J. Cazorla Almeida",2025-06-05T05:21:27Z,2025-06-05T05:21:27Z,http://arxiv.org/abs/2506.04640v1,http://arxiv.org/pdf/2506.04640v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
A simulation framework for autonomous lunar construction work,"We present a simulation framework for lunar construction work involving
multiple autonomous machines. The framework supports modelling of construction
scenarios and autonomy solutions, execution of the scenarios in simulation, and
analysis of work time and energy consumption throughout the construction
project. The simulations are based on physics-based models for contacting
multibody dynamics and deformable terrain, including vehicle-soil interaction
forces and soil flow in real time. A behaviour tree manages the operational
logic and error handling, which enables the representation of complex
behaviours through a discrete set of simpler tasks in a modular hierarchical
structure. High-level decision-making is separated from lower-level control
algorithms, with the two connected via ROS2. Excavation movements are
controlled through inverse kinematics and tracking controllers. The framework
is tested and demonstrated on two different lunar construction scenarios that
involve an excavator and dump truck with actively controlled articulated
crawlers.","Mattias Linde, Daniel Lindmark, Sandra Ålstig, Martin Servin",2025-05-28T08:16:05Z,2025-08-12T07:41:09Z,http://arxiv.org/abs/2505.22091v2,http://arxiv.org/pdf/2505.22091v2.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Semantic Intelligence: Integrating GPT-4 with A Planning in Low-Cost
  Robotics","Classical robot navigation often relies on hardcoded state machines and
purely geometric path planners, limiting a robot's ability to interpret
high-level semantic instructions. In this paper, we first assess GPT-4's
ability to act as a path planner compared to the A* algorithm, then present a
hybrid planning framework that integrates GPT-4's semantic reasoning with A* on
a low-cost robot platform operating on ROS2 Humble. Our approach eliminates
explicit finite state machine (FSM) coding by using prompt-based GPT-4
reasoning to handle task logic while maintaining the accurate paths computed by
A*. The GPT-4 module provides semantic understanding of instructions and
environmental cues (e.g., recognizing toxic obstacles or crowded areas to
avoid, or understanding low-battery situations requiring alternate route
selection), and dynamically adjusts the robot's occupancy grid via obstacle
buffering to enforce semantic constraints. We demonstrate multi-step reasoning
for sequential tasks, such as first navigating to a resource goal and then
reaching a final destination safely. Experiments on a Petoi Bittle robot with
an overhead camera and Raspberry Pi Zero 2W compare classical A* against
GPT-4-assisted planning. Results show that while A* is faster and more accurate
for basic route generation and obstacle avoidance, the GPT-4-integrated system
achieves high success rates (96-100%) on semantic tasks that are infeasible for
pure geometric planners. This work highlights how affordable robots can exhibit
intelligent, context-aware behaviors by leveraging large language model
reasoning with minimal hardware and no fine-tuning.","Jesse Barkley, Abraham George, Amir Barati Farimani",2025-05-03T21:49:14Z,2025-05-03T21:49:14Z,http://arxiv.org/abs/2505.01931v1,http://arxiv.org/pdf/2505.01931v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Real Time Semantic Segmentation of High Resolution Automotive LiDAR
  Scans","In recent studies, numerous previous works emphasize the importance of
semantic segmentation of LiDAR data as a critical component to the development
of driver-assistance systems and autonomous vehicles. However, many
state-of-the-art methods are tested on outdated, lower-resolution LiDAR sensors
and struggle with real-time constraints. This study introduces a novel semantic
segmentation framework tailored for modern high-resolution LiDAR sensors that
addresses both accuracy and real-time processing demands. We propose a novel
LiDAR dataset collected by a cutting-edge automotive 128 layer LiDAR in urban
traffic scenes. Furthermore, we propose a semantic segmentation method
utilizing surface normals as strong input features. Our approach is bridging
the gap between cutting-edge research and practical automotive applications.
Additionaly, we provide a Robot Operating System (ROS2) implementation that we
operate on our research vehicle. Our dataset and code are publicly available:
https://github.com/kav-institute/SemanticLiDAR.","Hannes Reichert, Benjamin Serfling, Elijah Schüssler, Kerim Turacan, Konrad Doll, Bernhard Sick",2025-04-30T13:00:50Z,2025-04-30T13:00:50Z,http://arxiv.org/abs/2504.21602v1,http://arxiv.org/pdf/2504.21602v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Trajectory Planning with Model Predictive Control for Obstacle Avoidance
  Considering Prediction Uncertainty","This paper introduces a novel trajectory planner for autonomous robots,
specifically designed to enhance navigation by incorporating dynamic obstacle
avoidance within the Robot Operating System 2 (ROS2) and Navigation 2 (Nav2)
framework. The proposed method utilizes Model Predictive Control (MPC) with a
focus on handling the uncertainties associated with the movement prediction of
dynamic obstacles. Unlike existing Nav2 trajectory planners which primarily
deal with static obstacles or react to the current position of dynamic
obstacles, this planner predicts future obstacle positions using a stochastic
Vector Auto-Regressive Model (VAR). The obstacles' future positions are
represented by probability distributions, and collision avoidance is achieved
through constraints based on the Mahalanobis distance, ensuring the robot
avoids regions where obstacles are likely to be. This approach considers the
robot's kinodynamic constraints, enabling it to track a reference path while
adapting to real-time changes in the environment. The paper details the
implementation, including obstacle prediction, tracking, and the construction
of feasible sets for MPC. Simulation results in a Gazebo environment
demonstrate the effectiveness of this method in scenarios where robots must
navigate around each other, showing improved collision avoidance capabilities.","Eric Schöneberg, Michael Schröder, Daniel Görges, Hans D. Schotten",2025-04-27T11:00:19Z,2025-04-27T11:00:19Z,http://arxiv.org/abs/2504.19193v1,http://arxiv.org/pdf/2504.19193v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
A ROS2-based software library for inverse dynamics computation,"Inverse dynamics computation is a critical component in robot control,
planning and simulation, enabling the calculation of joint torques required to
achieve a desired motion. This paper presents a ROS2-based software library
designed to solve the inverse dynamics problem for robotic systems. The library
is built around an abstract class with three concrete implementations: one for
simulated robots and two for real UR10 and Franka robots. This contribution
aims to provide a flexible, extensible, robot-agnostic solution to inverse
dynamics, suitable for both simulation and real-world scenarios involving
planning and control applications. The related software is available at
https://github.com/unisa-acg/inverse-dynamics-solver/tree/rap.","Vincenzo Petrone, Enrico Ferrentino, Pasquale Chiacchio",2025-04-08T14:50:17Z,2025-04-13T10:46:00Z,http://arxiv.org/abs/2504.06106v2,http://arxiv.org/pdf/2504.06106v2.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Optimizing AUV speed dynamics with a data-driven Koopman operator
  approach","Autonomous Underwater Vehicles (AUVs) play an essential role in modern ocean
exploration, and their speed control systems are fundamental
  to their efficient operation. Like many other robotic systems, AUVs exhibit
multivariable nonlinear dynamics and face various constraints,
  including state limitations, input constraints, and constraints on the
increment input, making controller design challenging
  and requiring significant effort and time. This paper addresses these
challenges by employing a data-driven Koopman operator theory combined
  with Model Predictive Control (MPC), which takes into account the
aforementioned constraints. The proposed approach not only ensures
  the performance of the AUV under state and input limitations but also
considers the variation in incremental input to prevent
  rapid and potentially damaging changes to the vehicle's operation.
Additionally, we develop a platform based on ROS2 and Gazebo
  to validate the effectiveness of the proposed algorithms, providing new
control strategies for underwater vehicles against the complex and dynamic
nature of underwater environments.","Zhiliang Liu, Xin Zhao, Peng Cai, Bing Cong",2025-03-11T05:30:36Z,2025-03-11T05:30:36Z,http://arxiv.org/abs/2503.09628v1,http://arxiv.org/pdf/2503.09628v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
QBIT: Quality-Aware Cloud-Based Benchmarking for Robotic Insertion Tasks,"Insertion tasks are fundamental yet challenging for robots, particularly in
autonomous operations, due to their continuous interaction with the
environment. AI-based approaches appear to be up to the challenge, but in
production they must not only achieve high success rates. They must also ensure
insertion quality and reliability. To address this, we introduce QBIT, a
quality-aware benchmarking framework that incorporates additional metrics such
as force energy, force smoothness and completion time to provide a
comprehensive assessment. To ensure statistical significance and minimize the
sim-to-real gap, we randomize contact parameters in the MuJoCo simulator,
account for perceptual uncertainty, and conduct large-scale experiments on a
Kubernetes-based infrastructure. Our microservice-oriented architecture ensures
extensibility, broad applicability, and improved reproducibility. To facilitate
seamless transitions to physical robotic testing, we use ROS2 with
containerization to reduce integration barriers. We evaluate QBIT using three
insertion approaches: geometricbased, force-based, and learning-based, in both
simulated and real-world environments. In simulation, we compare the accuracy
of contact simulation using different mesh decomposition techniques. Our
results demonstrate the effectiveness of QBIT in comparing different insertion
approaches and accelerating the transition from laboratory to real-world
applications. Code is available on GitHub.","Constantin Schempp, Yongzhou Zhang, Christian Friedrich, Bjorn Hein",2025-03-10T15:54:15Z,2025-03-10T15:54:15Z,http://arxiv.org/abs/2503.07479v1,http://arxiv.org/pdf/2503.07479v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"A Temporal Planning Framework for Multi-Agent Systems via LLM-Aided
  Knowledge Base Management","This paper presents a novel framework, called PLANTOR (PLanning with Natural
language for Task-Oriented Robots), that integrates Large Language Models
(LLMs) with Prolog-based knowledge management and planning for multi-robot
tasks. The system employs a two-phase generation of a robot-oriented knowledge
base, ensuring reusability and compositional reasoning, as well as a three-step
planning procedure that handles temporal dependencies, resource constraints,
and parallel task execution via mixed-integer linear programming. The final
plan is converted into a Behaviour Tree for direct use in ROS2. We tested the
framework in multi-robot assembly tasks within a block world and an
arch-building scenario. Results demonstrate that LLMs can produce accurate
knowledge bases with modest human feedback, while Prolog guarantees formal
correctness and explainability. This approach underscores the potential of LLM
integration for advanced robotics tasks requiring flexible, scalable, and
human-understandable planning.","Enrico Saccon, Ahmet Tikna, Davide De Martini, Edoardo Lamon, Luigi Palopoli, Marco Roveri",2025-02-26T13:51:28Z,2025-02-26T13:51:28Z,http://arxiv.org/abs/2502.19135v1,http://arxiv.org/pdf/2502.19135v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Efficient Coordination and Synchronization of Multi-Robot Systems Under
  Recurring Linear Temporal Logic","We consider multi-robot systems under recurring tasks formalized as linear
temporal logic (LTL) specifications. To solve the planning problem efficiently,
we propose a bottom-up approach combining offline plan synthesis with online
coordination, dynamically adjusting plans via real-time communication. To
address action delays, we introduce a synchronization mechanism ensuring
coordinated task execution, leading to a multi-agent coordination and
synchronization framework that is adaptable to a wide range of multi-robot
applications. The software package is developed in Python and ROS2 for broad
deployment. We validate our findings through lab experiments involving nine
robots showing enhanced adaptability compared to previous methods.
Additionally, we conduct simulations with up to ninety agents to demonstrate
the reduced computational complexity and the scalability features of our work.","Davide Peron, Victor Nan Fernandez-Ayala, Eleftherios E. Vlahakis, Dimos V. Dimarogonas",2025-02-23T10:35:04Z,2025-02-23T10:35:04Z,http://arxiv.org/abs/2502.16531v1,http://arxiv.org/pdf/2502.16531v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Asynchronous Stochastic Block Projection Algorithm for Solving Linear
  Systems under Predefined Communication Patterns","This paper proposes an event-triggered asynchronous distributed randomized
block Kaczmarz projection (ER-AD-RBKP) algorithm for efficiently solving
large-scale linear systems in resource-constrained and communication-unstable
environments. The algorithm enables each agent to update its local state
estimate independently and engage in communication only when specific
triggering conditions are satisfied, thereby significantly reducing
communication overhead. At each iteration, agents perform projections using
randomly selected partial local data blocks to lower per-iteration
computational costs and enhance scalability. By defining events that ensure
strong connectivity in the communication graph, we derive the sufficient
conditions for global convergence under a probabilistic framework, proving that
the algorithm converges exponentially in expectation as long as no extreme
events (e.g., permanent agent disconnection) occur. Besides, for inconsistent
systems, auxiliary variables are incorporated to transform the problem into an
equivalent consistent formulation, and theoretical error bounds are derived.
Moreover, we implement the ER-AD-RBKP algorithm in an asynchronous
communication environment built on ROS2, a distributed middleware framework for
real-time robotic systems. We evaluate the algorithm under various settings,
including varying numbers of agents, neighborhood sizes, communication
intervals, and failure scenarios such as communication disruptions and
processing faults. Experimental results demonstrate the robust performance of
the proposed algorithm in terms of computational efficiency, communication
cost, and system resilience, highlighting its strong potential for practical
applicability in real-world distributed systems.","Yanchen Yin, Yongli Wang",2025-02-20T02:47:12Z,2025-06-15T06:20:18Z,http://arxiv.org/abs/2502.14213v3,http://arxiv.org/pdf/2502.14213v3.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
The Dynamic Model of the UR10 Robot and its ROS2 Integration,"This paper presents the full dynamic model of the UR10 industrial robot. A
triple-stage identification approach is adopted to estimate the manipulator's
dynamic coefficients. First, linear parameters are computed using a standard
linear regression algorithm. Subsequently, nonlinear friction parameters are
estimated according to a sigmoidal model. Lastly, motor drive gains are devised
to map estimated joint currents to torques. The overall identified model can be
used for both control and planning purposes, as the accompanied ROS2 software
can be easily reconfigured to account for a generic payload. The estimated
robot model is experimentally validated against a set of exciting trajectories
and compared to the state-of-the-art model for the same manipulator, achieving
higher current prediction accuracy (up to a factor of 4.43) and more precise
motor gains. The related software is available at
https://codeocean.com/capsule/8515919/tree/v2.","Vincenzo Petrone, Enrico Ferrentino, Pasquale Chiacchio",2025-02-17T15:51:57Z,2025-02-17T15:51:57Z,http://arxiv.org/abs/2502.11940v1,http://arxiv.org/pdf/2502.11940v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Digital Twin Synchronization: Bridging the Sim-RL Agent to a Real-Time
  Robotic Additive Manufacturing Control","With the rapid development of deep reinforcement learning technology, it
gradually demonstrates excellent potential and is becoming the most promising
solution in the robotics. However, in the smart manufacturing domain, there is
still not too much research involved in dynamic adaptive control mechanisms
optimizing complex processes. This research advances the integration of Soft
Actor-Critic (SAC) with digital twins for industrial robotics applications,
providing a framework for enhanced adaptive real-time control for smart
additive manufacturing processing. The system architecture combines Unity's
simulation environment with ROS2 for seamless digital twin synchronization,
while leveraging transfer learning to efficiently adapt trained models across
tasks. We demonstrate our methodology using a Viper X300s robot arm with the
proposed hierarchical reward structure to address the common reinforcement
learning challenges in two distinct control scenarios. The results show rapid
policy convergence and robust task execution in both simulated and physical
environments demonstrating the effectiveness of our approach.","Matsive Ali, Sandesh Giri, Sen Liu, Qin Yang",2025-01-29T22:06:53Z,2025-06-09T23:33:56Z,http://arxiv.org/abs/2501.18016v2,http://arxiv.org/pdf/2501.18016v2.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"RoMu4o: A Robotic Manipulation Unit For Orchard Operations Automating
  Proximal Hyperspectral Leaf Sensing","Driven by the need to address labor shortages and meet the demands of a
rapidly growing population, robotic automation has become a critical component
in precision agriculture. Leaf-level hyperspectral spectroscopy is shown to be
a powerful tool for phenotyping, monitoring crop health, identifying essential
nutrients within plants as well as detecting diseases and water stress. This
work introduces RoMu4o, a robotic manipulation unit for orchard operations
offering an automated solution for proximal hyperspectral leaf sensing. This
ground robot is equipped with a 6DOF robotic arm and vision system for
real-time deep learning-based image processing and motion planning. We
developed robust perception and manipulation pipelines that enable the robot to
successfully grasp target leaves and perform spectroscopy. These frameworks
operate synergistically to identify and extract the 3D structure of leaves from
an observed batch of foliage, propose 6D poses, and generate collision-free
constraint-aware paths for precise leaf manipulation. The end-effector of the
arm features a compact design that integrates an independent lighting source
with a hyperspectral sensor, enabling high-fidelity data acquisition while
streamlining the calibration process for accurate measurements. Our ground
robot is engineered to operate in unstructured orchard environments. However,
the performance of the system is evaluated in both indoor and outdoor plant
models. The system demonstrated reliable performance for 1-LPB hyperspectral
sampling, achieving 95% success rate in lab trials and 79% in field trials.
Field experiments revealed an overall success rate of 70% for autonomous leaf
grasping and hyperspectral measurement in a pistachio orchard. The open-source
repository is available at: https://github.com/mehradmrt/UCM-AgBot-ROS2","Mehrad Mortazavi, David J. Cappelleri, Reza Ehsani",2025-01-18T01:04:02Z,2025-01-18T01:04:02Z,http://arxiv.org/abs/2501.10621v1,http://arxiv.org/pdf/2501.10621v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2025
"Investigating the Impact of Communication-Induced Action Space on
  Exploration of Unknown Environments with Decentralized Multi-Agent
  Reinforcement Learning","This paper introduces a novel enhancement to the Decentralized Multi-Agent
Reinforcement Learning (D-MARL) exploration by proposing communication-induced
action space to improve the mapping efficiency of unknown environments using
homogeneous agents. Efficient exploration of large environments relies heavily
on inter-agent communication as real-world scenarios are often constrained by
data transmission limits, such as signal latency and bandwidth. Our proposed
method optimizes each agent's policy using the heterogeneous-agent proximal
policy optimization algorithm, allowing agents to autonomously decide whether
to communicate or to explore, that is whether to share the locally collected
maps or continue the exploration. We propose and compare multiple novel reward
functions that integrate inter-agent communication and exploration, enhance
mapping efficiency and robustness, and minimize exploration overlap. This
article presents a framework developed in ROS2 to evaluate and validate the
investigated architecture. Specifically, four TurtleBot3 Burgers have been
deployed in a Gazebo-designed environment filled with obstacles to evaluate the
efficacy of the trained policies in mapping the exploration arena.","Gabriele Calzolari, Vidya Sumathy, Christoforos Kanellakis, George Nikolakopoulos",2024-12-28T08:02:45Z,2024-12-28T08:02:45Z,http://arxiv.org/abs/2412.20075v1,http://arxiv.org/pdf/2412.20075v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"A Paragraph is All It Takes: Rich Robot Behaviors from Interacting,
  Trusted LLMs","Large Language Models (LLMs) are compact representations of all public
knowledge of our physical environment and animal and human behaviors. The
application of LLMs to robotics may offer a path to highly capable robots that
perform well across most human tasks with limited or even zero tuning. Aside
from increasingly sophisticated reasoning and task planning, networks of
(suitably designed) LLMs offer ease of upgrading capabilities and allow humans
to directly observe the robot's thinking. Here we explore the advantages,
limitations, and particularities of using LLMs to control physical robots. The
basic system consists of four LLMs communicating via a human language data bus
implemented via web sockets and ROS2 message passing. Surprisingly, rich robot
behaviors and good performance across different tasks could be achieved despite
the robot's data fusion cycle running at only 1Hz and the central data bus
running at the extremely limited rates of the human brain, of around 40 bits/s.
The use of natural language for inter-LLM communication allowed the robot's
reasoning and decision making to be directly observed by humans and made it
trivial to bias the system's behavior with sets of rules written in plain
English. These rules were immutably written into Ethereum, a global, public,
and censorship resistant Turing-complete computer. We suggest that by using
natural language as the data bus among interacting AIs, and immutable public
ledgers to store behavior constraints, it is possible to build robots that
combine unexpectedly rich performance, upgradability, and durable alignment
with humans."," OpenMind, Shaohong Zhong, Adam Zhou, Boyuan Chen, Homin Luo, Jan Liphardt",2024-12-24T18:41:15Z,2024-12-24T18:41:15Z,http://arxiv.org/abs/2412.18588v1,http://arxiv.org/pdf/2412.18588v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Performance Evaluation of ROS2-DDS middleware implementations
  facilitating Cooperative Driving in Autonomous Vehicle","In the autonomous vehicle and self-driving paradigm, cooperative perception
or exchanging sensor information among vehicles over wireless communication has
added a new dimension. Generally, an autonomous vehicle is a special type of
robot that requires real-time, highly reliable sensor inputs due to functional
safety. Autonomous vehicles are equipped with a considerable number of sensors
to provide different required sensor data to make the driving decision and
share with other surrounding vehicles. The inclusion of Data Distribution
Service(DDS) as a communication middleware in ROS2 has proved its potential
capability to be a reliable real-time distributed system. DDS comes with a
scoping mechanism known as domain. Whenever a ROS2 process is initiated, it
creates a DDS participant. It is important to note that there is a limit to the
number of participants allowed in a single domain.
  The efficient handling of numerous in-vehicle sensors and their messages
demands the use of multiple ROS2 nodes in a single vehicle. Additionally, in
the cooperative perception paradigm, a significant number of ROS2 nodes can be
required when a vehicle functions as a single ROS2 node. These ROS2 nodes
cannot be part of a single domain due to DDS participant limitation; thus,
different domain communication is unavoidable. Moreover, there are different
vendor-specific implementations of DDS, and each vendor has their
configurations, which is an inevitable communication catalyst between the ROS2
nodes. The communication between vehicles or robots or ROS2 nodes depends
directly on the vendor-specific configuration, data type, data size, and the
DDS implementation used as middleware; in our study, we evaluate and
investigate the limitations, capabilities, and prospects of the different
domain communication for various vendor-specific DDS implementations for
diverse sensor data type.","Sumit Paul, Danh Lephuoc, Manfred Hauswirth",2024-12-10T13:07:26Z,2024-12-10T13:07:26Z,http://arxiv.org/abs/2412.07485v1,http://arxiv.org/pdf/2412.07485v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
Development of CPS Platform for Autonomous Construction,"In recent years, labor shortages due to the declining birthrate and aging
population have become significant challenges at construction sites in
developed countries, including Japan. To address these challenges, we are
developing an open platform called ROS2-TMS for Construction, a Cyber-Physical
System (CPS) for construction sites, to achieve both efficiency and safety in
earthwork operations. In ROS2-TMS for Construction, the system comprehensively
collects and stores environmental information from sensors placed throughout
the construction site. Based on these data, a real-time virtual construction
site is created in cyberspace. Then, based on the state of construction
machinery and environmental conditions in cyberspace, the optimal next actions
for actual construction machinery are determined, and the construction
machinery is operated accordingly. In this project, we decided to use the Open
Platform for Earthwork with Robotics and Autonomy (OPERA), developed by the
Public Works Research Institute (PWRI) in Japan, to control construction
machinery from ROS2-TMS for Construction with an originally extended behavior
tree. In this study, we present an overview of OPERA, focusing on the newly
developed navigation package for operating the crawler dump, as well as the
overall structure of ROS2-TMS for Construction as a Cyber-Physical System
(CPS). Additionally, we conducted experiments using a crawler dump and a
backhoe to verify the aforementioned functionalities.","Yuichiro Kasahara, Kota Akinari, Tomoya Kouno, Noriko Sano, Taro Abe, Genki Yamauchi, Daisuke Endo, Takeshi Hashimoto, Keiji Nagatani, Ryo Kurazume",2024-11-29T00:51:59Z,2024-11-29T00:51:59Z,http://arxiv.org/abs/2412.00147v1,http://arxiv.org/pdf/2412.00147v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
Performance evaluation of a ROS2 based Automated Driving System,"Automated driving is currently a prominent area of scientific work. In the
future, highly automated driving and new Advanced Driver Assistance Systems
will become reality. While Advanced Driver Assistance Systems and automated
driving functions for certain domains are already commercially available,
ubiquitous automated driving in complex scenarios remains a subject of ongoing
research. Contrarily to single-purpose Electronic Control Units, the software
for automated driving is often executed on high performance PCs. The Robot
Operating System 2 (ROS2) is commonly used to connect components in an
automated driving system. Due to the time critical nature of automated driving
systems, the performance of the framework is especially important. In this
paper, a thorough performance evaluation of ROS2 is conducted, both in terms of
timeliness and error rate. The results show that ROS2 is a suitable framework
for automated driving systems.","Jorin Kouril, Bernd Schäufele, Ilja Radusch, Bettina Schnor",2024-11-18T14:29:22Z,2024-11-19T08:52:04Z,http://arxiv.org/abs/2411.11607v2,http://arxiv.org/pdf/2411.11607v2.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"A generic approach for reactive stateful mitigation of application
  failures in distributed robotics systems deployed with Kubernetes","Offloading computationally expensive algorithms to the edge or even cloud
offers an attractive option to tackle limitations regarding on-board
computational and energy resources of robotic systems. In cloud-native
applications deployed with the container management system Kubernetes (K8s),
one key problem is ensuring resilience against various types of failures.
However, complex robotic systems interacting with the physical world pose a
very specific set of challenges and requirements that are not yet covered by
failure mitigation approaches from the cloud-native domain. In this paper, we
therefore propose a novel approach for robotic system monitoring and stateful,
reactive failure mitigation for distributed robotic systems deployed using
Kubernetes (K8s) and the Robot Operating System (ROS2). By employing the
generic substrate of Behaviour Trees, our approach can be applied to any
robotic workload and supports arbitrarily complex monitoring and failure
mitigation strategies. We demonstrate the effectiveness and
application-agnosticism of our approach on two example applications, namely
Autonomous Mobile Robot (AMR) navigation and robotic manipulation in a
simulated environment.","Florian Mirus, Frederik Pasch, Nikhil Singhal, Kay-Ulrich Scholl",2024-10-24T15:17:09Z,2024-11-04T10:59:17Z,http://arxiv.org/abs/2410.18825v2,http://arxiv.org/pdf/2410.18825v2.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Towards Safer Planetary Exploration: A Hybrid Architecture for Terrain
  Traversability Analysis in Mars Rovers","The field of autonomous navigation for unmanned ground vehicles (UGVs) is in
continuous growth and increasing levels of autonomy have been reached in the
last few years. However, the task becomes more challenging when the focus is on
the exploration of planet surfaces such as Mars. In those situations, UGVs are
forced to navigate through unstable and rugged terrains which, inevitably, open
the vehicle to more hazards, accidents, and, in extreme cases, complete mission
failure. The paper addresses the challenges of autonomous navigation for
unmanned ground vehicles in planetary exploration, particularly on Mars,
introducing a hybrid architecture for terrain traversability analysis that
combines two approaches: appearance-based and geometry-based. The
appearance-based method uses semantic segmentation via deep neural networks to
classify different terrain types. This is further refined by pixel-level
terrain roughness classification obtained from the same RGB image, assigning
different costs based on the physical properties of the soil. The
geometry-based method complements the appearance-based approach by evaluating
the terrain's geometrical features, identifying hazards that may not be
detectable by the appearance-based side. The outputs of both methods are
combined into a comprehensive hybrid cost map. The proposed architecture was
trained on synthetic datasets and developed as a ROS2 application to integrate
into broader autonomous navigation systems for harsh environments. Simulations
have been performed in Unity, showing the ability of the method to assess
online traversability analysis.","Achille Chiuchiarelli, Giacomo Franchini, Francesco Messina, Marcello Chiaberge",2024-10-23T10:12:14Z,2024-10-23T10:12:14Z,http://arxiv.org/abs/2410.17738v1,http://arxiv.org/pdf/2410.17738v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Advancing lunar exploration through virtual reality simulations: a
  framework for future human missions","In an era marked by renewed interest in lunar exploration and the prospect of
establishing a sustainable human presence on the Moon, innovative approaches
supporting mission preparation and astronaut training are imperative. To this
end, the advancements in Virtual Reality (VR) technology offer a promising
avenue to simulate and optimize future human missions to the Moon. Through VR
simulations, tests can be performed quickly, with different environment
parameters and a human-centered perspective can be maintained throughout the
experiments. This paper presents a comprehensive framework that harnesses VR
simulations to replicate the challenges and opportunities of lunar exploration,
aiming to enhance astronaut readiness and mission success. Multiple
environments with physical and visual characteristics that reflect those found
in interesting Moon regions have been modeled and integrated into simulations
based on the Unity graphical engine. We exploit VR to allow the user to fully
immerse in the simulations and interact with assets in the same way as in real
contexts. Different scenarios have been replicated, from upcoming exploration
missions where it is possible to deploy scientific payloads, collect samples,
and traverse the surrounding environment, to long-term habitation in a
futuristic lunar base, performing everyday activities. Moreover, our framework
allows us to simulate human-robot collaboration and surveillance directly
displaying sensor readings and scheduled tasks of autonomous agents which will
be part of future hybrid missions, leveraging the ROS2-Unity bridge. Thus, the
entire project can be summarized as a desire to define cornerstones for
human-machine design and interaction, astronaut training, and learning of
potential weak points in the context of future lunar missions, through targeted
operations in a variety of contexts as close to reality as possible.","Giacomo Franchini, Brenno Tuberga, Marcello Chiaberge",2024-10-22T16:06:00Z,2024-10-22T16:06:00Z,http://arxiv.org/abs/2410.17132v1,http://arxiv.org/pdf/2410.17132v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"SMART-TRACK: A Novel Kalman Filter-Guided Sensor Fusion For Robust UAV
  Object Tracking in Dynamic Environments","In the field of sensor fusion and state estimation for object detection and
localization, ensuring accurate tracking in dynamic environments poses
significant challenges. Traditional methods like the Kalman Filter (KF) often
fail when measurements are intermittent, leading to rapid divergence in state
estimations. To address this, we introduce SMART (Sensor Measurement
Augmentation and Reacquisition Tracker), a novel approach that leverages
high-frequency state estimates from the KF to guide the search for new
measurements, maintaining tracking continuity even when direct measurements
falter. This is crucial for dynamic environments where traditional methods
struggle. Our contributions include: 1) Versatile Measurement Augmentation
Using KF Feedback: We implement a versatile measurement augmentation system
that serves as a backup when primary object detectors fail intermittently. This
system is adaptable to various sensors, demonstrated using depth cameras where
KF's 3D predictions are projected into 2D depth image coordinates, integrating
nonlinear covariance propagation techniques simplified to first-order
approximations. 2) Open-source ROS2 Implementation: We provide an open-source
ROS2 implementation of the SMART-TRACK framework, validated in a realistic
simulation environment using Gazebo and ROS2, fostering broader adaptation and
further research. Our results showcase significant enhancements in tracking
stability, with estimation RMSE as low as 0.04 m during measurement
disruptions, advancing the robustness of UAV tracking and expanding the
potential for reliable autonomous UAV operations in complex scenarios. The
implementation is available at https://github.com/mzahana/SMART-TRACK.","Khaled Gabr, Mohamed Abdelkader, Imen Jarraya, Abdullah AlMusalami, Anis Koubaa",2024-10-14T12:01:01Z,2024-10-14T12:01:01Z,http://arxiv.org/abs/2410.10409v1,http://arxiv.org/pdf/2410.10409v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"ROS2-Based Simulation Framework for Cyberphysical Security Analysis of
  UAVs","We present a new simulator of Uncrewed Aerial Vehicles (UAVs) that is
  tailored to the needs of testing cyber-physical security attacks and
  defenses. Recent investigations into UAV safety have unveiled various attack
  surfaces and some defense mechanisms. However, due to escalating regulations
  imposed by aviation authorities on security research on real UAVs, and the
  substantial costs associated with hardware test-bed configurations, there
  arises a necessity for a simulator capable of substituting for hardware
  experiments, and/or narrowing down their scope to the strictly necessary.
  The study of different attack mechanisms requires specific features in a
  simulator. We propose a simulation framework based on ROS2, leveraging some
  of its key advantages, including modularity, replicability, customization,
  and the utilization of open-source tools such as Gazebo. Our framework has a
  built-in motion planner, controller, communication models and attack models.
  We share examples of research use cases that our framework can enable,
  demonstrating its utility.","Unmesh Patil, Akshith Gunasekaran, Rakesh Bobba, Houssam Abbas",2024-10-04T23:23:54Z,2024-10-04T23:23:54Z,http://arxiv.org/abs/2410.03971v1,http://arxiv.org/pdf/2410.03971v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Constrained Bandwidth Observation Sharing for Multi-Robot Navigation in
  Dynamic Environments via Intelligent Knapsack","Multi-robot navigation is increasingly crucial in various domains, including
disaster response, autonomous vehicles, and warehouse and manufacturing
automation. Robot teams often must operate in highly dynamic environments and
under strict bandwidth constraints imposed by communication infrastructure,
rendering effective observation sharing within the system a challenging
problem. This paper presents a novel optimal communication scheme, Intelligent
Knapsack (iKnap), for multi-robot navigation in dynamic environments under
bandwidth constraints. We model multi-robot communication as belief propagation
in a graph of inferential agents. We then formulate the combinatorial
optimization for observation sharing as a 0/1 knapsack problem, where each
potential pairwise communication between robots is assigned a decision-making
utility to be weighed against its bandwidth cost, and the system has some
cumulative bandwidth limit. We evaluate our approach in a simulated robotic
warehouse with human workers using ROS2 and the Open Robotics Middleware
Framework. Compared to state-of-the-art broadcast-based optimal communication
schemes, iKnap yields significant improvements in navigation performance with
respect to scenario complexity while maintaining a similar runtime.
Furthermore, iKnap utilizes allocated bandwidth and observational resources
more efficiently than existing approaches, especially in very low-resource and
high-uncertainty settings. Based on these results, we claim that the proposed
method enables more robust collaboration for multi-robot teams in real-world
navigation problems.","Anirudh Chari, Rui Chen, Han Zheng, Changliu Liu",2024-09-16T04:13:22Z,2025-03-03T16:24:36Z,http://arxiv.org/abs/2409.09975v2,http://arxiv.org/pdf/2409.09975v2.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Scenario Execution for Robotics: A generic, backend-agnostic library for
  running reproducible robotics experiments and tests","Testing and evaluation of robotics systems is a difficult and oftentimes
tedious task due to the systems' complexity and a lack of tools to conduct
reproducible robotics experiments. Additionally, almost all available tools are
either tailored towards a specific application domain, simulator or middleware.
Particularly scenario-based testing, a common practice in the domain of
automated driving, is not sufficiently covered in the robotics domain. In this
paper, we propose a novel backend- and middleware-agnostic approach for
conducting systematic, reproducible and automatable robotics experiments called
Scenario Execution for Robotics. Our approach is implemented as a Python
library built on top of the generic scenario description language OpenSCENARIO
2 and Behavior Trees and is made publicly available on GitHub. In extensive
experiments, we demonstrate that our approach supports multiple simulators as
backend and can be used as a standalone Python-library or as part of the ROS2
ecosystem. Furthermore, we demonstrate how our approach enables testing over
ranges of varying values. Finally, we show how Scenario Execution for Robotics
allows to move from simulation-based to real-world experiments with minimal
adaptations to the scenario description file.","Frederik Pasch, Florian Mirus, Yongzhou Zhang, Kay-Ulrich Scholl",2024-09-11T08:07:56Z,2024-09-11T08:07:56Z,http://arxiv.org/abs/2409.07080v1,http://arxiv.org/pdf/2409.07080v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
Newton-Raphson Flow for Aggressive Quadrotor Tracking Control,"We apply the Newton-Raphson flow tracking controller to aggressive quadrotor
flight and demonstrate that it achieves good tracking performance over a suite
of benchmark trajectories, beating the native trajectory tracking controller in
the popular PX4 Autopilot. The Newton-Raphson flow tracking controller is a
recently proposed integrator-type controller that aims to drive to zero the
error between a future predicted system output and the reference trajectory.
This controller is computationally lightweight, requiring only an imprecise
predictor, and achieves guaranteed asymptotic error bounds under certain
conditions. We show that these theoretical advantages are realizable on a
quadrotor hardware platform. Our experiments are conducted on a Holybrox x500v2
quadrotor using a Pixhawk 6x flight controller and a Rasbperry Pi 4 companion
computer which receives location information from an OptiTrack motion capture
system and sends input commands through the ROS2 API for the PX4 software
stack.","Evanns Morales-Cuadrado, Christian Llanes, Yorai Wardi, Samuel Coogan",2024-08-20T21:11:38Z,2024-08-20T21:11:38Z,http://arxiv.org/abs/2408.11197v1,http://arxiv.org/pdf/2408.11197v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Case Study: Runtime Safety Verification of Neural Network Controlled
  System","Neural networks are increasingly used in safety-critical applications such as
robotics and autonomous vehicles. However, the deployment of
neural-network-controlled systems (NNCSs) raises significant safety concerns.
Many recent advances overlook critical aspects of verifying control and
ensuring safety in real-time scenarios. This paper presents a case study on
using POLAR-Express, a state-of-the-art NNCS reachability analysis tool, for
runtime safety verification in a Turtlebot navigation system using LiDAR. The
Turtlebot, equipped with a neural network controller for steering, operates in
a complex environment with obstacles. We developed a safe online controller
switching strategy that switches between the original NNCS controller and an
obstacle avoidance controller based on the verification results. Our
experiments, conducted in a ROS2 Flatland simulation environment, explore the
capabilities and limitations of using POLAR-Express for runtime verification
and demonstrate the effectiveness of our switching strategy.","Frank Yang, Sinong Simon Zhan, Yixuan Wang, Chao Huang, Qi Zhu",2024-08-16T07:53:25Z,2024-08-16T07:53:25Z,http://arxiv.org/abs/2408.08592v1,http://arxiv.org/pdf/2408.08592v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"SoNIC: Safe Social Navigation with Adaptive Conformal Inference and
  Constrained Reinforcement Learning","Reinforcement learning (RL) enables social robots to generate trajectories
without relying on human-designed rules or interventions, making it generally
more effective than rule-based systems in adapting to complex, dynamic
real-world scenarios. However, social navigation is a safety-critical task that
requires robots to avoid collisions with pedestrians, whereas existing RL-based
solutions often fall short of ensuring safety in complex environments. In this
paper, we propose SoNIC, which to the best of our knowledge is the first
algorithm that integrates adaptive conformal inference (ACI) with constrained
reinforcement learning (CRL) to enable safe policy learning for social
navigation. Specifically, our method not only augments RL observations with
ACI-generated nonconformity scores, which inform the agent of the quantified
uncertainty but also employs these uncertainty estimates to effectively guide
the behaviors of RL agents by using constrained reinforcement learning. This
integration regulates the behaviors of RL agents and enables them to handle
safety-critical situations. On the standard CrowdNav benchmark, our method
achieves a success rate of 96.93%, which is 11.67% higher than the previous
state-of-the-art RL method and results in 4.5 times fewer collisions and 2.8
times fewer intrusions to ground-truth human future trajectories as well as
enhanced robustness in out-of-distribution scenarios. To further validate our
approach, we deploy our algorithm on a real robot by developing a ROS2-based
navigation system. Our experiments demonstrate that the system can generate
robust and socially polite decision-making when interacting with both sparse
and dense crowds. The video demos can be found on our project website:
https://sonic-social-nav.github.io/.","Jianpeng Yao, Xiaopan Zhang, Yu Xia, Zejin Wang, Amit K. Roy-Chowdhury, Jiachen Li",2024-07-24T17:57:21Z,2025-02-06T18:55:45Z,http://arxiv.org/abs/2407.17460v2,http://arxiv.org/pdf/2407.17460v2.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
Toychain: A Simple Blockchain for Research in Swarm Robotics,"This technical report describes the implementation of Toychain: a simple,
lightweight blockchain implemented in Python, designed for ease of deployment
and practicality in robotics research. It can be integrated with various
software and simulation tools used in robotics (we have integrated it with
ARGoS, Gazebo, and ROS2), and also be deployed on real robots capable of Wi-Fi
communications. The Toychain package supports the deployment of smart contracts
written in Python (computer programs that can be executed by and synchronized
across a distributed network). The nodes in the blockchain can execute smart
contract functions by broadcasting transactions, which update the state of the
blockchain upon agreement by all other nodes. The conditions for this agreement
are established by a consensus protocol. The Toychain package allows for custom
implementations of the consensus protocol, which can be useful for research or
meeting specific application requirements. Currently, Proof-of-Work and
Proof-of-Authority are implemented.","Alexandre Pacheco, Ulysse Denis, Raina Zakir, Volker Strobel, Andreagiovanni Reina, Marco Dorigo",2024-07-09T07:54:34Z,2024-07-09T07:54:34Z,http://arxiv.org/abs/2407.06630v1,http://arxiv.org/pdf/2407.06630v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"VIPS-Odom: Visual-Inertial Odometry Tightly-coupled with Parking Slots
  for Autonomous Parking","Precise localization is of great importance for autonomous parking task since
it provides service for the downstream planning and control modules, which
significantly affects the system performance. For parking scenarios, dynamic
lighting, sparse textures, and the instability of global positioning system
(GPS) signals pose challenges for most traditional localization methods. To
address these difficulties, we propose VIPS-Odom, a novel semantic
visual-inertial odometry framework for underground autonomous parking, which
adopts tightly-coupled optimization to fuse measurements from multi-modal
sensors and solves odometry. Our VIPS-Odom integrates parking slots detected
from the synthesized bird-eye-view (BEV) image with traditional feature points
in the frontend, and conducts tightly-coupled optimization with joint
constraints introduced by measurements from the inertial measurement unit,
wheel speed sensor and parking slots in the backend. We develop a multi-object
tracking framework to robustly track parking slots' states. To prove the
superiority of our method, we equip an electronic vehicle with related sensors
and build an experimental platform based on ROS2 system. Extensive experiments
demonstrate the efficacy and advantages of our method compared with other
baselines for parking scenarios.","Xuefeng Jiang, Fangyuan Wang, Rongzhang Zheng, Han Liu, Yixiong Huo, Jinzhang Peng, Lu Tian, Emad Barsoum",2024-07-06T09:21:25Z,2024-07-06T09:21:25Z,http://arxiv.org/abs/2407.05017v1,http://arxiv.org/pdf/2407.05017v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"VLM-Auto: VLM-based Autonomous Driving Assistant with Human-like
  Behavior and Understanding for Complex Road Scenes","Recent research on Large Language Models for autonomous driving shows promise
in planning and control. However, high computational demands and hallucinations
still challenge accurate trajectory prediction and control signal generation.
Deterministic algorithms offer reliability but lack adaptability to complex
driving scenarios and struggle with context and uncertainty. To address this
problem, we propose VLM-Auto, a novel autonomous driving assistant system to
empower the autonomous vehicles with adjustable driving behaviors based on the
understanding of road scenes. A pipeline involving the CARLA simulator and
Robot Operating System 2 (ROS2) verifying the effectiveness of our system is
presented, utilizing a single Nvidia 4090 24G GPU while exploiting the capacity
of textual output of the Visual Language Model (VLM). Besides, we also
contribute a dataset containing an image set and a corresponding prompt set for
fine-tuning the VLM module of our system. In CARLA experiments, our system
achieved $97.82\%$ average precision on 5 types of labels in our dataset. In
the real-world driving dataset, our system achieved $96.97\%$ prediction
accuracy in night scenes and gloomy scenes. Our VLM-Auto dataset will be
released at https://github.com/ZionGo6/VLM-Auto.","Ziang Guo, Zakhar Yagudin, Artem Lykov, Mikhail Konenkov, Dzmitry Tsetserukou",2024-05-09T16:17:41Z,2024-10-02T13:46:39Z,http://arxiv.org/abs/2405.05885v3,http://arxiv.org/pdf/2405.05885v3.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Hierarchically Decentralized Heterogeneous Multi-Robot Task Allocation
  System","With plans to send humans to the Moon and further, the supply of resources
like oxygen, water, fuel, etc., can be satiated by performing In-Situ Resource
Utilization (ISRU), where resources from the extra-terrestrial body are
extracted to be utilized. These ISRU missions can be carried out by a
Multi-Robot System (MRS). In this research, a high-level auction- based
Multi-Robot Task Allocation (MRTA) system is developed for coordinating tasks
amongst multiple robots with distinct capabilities. A hierarchical
decentralized coordination architecture is implemented in this research to
allocate the tasks amongst the robots for achieving intentional cooperation in
the Multi-Robot System (MRS). 3 different policies are formulated that govern
how robots should act in the multiple auction situations of the auction-based
task allocation system proposed in this research, and their performance is
evaluated in a 2D simulation called pyrobosim using ROS2. The decentralized
coordination architecture and the auction-based MRTA make the MRS highly
scalable, reliable, flexible, and robust.","Sujeet Kashid, Ashwin D. Kumat",2024-05-03T21:18:46Z,2024-05-03T21:18:46Z,http://arxiv.org/abs/2405.02484v1,http://arxiv.org/pdf/2405.02484v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
The Cambridge RoboMaster: An Agile Multi-Robot Research Platform,"Compact robotic platforms with powerful compute and actuation capabilities
are key enablers for practical, real-world deployments of multi-agent research.
This article introduces a tightly integrated hardware, control, and simulation
software stack on a fleet of holonomic ground robot platforms designed with
this motivation. Our robots, a fleet of customised DJI Robomaster S1 vehicles,
offer a balance between small robots that do not possess sufficient compute or
actuation capabilities and larger robots that are unsuitable for indoor
multi-robot tests. They run a modular ROS2-based optimal estimation and control
stack for full onboard autonomy, contain ad-hoc peer-to-peer communication
infrastructure, and can zero-shot run multi-agent reinforcement learning (MARL)
policies trained in our vectorized multi-agent simulation framework. We present
an in-depth review of other platforms currently available, showcase new
experimental validation of our system's capabilities, and introduce case
studies that highlight the versatility and reliability of our system as a
testbed for a wide range of research demonstrations. Our system as well as
supplementary material is available online.
https://proroklab.github.io/cambridge-robomaster","Jan Blumenkamp, Ajay Shankar, Matteo Bettini, Joshua Bird, Amanda Prorok",2024-05-03T15:54:20Z,2024-10-27T12:05:52Z,http://arxiv.org/abs/2405.02198v2,http://arxiv.org/pdf/2405.02198v2.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"HawkDrive: A Transformer-driven Visual Perception System for Autonomous
  Driving in Night Scene","Many established vision perception systems for autonomous driving scenarios
ignore the influence of light conditions, one of the key elements for driving
safety. To address this problem, we present HawkDrive, a novel perception
system with hardware and software solutions. Hardware that utilizes stereo
vision perception, which has been demonstrated to be a more reliable way of
estimating depth information than monocular vision, is partnered with the edge
computing device Nvidia Jetson Xavier AGX. Our software for low light
enhancement, depth estimation, and semantic segmentation tasks, is a
transformer-based neural network. Our software stack, which enables fast
inference and noise reduction, is packaged into system modules in Robot
Operating System 2 (ROS2). Our experimental results have shown that the
proposed end-to-end system is effective in improving the depth estimation and
semantic segmentation performance. Our dataset and codes will be released at
https://github.com/ZionGo6/HawkDrive.","Ziang Guo, Stepan Perminov, Mikhail Konenkov, Dzmitry Tsetserukou",2024-04-06T15:10:29Z,2024-05-06T12:34:34Z,http://arxiv.org/abs/2404.04653v2,http://arxiv.org/pdf/2404.04653v2.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"Hybrid Force Motion Control with Estimated Surface Normal for
  Manufacturing Applications","This paper proposes a hybrid force-motion framework that utilizes real-time
surface normal updates. The surface normal is estimated via a novel method that
leverages force sensing measurements and velocity commands to compensate the
friction bias. This approach is critical for robust execution of precision
force-controlled tasks in manufacturing, such as thermoplastic tape replacement
that traces surfaces or paths on a workpiece subject to uncertainties deviated
from the model. We formulated the proposed method and implemented the framework
in ROS2 environment. The approach was validated using kinematic simulations and
a hardware platform. Specifically, we demonstrated the approach on a 7-DoF
manipulator equipped with a force/torque sensor at the end-effector.","Ehsan Nasiri, Long Wang",2024-04-05T21:38:38Z,2024-04-05T21:38:38Z,http://arxiv.org/abs/2404.04419v1,http://arxiv.org/pdf/2404.04419v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"A Study on the Use of Simulation in Synthesizing Path-Following Control
  Policies for Autonomous Ground Robots","We report results obtained and insights gained while answering the following
question: how effective is it to use a simulator to establish path following
control policies for an autonomous ground robot? While the quality of the
simulator conditions the answer to this question, we found that for the
simulation platform used herein, producing four control policies for path
planning was straightforward once a digital twin of the controlled robot was
available. The control policies established in simulation and subsequently
demonstrated in the real world are PID control, MPC, and two neural network
(NN) based controllers. Training the two NN controllers via imitation learning
was accomplished expeditiously using seven simple maneuvers: follow three
circles clockwise, follow the same circles counter-clockwise, and drive
straight. A test randomization process that employs random micro-simulations is
used to rank the ``goodness'' of the four control policies. The policy ranking
noted in simulation correlates well with the ranking observed when the control
policies were tested in the real world. The simulation platform used is
publicly available and BSD3-released as open source; a public Docker image is
available for reproducibility studies. It contains a dynamics engine, a sensor
simulator, a ROS2 bridge, and a ROS2 autonomy stack the latter employed both in
the simulator and the real world experiments.","Harry Zhang, Stefan Caldararu, Aaron Young, Alexis Ruiz, Huzaifa Unjhawala, Ishaan Mahajan, Sriram Ashokkumar, Nevindu Batagoda, Zhenhao Zhou, Luning Bakke, Dan Negrut",2024-03-26T18:13:20Z,2024-03-26T18:13:20Z,http://arxiv.org/abs/2403.18021v1,http://arxiv.org/pdf/2403.18021v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
Lifelong LERF: Local 3D Semantic Inventory Monitoring Using FogROS2,"Inventory monitoring in homes, factories, and retail stores relies on
maintaining data despite objects being swapped, added, removed, or moved. We
introduce Lifelong LERF, a method that allows a mobile robot with minimal
compute to jointly optimize a dense language and geometric representation of
its surroundings. Lifelong LERF maintains this representation over time by
detecting semantic changes and selectively updating these regions of the
environment, avoiding the need to exhaustively remap. Human users can query
inventory by providing natural language queries and receiving a 3D heatmap of
potential object locations. To manage the computational load, we use Fog-ROS2,
a cloud robotics platform, to offload resource-intensive tasks. Lifelong LERF
obtains poses from a monocular RGBD SLAM backend, and uses these poses to
progressively optimize a Language Embedded Radiance Field (LERF) for semantic
monitoring. Experiments with 3-5 objects arranged on a tabletop and a Turtlebot
with a RealSense camera suggest that Lifelong LERF can persistently adapt to
changes in objects with up to 91% accuracy.","Adam Rashid, Chung Min Kim, Justin Kerr, Letian Fu, Kush Hari, Ayah Ahmad, Kaiyuan Chen, Huang Huang, Marcus Gualtieri, Michael Wang, Christian Juette, Nan Tian, Liu Ren, Ken Goldberg",2024-03-15T17:29:54Z,2024-03-15T17:29:54Z,http://arxiv.org/abs/2403.10494v1,http://arxiv.org/pdf/2403.10494v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
R4: rapid reproducible robotics research open hardware control system,"A key component of any robot is the interface between robotics middleware
software and physical motors. New robots often use arbitrary, messy mixtures of
closed and open motor drivers and error-prone physical mountings, wiring, and
connectors to interface them. There is a need for a standardizing OSH component
to abstract this complexity, as Arduino did for interfacing to smaller
components. We present a OSH printed circuit board to solve this problem once
and for all. On the high-level side, it interfaces to Arduino Giga - acting as
an unusually large and robust shield - and thus to existing open source
software stacks. A ROS2 interface is provided. On the lower-level side, it
interfaces to existing emerging standard open hardware including OSH motor
drivers and relays, which can already be used to drive fully open hardware
wheeled and arm robots. This enables the creation of a family of standardized,
fully open hardware, fully reproducible, research platforms.","Chris Waltham, Andy Perrett, Rakshit Soni, Charles Fox",2024-02-15T09:52:03Z,2025-06-29T17:51:46Z,http://arxiv.org/abs/2402.09833v3,http://arxiv.org/pdf/2402.09833v3.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"ROSpace: Intrusion Detection Dataset for a ROS2-Based Cyber-Physical
  System","Most of the intrusion detection datasets to research machine learning-based
intrusion detection systems (IDSs) are devoted to cyber-only systems, and they
typically collect data from one architectural layer. Additionally, often the
attacks are generated in dedicated attack sessions, without reproducing the
realistic alternation and overlap of normal and attack actions. We present a
dataset for intrusion detection by performing penetration testing on an
embedded cyber-physical system built over Robot Operating System 2 (ROS2).
Features are monitored from three architectural layers: the Linux operating
system, the network, and the ROS2 services. The dataset is structured as a time
series and describes the expected behavior of the system and its response to
ROS2-specific attacks: it repeatedly alternates periods of attack-free
operation with periods when a specific attack is being performed. Noteworthy,
this allows measuring the time to detect an attacker and the number of
malicious activities performed before detection. Also, it allows training an
intrusion detector to minimize both, by taking advantage of the numerous
alternating periods of normal and attack operations.","Tommaso Puccetti, Simone Nardi, Cosimo Cinquilli, Tommaso Zoppi, Andrea Ceccarelli",2024-02-13T13:54:47Z,2024-02-13T13:54:47Z,http://arxiv.org/abs/2402.08468v1,http://arxiv.org/pdf/2402.08468v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"GarchingSim: An Autonomous Driving Simulator with Photorealistic Scenes
  and Minimalist Workflow","Conducting real road testing for autonomous driving algorithms can be
expensive and sometimes impractical, particularly for small startups and
research institutes. Thus, simulation becomes an important method for
evaluating these algorithms. However, the availability of free and open-source
simulators is limited, and the installation and configuration process can be
daunting for beginners and interdisciplinary researchers. We introduce an
autonomous driving simulator with photorealistic scenes, meanwhile keeping a
user-friendly workflow. The simulator is able to communicate with external
algorithms through ROS2 or Socket.IO, making it compatible with existing
software stacks. Furthermore, we implement a highly accurate vehicle dynamics
model within the simulator to enhance the realism of the vehicle's physical
effects. The simulator is able to serve various functions, including generating
synthetic data and driving with machine learning-based algorithms. Moreover, we
prioritize simplicity in the deployment process, ensuring that beginners find
it approachable and user-friendly.","Liguo Zhou, Yinglei Song, Yichao Gao, Zhou Yu, Michael Sodamin, Hongshen Liu, Liang Ma, Lian Liu, Hao Liu, Yang Liu, Haichuan Li, Guang Chen, Alois Knoll",2024-01-28T23:26:15Z,2024-01-30T15:57:22Z,http://arxiv.org/abs/2401.15803v2,http://arxiv.org/pdf/2401.15803v2.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
"A Multi-Agent Security Testbed for the Analysis of Attacks and Defenses
  in Collaborative Sensor Fusion","The performance and safety of autonomous vehicles (AVs) deteriorates under
adverse environments and adversarial actors. The investment in multi-sensor,
multi-agent (MSMA) AVs is meant to promote improved efficiency of travel and
mitigate safety risks. Unfortunately, minimal investment has been made to
develop security-aware MSMA sensor fusion pipelines leaving them vulnerable to
adversaries. To advance security analysis of AVs, we develop the Multi-Agent
Security Testbed, MAST, in the Robot Operating System (ROS2). Our framework is
scalable for general AV scenarios and is integrated with recent multi-agent
datasets. We construct the first bridge between AVstack and ROS and develop
automated AV pipeline builds to enable rapid AV prototyping. We tackle the
challenge of deploying variable numbers of agent/adversary nodes at launch-time
with dynamic topic remapping. Using this testbed, we motivate the need for
security-aware AV architectures by exposing the vulnerability of centralized
multi-agent fusion pipelines to (un)coordinated adversary models in case
studies and Monte Carlo analysis.","R. Spencer Hallyburton, David Hunt, Shaocheng Luo, Miroslav Pajic",2024-01-17T17:59:46Z,2024-01-17T17:59:46Z,http://arxiv.org/abs/2401.09387v1,http://arxiv.org/pdf/2401.09387v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2024
b-it-bots RoboCup@Work Team Description Paper 2023,"This paper presents the b-it-bots RoboCup@Work team and its current hardware
and functional architecture for the KUKA youBot robot. We describe the
underlying software framework and the developed capabilities required for
operating in industrial environments including features such as reliable and
precise navigation, flexible manipulation, robust object recognition and task
planning. New developments include an approach to grasp vertical objects,
placement of objects by considering the empty space on a workstation, and the
process of porting our code to ROS2.","Kevin Patel, Vamsi Kalagaturu, Vivek Mannava, Ravisankar Selvaraju, Shubham Shinde, Dharmin Bakaraniya, Deebul Nair, Mohammad Wasil, Santosh Thoduka, Iman Awaad, Sven Schneider, Nico Hochgeschwender, Paul G. Plöger",2023-12-29T15:18:14Z,2023-12-29T15:18:14Z,http://arxiv.org/abs/2312.17643v1,http://arxiv.org/pdf/2312.17643v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2023
"An Industrial Perspective on Multi-Agent Decision Making for
  Interoperable Robot Navigation following the VDA5050 Standard","This paper provides a perspective on the literature and current challenges in
Multi-Agent Systems for interoperable robot navigation in industry. The focus
is on the multi-agent decision stack for Autonomous Mobile Robots operating in
mixed environments with humans, manually driven vehicles, and legacy Automated
Guided Vehicles. We provide typical characteristics of such Multi-Agent Systems
observed today and how these are expected to change on the short term due to
the new standard VDA5050 and the interoperability framework OpenRMF. We present
recent changes in fleet management standards and the role of open middleware
frameworks like ROS2 reaching industrial-grade quality. Approaches to increase
the robustness and performance of multi-robot navigation systems for
transportation are discussed, and research opportunities are derived.","Niels van Duijkeren, Luigi Palmieri, Ralph Lange, Alexander Kleiner",2023-11-24T17:16:57Z,2023-11-24T17:16:57Z,http://arxiv.org/abs/2311.14615v1,http://arxiv.org/pdf/2311.14615v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2023
"Trace-enabled Timing Model Synthesis for ROS2-based Autonomous
  Applications","Autonomous applications are typically developed over Robot Operating System
2.0 (ROS2) even in time-critical systems like automotive. Recent years have
seen increased interest in developing model-based timing analysis and schedule
optimization approaches for ROS2-based applications. To complement these
approaches, we propose a tracing and measurement framework to obtain timing
models of ROS2-based applications. It offers a tracer based on extended
Berkeley Packet Filter (eBPF) that probes different functions in ROS2
middleware and reads their arguments or return values to reason about the data
flow in applications. It combines event traces from ROS2 and the operating
system to generate a directed acyclic graph showing ROS2 callbacks, precedence
relations between them, and their timing attributes. While being compatible
with existing analyses, we also show how to model (i)~message synchronization,
e.g., in sensor fusion, and (ii)~service requests from multiple clients, e.g.,
in motion planning. Considering that, in real-world scenarios, the application
code might be confidential and formal models are unavailable, our framework
still enables the application of existing analysis and optimization techniques.","Hazem Abaza, Debayan Roy, Shiqing Fan, Selma Saidi, Antonios Motakis",2023-11-22T11:54:42Z,2023-11-23T10:09:31Z,http://arxiv.org/abs/2311.13333v2,http://arxiv.org/pdf/2311.13333v2.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2023
"FogROS2-Config: Optimizing Latency and Cost for Multi-Cloud Robot
  Applications","Cloud service providers provide over 50,000 distinct and dynamically changing
set of cloud server options. To help roboticists make cost-effective decisions,
we present FogROS2-Config, an open toolkit that takes ROS2 nodes as input and
automatically runs relevant benchmarks to quickly return a menu of cloud
compute services that tradeoff latency and cost. Because it is infeasible to
try every hardware configuration, FogROS2-Config quickly samples tests a small
set of edge case servers. We evaluate FogROS2-Config on three robotics
application tasks: visual SLAM, grasp planning. and motion planning.
FogROS2-Config can reduce the cost by up to 20x. By comparing with a Pareto
frontier for cost and latency by running the application task on feasible
server configurations, we evaluate cost and latency models and confirm that
FogROS2-Config selects efficient hardware configurations to balance cost and
latency.","Kaiyuan Chen, Kush Hari, Rohil Khare, Charlotte Le, Trinity Chung, Jaimyn Drake, Jeffrey Ichnowski, John Kubiatowicz, Ken Goldberg",2023-11-09T18:57:36Z,2024-05-13T05:46:34Z,http://arxiv.org/abs/2311.05600v2,http://arxiv.org/pdf/2311.05600v2.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2023
Risk-aware Control for Robots with Non-Gaussian Belief Spaces,"This paper addresses the problem of safety-critical control of autonomous
robots, considering the ubiquitous uncertainties arising from unmodeled
dynamics and noisy sensors. To take into account these uncertainties,
probabilistic state estimators are often deployed to obtain a belief over
possible states. Namely, Particle Filters (PFs) can handle arbitrary
non-Gaussian distributions in the robot's state. In this work, we define the
belief state and belief dynamics for continuous-discrete PFs and construct safe
sets in the underlying belief space. We design a controller that provably keeps
the robot's belief state within this safe set. As a result, we ensure that the
risk of the unknown robot's state violating a safety specification, such as
avoiding a dangerous area, is bounded. We provide an open-source implementation
as a ROS2 package and evaluate the solution in simulations and hardware
experiments involving high-dimensional belief spaces.","Matti Vahs, Jana Tumova",2023-09-22T13:31:29Z,2024-03-27T09:26:56Z,http://arxiv.org/abs/2309.12857v2,http://arxiv.org/pdf/2309.12857v2.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2023
OmniLRS: A Photorealistic Simulator for Lunar Robotics,"Developing algorithms for extra-terrestrial robotic exploration has always
been challenging. Along with the complexity associated with these environments,
one of the main issues remains the evaluation of said algorithms. With the
regained interest in lunar exploration, there is also a demand for quality
simulators that will enable the development of lunar robots. % In this paper,
we explain how we built a Lunar simulator based on Isaac Sim, Nvidia's robotic
simulator. In this paper, we propose Omniverse Lunar Robotic-Sim (OmniLRS) that
is a photorealistic Lunar simulator based on Nvidia's robotic simulator. This
simulation provides fast procedural environment generation, multi-robot
capabilities, along with synthetic data pipeline for machine-learning
applications. It comes with ROS1 and ROS2 bindings to control not only the
robots, but also the environments. This work also performs sim-to-real rock
instance segmentation to show the effectiveness of our simulator for
image-based perception. Trained on our synthetic data, a yolov8 model achieves
performance close to a model trained on real-world data, with 5% performance
gap. When finetuned with real data, the model achieves 14% higher average
precision than the model trained on real-world data, demonstrating our
simulator's photorealism.% to realize sim-to-real. The code is fully
open-source, accessible here: https://github.com/AntoineRichard/LunarSim, and
comes with demonstrations.","Antoine Richard, Junnosuke Kamohara, Kentaro Uno, Shreya Santra, Dave van der Meer, Miguel Olivares-Mendez, Kazuya Yoshida",2023-09-16T13:48:47Z,2023-09-16T13:48:47Z,http://arxiv.org/abs/2309.08997v1,http://arxiv.org/pdf/2309.08997v1.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2023
"Comparison of Middlewares in Edge-to-Edge and Edge-to-Cloud
  Communication for Distributed ROS2 Systems","The increased data transmission and number of devices involved in
communications among distributed systems make it challenging yet significantly
necessary to have an efficient and reliable networking middleware. In robotics
and autonomous systems, the wide application of ROS\,2 brings the possibility
of utilizing various networking middlewares together with DDS in ROS\,2 for
better communication among edge devices or between edge devices and the cloud.
However, there is a lack of comprehensive communication performance comparison
of integrating these networking middlewares with ROS\,2. In this study, we
provide a quantitative analysis for the communication performance of utilized
networking middlewares including MQTT and Zenoh alongside DDS in ROS\,2 among a
multiple host system. For a complete and reliable comparison, we calculate the
latency and throughput of these middlewares by sending distinct amounts and
types of data through different network setups including Ethernet, Wi-Fi, and
4G. To further extend the evaluation to real-world application scenarios, we
assess the drift error (the position changes) over time caused by these
networking middlewares with the robot moving in an identical square-shaped
path. Our results show that CycloneDDS performs better under Ethernet while
Zenoh performs better under Wi-Fi and 4G. In the actual robot test, the robot
moving trajectory drift error over time (96\,s) via Zenoh is the smallest. It
is worth noting we have a discussion of the CPU utilization of these networking
middlewares and the performance impact caused by enabling the security feature
in ROS\,2 at the end of the paper.","Jiaqiang Zhang, Xianjia Yu, Sier Ha, Jorge Pena Queralta, Tomi Westerlund",2023-09-14T08:01:11Z,2024-11-16T20:31:56Z,http://arxiv.org/abs/2309.07496v4,http://arxiv.org/pdf/2309.07496v4.pdf,all:ROS2 AND all:robot AND submittedDate:[202309062229 TO 202509052229],2023
RTPS Attack Dataset Description,"This paper explains all about our RTPS datasets. We collect malicious/benign
packet data by injecting attack data in an Unmanned Ground Vehicle (UGV) in the
normal state. We assembled the testbed, consisting of UGV, Controller, PC, and
Router. We collect this dataset in the UGV part of our testbed.
  We conducted two types of attack ""Command Injection"" and ""Command Injection
with ARP Spoofing"" on our testbed. The data collection time is 180, 300, 600,
and 1200. The scenario has 30 each on collection time, 240 total. We expect
this dataset to contribute to the development of defense technologies like
anomaly detection to address security threat issues in ROS2 networks and
Fast-DDS implements.","Dong Young Kim, Dongsung Kim, Yuchan Song, Gang Min Kim, Min Geun Song, Jeong Do Yoo, Huy Kang Kim",2023-11-24T14:10:17Z,2024-04-02T08:28:31Z,http://arxiv.org/abs/2311.14496v4,http://arxiv.org/pdf/2311.14496v4.pdf,all:ROS2 AND all:UGV AND submittedDate:[202309062229 TO 202509052229],2023
