title,summary,authors,published,updated,url,pdf_url,matched_query,year
"The Role of Embodiment in Intuitive Whole-Body Teleoperation for Mobile
  Manipulation","Intuitive Teleoperation interfaces are essential for mobile manipulation
robots to ensure high quality data collection while reducing operator workload.
A strong sense of embodiment combined with minimal physical and cognitive
demands not only enhances the user experience during large-scale data
collection, but also helps maintain data quality over extended periods. This
becomes especially crucial for challenging long-horizon mobile manipulation
tasks that require whole-body coordination. We compare two distinct robot
control paradigms: a coupled embodiment integrating arm manipulation and base
navigation functions, and a decoupled embodiment treating these systems as
separate control entities. Additionally, we evaluate two visual feedback
mechanisms: immersive virtual reality and conventional screen-based
visualization of the robot's field of view. These configurations were
systematically assessed across a complex, multi-stage task sequence requiring
integrated planning and execution. Our results show that the use of VR as a
feedback modality increases task completion time, cognitive workload, and
perceived effort of the teleoperator. Coupling manipulation and navigation
leads to a comparable workload on the user as decoupling the embodiments, while
preliminary experiments suggest that data acquired by coupled teleoperation
leads to better imitation learning performance. Our holistic view on intuitive
teleoperation interfaces provides valuable insight into collecting
high-quality, high-dimensional mobile manipulation data at scale with the human
operator in mind. Project
website:https://sophiamoyen.github.io/role-embodiment-wbc-moma-teleop/","Sophia Bianchi Moyen, Rickmer Krohn, Sophie Lueth, Kay Pompetzki, Jan Peters, Vignesh Prasad, Georgia Chalvatzaki",2025-09-03T11:25:36Z,2025-09-03T11:25:36Z,http://arxiv.org/abs/2509.03222v1,http://arxiv.org/pdf/2509.03222v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"MoTo: A Zero-shot Plug-in Interaction-aware Navigation for General
  Mobile Manipulation","Mobile manipulation stands as a core challenge in robotics, enabling robots
to assist humans across varied tasks and dynamic daily environments.
Conventional mobile manipulation approaches often struggle to generalize across
different tasks and environments due to the lack of large-scale training.
However, recent advances in manipulation foundation models demonstrate
impressive generalization capability on a wide range of fixed-base manipulation
tasks, which are still limited to a fixed setting. Therefore, we devise a
plug-in module named MoTo, which can be combined with any off-the-shelf
manipulation foundation model to empower them with mobile manipulation ability.
Specifically, we propose an interaction-aware navigation policy to generate
robot docking points for generalized mobile manipulation. To enable zero-shot
ability, we propose an interaction keypoints framework via vision-language
models (VLM) under multi-view consistency for both target object and robotic
arm following instructions, where fixed-base manipulation foundation models can
be employed. We further propose motion planning objectives for the mobile base
and robot arm, which minimize the distance between the two keypoints and
maintain the physical feasibility of trajectories. In this way, MoTo guides the
robot to move to the docking points where fixed-base manipulation can be
successfully performed, and leverages VLM generation and trajectory
optimization to achieve mobile manipulation in a zero-shot manner, without any
requirement on mobile manipulation expert data. Extensive experimental results
on OVMM and real-world demonstrate that MoTo achieves success rates of 2.68%
and 16.67% higher than the state-of-the-art mobile manipulation methods,
respectively, without requiring additional training data.","Zhenyu Wu, Angyuan Ma, Xiuwei Xu, Hang Yin, Yinan Liang, Ziwei Wang, Jiwen Lu, Haibin Yan",2025-09-01T17:59:03Z,2025-09-01T17:59:03Z,http://arxiv.org/abs/2509.01658v1,http://arxiv.org/pdf/2509.01658v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Minimizing AoI in Mobile Edge Computing: Nested Index Policy with
  Preemptive and Non-preemptive Structure","Mobile Edge Computing (MEC) leverages computational heterogeneity between
mobile devices and edge nodes to enable real-time applications requiring high
information freshness. The Age-of-Information (AoI) metric serves as a crucial
evaluator of information timeliness in such systems. Addressing AoI
minimization in multi-user MEC environments presents significant challenges due
to stochastic computing times. In this paper, we consider multiple users
offloading tasks to heterogeneous edge servers in an MEC system, focusing on
preemptive and non-preemptive task scheduling mechanisms. The problem is first
reformulated as a Restless Multi-Arm Bandit (RMAB) problem, with a multi-layer
Markov Decision Process (MDP) framework established to characterize AoI
dynamics in the MEC system. Based on the multi-layer MDP, we propose a nested
index framework and design a nested index policy with provably asymptotic
optimality. This establishes a theoretical framework adaptable to various
scheduling mechanisms, achieving efficient optimization through state
stratification and index design in both preemptive and non-preemptive modes.
Finally, the closed-form of the nested index is derived, facilitating
performance trade-offs between computational complexity and accuracy while
ensuring the universal applicability of the nested index policy across both
scheduling modes. The experimental results show that in non-preemptive
scheduling, compared with the benchmark method, the optimality gap is reduced
by 25.43%, while in preemptive scheduling, the gap has reduced by 61.84%. As
the system scale increases, it asymptotically converges in two scheduling modes
and especially provides near-optimal performance in non-preemptive structure.","Ning Yang, Yibo Liu, Shuo Chen, Meng Zhang, Haijun Zhang",2025-08-28T08:57:53Z,2025-08-28T08:57:53Z,http://arxiv.org/abs/2508.20564v1,http://arxiv.org/pdf/2508.20564v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Integration of Robot and Scene Kinematics for Sequential Mobile
  Manipulation Planning","We present a Sequential Mobile Manipulation Planning (SMMP) framework that
can solve long-horizon multi-step mobile manipulation tasks with coordinated
whole-body motion, even when interacting with articulated objects. By
abstracting environmental structures as kinematic models and integrating them
with the robot's kinematics, we construct an Augmented Configuration Apace
(A-Space) that unifies the previously separate task constraints for navigation
and manipulation, while accounting for the joint reachability of the robot
base, arm, and manipulated objects. This integration facilitates efficient
planning within a tri-level framework: a task planner generates symbolic action
sequences to model the evolution of A-Space, an optimization-based motion
planner computes continuous trajectories within A-Space to achieve desired
configurations for both the robot and scene elements, and an intermediate plan
refinement stage selects action goals that ensure long-horizon feasibility. Our
simulation studies first confirm that planning in A-Space achieves an 84.6\%
higher task success rate compared to baseline methods. Validation on real
robotic systems demonstrates fluid mobile manipulation involving (i) seven
types of rigid and articulated objects across 17 distinct contexts, and (ii)
long-horizon tasks of up to 14 sequential steps. Our results highlight the
significance of modeling scene kinematics into planning entities, rather than
encoding task-specific constraints, offering a scalable and generalizable
approach to complex robotic manipulation.","Ziyuan Jiao, Yida Niu, Zeyu Zhang, Yangyang Wu, Yao Su, Yixin Zhu, Hangxin Liu, Song-Chun Zhu",2025-08-26T03:08:54Z,2025-08-26T03:08:54Z,http://arxiv.org/abs/2508.18627v1,http://arxiv.org/pdf/2508.18627v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"A Vision-Based Shared-Control Teleoperation Scheme for Controlling the
  Robotic Arm of a Four-Legged Robot","In hazardous and remote environments, robotic systems perform critical tasks
demanding improved safety and efficiency. Among these, quadruped robots with
manipulator arms offer mobility and versatility for complex operations.
However, teleoperating quadruped robots is challenging due to the lack of
integrated obstacle detection and intuitive control methods for the robotic
arm, increasing collision risks in confined or dynamically changing workspaces.
Teleoperation via joysticks or pads can be non-intuitive and demands a high
level of expertise due to its complexity, culminating in a high cognitive load
on the operator. To address this challenge, a teleoperation approach that
directly maps human arm movements to the robotic manipulator offers a simpler
and more accessible solution. This work proposes an intuitive remote control by
leveraging a vision-based pose estimation pipeline that utilizes an external
camera with a machine learning-based model to detect the operator's wrist
position. The system maps these wrist movements into robotic arm commands to
control the robot's arm in real-time. A trajectory planner ensures safe
teleoperation by detecting and preventing collisions with both obstacles and
the robotic arm itself. The system was validated on the real robot,
demonstrating robust performance in real-time control. This teleoperation
approach provides a cost-effective solution for industrial applications where
safety, precision, and ease of use are paramount, ensuring reliable and
intuitive robotic control in high-risk environments.","Murilo Vinicius da Silva, Matheus Hipolito Carvalho, Juliano Negri, Thiago Segreto, Gustavo J. G. Lahr, Ricardo V. Godoy, Marcelo Becker",2025-08-20T18:31:57Z,2025-08-20T18:31:57Z,http://arxiv.org/abs/2508.14994v1,http://arxiv.org/pdf/2508.14994v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Bimanual Robot-Assisted Dressing: A Spherical Coordinate-Based Strategy
  for Tight-Fitting Garments","Robot-assisted dressing is a popular but challenging topic in the field of
robotic manipulation, offering significant potential to improve the quality of
life for individuals with mobility limitations. Currently, the majority of
research on robot-assisted dressing focuses on how to put on loose-fitting
clothing, with little attention paid to tight garments. For the former, since
the armscye is larger, a single robotic arm can usually complete the dressing
task successfully. However, for the latter, dressing with a single robotic arm
often fails due to the narrower armscye and the property of diminishing
rigidity in the armscye, which eventually causes the armscye to get stuck. This
paper proposes a bimanual dressing strategy suitable for dressing tight-fitting
clothing. To facilitate the encoding of dressing trajectories that adapt to
different human arm postures, a spherical coordinate system for dressing is
established. We uses the azimuthal angle of the spherical coordinate system as
a task-relevant feature for bimanual manipulation. Based on this new
coordinate, we employ Gaussian Mixture Model (GMM) and Gaussian Mixture
Regression (GMR) for imitation learning of bimanual dressing trajectories,
generating dressing strategies that adapt to different human arm postures. The
effectiveness of the proposed method is validated through various experiments.","Jian Zhao, Yunlong Lian, Andy M Tyrrell, Michael Gienger, Jihong Zhu",2025-08-17T07:45:34Z,2025-08-17T07:45:34Z,http://arxiv.org/abs/2508.12274v1,http://arxiv.org/pdf/2508.12274v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
Learning Task Execution Hierarchies for Redundant Robots,"Modern robotic systems, such as mobile manipulators, humanoids, and aerial
robots with arms, often possess high redundancy, enabling them to perform
multiple tasks simultaneously. Managing this redundancy is key to achieving
reliable and flexible behavior. A widely used approach is the Stack of Tasks
(SoT), which organizes control objectives by priority within a unified
framework. However, traditional SoTs are manually designed by experts, limiting
their adaptability and accessibility. This paper introduces a novel framework
that automatically learns both the hierarchy and parameters of a SoT from
user-defined objectives. By combining Reinforcement Learning and Genetic
Programming, the system discovers task priorities and control strategies
without manual intervention. A cost function based on intuitive metrics such as
precision, safety, and execution time guides the learning process. We validate
our method through simulations and experiments on the mobile-YuMi platform, a
dual-arm mobile manipulator with high redundancy. Results show that the learned
SoTs enable the robot to dynamically adapt to changing environments and inputs,
balancing competing objectives while maintaining robust task execution. This
approach provides a general and user-friendly solution for redundancy
management in complex robots, advancing human-centered robot programming and
reducing the need for expert design.","Alessandro Adami, Aris Synodinos, Matteo Iovino, Ruggero Carli, Pietro Falco",2025-08-14T16:06:58Z,2025-08-14T16:06:58Z,http://arxiv.org/abs/2508.10780v1,http://arxiv.org/pdf/2508.10780v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Decentralized Rank Scheduling for Energy-Constrained Multi-Task
  Federated Fine-Tuning in Edge-Assisted IoV Networks","Federated fine-tuning has emerged as a promising approach for adapting
foundation models (FMs) to diverse downstream tasks in edge environments. In
Internet of Vehicles (IoV) systems, enabling efficient and low-latency
multi-task adaptation is particularly challenging due to client mobility,
heterogeneous resources, and intermittent connectivity. This paper proposes a
hierarchical federated fine-tuning framework that coordinates roadside units
(RSUs) and vehicles to support resource-aware and mobility-resilient learning
across dynamic IoV scenarios. Leveraging Low-Rank Adaptation (LoRA), we
introduce a decentralized, energy-aware rank adaptation mechanism formulated as
a constrained multi-armed bandit problem. A novel UCB-DUAL algorithm is
developed to enable adaptive exploration under per-task energy budgets,
achieving provable sublinear regret. To evaluate our method, we construct a
large-scale IoV simulator based on real-world trajectories, capturing dynamic
participation, RSU handoffs, and communication variability. Extensive
experiments show that our approach achieves the best accuracy-efficiency
trade-off among all baselines, reducing latency by over 24\% and improving
average accuracy by more than 2.5\%.","Bokeng Zheng, Jianqiang Zhong, Jiayi Liu, Xiaoxi Zhang",2025-08-13T06:29:00Z,2025-08-13T06:29:00Z,http://arxiv.org/abs/2508.09532v1,http://arxiv.org/pdf/2508.09532v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"A Personalized Exercise Assistant using Reinforcement Learning (PEARL):
  Results from a four-arm Randomized-controlled Trial","Consistent physical inactivity poses a major global health challenge. Mobile
health (mHealth) interventions, particularly Just-in-Time Adaptive
Interventions (JITAIs), offer a promising avenue for scalable, personalized
physical activity (PA) promotion. However, developing and evaluating such
interventions at scale, while integrating robust behavioral science, presents
methodological hurdles. The PEARL study was the first large-scale, four-arm
randomized controlled trial to assess a reinforcement learning (RL) algorithm,
informed by health behavior change theory, to personalize the content and
timing of PA nudges via a Fitbit app.
  We enrolled and randomized 13,463 Fitbit users into four study arms: control,
random, fixed, and RL. The control arm received no nudges. The other three arms
received nudges from a bank of 155 nudges based on behavioral science
principles. The random arm received nudges selected at random. The fixed arm
received nudges based on a pre-set logic from survey responses about PA
barriers. The RL group received nudges selected by an adaptive RL algorithm. We
included 7,711 participants in primary analyses (mean age 42.1, 86.3% female,
baseline steps 5,618.2).
  We observed an increase in PA for the RL group compared to all other groups
from baseline to 1 and 2 months. The RL group had significantly increased
average daily step count at 1 month compared to all other groups: control (+296
steps, p=0.0002), random (+218 steps, p=0.005), and fixed (+238 steps,
p=0.002). At 2 months, the RL group sustained a significant increase compared
to the control group (+210 steps, p=0.0122). Generalized estimating equation
models also revealed a sustained increase in daily steps in the RL group vs.
control (+208 steps, p=0.002). These findings demonstrate the potential of a
scalable, behaviorally-informed RL approach to personalize digital health
interventions for PA.","Amy Armento Lee, Narayan Hegde, Nina Deliu, Emily Rosenzweig, Arun Suggala, Sriram Lakshminarasimhan, Qian He, John Hernandez, Martin Seneviratne, Rahul Singh, Pradnesh Kalkar, Karthikeyan Shanmugam, Aravindan Raghuveer, Abhimanyu Singh, My Nguyen, James Taylor, Jatin Alla, Sofia S. Villar, Hulya Emir-Farinas",2025-08-12T22:44:33Z,2025-08-12T22:44:33Z,http://arxiv.org/abs/2508.10060v1,http://arxiv.org/pdf/2508.10060v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
Hierarchical Placement Learning for Network Slice Provisioning,"In this work, we aim to address the challenge of slice provisioning in
edge-based mobile networks. We propose a solution that learns a service
function chain placement policy for Network Slice Requests, to maximize the
request acceptance rate, while minimizing the average node resource
utilization. To do this, we consider a Hierarchical Multi-Armed Bandit problem
and propose a two-level hierarchical bandit solution which aims to learn a
scalable placement policy that optimizes the stated objectives in an online
manner. Simulations on two real network topologies show that our proposed
approach achieves 5% average node resource utilization while admitting over 25%
more slice requests in certain scenarios, compared to baseline methods.","Jesutofunmi Ajayi, Antonio Di Maio, Torsten Braun",2025-08-08T16:20:24Z,2025-08-08T16:20:24Z,http://arxiv.org/abs/2508.06432v1,http://arxiv.org/pdf/2508.06432v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
CleanUpBench: Embodied Sweeping and Grasping Benchmark,"Embodied AI benchmarks have advanced navigation, manipulation, and reasoning,
but most target complex humanoid agents or large-scale simulations that are far
from real-world deployment. In contrast, mobile cleaning robots with dual mode
capabilities, such as sweeping and grasping, are rapidly emerging as realistic
and commercially viable platforms. However, no benchmark currently exists that
systematically evaluates these agents in structured, multi-target cleaning
tasks, revealing a critical gap between academic research and real-world
applications. We introduce CleanUpBench, a reproducible and extensible
benchmark for evaluating embodied agents in realistic indoor cleaning
scenarios. Built on NVIDIA Isaac Sim, CleanUpBench simulates a mobile service
robot equipped with a sweeping mechanism and a six-degree-of-freedom robotic
arm, enabling interaction with heterogeneous objects. The benchmark includes
manually designed environments and one procedurally generated layout to assess
generalization, along with a comprehensive evaluation suite covering task
completion, spatial efficiency, motion quality, and control performance. To
support comparative studies, we provide baseline agents based on heuristic
strategies and map-based planning. CleanUpBench bridges the gap between
low-level skill evaluation and full-scene testing, offering a scalable testbed
for grounded, embodied intelligence in everyday settings.","Wenbo Li, Guanting Chen, Tao Zhao, Jiyao Wang, Tianxin Hu, Yuwen Liao, Weixiang Guo, Shenghai Yuan",2025-08-07T16:20:31Z,2025-08-07T16:20:31Z,http://arxiv.org/abs/2508.05543v1,http://arxiv.org/pdf/2508.05543v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
Skin-Machine Interface with Multimodal Contact Motion Classifier,"This paper proposes a novel framework for utilizing skin sensors as a new
operation interface of complex robots. The skin sensors employed in this study
possess the capability to quantify multimodal tactile information at multiple
contact points. The time-series data generated from these sensors is
anticipated to facilitate the classification of diverse contact motions
exhibited by an operator. By mapping the classification results with robot
motion primitives, a diverse range of robot motions can be generated by
altering the manner in which the skin sensors are interacted with. In this
paper, we focus on a learning-based contact motion classifier employing
recurrent neural networks. This classifier is a pivotal factor in the success
of this framework. Furthermore, we elucidate the requisite conditions for
software-hardware designs. Firstly, multimodal sensing and its comprehensive
encoding significantly contribute to the enhancement of classification accuracy
and learning stability. Utilizing all modalities simultaneously as inputs to
the classifier proves to be an effective approach. Secondly, it is essential to
mount the skin sensors on a flexible and compliant support to enable the
activation of three-axis accelerometers. These accelerometers are capable of
measuring horizontal tactile information, thereby enhancing the correlation
with other modalities. Furthermore, they serve to absorb the noises generated
by the robot's movements during deployment. Through these discoveries, the
accuracy of the developed classifier surpassed 95 %, enabling the dual-arm
mobile manipulator to execute a diverse range of tasks via the Skin-Machine
Interface. https://youtu.be/UjUXT4Z4BC8","Alberto Confente, Takanori Jin, Taisuke Kobayashi, Julio Rogelio Guadarrama-Olvera, Gordon Cheng",2025-07-26T03:26:20Z,2025-07-26T03:26:20Z,http://arxiv.org/abs/2507.19760v1,http://arxiv.org/pdf/2507.19760v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Learning Accurate Whole-body Throwing with High-frequency Residual
  Policy and Pullback Tube Acceleration","Throwing is a fundamental skill that enables robots to manipulate objects in
ways that extend beyond the reach of their arms. We present a control framework
that combines learning and model-based control for prehensile whole-body
throwing with legged mobile manipulators. Our framework consists of three
components: a nominal tracking policy for the end-effector, a high-frequency
residual policy to enhance tracking accuracy, and an optimization-based module
to improve end-effector acceleration control. The proposed controller achieved
the average of 0.28 m landing error when throwing at targets located 6 m away.
Furthermore, in a comparative study with university students, the system
achieved a velocity tracking error of 0.398 m/s and a success rate of 56.8%,
hitting small targets randomly placed at distances of 3-5 m while throwing at a
specified speed of 6 m/s. In contrast, humans have a success rate of only
15.2%. This work provides an early demonstration of prehensile throwing with
quantified accuracy on hardware, contributing to progress in dynamic whole-body
manipulation.","Yuntao Ma, Yang Liu, Kaixian Qu, Marco Hutter",2025-06-20T13:29:59Z,2025-06-24T02:07:27Z,http://arxiv.org/abs/2506.16986v3,http://arxiv.org/pdf/2506.16986v3.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"STM32-Based IoT Framework for Real-Time Environmental Monitoring and
  Wireless Node Synchronization","The fast pace of technological growth has created a heightened need for
intelligent, autonomous monitoring systems in a variety of fields, especially
in environmental applications. This project shows the design process and
implementation of a proper dual node (master-slave) IoT-based monitoring system
using STM32F103C8T6 microcontrollers. The structure of the wireless monitoring
system studies the environmental conditions in real-time and can measure
parameters like temperature, humidity, soil moisture, raindrop detection and
obstacle distance. The relay of information occurs between the primary master
node (designated as the Green House) to the slave node (the Red House)
employing the HC-05 Bluetooth module for information transmission. Each node
displays the sensor data on OLED screens and a visual or auditory alert is
triggered based on predetermined thresholds. A comparative analysis of STM32
(ARM Cortex-M3) and Arduino (AVR) is presented to justify the STM32 used in
this work for greater processing power, less energy use, and better
peripherals. Practical challenges in this project arise from power distribution
and Bluetooth configuration limits. Future work will explore the transition of
a Wi-Fi communication protocol and develop a mobile monitoring robot to enhance
scalability of the system. Finally, this research shows that ARM based embedded
systems can provide real-time environmental monitoring systems that are
reliable and consume low power.","Ahmed Faizul Haque Dhrubo, Sazid Hasan, Mohammad Abdul Qayum",2025-06-17T07:42:11Z,2025-06-17T07:42:11Z,http://arxiv.org/abs/2506.17295v1,http://arxiv.org/pdf/2506.17295v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Uncertainty-Informed Active Perception for Open Vocabulary Object Goal
  Navigation","Mobile robots exploring indoor environments increasingly rely on
vision-language models to perceive high-level semantic cues in camera images,
such as object categories. Such models offer the potential to substantially
advance robot behaviour for tasks such as object-goal navigation (ObjectNav),
where the robot must locate objects specified in natural language by exploring
the environment. Current ObjectNav methods heavily depend on prompt engineering
for perception and do not address the semantic uncertainty induced by
variations in prompt phrasing. Ignoring semantic uncertainty can lead to
suboptimal exploration, which in turn limits performance. Hence, we propose a
semantic uncertainty-informed active perception pipeline for ObjectNav in
indoor environments. We introduce a novel probabilistic sensor model for
quantifying semantic uncertainty in vision-language models and incorporate it
into a probabilistic geometric-semantic map to enhance spatial understanding.
Based on this map, we develop a frontier exploration planner with an
uncertainty-informed multi-armed bandit objective to guide efficient object
search. Experimental results demonstrate that our method achieves ObjectNav
success rates comparable to those of state-of-the-art approaches, without
requiring extensive prompt engineering.","Utkarsh Bajpai, Julius Rückin, Cyrill Stachniss, Marija Popović",2025-06-16T11:17:15Z,2025-07-13T00:15:15Z,http://arxiv.org/abs/2506.13367v2,http://arxiv.org/pdf/2506.13367v2.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Distributed Learning for Reliable and Timely Communication in 6G
  Industrial Subnetworks","Emerging 6G industrial networks envision autonomous in-X subnetworks to
support efficient and cost-effective short range, localized connectivity for
autonomous control operations. Supporting timely transmission of event-driven,
critical control traffic is challenging in such networks is challenging due to
limited radio resources, dynamic device activity, and high mobility. In this
paper, we propose a distributed, learning-based random access protocol that
establishes implicit inter-subnetwork coordination to minimize the collision
probability and improves timely delivery. Each subnetwork independently learns
and selects access configurations based on a contention signature signal
broadcast by a central access point, enabling adaptive, collision-aware access
under dynamic traffic and mobility conditions. The proposed approach features
lightweight neural models and online training, making it suitable for
deployment in constrained industrial subnetworks. Simulation results show that
our method significantly improves the probability of timely packet delivery
compared to baseline methods, particularly in dense and high-load scenarios.
For instance, our proposed method achieves 21% gain in the probability of
timely packet delivery compared to a classical Multi-Armed Bandit (MAB) for an
industrial setting of 60 subnetworks and 5 radio channels.","Samira Abdelrahman, Hossam Farag, Gilberto Berardinelli",2025-06-13T13:01:45Z,2025-06-13T13:01:45Z,http://arxiv.org/abs/2506.11749v1,http://arxiv.org/pdf/2506.11749v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"HoMeR: Learning In-the-Wild Mobile Manipulation via Hybrid Imitation and
  Whole-Body Control","We introduce HoMeR, an imitation learning framework for mobile manipulation
that combines whole-body control with hybrid action modes that handle both
long-range and fine-grained motion, enabling effective performance on realistic
in-the-wild tasks. At its core is a fast, kinematics-based whole-body
controller that maps desired end-effector poses to coordinated motion across
the mobile base and arm. Within this reduced end-effector action space, HoMeR
learns to switch between absolute pose predictions for long-range movement and
relative pose predictions for fine-grained manipulation, offloading low-level
coordination to the controller and focusing learning on task-level decisions.
We deploy HoMeR on a holonomic mobile manipulator with a 7-DoF arm in a real
home. We compare HoMeR to baselines without hybrid actions or whole-body
control across 3 simulated and 3 real household tasks such as opening cabinets,
sweeping trash, and rearranging pillows. Across tasks, HoMeR achieves an
overall success rate of 79.17% using just 20 demonstrations per task,
outperforming the next best baseline by 29.17 on average. HoMeR is also
compatible with vision-language models and can leverage their internet-scale
priors to better generalize to novel object appearances, layouts, and cluttered
scenes. In summary, HoMeR moves beyond tabletop settings and demonstrates a
scalable path toward sample-efficient, generalizable manipulation in everyday
indoor spaces. Code, videos, and supplementary material are available at:
http://homer-manip.github.io","Priya Sundaresan, Rhea Malhotra, Phillip Miao, Jingyun Yang, Jimmy Wu, Hengyuan Hu, Rika Antonova, Francis Engelmann, Dorsa Sadigh, Jeannette Bohg",2025-06-01T21:43:35Z,2025-06-01T21:43:35Z,http://arxiv.org/abs/2506.01185v1,http://arxiv.org/pdf/2506.01185v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
Learning coordinated badminton skills for legged manipulators,"Coordinating the motion between lower and upper limbs and aligning limb
control with perception are substantial challenges in robotics, particularly in
dynamic environments. To this end, we introduce an approach for enabling legged
mobile manipulators to play badminton, a task that requires precise
coordination of perception, locomotion, and arm swinging. We propose a unified
reinforcement learning-based control policy for whole-body visuomotor skills
involving all degrees of freedom to achieve effective shuttlecock tracking and
striking. This policy is informed by a perception noise model that utilizes
real-world camera data, allowing for consistent perception error levels between
simulation and deployment and encouraging learned active perception behaviors.
Our method includes a shuttlecock prediction model, constrained reinforcement
learning for robust motion control, and integrated system identification
techniques to enhance deployment readiness. Extensive experimental results in a
variety of environments validate the robot's capability to predict shuttlecock
trajectories, navigate the service area effectively, and execute precise
strikes against human players, demonstrating the feasibility of using legged
mobile manipulators in complex and dynamic sports scenarios.","Yuntao Ma, Andrei Cramariuc, Farbod Farshidian, Marco Hutter",2025-05-29T01:26:30Z,2025-05-29T01:26:30Z,http://arxiv.org/abs/2505.22974v1,http://arxiv.org/pdf/2505.22974v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"BRAVE: Brain-Controlled Prosthetic Arm with Voice Integration and
  Embodied Learning for Enhanced Mobility","Non-invasive brain-computer interfaces (BCIs) have the potential to enable
intuitive control of prosthetic limbs for individuals with upper limb
amputations. However, existing EEG-based control systems face challenges
related to signal noise, classification accuracy, and real-time adaptability.
In this work, we present BRAVE, a hybrid EEG and voice-controlled prosthetic
system that integrates ensemble learning-based EEG classification with a
human-in-the-loop (HITL) correction framework for enhanced responsiveness.
Unlike traditional electromyography (EMG)-based prosthetic control, BRAVE aims
to interpret EEG-driven motor intent, enabling movement control without
reliance on residual muscle activity. To improve classification robustness,
BRAVE combines LSTM, CNN, and Random Forest models in an ensemble framework,
achieving a classification accuracy of 96% across test subjects. EEG signals
are preprocessed using a bandpass filter (0.5-45 Hz), Independent Component
Analysis (ICA) for artifact removal, and Common Spatial Pattern (CSP) feature
extraction to minimize contamination from electromyographic (EMG) and
electrooculographic (EOG) signals. Additionally, BRAVE incorporates automatic
speech recognition (ASR) to facilitate intuitive mode switching between
different degrees of freedom (DOF) in the prosthetic arm. The system operates
in real time, with a response latency of 150 ms, leveraging Lab Streaming Layer
(LSL) networking for synchronized data acquisition. The system is evaluated on
an in-house fabricated prosthetic arm and on multiple participants highlighting
the generalizability across users. The system is optimized for low-power
embedded deployment, ensuring practical real-world application beyond
high-performance computing environments. Our results indicate that BRAVE offers
a promising step towards robust, real-time, non-invasive prosthetic control.","Abdul Basit, Maha Nawaz, Muhammad Shafique",2025-05-23T11:44:33Z,2025-05-23T11:44:33Z,http://arxiv.org/abs/2506.18749v1,http://arxiv.org/pdf/2506.18749v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Learning Driven Elastic Task Multi-Connectivity Immersive Computing
  Systems","In virtual reality (VR) environments, computational tasks exhibit an elastic
nature, meaning they can dynamically adjust based on various user and system
constraints. This elasticity is essential for maintaining immersive
experiences; however, it also introduces challenges for communication and
computing in VR systems. In this paper, we investigate elastic task offloading
for multi-user edge-computing-enabled VR systems with multi-connectivity,
aiming to maximize the computational energy-efficiency (computational
throughput per unit of energy consumed). To balance the induced communication,
computation, energy consumption, and quality of experience trade-offs due to
the elasticity of VR tasks, we formulate a constrained stochastic computational
energy-efficiency optimization problem that integrates the
multi-connectivity/multi-user action space and the elastic nature of VR
computational tasks. We formulate a centralized phasic policy gradient (CPPG)
framework to solve the problem of interest online, using only prior elastic
task offloading statistics (energy consumption, response time, and transmission
time), and task information (i.e., task size and computational intensity),
while observing the induced system performance (energy consumption and
latency). We further extend our approach to decentralized learning by
formulating an independent phasic policy gradient (IPPG) method and a
decentralized shared multi-armed bandit (DSMAB) method. We train our methods
with real-world 4G, 5G, and WiGig network traces and 360 video datasets to
evaluate their performance in terms of response time, energy efficiency,
scalability, and delivered quality of experience. We also provide a
comprehensive analysis of task size and its effect on offloading policy and
system performance. In particular, we show that CPPG reduces latency by 28% and
energy consumption by 78% compared to IPPG.","Babak Badnava, Jacob Chakareski, Morteza Hashemi",2025-05-19T16:44:02Z,2025-05-19T16:44:02Z,http://arxiv.org/abs/2505.13331v1,http://arxiv.org/pdf/2505.13331v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Online Learning-based Adaptive Beam Switching for 6G Networks: Enhancing
  Efficiency and Resilience","Adaptive beam switching in 6G networks is challenged by high frequencies,
mobility, and blockage. We propose an Online Learning framework using Deep
Reinforcement Learning (DRL) with an enhanced state representation (velocity
and blockage history), a GRU architecture, and prioritized experience replay
for real-time beam optimization. Validated via Nvidia Sionna under
time-correlated blockage, our approach significantly enhances resilience in
SNR, throughput, and accuracy compared to a conventional heuristic.
Furthermore, the enhanced DRL agent outperforms a reactive Multi-Armed Bandit
(MAB) baseline by leveraging temporal dependencies, achieving lower performance
variability. This demonstrates the benefits of memory and prioritized learning
for robust 6G beam management, while confirming MAB as a strong baseline.","Seyed Bagher Hashemi Natanzi, Zhicong Zhu, Bo Tang",2025-05-12T19:59:05Z,2025-05-12T19:59:05Z,http://arxiv.org/abs/2505.08032v1,http://arxiv.org/pdf/2505.08032v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Intuitive Human-Robot Interfaces Leveraging on Autonomy Features for the
  Control of Highly-redundant Robots","[...] With the TelePhysicalOperation interface, the user can teleoperate the
different capabilities of a robot (e.g., single/double arm manipulation,
wheel/leg locomotion) by applying virtual forces on selected robot body parts.
This approach emulates the intuitiveness of physical human-robot interaction,
but at the same time it permits to teleoperate the robot from a safe distance,
in a way that resembles a ""Marionette"" interface. The system is further
enhanced with wearable haptic feedback functions to align better with the
""Marionette"" metaphor, and a user study has been conducted to validate its
efficacy with and without the haptic channel enabled. Considering the
importance of robot independence, the TelePhysicalOperation interface
incorporates autonomy modules to face, for example, the teleoperation of
dual-arm mobile base robots for bimanual object grasping and transportation
tasks.
  With the laser-guided interface, the user can indicate points of interest to
the robot through the utilization of a simple but effective laser emitter
device. With a neural network-based vision system, the robot tracks the laser
projection in real time, allowing the user to indicate not only fixed goals,
like objects, but also paths to follow. With the implemented autonomous
behavior, a mobile manipulator employs its locomanipulation abilities to follow
the indicated goals. The behavior is modeled using Behavior Trees, exploiting
their reactivity to promptly respond to changes in goal positions, and their
modularity to adapt the motion planning to the task needs. The proposed laser
interface has also been employed in an assistive scenario. In this case, users
with upper limbs impairments can control an assistive manipulator by directing
a head-worn laser emitter to the point of interests, to collaboratively address
activities of everyday life. [...]",Davide Torielli,2025-05-12T15:33:43Z,2025-05-12T15:33:43Z,http://arxiv.org/abs/2505.07668v1,http://arxiv.org/pdf/2505.07668v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Augmented Body Communicator: Enhancing daily body expression for people
  with upper limb limitations through LLM and a robotic arm","Individuals with upper limb movement limitations face challenges in
interacting with others. Although robotic arms are currently used primarily for
functional tasks, there is considerable potential to explore ways to enhance
users' body language capabilities during social interactions. This paper
introduces an Augmented Body Communicator system that integrates robotic arms
and a large language model. Through the incorporation of kinetic memory,
disabled users and their supporters can collaboratively design actions for the
robot arm. The LLM system then provides suggestions on the most suitable action
based on contextual cues during interactions. The system underwent thorough
user testing with six participants who have conditions affecting upper limb
mobility. Results indicate that the system improves users' ability to express
themselves. Based on our findings, we offer recommendations for developing
robotic arms that support disabled individuals with body language capabilities
and functional tasks.","Songchen Zhou, Mark Armstrong, Giulia Barbareschi, Toshihiro Ajioka, Zheng Hu, Ryoichi Ando, Kentaro Yoshifuji, Masatane Muto, Kouta Minamizawa",2025-05-09T07:00:11Z,2025-05-09T07:00:11Z,http://arxiv.org/abs/2505.05832v1,http://arxiv.org/pdf/2505.05832v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Human-Robot Collaboration for the Remote Control of Mobile Humanoid
  Robots with Torso-Arm Coordination","Recently, many humanoid robots have been increasingly deployed in various
facilities, including hospitals and assisted living environments, where they
are often remotely controlled by human operators. Their kinematic redundancy
enhances reachability and manipulability, enabling them to navigate complex,
cluttered environments and perform a wide range of tasks. However, this
redundancy also presents significant control challenges, particularly in
coordinating the movements of the robot's macro-micro structure (torso and
arms). Therefore, we propose various human-robot collaborative (HRC) methods
for coordinating the torso and arm of remotely controlled mobile humanoid
robots, aiming to balance autonomy and human input to enhance system efficiency
and task execution. The proposed methods include human-initiated approaches,
where users manually control torso movements, and robot-initiated approaches,
which autonomously coordinate torso and arm based on factors such as
reachability, task goal, or inferred human intent. We conducted a user study
with N=17 participants to compare the proposed approaches in terms of task
performance, manipulability, and energy efficiency, and analyzed which methods
were preferred by participants.","Nikita Boguslavskii, Lorena Maria Genua, Zhi Li",2025-05-09T04:20:47Z,2025-05-09T04:20:47Z,http://arxiv.org/abs/2505.05773v1,http://arxiv.org/pdf/2505.05773v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"IK Seed Generator for Dual-Arm Human-like Physicality Robot with Mobile
  Base","Robots are strongly expected as a means of replacing human tasks. If a robot
has a human-like physicality, the possibility of replacing human tasks
increases. In the case of household service robots, it is desirable for them to
be on a human-like size so that they do not become excessively large in order
to coexist with humans in their operating environment. However, robots with
size limitations tend to have difficulty solving inverse kinematics (IK) due to
mechanical limitations, such as joint angle limitations. Conversely, if the
difficulty coming from this limitation could be mitigated, one can expect that
the use of such robots becomes more valuable. In numerical IK solver, which is
commonly used for robots with higher degrees-of-freedom (DOF), the solvability
of IK depends on the initial guess given to the solver. Thus, this paper
proposes a method for generating a good initial guess for a numerical IK solver
given the target hand configuration. For the purpose, we define the goodness of
an initial guess using the scaled Jacobian matrix, which can calculate the
manipulability index considering the joint limits. These two factors are
related to the difficulty of solving IK. We generate the initial guess by
optimizing the goodness using the genetic algorithm (GA). To enumerate much
possible IK solutions, we use the reachability map that represents the
reachable area of the robot hand in the arm-base coordinate system. We conduct
quantitative evaluation and prove that using an initial guess that is judged to
be better using the goodness value increases the probability that IK is solved.
Finally, as an application of the proposed method, we show that by generating
good initial guesses for IK a robot actually achieves three typical scenarios.","Jun Takamatsu, Atsushi Kanehira, Kazuhiro Sasabuchi, Naoki Wake, Katsushi Ikeuchi",2025-05-01T21:33:23Z,2025-05-01T21:33:23Z,http://arxiv.org/abs/2505.00871v1,http://arxiv.org/pdf/2505.00871v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Investigating Adaptive Tuning of Assistive Exoskeletons Using Offline
  Reinforcement Learning: Challenges and Insights","Assistive exoskeletons have shown great potential in enhancing mobility for
individuals with motor impairments, yet their effectiveness relies on precise
parameter tuning for personalized assistance. In this study, we investigate the
potential of offline reinforcement learning for optimizing effort thresholds in
upper-limb assistive exoskeletons, aiming to reduce reliance on manual
calibration. Specifically, we frame the problem as a multi-agent system where
separate agents optimize biceps and triceps effort thresholds, enabling a more
adaptive and data-driven approach to exoskeleton control. Mixed Q-Functionals
(MQF) is employed to efficiently handle continuous action spaces while
leveraging pre-collected data, thereby mitigating the risks associated with
real-time exploration. Experiments were conducted using the MyoPro 2
exoskeleton across two distinct tasks involving horizontal and vertical arm
movements. Our results indicate that the proposed approach can dynamically
adjust threshold values based on learned patterns, potentially improving user
interaction and control, though performance evaluation remains challenging due
to dataset limitations.","Yasin Findik, Christopher Coco, Reza Azadeh",2025-04-30T21:59:56Z,2025-04-30T21:59:56Z,http://arxiv.org/abs/2505.00201v1,http://arxiv.org/pdf/2505.00201v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Joint Resource Estimation and Trajectory Optimization for eVTOL-involved
  CR network: A Monte Carlo Tree Search-based Approach","Electric Vertical Take-Off and Landing (eVTOL) aircraft, pivotal to Advanced
Air Mobility (AAM), are emerging as a transformative transportation paradigm
with the potential to redefine urban and regional mobility. While these systems
offer unprecedented efficiency in transporting people and goods, they rely
heavily on computation capability, safety-critical operations such as real-time
navigation, environmental sensing, and trajectory tracking--necessitating
robust offboard computational support. A widely adopted solution involves
offloading these tasks to terrestrial base stations (BSs) along the flight
path. However, air-to-ground connectivity is often constrained by spectrum
conflicts with terrestrial users, which poses a significant challenge to
maintaining reliable task execution. Cognitive radio (CR) techniques offer
promising capabilities for dynamic spectrum access, making them a natural fit
for addressing this issue. Existing studies often overlook the time-varying
nature of BS resources, such as spectrum availability and CPU cycles, which
leads to inaccurate trajectory planning, suboptimal offloading success rates,
excessive energy consumption, and operational delays. To address these
challenges, we propose a trajectory optimization framework for eVTOL swarms
that maximizes task offloading success probability while minimizing both energy
consumption and resource competition (e.g., spectrum and CPU cycles) with
primary terrestrial users. The proposed algorithm integrates a Multi-Armed
Bandit (MAB) model to dynamically estimate BS resource availability and a Monte
Carlo Tree Search (MCTS) algorithm to determine optimal offloading decisions,
selecting both the BSs and access time windows that align with energy and
temporal constraints.","Kai Xiong, Chenxin Yang, Yujie Qin, Chau Yuen",2025-04-25T02:48:48Z,2025-04-25T02:48:48Z,http://arxiv.org/abs/2504.18031v1,http://arxiv.org/pdf/2504.18031v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Integrating Learning-Based Manipulation and Physics-Based Locomotion for
  Whole-Body Badminton Robot Control","Learning-based methods, such as imitation learning (IL) and reinforcement
learning (RL), can produce excel control policies over challenging agile robot
tasks, such as sports robot. However, no existing work has harmonized
learning-based policy with model-based methods to reduce training complexity
and ensure the safety and stability for agile badminton robot control. In this
paper, we introduce Hamlet, a novel hybrid control system for agile badminton
robots. Specifically, we propose a model-based strategy for chassis locomotion
which provides a base for arm policy. We introduce a physics-informed ""IL+RL""
training framework for learning-based arm policy. In this train framework, a
model-based strategy with privileged information is used to guide arm policy
training during both IL and RL phases. In addition, we train the critic model
during IL phase to alleviate the performance drop issue when transitioning from
IL to RL. We present results on our self-engineered badminton robot, achieving
94.5% success rate against the serving machine and 90.7% success rate against
human players. Our system can be easily generalized to other agile mobile
manipulation tasks such as agile catching and table tennis. Our project
website: https://dreamstarring.github.io/HAMLET/.","Haochen Wang, Zhiwei Shi, Chengxi Zhu, Yafei Qiao, Cheng Zhang, Fan Yang, Pengjie Ren, Lan Lu, Dong Xuan",2025-04-24T17:46:29Z,2025-04-27T14:23:00Z,http://arxiv.org/abs/2504.17771v2,http://arxiv.org/pdf/2504.17771v2.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"TOCALib: Optimal control library with interpolation for bimanual
  manipulation and obstacles avoidance","The paper presents a new approach for constructing a library of optimal
trajectories for two robotic manipulators, Two-Arm Optimal Control and
Avoidance Library (TOCALib). The optimisation takes into account kinodynamic
and other constraints within the FROST framework. The novelty of the method
lies in the consideration of collisions using the DCOL method, which allows
obtaining symbolic expressions for assessing the presence of collisions and
using them in gradient-based optimization control methods. The proposed
approach allowed the implementation of complex bimanual manipulations. In this
paper we used Mobile Aloha as an example of TOCALib application. The approach
can be extended to other bimanual robots, as well as to gait control of bipedal
robots. It can also be used to construct training data for machine learning
tasks for manipulation.","Yulia Danik, Dmitry Makarov, Aleksandra Arkhipova, Sergei Davidenko, Aleksandr Panov",2025-04-10T12:53:01Z,2025-04-10T12:53:01Z,http://arxiv.org/abs/2504.07708v1,http://arxiv.org/pdf/2504.07708v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
8-DoFs Cable Driven Parallel Robots for Bimanual Teleportation,"Teleoperation plays a critical role in intuitive robot control and imitation
learning, particularly for complex tasks involving mobile manipulators with
redundant degrees of freedom (DoFs). However, most existing master controllers
are limited to 6-DoF spatial control and basic gripper control, making them
insufficient for controlling high-DoF robots and restricting the operator to a
small workspace. In this work, we present a novel, low-cost, high-DoF master
controller based on Cable-Driven Parallel Robots (CDPRs), designed to overcome
these limitations. The system decouples translation and orientation control,
following a scalable 3 + 3 + n DoF structure: 3 DoFs for large-range
translation using a CDPR, 3 DoFs for orientation using a gimbal mechanism, and
n additional DoFs for gripper and redundant joint control. Its lightweight
cable-driven design enables a large and adaptable workspace while minimizing
actuator load. The end-effector remains stable without requiring continuous
high-torque input, unlike most serial robot arms. We developed the first
dual-arm CDPR-based master controller using cost-effective actuators and a
simple mechanical structure. In demonstrations, the system successfully
controlled an 8-DoF robotic arm with a 2-DoF pan-tilt camera, performing tasks
such as pick-and-place, knot tying, object sorting, and tape application. The
results show precise, versatile, and practical high-DoF teleoperation.","Hung Hon Cheng, Josie Hughes",2025-04-02T09:54:16Z,2025-04-02T09:54:16Z,http://arxiv.org/abs/2504.01554v1,http://arxiv.org/pdf/2504.01554v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Using Mobile AR for Rapid Feasibility Analysis for Deployment of Robots:
  A Usability Study with Non-Expert Users","Automating a production line with robotic arms is a complex, demanding task
that requires not only substantial resources but also a deep understanding of
the automated processes and available technologies and tools. Expert
integrators must consider factors such as placement, payload, and robot reach
requirements to determine the feasibility of automation. Ideally, such
considerations are based on a detailed digital simulation developed before any
hardware is deployed. However, this process is often time-consuming and
challenging. To simplify these processes, we introduce a much simpler method
for the feasibility analysis of robotic arms' reachability, designed for
non-experts. We implement this method through a mobile, sensing-based prototype
tool. The two-step experimental evaluation included the expert user study
results, which helped us identify the difficulty levels of various deployment
scenarios and refine the initial prototype. The results of the subsequent
quantitative study with 22 non-expert participants utilizing both scenarios
indicate that users could complete both simple and complex feasibility analyses
in under ten minutes, exhibiting similar cognitive loads and high engagement.
Overall, the results suggest that the tool was well-received and rated as
highly usable, thereby showing a new path for changing the ease of feasibility
analysis for automation.","Krzysztof Zielinski, Slawomir Tadeja, Bruce Blumberg, Mikkel Baun Kjærgaard",2025-03-18T20:54:48Z,2025-03-18T20:54:48Z,http://arxiv.org/abs/2503.14725v1,http://arxiv.org/pdf/2503.14725v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"MoManipVLA: Transferring Vision-language-action Models for General
  Mobile Manipulation","Mobile manipulation is the fundamental challenge for robotics to assist
humans with diverse tasks and environments in everyday life. However,
conventional mobile manipulation approaches often struggle to generalize across
different tasks and environments because of the lack of large-scale training.
In contrast, recent advances in vision-language-action (VLA) models have shown
impressive generalization capabilities, but these foundation models are
developed for fixed-base manipulation tasks. Therefore, we propose an efficient
policy adaptation framework named MoManipVLA to transfer pre-trained VLA models
of fix-base manipulation to mobile manipulation, so that high generalization
ability across tasks and environments can be achieved in mobile manipulation
policy. Specifically, we utilize pre-trained VLA models to generate waypoints
of the end-effector with high generalization ability. We design motion planning
objectives for the mobile base and the robot arm, which aim at maximizing the
physical feasibility of the trajectory. Finally, we present an efficient
bi-level objective optimization framework for trajectory generation, where the
upper-level optimization predicts waypoints for base movement to enhance the
manipulator policy space, and the lower-level optimization selects the optimal
end-effector trajectory to complete the manipulation task. In this way,
MoManipVLA can adjust the position of the robot base in a zero-shot manner,
thus making the waypoints predicted from the fixed-base VLA models feasible.
Extensive experimental results on OVMM and the real world demonstrate that
MoManipVLA achieves a 4.2% higher success rate than the state-of-the-art mobile
manipulation, and only requires 50 training cost for real world deployment due
to the strong generalization ability in the pre-trained VLA models.","Zhenyu Wu, Yuheng Zhou, Xiuwei Xu, Ziwei Wang, Haibin Yan",2025-03-17T17:59:52Z,2025-03-17T17:59:52Z,http://arxiv.org/abs/2503.13446v1,http://arxiv.org/pdf/2503.13446v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"MoMa-Kitchen: A 100K+ Benchmark for Affordance-Grounded Last-Mile
  Navigation in Mobile Manipulation","In mobile manipulation, navigation and manipulation are often treated as
separate problems, resulting in a significant gap between merely approaching an
object and engaging with it effectively. Many navigation approaches primarily
define success by proximity to the target, often overlooking the necessity for
optimal positioning that facilitates subsequent manipulation. To address this,
we introduce MoMa-Kitchen, a benchmark dataset comprising over 100k samples
that provide training data for models to learn optimal final navigation
positions for seamless transition to manipulation. Our dataset includes
affordance-grounded floor labels collected from diverse kitchen environments,
in which robotic mobile manipulators of different models attempt to grasp
target objects amidst clutter. Using a fully automated pipeline, we simulate
diverse real-world scenarios and generate affordance labels for optimal
manipulation positions. Visual data are collected from RGB-D inputs captured by
a first-person view camera mounted on the robotic arm, ensuring consistency in
viewpoint during data collection. We also develop a lightweight baseline model,
NavAff, for navigation affordance grounding that demonstrates promising
performance on the MoMa-Kitchen benchmark. Our approach enables models to learn
affordance-based final positioning that accommodates different arm types and
platform heights, thereby paving the way for more robust and generalizable
integration of navigation and manipulation in embodied AI. Project page:
\href{https://momakitchen.github.io/}{https://momakitchen.github.io/}.","Pingrui Zhang, Xianqiang Gao, Yuhan Wu, Kehui Liu, Dong Wang, Zhigang Wang, Bin Zhao, Yan Ding, Xuelong Li",2025-03-14T04:47:38Z,2025-03-14T04:47:38Z,http://arxiv.org/abs/2503.11081v1,http://arxiv.org/pdf/2503.11081v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"ArticuBot: Learning Universal Articulated Object Manipulation Policy via
  Large Scale Simulation","This paper presents ArticuBot, in which a single learned policy enables a
robotics system to open diverse categories of unseen articulated objects in the
real world. This task has long been challenging for robotics due to the large
variations in the geometry, size, and articulation types of such objects. Our
system, Articubot, consists of three parts: generating a large number of
demonstrations in physics-based simulation, distilling all generated
demonstrations into a point cloud-based neural policy via imitation learning,
and performing zero-shot sim2real transfer to real robotics systems. Utilizing
sampling-based grasping and motion planning, our demonstration generalization
pipeline is fast and effective, generating a total of 42.3k demonstrations over
322 training articulated objects. For policy learning, we propose a novel
hierarchical policy representation, in which the high-level policy learns the
sub-goal for the end-effector, and the low-level policy learns how to move the
end-effector conditioned on the predicted goal. We demonstrate that this
hierarchical approach achieves much better object-level generalization compared
to the non-hierarchical version. We further propose a novel weighted
displacement model for the high-level policy that grounds the prediction into
the existing 3D structure of the scene, outperforming alternative policy
representations. We show that our learned policy can zero-shot transfer to
three different real robot settings: a fixed table-top Franka arm across two
different labs, and an X-Arm on a mobile base, opening multiple unseen
articulated objects across two labs, real lounges, and kitchens. Videos and
code can be found on our project website: https://articubot.github.io/.","Yufei Wang, Ziyu Wang, Mino Nakura, Pratik Bhowal, Chia-Liang Kuo, Yi-Ting Chen, Zackory Erickson, David Held",2025-03-04T22:51:50Z,2025-05-01T21:26:22Z,http://arxiv.org/abs/2503.03045v2,http://arxiv.org/pdf/2503.03045v2.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"MAB-Based Channel Scheduling for Asynchronous Federated Learning in
  Non-Stationary Environments","Federated learning enables distributed model training across clients without
raw data exchange, but in wireless implementations, frequent parameter updates
cause high communication overhead. Existing research often assumes known
channel state information (CSI) or stationary channels, though practical
wireless channels are non-stationary due to fading, user mobility, and attacks,
leading to unpredictable transmission failures and exacerbating client
staleness, which hampers model convergence. To tackle these challenges, we
propose an asynchronous federated learning scheduling framework for
non-stationary channels that aims to reduce client staleness while enhancing
communication efficiency and fairness. Our framework considers two scenarios:
extremely non-stationary and piecewise-stationary channels. Age of Information
(AoI) quantifies client staleness under these conditions. We conduct
convergence analysis to examine the impact of AoI and per-round client
participation on learning performance and formulate the scheduling problem as a
multi-armed bandit (MAB) problem. We derive theoretical lower bounds on AoI
regret and develop scheduling strategies based on GLR-CUCB and M-exp3
algorithms, including upper bounds on AoI regret. To address imbalanced client
updates, we propose an adaptive matching strategy that incorporates marginal
utility and fairness considerations. Simulation results show that our algorithm
achieves sub-linear AoI regret, accelerates convergence, and promotes fairer
aggregation.","Zhiyin Li, Yubo Yang, Tao Yang, Ziyu Guo, Xiaofeng Wu, Bo Hu",2025-03-03T09:05:04Z,2025-03-23T06:54:42Z,http://arxiv.org/abs/2503.01324v2,http://arxiv.org/pdf/2503.01324v2.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Statistical and Deterministic RCS Characterization for ISAC Channel
  Modeling","In this study, we perform a statistical analysis of the radar cross section
(RCS) for various test targets in an indoor factory at \(25\)-\(28\) GHz, with
the goal of formulating parameters that may be used for target identification
and other sensing applications for future wireless systems. The analysis is
conducted based on measurements in monostatic and bistatic configurations for
bistatic angles of \(20^\circ\), \(40^\circ\), and \(60^\circ\), which are
functions of transmitter-receiver (T-R) and target positions, via accurate
\(3\)dB beamwidth of \(10^\circ\) in both azimuth and elevation planes. The
test targets include unmanned aerial vehicles, an autonomous mobile robot, and
a robotic arm. We utilize parametric statistical distributions to fit the
measured RCS data. The analysis reveals that the \textit{lognormal and gamma
distributions} are effective in modeling the RCS of the test targets over
different reflecting points of the target itself, i.e. when target is in
motion. Additionally, we provide a framework for evaluating the deterministic
bistatic RCS of a rectangular sheet of laminated wood, due to its widespread
use in indoor hotspot environments. Novel deterministic and statistical RCS
models are evaluated, incorporating dependencies on the bistatic angle, T-R
distance (\(2\)m -\(10\)m) and the target. The results demonstrate that some
proposed RCS models accurately fit the measured data, highlighting their
applicability in bistatic configurations.","Ali Waqar Azim, Ahmad Bazzi, Roberto Bomfin, Nikolaos Giakoumidis, Theodore S. Rappaport, Marwa Chafii",2025-02-17T08:10:09Z,2025-02-17T08:10:09Z,http://arxiv.org/abs/2502.11540v1,http://arxiv.org/pdf/2502.11540v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
Motion Planning of Nonholonomic Cooperative Mobile Manipulators,"We propose a real-time implementable motion planning technique for
cooperative object transportation by nonholonomic mobile manipulator robots
(MMRs) in an environment with static and dynamic obstacles. The proposed motion
planning technique works in two steps. A novel visibility vertices-based path
planning algorithm computes a global piece-wise linear path between the start
and the goal location in the presence of static obstacles offline. It defines
the static obstacle free space around the path with a set of convex polygons
for the online motion planner. We employ a Nonliner Model Predictive Control
(NMPC) based online motion planning technique for nonholonomic MMRs that
jointly plans for the mobile base and the manipulators arm. It efficiently
utilizes the locomotion capability of the mobile base and the manipulation
capability of the arm. The motion planner plans feasible motion for the MMRs
and generates trajectory for object transportation considering the kinodynamic
constraints and the static and dynamic obstacles. The efficiency of our
approach is validated by numerical simulation and hardware experiments in
varied environments.","Keshab Patra, Arpita Sinha, Anirban Guha",2025-02-08T06:05:43Z,2025-02-08T06:05:43Z,http://arxiv.org/abs/2502.05462v1,http://arxiv.org/pdf/2502.05462v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
Malleable Robots,"This chapter is about the fundamentals of fabrication, control, and
human-robot interaction of a new type of collaborative robotic manipulators,
called malleable robots, which are based on adjustable architectures of varying
stiffness for achieving high dexterity with lower mobility arms. Collaborative
robots, or cobots, commonly integrate six or more degrees of freedom (DOF) in a
serial arm in order to allow positioning in constrained spaces and adaptability
across tasks. Increasing the dexterity of robotic arms has been indeed
traditionally accomplished by increasing the number of degrees of freedom of
the system; however, once a robotic task has been established (e.g., a
pick-and-place operation), the motion of the end-effector can be normally
achieved using less than 6-DOF (i.e., lower mobility). The aim of malleable
robots is to close the technological gap that separates current cobots from
achieving flexible, accessible manufacturing automation with a reduced number
of actuators.","Angus B. Clark, Xinran Wang, Alex Ranne, Nicolas Rojas",2025-02-06T12:14:17Z,2025-02-06T12:14:17Z,http://arxiv.org/abs/2502.04012v1,http://arxiv.org/pdf/2502.04012v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Dynamic object goal pushing with mobile manipulators through model-free
  constrained reinforcement learning","Non-prehensile pushing to move and reorient objects to a goal is a versatile
loco-manipulation skill. In the real world, the object's physical properties
and friction with the floor contain significant uncertainties, which makes the
task challenging for a mobile manipulator. In this paper, we develop a
learning-based controller for a mobile manipulator to move an unknown object to
a desired position and yaw orientation through a sequence of pushing actions.
The proposed controller for the robotic arm and the mobile base motion is
trained using a constrained Reinforcement Learning (RL) formulation. We
demonstrate its capability in experiments with a quadrupedal robot equipped
with an arm. The learned policy achieves a success rate of 91.35% in simulation
and at least 80% on hardware in challenging scenarios. Through our extensive
hardware experiments, we show that the approach demonstrates high robustness
against unknown objects of different masses, materials, sizes, and shapes. It
reactively discovers the pushing location and direction, thus achieving
contact-rich behavior while observing only the pose of the object.
Additionally, we demonstrate the adaptive behavior of the learned policy
towards preventing the object from toppling.","Ioannis Dadiotis, Mayank Mittal, Nikos Tsagarakis, Marco Hutter",2025-02-03T17:28:35Z,2025-02-03T17:28:35Z,http://arxiv.org/abs/2502.01546v1,http://arxiv.org/pdf/2502.01546v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
Multi-Dimensional Vector ISA Extension for Mobile In-Cache Computing,"In-cache computing technology transforms existing caches into long-vector
compute units and offers low-cost alternatives to building expensive vector
engines for mobile CPUs. Unfortunately, existing long-vector Instruction Set
Architecture (ISA) extensions, such as RISC-V Vector Extension (RVV) and Arm
Scalable Vector Extension (SVE), provide only one-dimensional strided and
random memory accesses. While this is sufficient for typical vector engines, it
fails to effectively utilize the large Single Instruction, Multiple Data (SIMD)
widths of in-cache vector engines. This is because mobile data-parallel kernels
expose limited parallelism across a single dimension.
  Based on our analysis of mobile vector kernels, we introduce a long-vector
Multi-dimensional Vector ISA Extension (MVE) for mobile in-cache computing. MVE
achieves high SIMD resource utilization and enables flexible programming by
abstracting cache geometry and data layout. The proposed ISA features
multi-dimensional strided and random memory accesses and efficient
dimension-level masked execution to encode parallelism across multiple
dimensions. Using a wide range of data-parallel mobile workloads, we
demonstrate that MVE offers significant performance and energy reduction
benefits of 2.9x and 8.8x, on average, compared to the SIMD units of a
commercial mobile processor, at an area overhead of 3.6%.","Alireza Khadem, Daichi Fujiki, Hilbert Chen, Yufeng Gu, Nishil Talati, Scott Mahlke, Reetuparna Das",2025-01-17T01:24:12Z,2025-01-17T01:24:12Z,http://arxiv.org/abs/2501.09902v1,http://arxiv.org/pdf/2501.09902v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Towards Federated Multi-Armed Bandit Learning for Content Dissemination
  using Swarm of UAVs","This paper introduces an Unmanned Aerial Vehicle - enabled content management
architecture that is suitable for critical content access in communities of
users that are communication-isolated during diverse types of disaster
scenarios. The proposed architecture leverages a hybrid network of stationary
anchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The
anchor UAVs are equipped with both vertical and lateral communication links,
and they serve local users, while the mobile micro-ferrying UAVs extend
coverage across communities with increased mobility. The focus is on developing
a content dissemination system that dynamically learns optimal caching policies
to maximize content availability. The core innovation is an adaptive content
dissemination framework based on distributed Federated Multi-Armed Bandit
learning. The goal is to optimize UAV content caching decisions based on
geo-temporal content popularity and user demand variations. A Selective Caching
Algorithm is also introduced to reduce redundant content replication by
incorporating inter-UAV information sharing. This method strategically
preserves the uniqueness in user preferences while amalgamating the
intelligence across a distributed learning system. This approach improves the
learning algorithm's ability to adapt to diverse user preferences. Functional
verification and performance evaluation confirm the proposed architecture's
utility across different network sizes, UAV swarms, and content popularity
patterns.","Amit Kumar Bhuyan, Hrishikesh Dutta, Subir Biswas",2025-01-15T20:55:13Z,2025-04-18T05:13:52Z,http://arxiv.org/abs/2501.09146v2,http://arxiv.org/pdf/2501.09146v2.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Data-driven Spatial Classification using Multi-Arm Bandits for
  Monitoring with Energy-Constrained Mobile Robots","We consider the spatial classification problem for monitoring using data
collected by a coordinated team of mobile robots. Such classification problems
arise in several applications including search-and-rescue and precision
agriculture. Specifically, we want to classify the regions of a search
environment into interesting and uninteresting as quickly as possible using a
team of mobile sensors and mobile charging stations. We develop a data-driven
strategy that accommodates the noise in sensed data and the limited energy
capacity of the sensors, and generates collision-free motion plans for the
team. We propose a bi-level approach, where a high-level planner leverages a
multi-armed bandit framework to determine the potential regions of interest for
the drones to visit next based on the data collected online. Then, a low-level
path planner based on integer programming coordinates the paths for the team to
visit the target regions subject to the physical constraints. We characterize
several theoretical properties of the proposed approach, including anytime
guarantees and task completion time. We show the efficacy of our approach in
simulation, and further validate these observations in physical experiments
using mobile robots.","Xiaoshan Lin, Siddharth Nayak, Stefano Di Cairano, Abraham P. Vinod",2025-01-14T16:05:32Z,2025-01-14T16:05:32Z,http://arxiv.org/abs/2501.08222v1,http://arxiv.org/pdf/2501.08222v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"MobileH2R: Learning Generalizable Human to Mobile Robot Handover
  Exclusively from Scalable and Diverse Synthetic Data","This paper introduces MobileH2R, a framework for learning generalizable
vision-based human-to-mobile-robot (H2MR) handover skills. Unlike traditional
fixed-base handovers, this task requires a mobile robot to reliably receive
objects in a large workspace enabled by its mobility. Our key insight is that
generalizable handover skills can be developed in simulators using high-quality
synthetic data, without the need for real-world demonstrations. To achieve
this, we propose a scalable pipeline for generating diverse synthetic full-body
human motion data, an automated method for creating safe and imitation-friendly
demonstrations, and an efficient 4D imitation learning method for distilling
large-scale demonstrations into closed-loop policies with base-arm
coordination. Experimental evaluations in both simulators and the real world
show significant improvements (at least +15% success rate) over baseline
methods in all cases. Experiments also validate that large-scale and diverse
synthetic data greatly enhances robot learning, highlighting our scalable
framework.","Zifan Wang, Ziqing Chen, Junyu Chen, Jilong Wang, Yuxin Yang, Yunze Liu, Xueyi Liu, He Wang, Li Yi",2025-01-08T16:23:56Z,2025-01-09T04:13:45Z,http://arxiv.org/abs/2501.04595v2,http://arxiv.org/pdf/2501.04595v2.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"A Proof of Concept Resource Management Scheme for Augmented Reality
  Applications in 5G Systems","Augmented reality applications are bitrate intensive, delay-sensitive, and
computationally demanding. To support them, mobile edge computing systems need
to carefully manage both their networking and computing resources. To this end,
we present a proof of concept resource management scheme that adapts the
bandwidth at the base station and the GPU frequency at the edge to efficiently
fulfill roundtrip delay constrains. Resource adaptation is performed using a
Multi-Armed Bandit algorithm that accounts for the monotonic relationship
between allocated resources and performance. We evaluate our scheme by
experimentation on an OpenAirInterface 5G testbed where the considered
application is OpenRTiST. The results indicate that our resource management
scheme can substantially reduce both bandwidth usage and power consumption
while delivering high quality of service. Overall, this work demonstrates that
intelligent resource control can potentially establish systems that are not
only more efficient but also more sustainable.","Panagiotis Nikolaidis, Samie Mostafavi, James Gross, John Baras",2025-01-02T18:18:38Z,2025-01-02T18:18:38Z,http://arxiv.org/abs/2501.01398v1,http://arxiv.org/pdf/2501.01398v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
Task-Driven Co-Design of Mobile Manipulators,"Recent interest in mobile manipulation has resulted in a wide range of new
robot designs. A large family of these designs focuses on modular platforms
that combine existing mobile bases with static manipulator arms. They combine
these modules by mounting the arm in a tabletop configuration. However, the
operating workspaces and heights for common mobile manipulation tasks, such as
opening articulated objects, significantly differ from tabletop manipulation
tasks. As a result, these standard arm mounting configurations can result in
kinematics with restricted joint ranges and motions. To address these problems,
we present the first Concurrent Design approach for mobile manipulators to
optimize key arm-mounting parameters. Our approach directly targets task
performance across representative household tasks by training a powerful
multitask-capable reinforcement learning policy in an inner loop while
optimizing over a distribution of design configurations guided by Bayesian
Optimization and HyperBand (BOHB) in an outer loop. This results in novel
designs that significantly improve performance across both seen and unseen test
tasks, and outperform designs generated by heuristic-based performance indices
that are cheaper to evaluate but only weakly correlated with the motions of
interest. We evaluate the physical feasibility of the resulting designs and
show that they are practical and remain modular, affordable, and compatible
with existing commercial components. We open-source the approach and generated
designs to facilitate further improvements of these platforms.","Raphael Schneider, Daniel Honerkamp, Tim Welschehold, Abhinav Valada",2024-12-21T14:00:41Z,2025-05-17T21:14:51Z,http://arxiv.org/abs/2412.16635v2,http://arxiv.org/pdf/2412.16635v2.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Tabletop Object Rearrangement: Structure, Complexity, and Efficient
  Combinatorial Search-Based Solutions","This thesis provides an in-depth structural analysis and efficient
algorithmic solutions for tabletop object rearrangement with overhand grasps
(TORO), a foundational task in advancing intelligent robotic manipulation.
Rearranging multiple objects in a confined workspace presents two primary
challenges: sequencing actions to minimize pick-and-place operations - an
NP-hard problem in TORO - and determining temporary object placements (""buffer
poses"") within a cluttered environment, which is essential yet highly complex.
For TORO with available external free space, this work investigates the minimum
buffer space, or ""running buffer size,"" required for temporary relocations,
presenting both theoretical insights and exact algorithms. For TORO without
external free space, the concept of lazy buffer verification is introduced,
with its efficiency evaluated across various manipulator configurations,
including single-arm, dual-arm, and mobile manipulators.",Kai Gao,2024-12-19T21:03:56Z,2025-01-31T14:28:46Z,http://arxiv.org/abs/2412.15398v2,http://arxiv.org/pdf/2412.15398v2.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"TidyBot++: An Open-Source Holonomic Mobile Manipulator for Robot
  Learning","Exploiting the promise of recent advances in imitation learning for mobile
manipulation will require the collection of large numbers of human-guided
demonstrations. This paper proposes an open-source design for an inexpensive,
robust, and flexible mobile manipulator that can support arbitrary arms,
enabling a wide range of real-world household mobile manipulation tasks.
Crucially, our design uses powered casters to enable the mobile base to be
fully holonomic, able to control all planar degrees of freedom independently
and simultaneously. This feature makes the base more maneuverable and
simplifies many mobile manipulation tasks, eliminating the kinematic
constraints that create complex and time-consuming motions in nonholonomic
bases. We equip our robot with an intuitive mobile phone teleoperation
interface to enable easy data acquisition for imitation learning. In our
experiments, we use this interface to collect data and show that the resulting
learned policies can successfully perform a variety of common household mobile
manipulation tasks.","Jimmy Wu, William Chong, Robert Holmberg, Aaditya Prasad, Yihuai Gao, Oussama Khatib, Shuran Song, Szymon Rusinkiewicz, Jeannette Bohg",2024-12-11T18:54:22Z,2024-12-11T18:54:22Z,http://arxiv.org/abs/2412.10447v1,http://arxiv.org/pdf/2412.10447v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"EmbodiedRAG: Dynamic 3D Scene Graph Retrieval for Efficient and Scalable
  Robot Task Planning","Recent advances in Large Language Models (LLMs) have helped facilitate
exciting progress for robotic planning in real, open-world environments. 3D
scene graphs (3DSGs) offer a promising environment representation for grounding
such LLM-based planners as they are compact and semantically rich. However, as
the robot's environment scales (e.g., number of entities tracked) and the
complexity of scene graph information increases (e.g., maintaining more
attributes), providing the 3DSG as-is to an LLM-based planner quickly becomes
infeasible due to input token count limits and attentional biases present in
LLMs. Inspired by the successes of Retrieval-Augmented Generation (RAG) methods
that retrieve query-relevant document chunks for LLM question and answering, we
adapt the paradigm for our embodied domain. Specifically, we propose a 3D scene
subgraph retrieval framework, called EmbodiedRAG, that we augment an LLM-based
planner with for executing natural language robotic tasks. Notably, our
retrieved subgraphs adapt to changes in the environment as well as changes in
task-relevancy as the robot executes its plan. We demonstrate EmbodiedRAG's
ability to significantly reduce input token counts (by an order of magnitude)
and planning time (up to 70% reduction in average time per planning step) while
improving success rates on AI2Thor simulated household tasks with a single-arm,
mobile manipulator. Additionally, we implement EmbodiedRAG on a quadruped with
a manipulator to highlight the performance benefits for robot deployment at the
edge in real environments.","Meghan Booker, Grayson Byrd, Bethany Kemp, Aurora Schmidt, Corban Rivera",2024-10-31T14:22:20Z,2024-10-31T14:22:20Z,http://arxiv.org/abs/2410.23968v1,http://arxiv.org/pdf/2410.23968v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"An Efficient Representation of Whole-body Model Predictive Control for
  Online Compliant Dual-arm Mobile Manipulation","Dual-arm mobile manipulators can transport and manipulate large-size objects
with simple end-effectors. To interact with dynamic environments with strict
safety and compliance requirements, achieving whole-body motion planning online
while meeting various hard constraints for such highly redundant mobile
manipulators poses a significant challenge. We tackle this challenge by
presenting an efficient representation of whole-body motion trajectories within
our bilevel model-based predictive control (MPC) framework. We utilize
B\'ezier-curve parameterization to represent the optimized collision-free
trajectories of two collaborating end-effectors in the first MPC, facilitating
fast long-horizon object-oriented motion planning in SE(3) while considering
approximated feasibility constraints. This approach is further applied to
parameterize whole-body trajectories in the second MPC for whole-body motion
generation with predictive admittance control in a relatively short horizon
while satisfying whole-body hard constraints. This representation enables two
MPCs with continuous properties, thereby avoiding inaccurate model-state
transition and dense decision-variable settings in existing MPCs using the
discretization method. It strengthens the online execution of the bilevel MPC
framework in high-dimensional space and facilitates the generation of
consistent commands for our hybrid position/velocity-controlled robot. The
simulation comparisons and real-world experiments demonstrate the efficiency
and robustness of this approach in various scenarios for static and dynamic
obstacle avoidance, and compliant interaction control with the manipulated
object and external disturbances.","Wenqian Du, Ran Long, João Moura, Jiayi Wang, Saeid Samadi, Sethu Vijayakumar",2024-10-30T11:06:43Z,2024-10-30T11:06:43Z,http://arxiv.org/abs/2410.22910v1,http://arxiv.org/pdf/2410.22910v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Constrained Nonlinear Kaczmarz Projection on Intersections of Manifolds
  for Coordinated Multi-Robot Mobile Manipulation","Cooperative manipulation tasks impose various structure-, task-, and
robot-specific constraints on mobile manipulators. However, current methods
struggle to model and solve these myriad constraints simultaneously. We propose
a twofold solution: first, we model constraints as a family of manifolds
amenable to simultaneous solving. Second, we introduce the constrained
nonlinear Kaczmarz (cNKZ) projection technique to produce constraint-satisfying
solutions. Experiments show that cNKZ dramatically outperforms baseline
approaches, which cannot find solutions at all. We integrate cNKZ with a
sampling-based motion planning algorithm to generate complex, coordinated
motions for 3 to 6 mobile manipulators (18--36 DoF), with cNKZ solving up to 80
nonlinear constraints simultaneously and achieving up to a 92% success rate in
cluttered environments. We also demonstrate our approach on hardware using
three Turtlebot3 Waffle Pi robots with OpenMANIPULATOR-X arms.","Akshaya Agrawal, Parker Mayer, Zachary Kingston, Geoffrey A. Hollinger",2024-10-29T00:22:02Z,2025-03-27T10:01:08Z,http://arxiv.org/abs/2410.21630v2,http://arxiv.org/pdf/2410.21630v2.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Predictive Reachability for Embodiment Selection in Mobile Manipulation
  Behaviors","Mobile manipulators require coordinated control between navigation and
manipulation to accomplish tasks. Typically, coordinated mobile manipulation
behaviors have base navigation to approach the goal followed by arm
manipulation to reach the desired pose. Selecting the embodiment between the
base and arm can be determined based on reachability. Previous methods evaluate
reachability by computing inverse kinematics and activate arm motions once
solutions are identified. In this study, we introduce a new approach called
predictive reachability that decides reachability based on predicted arm
motions. Our model utilizes a hierarchical policy framework built upon a world
model. The world model allows the prediction of future trajectories and the
evaluation of reachability. The hierarchical policy selects the embodiment
based on the predicted reachability and plans accordingly. Unlike methods that
require prior knowledge about robots and environments for inverse kinematics,
our method only relies on image-based observations. We evaluate our approach
through basic reaching tasks across various environments. The results
demonstrate that our method outperforms previous model-based approaches in both
sample efficiency and performance, while enabling more reasonable embodiment
selection based on predictive reachability.","Xiaoxu Feng, Takato Horii, Takayuki Nagai",2024-10-28T14:17:08Z,2024-10-28T14:17:08Z,http://arxiv.org/abs/2410.21059v1,http://arxiv.org/pdf/2410.21059v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Mobility-Aware Federated Learning: Multi-Armed Bandit Based Selection in
  Vehicular Network","In this paper, we study a vehicle selection problem for federated learning
(FL) over vehicular networks. Specifically, we design a mobility-aware
vehicular federated learning (MAVFL) scheme in which vehicles drive through a
road segment to perform FL. Some vehicles may drive out of the segment which
leads to unsuccessful training. In the proposed scheme, the real-time
successful training participation ratio is utilized to implement vehicle
selection. We conduct the convergence analysis to indicate the influence of
vehicle mobility on training loss. Furthermore, we propose a multi-armed
bandit-based vehicle selection algorithm to minimize the utility function
considering training loss and delay. The simulation results show that compared
with baselines, the proposed algorithm can achieve better training performance
with approximately 28\% faster convergence.","Haoyu Tu, Lin Chen, Zuguang Li, Xiaopei Chen, Wen Wu",2024-10-14T12:45:15Z,2024-10-15T03:16:09Z,http://arxiv.org/abs/2410.10451v2,http://arxiv.org/pdf/2410.10451v2.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"RM4D: A Combined Reachability and Inverse Reachability Map for Common
  6-/7-axis Robot Arms by Dimensionality Reduction to 4D","Knowledge of a manipulator's workspace is fundamental for a variety of tasks
including robot design, grasp planning and robot base placement. Consequently,
workspace representations are well studied in robotics. Two important
representations are reachability maps and inverse reachability maps. The former
predicts whether a given end-effector pose is reachable from where the robot
currently is, and the latter suggests suitable base positions for a desired
end-effector pose. Typically, the reachability map is built by discretizing the
6D space containing the robot's workspace and determining, for each cell,
whether it is reachable or not. The reachability map is subsequently inverted
to build the inverse map. This is a cumbersome process which restricts the
applications of such maps. In this work, we exploit commonalities of existing
six and seven axis robot arms to reduce the dimension of the discretization
from 6D to 4D. We propose Reachability Map 4D (RM4D), a map that only requires
a single 4D data structure for both forward and inverse queries. This gives a
much more compact map that can be constructed by an order of magnitude faster
than existing maps, with no inversion overheads and no loss in accuracy. Our
experiments showcase the usefulness of RM4D for grasp planning with a mobile
manipulator.",Martin Rudorfer,2024-10-09T15:07:41Z,2024-10-09T15:07:41Z,http://arxiv.org/abs/2410.06968v1,http://arxiv.org/pdf/2410.06968v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"M3Bench: Benchmarking Whole-body Motion Generation for Mobile
  Manipulation in 3D Scenes","We propose M3Bench, a new benchmark for whole-body motion generation in
mobile manipulation tasks. Given a 3D scene context, M3Bench requires an
embodied agent to reason about its configuration, environmental constraints,
and task objectives to generate coordinated whole-body motion trajectories for
object rearrangement. M3Bench features 30,000 object rearrangement tasks across
119 diverse scenes, providing expert demonstrations generated by our newly
developed M3BenchMaker, an automatic data generation tool that produces
whole-body motion trajectories from high-level task instructions using only
basic scene and robot information. Our benchmark includes various task splits
to evaluate generalization across different dimensions and leverages realistic
physics simulation for trajectory assessment. Extensive evaluation analysis
reveals that state-of-the-art models struggle with coordinating base-arm motion
while adhering to environmental and task-specific constraints, underscoring the
need for new models to bridge this gap. By releasing M3Bench and M3BenchMaker
we aim to advance robotics research toward more adaptive and capable mobile
manipulation in diverse, real-world environments.","Zeyu Zhang, Sixu Yan, Muzhi Han, Zaijin Wang, Xinggang Wang, Song-Chun Zhu, Hangxin Liu",2024-10-09T08:38:21Z,2025-05-29T04:19:21Z,http://arxiv.org/abs/2410.06678v3,http://arxiv.org/pdf/2410.06678v3.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
Whole-body End-Effector Pose Tracking,"Combining manipulation with the mobility of legged robots is essential for a
wide range of robotic applications. However, integrating an arm with a mobile
base significantly increases the system's complexity, making precise
end-effector control challenging. Existing model-based approaches are often
constrained by their modeling assumptions, leading to limited robustness.
Meanwhile, recent Reinforcement Learning (RL) implementations restrict the
arm's workspace to be in front of the robot or track only the position to
obtain decent tracking accuracy. In this work, we address these limitations by
introducing a whole-body RL formulation for end-effector pose tracking in a
large workspace on rough, unstructured terrains. Our proposed method involves a
terrain-aware sampling strategy for the robot's initial configuration and
end-effector pose commands, as well as a game-based curriculum to extend the
robot's operating range. We validate our approach on the ANYmal quadrupedal
robot with a six DoF robotic arm. Through our experiments, we show that the
learned controller achieves precise command tracking over a large workspace and
adapts across varying terrains such as stairs and slopes. On deployment, it
achieves a pose-tracking error of 2.64 cm and 3.64 degrees, outperforming
existing competitive baselines.","Tifanny Portela, Andrei Cramariuc, Mayank Mittal, Marco Hutter",2024-09-24T12:51:32Z,2025-04-25T09:59:56Z,http://arxiv.org/abs/2409.16048v2,http://arxiv.org/pdf/2409.16048v2.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Kinodynamic Motion Planning for Collaborative Object Transportation by
  Multiple Mobile Manipulators","This work proposes a kinodynamic motion planning technique for collaborative
object transportation by multiple mobile manipulators in dynamic environments.
A global path planner computes a linear piecewise path from start to goal. A
novel algorithm detects the narrow regions between the static obstacles and
aids in defining the obstacle-free region to enhance the feasibility of the
global path. We then formulate a local online motion planning technique for
trajectory generation that minimizes the control efforts in a receding horizon
manner. It plans the trajectory for finite time horizons, considering the
kinodynamic constraints and the static and dynamic obstacles. The planning
technique jointly plans for the mobile bases and the arms to utilize the
locomotion capability of the mobile base and the manipulation capability of the
arm efficiently. We use a convex cone approach to avoid self-collision of the
formation by modifying the mobile manipulators admissible state without
imposing additional constraints. Numerical simulations and hardware experiments
showcase the efficiency of the proposed approach.","Keshab Patra, Arpita Sinha, Anirban Guha",2024-09-23T11:03:16Z,2024-09-23T11:03:16Z,http://arxiv.org/abs/2409.14910v1,http://arxiv.org/pdf/2409.14910v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
Catch It! Learning to Catch in Flight with Mobile Dexterous Hands,"Catching objects in flight (i.e., thrown objects) is a common daily skill for
humans, yet it presents a significant challenge for robots. This task requires
a robot with agile and accurate motion, a large spatial workspace, and the
ability to interact with diverse objects. In this paper, we build a mobile
manipulator composed of a mobile base, a 6-DoF arm, and a 12-DoF dexterous hand
to tackle such a challenging task. We propose a two-stage reinforcement
learning framework to efficiently train a whole-body-control catching policy
for this high-DoF system in simulation. The objects' throwing configurations,
shapes, and sizes are randomized during training to enhance policy adaptivity
to various trajectories and object characteristics in flight. The results show
that our trained policy catches diverse objects with randomly thrown
trajectories, at a high success rate of about 80\% in simulation, with a
significant improvement over the baselines. The policy trained in simulation
can be directly deployed in the real world with onboard sensing and
computation, which achieves catching sandbags in various shapes, randomly
thrown by humans. Our project page is available at
https://mobile-dex-catch.github.io/.","Yuanhang Zhang, Tianhai Liang, Zhenyang Chen, Yanjie Ze, Huazhe Xu",2024-09-16T14:32:25Z,2024-09-16T14:32:25Z,http://arxiv.org/abs/2409.10319v1,http://arxiv.org/pdf/2409.10319v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"PeriGuru: A Peripheral Robotic Mobile App Operation Assistant based on
  GUI Image Understanding and Prompting with LLM","Smartphones have significantly enhanced our daily learning, communication,
and entertainment, becoming an essential component of modern life. However,
certain populations, including the elderly and individuals with disabilities,
encounter challenges in utilizing smartphones, thus necessitating mobile app
operation assistants, a.k.a. mobile app agent. With considerations for privacy,
permissions, and cross-platform compatibility issues, we endeavor to devise and
develop PeriGuru in this work, a peripheral robotic mobile app operation
assistant based on GUI image understanding and prompting with Large Language
Model (LLM). PeriGuru leverages a suite of computer vision techniques to
analyze GUI screenshot images and employs LLM to inform action decisions, which
are then executed by robotic arms. PeriGuru achieves a success rate of 81.94%
on the test task set, which surpasses by more than double the method without
PeriGuru's GUI image interpreting and prompting design. Our code is available
on https://github.com/Z2sJ4t/PeriGuru.","Kelin Fu, Yang Tian, Kaigui Bian",2024-09-14T07:54:25Z,2024-09-14T07:54:25Z,http://arxiv.org/abs/2409.09354v1,http://arxiv.org/pdf/2409.09354v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
EHC-MM: Embodied Holistic Control for Mobile Manipulation,"Mobile manipulation typically entails the base for mobility, the arm for
accurate manipulation, and the camera for perception. The principle of Distant
Mobility, Close Grasping(DMCG) is essential for holistic control. We propose
Embodied Holistic Control for Mobile Manipulation(EHC-MM) with the embodied
function of sig(w): By formulating the DMCG principle as a Quadratic
Programming (QP) problem, sig(w) dynamically balances the robot's emphasis
between movement and manipulation with the consideration of the robot's state
and environment. In addition, we propose the Monitor-Position-Based Servoing
(MPBS) with sig(w), enabling the tracking of the target during the operation.
This approach enables coordinated control among the robot's base, arm, and
camera, enhancing task efficiency. Through extensive simulations and real-world
experiments, our approach significantly improves both the success rate and
efficiency of mobile manipulation tasks, achieving a 95.6% success rate in
real-world scenarios and a 52.8% increase in time efficiency.","Jiawen Wang, Yixiang Jin, Jun Shi, Yong A, Dingzhe Li, Fuchun Sun, Dingsheng Luo, Bin Fang",2024-09-13T04:16:44Z,2025-03-12T14:42:34Z,http://arxiv.org/abs/2409.08527v2,http://arxiv.org/pdf/2409.08527v2.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
Perceptive Pedipulation with Local Obstacle Avoidance,"Pedipulation leverages the feet of legged robots for mobile manipulation,
eliminating the need for dedicated robotic arms. While previous works have
showcased blind and task-specific pedipulation skills, they fail to account for
static and dynamic obstacles in the environment. To address this limitation, we
introduce a reinforcement learning-based approach to train a whole-body
obstacle-aware policy that tracks foot position commands while simultaneously
avoiding obstacles. Despite training the policy in only five different static
scenarios in simulation, we show that it generalizes to unknown environments
with different numbers and types of obstacles. We analyze the performance of
our method through a set of simulation experiments and successfully deploy the
learned policy on the ANYmal quadruped, demonstrating its capability to follow
foot commands while navigating around static and dynamic obstacles. Videos of
the experiments are available at
sites.google.com/leggedrobotics.com/perceptive-pedipulation.","Jonas Stolle, Philip Arm, Mayank Mittal, Marco Hutter",2024-09-11T11:34:43Z,2024-11-04T10:19:37Z,http://arxiv.org/abs/2409.07195v3,http://arxiv.org/pdf/2409.07195v3.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Hevelius Report: Visualizing Web-Based Mobility Test Data For Clinical
  Decision and Learning Support","Hevelius, a web-based computer mouse test, measures arm movement and has been
shown to accurately evaluate severity for patients with Parkinson's disease and
ataxias. A Hevelius session produces 32 numeric features, which may be hard to
interpret, especially in time-constrained clinical settings. This work aims to
support clinicians (and other stakeholders) in interpreting and connecting
Hevelius features to clinical concepts. Through an iterative design process, we
developed a visualization tool (Hevelius Report) that (1) abstracts six
clinically relevant concepts from 32 features, (2) visualizes patient test
results, and compares them to results from healthy controls and other patients,
and (3) is an interactive app to meet the specific needs in different usage
scenarios. Then, we conducted a preliminary user study through an online
interview with three clinicians who were not involved in the project. They
expressed interest in using Hevelius Report, especially for identifying subtle
changes in their patients' mobility that are hard to capture with existing
clinical tests. Future work will integrate the visualization tool into the
current clinical workflow of a neurology team and conduct systematic
evaluations of the tool's usefulness, usability, and effectiveness. Hevelius
Report represents a promising solution for analyzing fine-motor test results
and monitoring patients' conditions and progressions.","Hongjin Lin, Tessa Han, Krzysztof Z. Gajos, Anoopum S. Gupta",2024-09-09T21:51:09Z,2024-09-09T21:51:09Z,http://arxiv.org/abs/2409.06088v1,http://arxiv.org/pdf/2409.06088v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
Grounding Language Models in Autonomous Loco-manipulation Tasks,"Humanoid robots with behavioral autonomy have consistently been regarded as
ideal collaborators in our daily lives and promising representations of
embodied intelligence. Compared to fixed-based robotic arms, humanoid robots
offer a larger operational space while significantly increasing the difficulty
of control and planning. Despite the rapid progress towards general-purpose
humanoid robots, most studies remain focused on locomotion ability with few
investigations into whole-body coordination and tasks planning, thus limiting
the potential to demonstrate long-horizon tasks involving both mobility and
manipulation under open-ended verbal instructions. In this work, we propose a
novel framework that learns, selects, and plans behaviors based on tasks in
different scenarios. We combine reinforcement learning (RL) with whole-body
optimization to generate robot motions and store them into a motion library. We
further leverage the planning and reasoning features of the large language
model (LLM), constructing a hierarchical task graph that comprises a series of
motion primitives to bridge lower-level execution with higher-level planning.
Experiments in simulation and real-world using the CENTAURO robot show that the
language model based planner can efficiently adapt to new loco-manipulation
tasks, demonstrating high autonomy from free-text commands in unstructured
scenes.","Jin Wang, Nikos Tsagarakis",2024-09-02T15:27:48Z,2024-09-02T15:27:48Z,http://arxiv.org/abs/2409.01326v1,http://arxiv.org/pdf/2409.01326v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
Learning Multi-agent Multi-machine Tending by Mobile Robots,"Robotics can help address the growing worker shortage challenge of the
manufacturing industry. As such, machine tending is a task collaborative robots
can tackle that can also highly boost productivity. Nevertheless, existing
robotics systems deployed in that sector rely on a fixed single-arm setup,
whereas mobile robots can provide more flexibility and scalability. In this
work, we introduce a multi-agent multi-machine tending learning framework by
mobile robots based on Multi-agent Reinforcement Learning (MARL) techniques
with the design of a suitable observation and reward. Moreover, an
attention-based encoding mechanism is developed and integrated into Multi-agent
Proximal Policy Optimization (MAPPO) algorithm to boost its performance for
machine tending scenarios. Our model (AB-MAPPO) outperformed MAPPO in this new
challenging scenario in terms of task success, safety, and resources
utilization. Furthermore, we provided an extensive ablation study to support
our various design decisions.","Abdalwhab Abdalwhab, Giovanni Beltrame, Samira Ebrahimi Kahou, David St-Onge",2024-08-29T19:57:52Z,2025-02-28T04:41:20Z,http://arxiv.org/abs/2408.16875v3,http://arxiv.org/pdf/2408.16875v3.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
RMMI: Reactive Mobile Manipulation using an Implicit Neural Map,"Mobile manipulator robots operating in complex domestic and industrial
environments must effectively coordinate their base and arm motions while
avoiding obstacles. While current reactive control methods gracefully achieve
this coordination, they rely on simplified and idealised geometric
representations of the environment to avoid collisions. This limits their
performance in cluttered environments. To address this problem, we introduce
RMMI, a reactive control framework that leverages the ability of neural Signed
Distance Fields (SDFs) to provide a continuous and differentiable
representation of the environment's geometry. RMMI formulates a quadratic
program that optimises jointly for robot base and arm motion, maximises the
manipulability, and avoids collisions through a set of inequality constraints.
These constraints are constructed by querying the SDF for the distance and
direction to the closest obstacle for a large number of sampling points on the
robot. We evaluate RMMI both in simulation and in a set of real-world
experiments. For reaching in cluttered environments, we observe a 25% increase
in success rate. For additional details, code, and experiment videos, please
visit https://rmmi.github.io/.","Nicolas Marticorena, Tobias Fischer, Jesse Haviland, Niko Suenderhauf",2024-08-29T01:57:57Z,2025-09-03T04:01:03Z,http://arxiv.org/abs/2408.16206v2,http://arxiv.org/pdf/2408.16206v2.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Beyond Shortsighted Navigation: Merging Best View Trajectory Planning
  with Robot Navigation","Gathering visual information effectively to monitor known environments is a
key challenge in robotics. To be as efficient as human surveyors, robotic
systems must continuously collect observational data required to complete their
survey task. Inspection personnel instinctively know to look at relevant
equipment that happens to be ``along the way.'' In this paper, we introduce a
novel framework for continuous long-horizon viewpoint planning, for ground
robots, applied to tasks involving patrolling, monitoring or visual data
gathering in known environments. Our approach to Long Horizon Viewpoint
Planning (LHVP), enables the robot to autonomously navigate and collect
environmental data optimizing for coverage over the horizon of the patrol.
Leveraging a quadruped's mobility and sensory capabilities, our LHVP framework
plans patrol paths that account for coupling the viewpoint planner for the arm
camera with the mobile base's navigation planner. The viewpath optimization
algorithm seeks a balance between comprehensive environmental coverage and
dynamically feasible movements, thus ensuring prolonged and effective operation
in scenarios including monitoring, security surveillance, and disaster
response. We validate our approach through simulations and in the real world
and show that our LHVP significantly outperforms naive patrolling methods in
terms of area coverage generating information-gathering trajectories for the
robot arm. Our results indicate a promising direction for the deployment of
mobile robots in long-term, autonomous surveying, and environmental data
collection tasks, highlighting the potential of intelligent robotic systems in
challenging real-world applications.","Srinath Tankasala, Roberto Martín-Martín, Mitch Pryor",2024-08-22T16:08:45Z,2024-08-22T16:08:45Z,http://arxiv.org/abs/2408.12513v1,http://arxiv.org/pdf/2408.12513v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
Cage: Hardware-Accelerated Safe WebAssembly,"WebAssembly (WASM) is an immensely versatile and increasingly popular
compilation target. It executes applications written in several languages
(e.g., C/C++) with near-native performance in various domains (e.g., mobile,
edge, cloud). Despite WASM's sandboxing feature, which isolates applications
from other instances and the host platform, WASM does not inherently provide
any memory safety guarantees for applications written in low-level, unsafe
languages. To this end, we propose Cage, a hardware-accelerated toolchain for
WASM that supports unmodified applications compiled to WASM and utilizes
diverse Arm hardware features aiming to enrich the memory safety properties of
WASM. Precisely, Cage leverages Arm's Memory Tagging Extension (MTE) to (i)
provide spatial and temporal memory safety for heap and stack allocations and
(ii) improve the performance of WASM's sandboxing mechanism. Cage further
employs Arm's Pointer Authentication (PAC) to prevent leaked pointers from
being reused by other WASM instances, thus enhancing WASM's security
properties. We implement our system based on 64-bit WASM. We provide a WASM
compiler and runtime with support for Arm's MTE and PAC. On top of that, Cage's
LLVM-based compiler toolchain transforms unmodified applications to provide
spatial and temporal memory safety for stack and heap allocations and prevent
function pointer reuse. Our evaluation on real hardware shows that Cage incurs
minimal runtime (<5.8%) and memory (<3.7%) overheads and can improve the
performance of WASM's sandboxing mechanism, achieving a speedup of over 5.1%,
while offering efficient memory safety guarantees.","Martin Fink, Dimitrios Stavrakakis, Dennis Sprokholt, Soham Chakraborty, Jan-Erik Ekberg, Pramod Bhatotia",2024-08-21T09:22:28Z,2024-12-19T12:38:38Z,http://arxiv.org/abs/2408.11456v3,http://arxiv.org/pdf/2408.11456v3.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
Motion Planning for Minimally Actuated Serial Robots,"Modern manipulators are acclaimed for their precision but often struggle to
operate in confined spaces. This limitation has driven the development of
hyper-redundant and continuum robots. While these present unique advantages,
they face challenges in, for instance, weight, mechanical complexity, modeling
and costs. The Minimally Actuated Serial Robot (MASR) has been proposed as a
light-weight, low-cost and simpler alternative where passive joints are
actuated with a Mobile Actuator (MA) moving along the arm. Yet, Inverse
Kinematics (IK) and a general motion planning algorithm for the MASR have not
be addressed. In this letter, we propose the MASR-RRT* motion planning
algorithm specifically developed for the unique kinematics of MASR. The main
component of the algorithm is a data-based model for solving the IK problem
while considering minimal traverse of the MA. The model is trained solely using
the forward kinematics of the MASR and does not require real data. With the
model as a local-connection mechanism, MASR-RRT* minimizes a cost function
expressing the action time. In a comprehensive analysis, we show that MASR-RRT*
is superior in performance to the straight-forward implementation of the
standard RRT*. Experiments on a real robot in different environments with
obstacles validate the proposed algorithm.","Avi Cohen, Avishai Sintov, David Zarrouk",2024-08-12T13:38:07Z,2024-08-12T13:38:07Z,http://arxiv.org/abs/2408.06143v1,http://arxiv.org/pdf/2408.06143v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"A New Tightly-Coupled Dual-VIO for a Mobile Manipulator With Dynamic
  Locomotion","This paper introduces a new dual monocular visualinertial odometry (dual-VIO)
strategy for a mobile manipulator operating under dynamic locomotion, i.e.
coordinated movement involving both the base platform and the manipulator arm.
Our approach has been motivated by challenges arising from inaccurate
estimation due to coupled excitation when the mobile manipulator is engaged in
dynamic locomotion in cluttered environments. The technique maintains two
independent monocular VIO modules, with one at the mobile base and the other at
the end-effector (EE), which are tightly coupled at the low level of the factor
graph. The proposed method treats each monocular VIO with respect to each other
as a positional anchor through arm-kinematics. These anchor points provide a
soft geometric constraint during the VIO pose optimization. This allows us to
stabilize both estimators in case of instability of one estimator in highly
dynamic locomotions. The performance of our approach has been demonstrated
through extensive experimental testing with a mobile manipulator tested in
comparison to running dual VINS-Mono in parallel. We envision that our method
can also provide a foundation towards active-SLAM (ASLAM) with a new
perspective on multi-VIO fusion and system redundancy.","Jianxiang Xu, Soo Jeon",2024-07-18T19:53:48Z,2024-07-18T19:53:48Z,http://arxiv.org/abs/2407.13878v1,http://arxiv.org/pdf/2407.13878v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Dual-arm Motion Generation for Repositioning Care based on Deep
  Predictive Learning with Somatosensory Attention Mechanism","Caregiving is a vital role for domestic robots, especially the repositioning
care has immense societal value, critically improving the health and quality of
life of individuals with limited mobility. However, repositioning task is a
challenging area of research, as it requires robots to adapt their motions
while interacting flexibly with patients. The task involves several key
challenges: (1) applying appropriate force to specific target areas; (2)
performing multiple actions seamlessly, each requiring different force
application policies; and (3) motion adaptation under uncertain positional
conditions. To address these, we propose a deep neural network (DNN)-based
architecture utilizing proprioceptive and visual attention mechanisms, along
with impedance control to regulate the robot's movements. Using the dual-arm
humanoid robot Dry-AIREC, the proposed model successfully generated motions to
insert the robot's hand between the bed and a mannequin's back without applying
excessive force, and it supported the transition from a supine to a lifted-up
position. The project page is here:
https://sites.google.com/view/caregiving-robot-airec/repositioning","Tamon Miyake, Namiko Saito, Tetsuya Ogata, Yushi Wang, Shigeki Sugano",2024-07-18T10:34:16Z,2025-06-19T10:31:58Z,http://arxiv.org/abs/2407.13376v2,http://arxiv.org/pdf/2407.13376v2.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"UMI on Legs: Making Manipulation Policies Mobile with
  Manipulation-Centric Whole-body Controllers","We introduce UMI-on-Legs, a new framework that combines real-world and
simulation data for quadruped manipulation systems. We scale task-centric data
collection in the real world using a hand-held gripper (UMI), providing a cheap
way to demonstrate task-relevant manipulation skills without a robot.
Simultaneously, we scale robot-centric data in simulation by training
whole-body controller for task-tracking without task simulation setups. The
interface between these two policies is end-effector trajectories in the task
frame, inferred by the manipulation policy and passed to the whole-body
controller for tracking. We evaluate UMI-on-Legs on prehensile, non-prehensile,
and dynamic manipulation tasks, and report over 70% success rate on all tasks.
Lastly, we demonstrate the zero-shot cross-embodiment deployment of a
pre-trained manipulation policy checkpoint from prior work, originally intended
for a fixed-base robot arm, on our quadruped system. We believe this framework
provides a scalable path towards learning expressive manipulation skills on
dynamic robot embodiments. Please checkout our website for robot videos, code,
and data: https://umi-on-legs.github.io","Huy Ha, Yihuai Gao, Zipeng Fu, Jie Tan, Shuran Song",2024-07-14T23:03:23Z,2024-07-14T23:03:23Z,http://arxiv.org/abs/2407.10353v1,http://arxiv.org/pdf/2407.10353v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Enhancing Robotic Arm Activity Recognition with Vision Transformers and
  Wavelet-Transformed Channel State Information","Vision-based methods are commonly used in robotic arm activity recognition.
These approaches typically rely on line-of-sight (LoS) and raise privacy
concerns, particularly in smart home applications. Passive Wi-Fi sensing
represents a new paradigm for recognizing human and robotic arm activities,
utilizing channel state information (CSI) measurements to identify activities
in indoor environments. In this paper, a novel machine learning approach based
on discrete wavelet transform and vision transformers for robotic arm activity
recognition from CSI measurements in indoor settings is proposed. This method
outperforms convolutional neural network (CNN) and long short-term memory
(LSTM) models in robotic arm activity recognition, particularly when LoS is
obstructed by barriers, without relying on external or internal sensors or
visual aids. Experiments are conducted using four different data collection
scenarios and four different robotic arm activities. Performance results
demonstrate that wavelet transform can significantly enhance the accuracy of
visual transformer networks in robotic arms activity recognition.","Rojin Zandi, Kian Behzad, Elaheh Motamedi, Hojjat Salehinejad, Milad Siami",2024-07-08T17:28:16Z,2024-07-08T17:28:16Z,http://arxiv.org/abs/2407.06154v1,http://arxiv.org/pdf/2407.06154v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
Safe MPC Alignment with Human Directional Feedback,"In safety-critical robot planning or control, manually specifying safety
constraints or learning them from demonstrations can be challenging. In this
article, we propose a certifiable alignment method for a robot to learn a
safety constraint in its model predictive control (MPC) policy with human
online directional feedback. To our knowledge, it is the first method to learn
safety constraints from human feedback. The proposed method is based on an
empirical observation: human directional feedback, when available, tends to
guide the robot toward safer regions. The method only requires the direction of
human feedback to update the learning hypothesis space. It is certifiable,
providing an upper bound on the total number of human feedback in the case of
successful learning, or declaring the hypothesis misspecification, i.e., the
true implicit safety constraint cannot be found within the specified hypothesis
space. We evaluated the proposed method using numerical examples and user
studies in two simulation games. Additionally, we implemented and tested the
proposed method on a real-world Franka robot arm performing mobile
water-pouring tasks. The results demonstrate the efficacy and efficiency of our
method, showing that it enables a robot to successfully learn safety
constraints with a small handful (tens) of human directional corrections.","Zhixian Xie, Wenlong Zhang, Yi Ren, Zhaoran Wang, George J. Pappas, Wanxin Jin",2024-07-05T02:00:47Z,2025-01-08T01:16:25Z,http://arxiv.org/abs/2407.04216v2,http://arxiv.org/pdf/2407.04216v2.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
Humanoid Parkour Learning,"Parkour is a grand challenge for legged locomotion, even for quadruped
robots, requiring active perception and various maneuvers to overcome multiple
challenging obstacles. Existing methods for humanoid locomotion either optimize
a trajectory for a single parkour track or train a reinforcement learning
policy only to walk with a significant amount of motion references. In this
work, we propose a framework for learning an end-to-end vision-based
whole-body-control parkour policy for humanoid robots that overcomes multiple
parkour skills without any motion prior. Using the parkour policy, the humanoid
robot can jump on a 0.42m platform, leap over hurdles, 0.8m gaps, and much
more. It can also run at 1.8m/s in the wild and walk robustly on different
terrains. We test our policy in indoor and outdoor environments to demonstrate
that it can autonomously select parkour skills while following the rotation
command of the joystick. We override the arm actions and show that this
framework can easily transfer to humanoid mobile manipulation tasks. Videos can
be found at https://humanoid4parkour.github.io","Ziwen Zhuang, Shenzhe Yao, Hang Zhao",2024-06-15T23:21:10Z,2024-09-26T11:13:10Z,http://arxiv.org/abs/2406.10759v2,http://arxiv.org/pdf/2406.10759v2.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"A quantitative investigation for deployment of mobile collaborative
  robots in high-value manufacturing","Component inspection is often the bottleneck in high-value manufacturing,
driving industries like aerospace toward automated inspection technologies.
Current systems often employ fixed arm robots, but they lack the flexibility in
adapting to new components or orientations Advanced mobile robotic platforms
with updated sensor technologies and algorithms have improved localization and
path planning capabilities, making them ideal for bringing inspection processes
directly to parts. However, mobile platforms introduce challenges in
localization and maneuverability, leading to potential errors. Their positional
uncertainty is higher than fixed systems due to the lack of a fixed calibrated
location, posing challenges for position-sensitive inspection sensors.
Therefore, it's essential to assess the positional accuracy and repeatability
of mobile manipulator platforms. The KUKA KMR iiwa was chosen for its
collaborative features, robust build, and scalability within the KUKA product
range. The accuracy and repeatability of the mobile platform were evaluated
through a series of tests to evaluate the performance of its integrated feature
mapping, the effect of various speeds on positional accuracy, and the
efficiency of the omnidirectional wheels for a range of translation
orientations. Experimental evaluation revealed that enabling feature mapping
substantially improves the KUKA KMR iiwa's performance, with accuracy gains and
error reductions exceeding 90%. Repeatability errors were under 7 mm with
mapping activated and around 2.5 mm in practical scenarios, demonstrating that
mobile manipulators, incorporating both the manipulator and platform, can
fulfil the precise requirements of industries with high precision needs.
Providing a highly diverse alternative to traditional fixed-base industrial
manipulators.","Amine Hifi, W. Jackson, C. Loukas, M. Shields, A. Poole, E. Mohseni, C. N. MacLeod, G. Dobie, S. G. Pierce, T. O'Hare, G. Munro, J. O'Brian-O'Reilly, R. W. K. Vithanage",2024-06-10T15:13:57Z,2024-06-10T15:13:57Z,http://arxiv.org/abs/2406.06353v1,http://arxiv.org/pdf/2406.06353v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Gate- and flux-tunable sin(2$\varphi$) Josephson element with
  proximitized Ge-based junctions","Hybrid superconductor-semiconductor Josephson field-effect transistors
(JoFETs) function as Josephson junctions with a gate-tunable critical current.
Additionally, they can feature a non-sinusoidal current-phase relation (CPR)
containing multiple harmonics of the superconducting phase difference, a so-far
underutilized property. In this work, we exploit this multi-harmonicity to
create a Josephson circuit element with an almost perfectly $\pi$-periodic CPR,
indicative of a largely dominant charge-4e supercurrent transport. Such a
Josephson element was recently proposed as the basic building block of a
protected superconducting qubit. Here, it is realized using a superconducting
quantum interference device (SQUID) with low-inductance aluminum arms and two
nominally identical JoFETs. The latter are fabricated from a SiGe/Ge/SiGe
quantum-well heterostructure embedding a high-mobility two-dimensional hole
gas. By carefully adjusting the JoFET gate voltages and finely tuning the
magnetic flux through the SQUID close to half a flux quantum, we achieve a
regime where the $\sin(2\varphi)$ component accounts for more than
\SI{95}{\percent} of the total supercurrent. This result demonstrates a new
promising route for the realization of superconducting qubits with enhanced
coherence properties.","Axel Leblanc, Chotivut Tangchingchai, Zahra Sadre Momtaz, Elyjah Kiyooka, Jean-Michel Hartmann, Frederic Gustavo, Jean-Luc Thomassin, Boris Brun, Vivien Schmitt, Simon Zihlmann, Romain Maurand, Etienne Dumur, Silvano De Franceschi, Francois Lefloch",2024-05-23T15:31:13Z,2024-06-17T14:21:29Z,http://arxiv.org/abs/2405.14695v2,http://arxiv.org/pdf/2405.14695v2.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Optimal Whole Body Trajectory Planning for Mobile Manipulators in
  Planetary Exploration and Construction","Space robotics poses unique challenges arising from the limitation of energy
and computational resources, and the complexity of the environment and employed
platforms. At the control center, offline motion planning is fundamental in the
computation of optimized trajectories accounting for the system's constraints.
Smooth movements, collision and forbidden areas avoidance, target visibility
and energy consumption are all important factors to consider to be able to
generate feasible and optimal plans. When mobile manipulators (terrestrial,
aerial) are employed, the base and the arm movements are often separately
planned, ultimately resulting in sub-optimal solutions. We propose an Optimal
Whole Body Planner (OptiWB) based on Discrete Dynamic Programming (DDP) and
optimal interpolation. Kinematic redundancy is exploited for collision and
forbidden areas avoidance, and to improve target illumination and visibility
from onboard cameras. The planner, implemented in ROS (Robot Operating System),
interfaces 3DROCS, a mission planner used in several programs of the European
Space Agency (ESA) to support planetary exploration surface missions and part
of the ExoMars Rover's planning software. The proposed approach is exercised on
a simplified version of the Analog-1 Interact rover by ESA, a 7-DOFs robotic
arm mounted on a four wheels non-holonomic platform.","Federica Storiale, Enrico Ferrentino, Federico Salvioli, Konstantinos Kapellos, Pasquale Chiacchio",2024-05-23T09:39:23Z,2024-05-23T09:39:23Z,http://arxiv.org/abs/2405.14363v1,http://arxiv.org/pdf/2405.14363v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Energy-Efficient Sleep Mode Optimization of 5G mmWave Networks Using
  Deep Contextual MAB","Millimeter-wave (mmWave) networks, integral to 5G communication, offer a vast
spectrum that addresses the issue of spectrum scarcity and enhances peak rate
and capacity. However, their dense deployment, necessary to counteract
propagation losses, leads to high power consumption. An effective strategy to
reduce this energy consumption in mobile networks is the sleep mode
optimization (SMO) of base stations (BSs). In this paper, we propose a novel
SMO approach for mmWave BSs in a 3D urban environment. This approach, which
incorporates a neural network (NN) based contextual multi-armed bandit (C-MAB)
with an epsilon decay algorithm, accommodates the dynamic and diverse traffic
of user equipment (UE) by clustering the UEs in their respective tracking areas
(TAs). Our strategy includes beamforming, which helps reduce energy consumption
from the UE side, while SMO minimizes energy use from the BS perspective. We
extended our investigation to include Random, Epsilon Greedy, Upper Confidence
Bound (UCB), and Load Based sleep mode (SM) strategies. We compared the
performance of our proposed C-MAB based SM algorithm with those of All On and
other alternative approaches. Simulation results show that our proposed method
outperforms all other SM strategies in terms of the $10^{th}$ percentile of
user rate and average throughput while demonstrating comparable average
throughput to the All On approach. Importantly, it outperforms all approaches
in terms of energy efficiency (EE).","Saad Masrur, Ismail Guvenc, David Lopez-Perez",2024-05-15T17:37:28Z,2024-05-15T17:37:28Z,http://arxiv.org/abs/2405.09528v1,http://arxiv.org/pdf/2405.09528v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"SPIN: Simultaneous Perception, Interaction and Navigation","While there has been remarkable progress recently in the fields of
manipulation and locomotion, mobile manipulation remains a long-standing
challenge. Compared to locomotion or static manipulation, a mobile system must
make a diverse range of long-horizon tasks feasible in unstructured and dynamic
environments. While the applications are broad and interesting, there are a
plethora of challenges in developing these systems such as coordination between
the base and arm, reliance on onboard perception for perceiving and interacting
with the environment, and most importantly, simultaneously integrating all
these parts together. Prior works approach the problem using disentangled
modular skills for mobility and manipulation that are trivially tied together.
This causes several limitations such as compounding errors, delays in
decision-making, and no whole-body coordination. In this work, we present a
reactive mobile manipulation framework that uses an active visual system to
consciously perceive and react to its environment. Similar to how humans
leverage whole-body and hand-eye coordination, we develop a mobile manipulator
that exploits its ability to move and see, more specifically -- to move in
order to see and to see in order to move. This allows it to not only move
around and interact with its environment but also, choose ""when"" to perceive
""what"" using an active visual system. We observe that such an agent learns to
navigate around complex cluttered scenarios while displaying agile whole-body
coordination using only ego-vision without needing to create environment maps.
Results visualizations and videos at https://spin-robot.github.io/","Shagun Uppal, Ananye Agarwal, Haoyu Xiong, Kenneth Shaw, Deepak Pathak",2024-05-13T17:59:36Z,2024-05-13T17:59:36Z,http://arxiv.org/abs/2405.07991v1,http://arxiv.org/pdf/2405.07991v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"vMCU: Coordinated Memory Management and Kernel Optimization for DNN
  Inference on MCUs","IoT devices based on microcontroller units (MCU) provide ultra-low power
consumption and ubiquitous computation for near-sensor deep learning models
(DNN). However, the memory of MCU is usually 2-3 orders of magnitude smaller
than mobile devices, which makes it challenging to map DNNs onto MCUs. Previous
work separates memory management and kernel implementation for MCU and relies
on coarse-grained memory management techniques such as inplace update to reduce
memory consumption.
  In this paper, we propose to coordinate memory management and kernel
optimization for DNN inference on MCUs to enable fine-grained memory
management. The key idea is to virtualize the limited memory of MCU as a large
memory pool. Each kernel divides the memory pool into kernel-specific segments
and handles segment load and store while computing DNN layers. Memory
consumption can be reduced because using the fine-grained segment-level memory
control, we can overlap the memory footprint of different tensors without the
need to materialize them at the same time. Following this idea, we implement
\ours{} for DNN inference on MCU. Evaluation for single layers on ARM Cortex-M4
and Cortex-M7 processors shows that \ours{} can reduce from $12.0\%$ to
$49.5\%$ RAM usage and from $20.6\%$ to $53.0\%$ energy consumption compared to
state-of-the-art work. For full DNN evaluation, \ours{} can reduce the memory
bottleneck by $61.5\%$, enabling more models to be deployed on low-end MCUs.","Size Zheng, Renze Chen, Meng Li, Zihao Ye, Luis Ceze, Yun Liang",2024-05-01T16:24:53Z,2024-05-01T16:24:53Z,http://arxiv.org/abs/2406.06542v1,http://arxiv.org/pdf/2406.06542v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Innovative Integration of Visual Foundation Model with a Robotic Arm on
  a Mobile Platform","In the rapidly advancing field of robotics, the fusion of state-of-the-art
visual technologies with mobile robotic arms has emerged as a critical
integration. This paper introduces a novel system that combines the Segment
Anything model (SAM) -- a transformer-based visual foundation model -- with a
robotic arm on a mobile platform. The design of integrating a depth camera on
the robotic arm's end-effector ensures continuous object tracking,
significantly mitigating environmental uncertainties. By deploying on a mobile
platform, our grasping system has an enhanced mobility, playing a key role in
dynamic environments where adaptability are critical. This synthesis enables
dynamic object segmentation, tracking, and grasping. It also elevates user
interaction, allowing the robot to intuitively respond to various modalities
such as clicks, drawings, or voice commands, beyond traditional robotic
systems. Empirical assessments in both simulated and real-world demonstrate the
system's capabilities. This configuration opens avenues for wide-ranging
applications, from industrial settings, agriculture, and household tasks, to
specialized assignments and beyond.","Shimian Zhang, Qiuhong Lu",2024-04-29T14:10:08Z,2024-04-29T14:10:08Z,http://arxiv.org/abs/2404.18720v1,http://arxiv.org/pdf/2404.18720v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"ESPM-D: Efficient Sparse Polynomial Multiplication for Dilithium on ARM
  Cortex-M4 and Apple M2","Dilithium is a lattice-based digital signature scheme standardized by the
NIST post-quantum cryptography (PQC) project. In this study, we focus on
developing efficient sparse polynomial multiplication implementations of
Dilithium for ARM Cortex-M4 and Apple M2, which are both based on the ARM
architecture. The ARM Cortex-M4 is commonly utilized in resource-constrained
devices such as sensors. Conversely, the Apple M2 is typically found on mobile
devices, emphasizing high performance and versatility. Accordingly, our
optimization strategies differ between ARM Cortex-M4 and Apple M2. We
prioritize optimizing stack usage for the former while enhancing computational
efficiency for the latter. Our optimized sparse polynomial multiplication
achieves significant speedups of up to 30% on ARM Cortex-M4 and 55% on Apple M2
compared to the state-of-the-art Number-Theoretic Transform (NTT)
implementation. Additionally, we integrate the sparse polynomial multiplication
with the infinity norm judgments in the Dilithium signing process, further
enhancing signing efficiency. Our optimized implementation not only reduces
stack usage by 10.8%, 1.2%, and 7.7% in the signing procedure of Dilithium2,
Dilithium3, and Dilithium5, respectively, but also enhances signing performance
by 0.4% to 0.8% compared to the state-of-the-art ARM Cortex-M4 implementation.
Furthermore, we optimize polynomial sampling, rounding functions, and
polynomial packing and unpacking using ARM Cortex-M4 DSP instructions,
resulting in a 0.4%-3.2% improvement in key generation and verification
procedures. On the MacBook Air 2022, our Dilithium implementation achieves 10%
to 11% speedups in the signing procedure. To the best of our knowledge, our
work sets new performance records for Dilithium on both ARM Cortex-M4 and Apple
M2 platforms.","Jieyu Zheng, Hong Zhang, Le Tian, Zhuo Zhang, Hanyu Wei, Zhiwei Chu, Yafang Yang, Yunlei Zhao",2024-04-19T07:21:55Z,2024-04-19T07:21:55Z,http://arxiv.org/abs/2404.12675v1,http://arxiv.org/pdf/2404.12675v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Human-Robot Co-Transportation using Disturbance-Aware MPC with Pose
  Optimization","This paper proposes a new control algorithm for human-robot co-transportation
using a robot manipulator equipped with a mobile base and a robotic arm. We
integrate the regular Model Predictive Control (MPC) with a novel pose
optimization mechanism to more efficiently mitigate disturbances (such as human
behavioral uncertainties or robot actuation noise) during the task. The core of
our methodology involves a two-step iterative design: At each planning horizon,
we determine the optimal pose of the robotic arm (joint angle configuration)
from a candidate set, aiming to achieve the lowest estimated control cost. This
selection is based on solving a disturbance-aware Discrete Algebraic Ricatti
Equation (DARE), which also determines the optimal inputs for the robot's whole
body control (including both the mobile base and the robotic arm). To validate
the effectiveness of the proposed approach, we provide theoretical derivation
for the disturbance-aware DARE and perform simulated experiments and hardware
demos using a Fetch robot under varying conditions, including different
trajectories and different levels of disturbances. The results reveal that our
proposed approach outperforms baseline algorithms.","Al Jaber Mahmud, Amir Hossain Raj, Duc M. Nguyen, Weizi Li, Xuesu Xiao, Xuan Wang",2024-03-31T01:21:55Z,2025-06-18T01:40:25Z,http://arxiv.org/abs/2404.00514v3,http://arxiv.org/pdf/2404.00514v3.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
Visual Whole-Body Control for Legged Loco-Manipulation,"We study the problem of mobile manipulation using legged robots equipped with
an arm, namely legged loco-manipulation. The robot legs, while usually utilized
for mobility, offer an opportunity to amplify the manipulation capabilities by
conducting whole-body control. That is, the robot can control the legs and the
arm at the same time to extend its workspace. We propose a framework that can
conduct the whole-body control autonomously with visual observations. Our
approach, namely Visual Whole-Body Control(VBC), is composed of a low-level
policy using all degrees of freedom to track the body velocities along with the
end-effector position, and a high-level policy proposing the velocities and
end-effector position based on visual inputs. We train both levels of policies
in simulation and perform Sim2Real transfer for real robot deployment. We
perform extensive experiments and show significant improvements over baselines
in picking up diverse objects in different configurations (heights, locations,
orientations) and environments.","Minghuan Liu, Zixuan Chen, Xuxin Cheng, Yandong Ji, Ri-Zhao Qiu, Ruihan Yang, Xiaolong Wang",2024-03-25T17:26:08Z,2024-11-02T18:04:23Z,http://arxiv.org/abs/2403.16967v5,http://arxiv.org/pdf/2403.16967v5.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
Current-Based Impedance Control for Interacting with Mobile Manipulators,"As robots shift from industrial to human-centered spaces, adopting mobile
manipulators, which expand workspace capabilities, becomes crucial. In these
settings, seamless interaction with humans necessitates compliant control. Two
common methods for safe interaction, admittance, and impedance control, require
force or torque sensors, often absent in lower-cost or lightweight robots. This
paper presents an adaption of impedance control that can be used on
current-controlled robots without the use of force or torque sensors and its
application for compliant control of a mobile manipulator. A calibration method
is designed that enables estimation of the actuators' current/torque ratios and
frictions, used by the adapted impedance controller, and that can handle model
errors. The calibration method and the performance of the designed controller
are experimentally validated using the Kinova GEN3 Lite arm. Results show that
the calibration method is consistent and that the designed controller for the
arm is compliant while also being able to track targets with five-millimeter
precision when no interaction is present. Additionally, this paper presents two
operational modes for interacting with the mobile manipulator: one for guiding
the robot around the workspace through interacting with the arm and another for
executing a tracking task, both maintaining compliance to external forces.
These operational modes were tested in real-world experiments, affirming their
practical applicability and effectiveness.","Jelmer de Wolde, Luzia Knoedler, Gianluca Garofalo, Javier Alonso-Mora",2024-03-19T18:15:35Z,2024-03-19T18:15:35Z,http://arxiv.org/abs/2403.13079v1,http://arxiv.org/pdf/2403.13079v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
LAVA: Long-horizon Visual Action based Food Acquisition,"Robotic Assisted Feeding (RAF) addresses the fundamental need for individuals
with mobility impairments to regain autonomy in feeding themselves. The goal of
RAF is to use a robot arm to acquire and transfer food to individuals from the
table. Existing RAF methods primarily focus on solid foods, leaving a gap in
manipulation strategies for semi-solid and deformable foods. This study
introduces Long-horizon Visual Action (LAVA) based food acquisition of liquid,
semisolid, and deformable foods. Long-horizon refers to the goal of ""clearing
the bowl"" by sequentially acquiring the food from the bowl. LAVA employs a
hierarchical policy for long-horizon food acquisition tasks. The framework uses
high-level policy to determine primitives by leveraging ScoopNet. At the
mid-level, LAVA finds parameters for primitives using vision. To carry out
sequential plans in the real world, LAVA delegates action execution which is
driven by Low-level policy that uses parameters received from mid-level policy
and behavior cloning ensuring precise trajectory execution. We validate our
approach on complex real-world acquisition trials involving granular, liquid,
semisolid, and deformable food types along with fruit chunks and soup
acquisition. Across 46 bowls, LAVA acquires much more efficiently than
baselines with a success rate of 89 +/- 4% and generalizes across realistic
plate variations such as different positions, varieties, and amount of food in
the bowl. Code, datasets, videos, and supplementary materials can be found on
our website.","Amisha Bhaskar, Rui Liu, Vishnu D. Sharma, Guangyao Shi, Pratap Tokekar",2024-03-19T16:21:40Z,2024-03-19T16:21:40Z,http://arxiv.org/abs/2403.12876v1,http://arxiv.org/pdf/2403.12876v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"CEASE: Collision-Evaluation-based Active Sense System for Collaborative
  Robotic Arms","Collision detection via visual fences can significantly enhance the safety of
collaborative robotic arms. Existing work typically performs such detection
based on pre-deployed stationary cameras outside the robotic arm's workspace.
These stationary cameras can only provide a restricted detection range and
constrain the mobility of the robotic system. To cope with this issue, we
propose an active sense method enabling a wide range of collision risk
evaluation in dynamic scenarios. First, an active vision mechanism is
implemented by equipping cameras with additional degrees of rotation.
Considering the uncertainty in the active sense, we design a state confidence
envelope to uniformly characterize both known and potential dynamic obstacles.
Subsequently, using the observation-based uncertainty evolution, collision risk
is evaluated by the prediction of obstacle envelopes. On this basis, a Markov
decision process was employed to search for an optimal observation sequence of
the active sense system, which enlarges the field of observation and reduces
uncertainties in the state estimation of surrounding obstacles. Simulation and
real-world experiments consistently demonstrate a 168% increase in the
observation time coverage of typical dynamic humanoid obstacles compared to the
method using stationary cameras, which underscores our system's effectiveness
in collision risk tracking and enhancing the safety of robotic arms.","Xian Huang, Yuanjiong Ying, Wei Dong",2024-03-09T02:11:27Z,2024-03-09T02:11:27Z,http://arxiv.org/abs/2403.05761v1,http://arxiv.org/pdf/2403.05761v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
Grasping Trajectory Optimization with Point Clouds,"We introduce a new trajectory optimization method for robotic grasping based
on a point-cloud representation of robots and task spaces. In our method,
robots are represented by 3D points on their link surfaces. The task space of a
robot is represented by a point cloud that can be obtained from depth sensors.
Using the point-cloud representation, goal reaching in grasping can be
formulated as point matching, while collision avoidance can be efficiently
achieved by querying the signed distance values of the robot points in the
signed distance field of the scene points. Consequently, a constrained
nonlinear optimization problem is formulated to solve the joint motion and
grasp planning problem. The advantage of our method is that the point-cloud
representation is general to be used with any robot in any environment. We
demonstrate the effectiveness of our method by performing experiments on a
tabletop scene and a shelf scene for grasping with a Fetch mobile manipulator
and a Franka Panda arm. The project page is available at
\url{https://irvlutd.github.io/GraspTrajOpt}","Yu Xiang, Sai Haneesh Allu, Rohith Peddi, Tyler Summers, Vibhav Gogate",2024-03-08T17:29:51Z,2024-08-07T20:33:27Z,http://arxiv.org/abs/2403.05466v2,http://arxiv.org/pdf/2403.05466v2.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Multimodal Anomaly Detection based on Deep Auto-Encoder for Object Slip
  Perception of Mobile Manipulation Robots","Object slip perception is essential for mobile manipulation robots to perform
manipulation tasks reliably in the dynamic real-world. Traditional approaches
to robot arms' slip perception use tactile or vision sensors. However, mobile
robots still have to deal with noise in their sensor signals caused by the
robot's movement in a changing environment. To solve this problem, we present
an anomaly detection method that utilizes multisensory data based on a deep
autoencoder model. The proposed framework integrates heterogeneous data streams
collected from various robot sensors, including RGB and depth cameras, a
microphone, and a force-torque sensor. The integrated data is used to train a
deep autoencoder to construct latent representations of the multisensory data
that indicate the normal status. Anomalies can then be identified by error
scores measured by the difference between the trained encoder's latent values
and the latent values of reconstructed input data. In order to evaluate the
proposed framework, we conducted an experiment that mimics an object slip by a
mobile service robot operating in a real-world environment with diverse
household objects and different moving patterns. The experimental results
verified that the proposed framework reliably detects anomalies in object slip
situations despite various object types and robot behaviors, and visual and
auditory noise in the environment.","Youngjae Yoo, Chung-Yeon Lee, Byoung-Tak Zhang",2024-03-06T09:15:53Z,2024-03-06T09:15:53Z,http://arxiv.org/abs/2403.03563v1,http://arxiv.org/pdf/2403.03563v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Low Complexity Deep Learning Augmented Wireless Channel Estimation for
  Pilot-Based OFDM on Zynq System on Chip","Channel estimation (CE) is one of the critical signal-processing tasks of the
wireless physical layer (PHY). Recent deep learning (DL) based CE have
outperformed statistical approaches such as least-square-based CE (LS) and
linear minimum mean square error-based CE (LMMSE). However, existing CE
approaches have not yet been realized on system-on-chip (SoC). The first
contribution of this paper is to efficiently implement the existing
state-of-the-art CE algorithms on Zynq SoC (ZSoC), comprising of ARM processor
and field programmable gate array (FPGA), via hardware-software co-design and
fixed point analysis. We validate the superiority of DL-based CE and LMMSE over
LS for various signal-to-noise ratios (SNR) and wireless channels in terms of
mean square error (MSE) and bit error rate (BER). We also highlight the high
complexity, execution time, and power consumption of DL-based CE and LMMSE
approaches. To address this, we propose a novel compute-efficient LS-augmented
interpolated deep neural network (LSiDNN) based CE algorithm and realize it on
ZSoC. The proposed LSiDNN offers 88-90% lower execution time and 38-85% lower
resource utilization than state-of-the-art DL-based CE for identical MSE and
BER. LSiDNN offers significantly lower MSE and BER than LMMSE, and the gain
improves with increased mobility between transceivers. It offers 75% lower
execution time and 90-94% lower resource utilization than LMMSE.","Animesh Sharma, Syed Asrar Ul Haq, Sumit J. Darak",2024-03-02T05:27:41Z,2024-03-02T05:27:41Z,http://arxiv.org/abs/2403.01098v1,http://arxiv.org/pdf/2403.01098v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Pushing the Limits of Cross-Embodiment Learning for Manipulation and
  Navigation","Recent years in robotics and imitation learning have shown remarkable
progress in training large-scale foundation models by leveraging data across a
multitude of embodiments. The success of such policies might lead us to wonder:
just how diverse can the robots in the training set be while still facilitating
positive transfer? In this work, we study this question in the context of
heterogeneous embodiments, examining how even seemingly very different domains,
such as robotic navigation and manipulation, can provide benefits when included
in the training data for the same model. We train a single goal-conditioned
policy that is capable of controlling robotic arms, quadcopters, quadrupeds,
and mobile bases. We then investigate the extent to which transfer can occur
across navigation and manipulation on these embodiments by framing them as a
single goal-reaching task. We find that co-training with navigation data can
enhance robustness and performance in goal-conditioned manipulation with a
wrist-mounted camera. We then deploy our policy trained only from
navigation-only and static manipulation-only data on a mobile manipulator,
showing that it can control a novel embodiment in a zero-shot manner. These
results provide evidence that large-scale robotic policies can benefit from
data collected across various embodiments. Further information and robot videos
can be found on our project website http://extreme-cross-embodiment.github.io.","Jonathan Yang, Catherine Glossop, Arjun Bhorkar, Dhruv Shah, Quan Vuong, Chelsea Finn, Dorsa Sadigh, Sergey Levine",2024-02-29T18:30:32Z,2024-02-29T18:30:32Z,http://arxiv.org/abs/2402.19432v1,http://arxiv.org/pdf/2402.19432v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Real-time 3D Semantic Scene Perception for Egocentric Robots with
  Binocular Vision","Perceiving a three-dimensional (3D) scene with multiple objects while moving
indoors is essential for vision-based mobile cobots, especially for enhancing
their manipulation tasks. In this work, we present an end-to-end pipeline with
instance segmentation, feature matching, and point-set registration for
egocentric robots with binocular vision, and demonstrate the robot's grasping
capability through the proposed pipeline. First, we design an RGB image-based
segmentation approach for single-view 3D semantic scene segmentation,
leveraging common object classes in 2D datasets to encapsulate 3D points into
point clouds of object instances through corresponding depth maps. Next, 3D
correspondences of two consecutive segmented point clouds are extracted based
on matched keypoints between objects of interest in RGB images from the prior
step. In addition, to be aware of spatial changes in 3D feature distribution,
we also weigh each 3D point pair based on the estimated distribution using
kernel density estimation (KDE), which subsequently gives robustness with less
central correspondences while solving for rigid transformations between point
clouds. Finally, we test our proposed pipeline on the 7-DOF dual-arm Baxter
robot with a mounted Intel RealSense D435i RGB-D camera. The result shows that
our robot can segment objects of interest, register multiple views while
moving, and grasp the target object. The source code is available at
https://github.com/mkhangg/semantic_scene_perception.","K. Nguyen, T. Dang, M. Huber",2024-02-19T06:28:40Z,2024-02-19T06:28:40Z,http://arxiv.org/abs/2402.11872v1,http://arxiv.org/pdf/2402.11872v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"MorpheusNet: Resource efficient sleep stage classifier for embedded
  on-line systems","Sleep Stage Classification (SSC) is a labor-intensive task, requiring experts
to examine hours of electrophysiological recordings for manual classification.
This is a limiting factor when it comes to leveraging sleep stages for
therapeutic purposes. With increasing affordability and expansion of wearable
devices, automating SSC may enable deployment of sleep-based therapies at
scale. Deep Learning has gained increasing attention as a potential method to
automate this process. Previous research has shown accuracy comparable to
manual expert scores. However, previous approaches require sizable amount of
memory and computational resources. This constrains the ability to classify in
real time and deploy models on the edge. To address this gap, we aim to provide
a model capable of predicting sleep stages in real-time, without requiring
access to external computational sources (e.g., mobile phone, cloud). The
algorithm is power efficient to enable use on embedded battery powered systems.
Our compact sleep stage classifier can be deployed on most off-the-shelf
microcontrollers (MCU) with constrained hardware settings. This is due to the
memory footprint of our approach requiring significantly fewer operations. The
model was tested on three publicly available data bases and achieved
performance comparable to the state of the art, whilst reducing model
complexity by orders of magnitude (up to 280 times smaller compared to state of
the art). We further optimized the model with quantization of parameters to 8
bits with only an average drop of 0.95% in accuracy. When implemented in
firmware, the quantized model achieves a latency of 1.6 seconds on an Arm
CortexM4 processor, allowing its use for on-line SSC-based therapies.","Ali Kavoosi, Morgan P. Mitchell, Raveen Kariyawasam, John E. Fleming, Penny Lewis, Heidi Johansen-Berg, Hayriye Cagnan, Timothy Denison",2024-01-14T17:52:08Z,2024-01-14T17:52:08Z,http://arxiv.org/abs/2401.10284v1,http://arxiv.org/pdf/2401.10284v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Diversity-Based Recruitment in Crowdsensing By Combinatorial Multi-Armed
  Bandits","This paper explores mobile crowdsensing, which leverages mobile devices and
their users for collective sensing tasks under the coordination of a central
requester. The primary challenge here is the variability in the sensing
capabilities of individual workers, which are initially unknown and must be
progressively learned. In each round of task assignment, the requester selects
a group of workers to handle specific tasks. This process inherently leads to
task overlaps in the same round and repetitions across rounds. We propose a
novel model that enhances task diversity over the rounds by dynamically
adjusting the weight of tasks in each round based on their frequency of
assignment. Additionally, it accommodates the variability in task completion
quality caused by overlaps in the same round, which can range from the maximum
individual worker's quality to the summation of qualities of all assigned
workers in the overlap. A significant constraint in this process is the
requester's budget, which demands an efficient strategy for worker recruitment.
Our solution is to maximize the overall weighted quality of tasks completed in
each round. We employ a combinatorial multi-armed bandit framework with an
upper confidence bound approach for this purpose. The paper further presents a
regret analysis and simulations using realistic data to demonstrate the
efficacy of our model.","Abdalaziz Sawwan, Jie Wu",2023-12-25T13:54:58Z,2023-12-25T13:54:58Z,http://arxiv.org/abs/2312.15729v1,http://arxiv.org/pdf/2312.15729v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2023
"Practical Non-Intrusive GUI Exploration Testing with Visual-based
  Robotic Arms","GUI testing is significant in the SE community. Most existing frameworks are
intrusive and only support some specific platforms. With the development of
distinct scenarios, diverse embedded systems or customized operating systems on
different devices do not support existing intrusive GUI testing frameworks.
Some approaches adopt robotic arms to replace the interface invoking of mobile
apps under test and use computer vision technologies to identify GUI elements.
However, some challenges are unsolved. First, existing approaches assume that
GUI screens are fixed so that they cannot be adapted to diverse systems with
different screen conditions. Second, existing approaches use XY-plane robotic
arms, which cannot flexibly simulate testing operations. Third, existing
approaches ignore compatibility bugs and only focus on crash bugs. A more
practical approach is required for the non-intrusive scenario. We propose a
practical non-intrusive GUI testing framework with visual robotic arms.
RoboTest integrates novel GUI screen and widget detection algorithms, adaptive
to detecting screens of different sizes and then to extracting GUI widgets from
the detected screens. Then, a set of testing operations is applied with a 4-DOF
robotic arm, which effectively and flexibly simulates human testing operations.
During app exploration, RoboTest integrates the Principle of Proximity-guided
exploration strategy, choosing close widgets of the previous targets to reduce
robotic arm movement overhead and improve exploration efficiency. RoboTest can
effectively detect some compatibility bugs beyond crash bugs with a GUI
comparison on different devices of the same test operations. We evaluate
RoboTest with 20 mobile apps, with a case study on an embedded system. The
results show that RoboTest can effectively, efficiently, and generally explore
AUTs to find bugs and reduce exploration time overhead.","Shengcheng Yu, Chunrong Fang, Mingzhe Du, Yuchen Ling, Zhenyu Chen, Zhendong Su",2023-12-17T09:05:39Z,2023-12-17T09:05:39Z,http://arxiv.org/abs/2312.10655v1,http://arxiv.org/pdf/2312.10655v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2023
RoboPainter -- a conceptual towards robotized interior finishes,"High demand for painters is required nowadays and foreseen in the near future
for both developed and developing countries. To satisfy such demand, this paper
presents the detailed computer aided design (CAD) model of a fully functional
wall painting robot for interior finishes. The RoboPainter is capable of
performing full scale wall-ceil painting in addition to decorative wall
drawings. The 8 degrees of freedom (DOF) mobile robot structure consists of a
6DOF spray painting arm mounted on a 2DOF differentially driven mobile base.
The design presented endows several achievements in terms of total robot mass
and painting rate as compared to existing literature. Detailed dynamic model
parameters are presented to allow for further enhancement in terms of robot
motion control.",Mohamed Sorour,2023-12-16T09:47:42Z,2023-12-16T09:47:42Z,http://arxiv.org/abs/2312.10395v1,http://arxiv.org/pdf/2312.10395v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2023
"Using Augmented Reality to Assess and Modify Mobile Manipulator Surface
  Repair Plans","Industrial robotics are redefining inspection and maintenance routines across
multiple sectors, enhancing safety, efficiency, and environmental
sustainability. In outdoor industrial facilities, it is crucial to inspect and
repair complex surfaces affected by corrosion. To address this challenge,
mobile manipulators have been developed to navigate these facilities, identify
corroded areas, and apply protective coatings. However, given that this
technology is still in its infancy and the consequences of improperly coating
essential equipment can be significant, human oversight is necessary to review
the robot's corrosion identification and repair plan. We present a practical
and scalable Augmented Reality (AR)-based system designed to empower
non-experts to visualize, modify, and approve robot-generated surface corrosion
repair plans in real-time. Built upon an AR-based human-robot interaction
framework, Augmented Robot Environment (AugRE), we developed a comprehensive AR
application module called Situational Task Accept and Repair (STAR). STAR
allows users to examine identified corrosion images, point cloud data, and
robot navigation objectives overlaid on the physical environment within these
industrial environments. Users are able to additionally make adjustments to the
robot repair plan in real-time using interactive holographic volumes, excluding
critical nearby equipment that might be at risk of coating overspray. We
demonstrate the entire system using a Microsoft HoloLens 2 and a dual-arm
mobile manipulator. Our future research will focus on evaluating user
experience, system robustness, and real-world validation.","Frank Regal, Steven Swanbeck, Fabian Parra, Jared Rosenbaum, Mitch Pryor",2023-11-02T04:34:57Z,2023-11-02T04:34:57Z,http://arxiv.org/abs/2311.00988v1,http://arxiv.org/pdf/2311.00988v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2023
"SparseByteNN: A Novel Mobile Inference Acceleration Framework Based on
  Fine-Grained Group Sparsity","To address the challenge of increasing network size, researchers have
developed sparse models through network pruning. However, maintaining model
accuracy while achieving significant speedups on general computing devices
remains an open problem. In this paper, we present a novel mobile inference
acceleration framework SparseByteNN, which leverages fine-grained kernel
sparsity to achieve real-time execution as well as high accuracy. Our framework
consists of two parts: (a) A fine-grained kernel sparsity schema with a
sparsity granularity between structured pruning and unstructured pruning. It
designs multiple sparse patterns for different operators. Combined with our
proposed whole network rearrangement strategy, the schema achieves a high
compression rate and high precision at the same time. (b) Inference engine
co-optimized with the sparse pattern. The conventional wisdom is that this
reduction in theoretical FLOPs does not translate into real-world efficiency
gains. We aim to correct this misconception by introducing a family of
efficient sparse kernels for ARM and WebAssembly. Equipped with our efficient
implementation of sparse primitives, we show that sparse versions of
MobileNet-v1 outperform strong dense baselines on the efficiency-accuracy
curve. Experimental results on Qualcomm 855 show that for 30% sparse
MobileNet-v1, SparseByteNN achieves 1.27x speedup over the dense version and
1.29x speedup over the state-of-the-art sparse inference engine MNN with a
slight accuracy drop of 0.224%. The source code of SparseByteNN will be
available at https://github.com/lswzjuer/SparseByteNN","Haitao Xu, Songwei Liu, Yuyang Xu, Shuai Wang, Jiashi Li, Chenqian Yan, Liangqiang Li, Lean Fu, Xin Pan, Fangmin Chen",2023-10-30T13:08:48Z,2023-10-30T13:08:48Z,http://arxiv.org/abs/2310.19509v1,http://arxiv.org/pdf/2310.19509v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2023
"AdaptiX -- A Transitional XR Framework for Development and Evaluation of
  Shared Control Applications in Assistive Robotics","With the ongoing efforts to empower people with mobility impairments and the
increase in technological acceptance by the general public, assistive
technologies, such as collaborative robotic arms, are gaining popularity. Yet,
their widespread success is limited by usability issues, specifically the
disparity between user input and software control along the autonomy continuum.
To address this, shared control concepts provide opportunities to combine the
targeted increase of user autonomy with a certain level of computer assistance.
This paper presents the free and open-source AdaptiX XR framework for
developing and evaluating shared control applications in a high-resolution
simulation environment. The initial framework consists of a simulated robotic
arm with an example scenario in Virtual Reality (VR), multiple standard control
interfaces, and a specialized recording/replay system. AdaptiX can easily be
extended for specific research needs, allowing Human-Robot Interaction (HRI)
researchers to rapidly design and test novel interaction methods, intervention
strategies, and multi-modal feedback techniques, without requiring an actual
physical robotic arm during the early phases of ideation, prototyping, and
evaluation. Also, a Robot Operating System (ROS) integration enables the
controlling of a real robotic arm in a PhysicalTwin approach without any
simulation-reality gap. Here, we review the capabilities and limitations of
AdaptiX in detail and present three bodies of research based on the framework.
AdaptiX can be accessed at https://adaptix.robot-research.de.","Max Pascher, Felix Ferdinand Goldau, Kirill Kronhardt, Udo Frese, Jens Gerken",2023-10-24T14:44:41Z,2024-05-17T16:43:15Z,http://arxiv.org/abs/2310.15887v3,http://arxiv.org/pdf/2310.15887v3.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2023
"Robot-Assisted Navigation for Visually Impaired through Adaptive
  Impedance and Path Planning","This paper presents a framework to navigate visually impaired people through
unfamiliar environments by means of a mobile manipulator. The Human-Robot
system consists of three key components: a mobile base, a robotic arm, and the
human subject who gets guided by the robotic arm via physically coupling their
hand with the cobot's end-effector. These components, receiving a goal from the
user, traverse a collision-free set of waypoints in a coordinated manner, while
avoiding static and dynamic obstacles through an obstacle avoidance unit and a
novel human guidance planner. With this aim, we also present a legs tracking
algorithm that utilizes 2D LiDAR sensors integrated into the mobile base to
monitor the human pose. Additionally, we introduce an adaptive pulling planner
responsible for guiding the individual back to the intended path if they veer
off course. This is achieved by establishing a target arm end-effector position
and dynamically adjusting the impedance parameters in real-time through a
impedance tuning unit. To validate the framework we present a set of
experiments both in laboratory settings with 12 healthy blindfolded subjects
and a proof-of-concept demonstration in a real-world scenario.","Pietro Balatti, Idil Ozdamar, Doganay Sirintuna, Luca Fortini, Mattia Leonori, Juan M. Gandarias, Arash Ajoudani",2023-10-23T08:46:14Z,2023-10-23T08:46:14Z,http://arxiv.org/abs/2310.14705v1,http://arxiv.org/pdf/2310.14705v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2023
"EdgeAISim: A Toolkit for Simulation and Modelling of AI Models in Edge
  Computing Environments","To meet next-generation IoT application demands, edge computing moves
processing power and storage closer to the network edge to minimise latency and
bandwidth utilisation. Edge computing is becoming popular as a result of these
benefits, but resource management is still challenging. Researchers are
utilising AI models to solve the challenge of resource management in edge
computing systems. However, existing simulation tools are only concerned with
typical resource management policies, not the adoption and implementation of AI
models for resource management, especially. Consequently, researchers continue
to face significant challenges, making it hard and time-consuming to use AI
models when designing novel resource management policies for edge computing
with existing simulation tools. To overcome these issues, we propose a
lightweight Python-based toolkit called EdgeAISim for the simulation and
modelling of AI models for designing resource management policies in edge
computing environments. In EdgeAISim, we extended the basic components of the
EdgeSimPy framework and developed new AI-based simulation models for task
scheduling, energy management, service migration, network flow scheduling, and
mobility support for edge computing environments. In EdgeAISim, we have
utilised advanced AI models such as Multi-Armed Bandit with Upper Confidence
Bound, Deep Q-Networks, Deep Q-Networks with Graphical Neural Network, and
ActorCritic Network to optimize power usage while efficiently managing task
migration within the edge computing environment. The performance of these
proposed models of EdgeAISim is compared with the baseline, which uses a
worst-fit algorithm-based resource management policy in different settings.
Experimental results indicate that EdgeAISim exhibits a substantial reduction
in power consumption, highlighting the compelling success of power optimization
strategies in EdgeAISim.","Aadharsh Roshan Nandhakumar, Ayush Baranwal, Priyanshukumar Choudhary, Muhammed Golec, Sukhpal Singh Gill",2023-10-09T10:47:36Z,2023-10-09T10:47:36Z,http://arxiv.org/abs/2310.05605v1,http://arxiv.org/pdf/2310.05605v1.pdf,all:robot arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2023
MR6D: Benchmarking 6D Pose Estimation for Mobile Robots,"Existing 6D pose estimation datasets primarily focus on small household
objects typically handled by robot arm manipulators, limiting their relevance
to mobile robotics. Mobile platforms often operate without manipulators,
interact with larger objects, and face challenges such as long-range
perception, heavy self-occlusion, and diverse camera perspectives. While recent
models generalize well to unseen objects, evaluations remain confined to
household-like settings that overlook these factors. We introduce MR6D, a
dataset designed for 6D pose estimation for mobile robots in industrial
environments. It includes 92 real-world scenes featuring 16 unique objects
across static and dynamic interactions. MR6D captures the challenges specific
to mobile platforms, including distant viewpoints, varied object
configurations, larger object sizes, and complex occlusion/self-occlusion
patterns. Initial experiments reveal that current 6D pipelines underperform in
these settings, with 2D segmentation being another hurdle. MR6D establishes a
foundation for developing and evaluating pose estimation methods tailored to
the demands of mobile robotics. The dataset is available at
https://huggingface.co/datasets/anas-gouda/mr6d.","Anas Gouda, Shrutarv Awasthi, Christian Blesing, Lokeshwaran Manohar, Frank Hoffmann, Alice Kirchheim",2025-08-19T12:21:34Z,2025-08-19T12:21:34Z,http://arxiv.org/abs/2508.13775v1,http://arxiv.org/pdf/2508.13775v1.pdf,all:robot arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
A novel autonomous microplastics surveying robot for beach environments,"Microplastics, defined as plastic particles smaller than 5 millimeters, have
become a pervasive environmental contaminant that accumulates on beaches due to
wind patterns and tidal forcing. Detecting microplastics and mapping their
concentration in the wild remains one of the primary challenges in addressing
this environmental issue. This paper introduces a novel robotic platform that
automatically detects and chemically analyzes microplastics on beach surfaces.
This mobile manipulator system scans areas for microplastics using a camera
mounted on the robotic arm's end effector. The system effectively segments
candidate microplastic particles on sand surfaces even in the presence of
organic matter such as leaves and clams. Once a candidate microplastic particle
is detected, the system steers a near-infrared (NIR) spectroscopic sensor onto
the particle using both NIR and visual feedback to chemically analyze it in
real-time. Through experiments in lab and beach environments, the system is
shown to achieve an excellent positional precision in manipulation control and
high microplastic classification accuracy.","Hassan Iqbal, Kobiny Rex, Joseph Shirley, Carlos Baiz, Christian Claudel",2025-08-04T23:20:00Z,2025-08-04T23:20:00Z,http://arxiv.org/abs/2508.02952v1,http://arxiv.org/pdf/2508.02952v1.pdf,all:robot arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"ULC: A Unified and Fine-Grained Controller for Humanoid
  Loco-Manipulation","Loco-Manipulation for humanoid robots aims to enable robots to integrate
mobility with upper-body tracking capabilities. Most existing approaches adopt
hierarchical architectures that decompose control into isolated upper-body
(manipulation) and lower-body (locomotion) policies. While this decomposition
reduces training complexity, it inherently limits coordination between
subsystems and contradicts the unified whole-body control exhibited by humans.
We demonstrate that a single unified policy can achieve a combination of
tracking accuracy, large workspace, and robustness for humanoid
loco-manipulation. We propose the Unified Loco-Manipulation Controller (ULC), a
single-policy framework that simultaneously tracks root velocity, root height,
torso rotation, and dual-arm joint positions in an end-to-end manner, proving
the feasibility of unified control without sacrificing performance. We achieve
this unified control through key technologies: sequence skill acquisition for
progressive learning complexity, residual action modeling for fine-grained
control adjustments, command polynomial interpolation for smooth motion
transitions, random delay release for robustness to deploy variations, load
randomization for generalization to external disturbances, and
center-of-gravity tracking for providing explicit policy gradients to maintain
stability. We validate our method on the Unitree G1 humanoid robot with 3-DOF
(degrees-of-freedom) waist. Compared with strong baselines, ULC shows better
tracking performance to disentangled methods and demonstrating larger workspace
coverage. The unified dual-arm tracking enables precise manipulation under
external loads while maintaining coordinated whole-body control for complex
loco-manipulation tasks.","Wandong Sun, Luying Feng, Baoshi Cao, Yang Liu, Yaochu Jin, Zongwu Xie",2025-07-09T14:44:52Z,2025-07-09T14:44:52Z,http://arxiv.org/abs/2507.06905v1,http://arxiv.org/pdf/2507.06905v1.pdf,all:robot arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"A comprehensive control architecture for semi-autonomous dual-arm robots
  in agriculture settings","The adoption of mobile robotic platforms in complex environments, such as
agricultural settings, requires these systems to exhibit a flexible yet
effective architecture that integrates perception and control. In such
scenarios, several tasks need to be accomplished simultaneously, ranging from
managing robot limits to performing operational tasks and handling human
inputs. The purpose of this paper is to present a comprehensive control
architecture for achieving complex tasks such as robotized harvesting in
vineyards within the framework of the European project CANOPIES. In detail, a
16-DOF dual-arm mobile robot is employed, controlled via a Hierarchical
Quadratic Programming (HQP) approach capable of handling both equality and
inequality constraints at various priorities to harvest grape bunches selected
by the perception system developed within the project. Furthermore, given the
complexity of the scenario and the uncertainty in the perception system, which
could potentially lead to collisions with the environment, the handling of
interaction forces is necessary. Remarkably, this was achieved using the same
HQP framework. This feature is further leveraged to enable semi-autonomous
operations, allowing a human operator to assist the robotic counterpart in
completing harvesting tasks. Finally, the obtained results are validated
through extensive testing conducted first in a laboratory environment to prove
individual functionalities, then in a real vineyard, encompassing both
autonomous and semi-autonomous grape harvesting operations.","Jozsef Palmieri, Paolo Di Lillo, Stefano Chiaverini, Alessandro Marino",2025-06-30T10:54:43Z,2025-06-30T10:54:43Z,http://arxiv.org/abs/2506.23723v1,http://arxiv.org/pdf/2506.23723v1.pdf,all:robot arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"HARMONI: Haptic-Guided Assistance for Unified Robotic Tele-Manipulation
  and Tele-Navigation","Shared control, which combines human expertise with autonomous assistance, is
critical for effective teleoperation in complex environments. While recent
advances in haptic-guided teleoperation have shown promise, they are often
limited to simplified tasks involving 6- or 7-DoF manipulators and rely on
separate control strategies for navigation and manipulation. This increases
both cognitive load and operational overhead. In this paper, we present a
unified tele-mobile manipulation framework that leverages haptic-guided shared
control. The system integrates a 9-DoF follower mobile manipulator and a 7-DoF
leader robotic arm, enabling seamless transitions between tele-navigation and
tele-manipulation through real-time haptic feedback. A user study with 20
participants under real-world conditions demonstrates that our framework
significantly improves task accuracy and efficiency without increasing
cognitive load. These findings highlight the potential of haptic-guided shared
control for enhancing operator performance in demanding teleoperation
scenarios.","V. Sripada, A. Khan, J. Föcker, S. Parsa, Susmitha P, H Maior, A. Ghalamzan-E",2025-06-16T17:11:34Z,2025-06-16T17:11:34Z,http://arxiv.org/abs/2506.13704v1,http://arxiv.org/pdf/2506.13704v1.pdf,all:robot arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
Heavy lifting tasks via haptic teleoperation of a wheeled humanoid,"Humanoid robots can support human workers in physically demanding
environments by performing tasks that require whole-body coordination, such as
lifting and transporting heavy objects.These tasks, which we refer to as
Dynamic Mobile Manipulation (DMM), require the simultaneous control of
locomotion, manipulation, and posture under dynamic interaction forces. This
paper presents a teleoperation framework for DMM on a height-adjustable wheeled
humanoid robot for carrying heavy payloads. A Human-Machine Interface (HMI)
enables whole-body motion retargeting from the human pilot to the robot by
capturing the motion of the human and applying haptic feedback. The pilot uses
body motion to regulate robot posture and locomotion, while arm movements guide
manipulation.Real time haptic feedback delivers end effector wrenches and
balance related cues, closing the loop between human perception and robot
environment interaction. We evaluate the different telelocomotion mappings that
offer varying levels of balance assistance, allowing the pilot to either
manually or automatically regulate the robot's lean in response to
payload-induced disturbances. The system is validated in experiments involving
dynamic lifting of barbells and boxes up to 2.5 kg (21% of robot mass),
demonstrating coordinated whole-body control, height variation, and disturbance
handling under pilot guidance. Video demo can be found at:
https://youtu.be/jF270_bG1h8?feature=shared","Amartya Purushottam, Jack Yan, Christopher Yu, Joao Ramos",2025-05-26T05:31:44Z,2025-05-26T05:31:44Z,http://arxiv.org/abs/2505.19530v1,http://arxiv.org/pdf/2505.19530v1.pdf,all:robot arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"MIHRaGe: A Mixed-Reality Interface for Human-Robot Interaction via
  Gaze-Oriented Control","Individuals with upper limb mobility impairments often require assistive
technologies to perform activities of daily living. While gaze-tracking has
emerged as a promising method for robotic assistance, existing solutions lack
sufficient feedback mechanisms, leading to uncertainty in user intent
recognition and reduced adaptability. This paper presents the MIHRAGe
interface, an integrated system that combines gaze-tracking, robotic
assistance, and a mixed-reality to create an immersive environment for
controlling the robot using only eye movements. The system was evaluated
through an experimental protocol involving four participants, assessing gaze
accuracy, robotic positioning precision, and the overall success of a pick and
place task. Results showed an average gaze fixation error of 1.46 cm, with
individual variations ranging from 1.28 cm to 2.14 cm. The robotic arm
demonstrated an average positioning error of +-1.53 cm, with discrepancies
attributed to interface resolution and calibration constraints. In a pick and
place task, the system achieved a success rate of 80%, highlighting its
potential for improving accessibility in human-robot interaction with visual
feedback to the user.","Rafael R. Baptista, Nina R. Gerszberg, Ricardo V. Godoy, Gustavo J. G. Lahr",2025-05-06T19:08:53Z,2025-05-06T19:08:53Z,http://arxiv.org/abs/2505.03929v1,http://arxiv.org/pdf/2505.03929v1.pdf,all:robot arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"KITchen: A Real-World Benchmark and Dataset for 6D Object Pose
  Estimation in Kitchen Environments","Despite the recent progress on 6D object pose estimation methods for robotic
grasping, a substantial performance gap persists between the capabilities of
these methods on existing datasets and their efficacy in real-world grasping
and mobile manipulation tasks, particularly when robots rely solely on their
monocular egocentric field of view (FOV). Existing real-world datasets
primarily focus on table-top grasping scenarios, where a robot arm is placed in
a fixed position and the objects are centralized within the FOV of fixed
external camera(s). Assessing performance on such datasets may not accurately
reflect the challenges encountered in everyday grasping and mobile manipulation
tasks within kitchen environments such as retrieving objects from higher
shelves, sinks, dishwashers, ovens, refrigerators, or microwaves. To address
this gap, we present KITchen, a novel benchmark designed specifically for
estimating the 6D poses of objects located in diverse positions within kitchen
settings. For this purpose, we recorded a comprehensive dataset comprising
around 205k real-world RGBD images for 111 kitchen objects captured in two
distinct kitchens, utilizing a humanoid robot with its egocentric perspectives.
Subsequently, we developed a semi-automated annotation pipeline, to streamline
the labeling process of such datasets, resulting in the generation of 2D object
labels, 2D object segmentation masks, and 6D object poses with minimal human
effort. The benchmark, the dataset, and the annotation pipeline will be
publicly available at https://kitchen-dataset.github.io/KITchen.","Abdelrahman Younes, Tamim Asfour",2024-03-24T17:00:01Z,2024-12-17T11:08:51Z,http://arxiv.org/abs/2403.16238v3,http://arxiv.org/pdf/2403.16238v3.pdf,all:robot arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2024
"Autonomous Monitoring of Pharmaceutical R&D Laboratories with 6 Axis Arm
  Equipped Quadruped Robot and Generative AI: A Preliminary Study","This paper presents a proof-of-concept study that examines the utilization of
generative AI and mobile robotics for autonomous laboratory monitoring in the
pharmaceutical R&D laboratory. The study investigates the potential advantages
of anomaly detection and automated reporting by multi-modal model and Vision
Foundation Model (VFM), which have the potential to enhance compliance and
safety in laboratory environments. Additionally, the paper discusses the
current limitations of the generative AI approach and proposes future
directions for its application in lab monitoring.","Shunichi Hato, Nozomi Ogawa",2024-03-15T08:51:38Z,2024-03-15T08:51:38Z,http://arxiv.org/abs/2403.10108v1,http://arxiv.org/pdf/2403.10108v1.pdf,all:robot arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2024
"SpaceHopper: A Small-Scale Legged Robot for Exploring Low-Gravity
  Celestial Bodies","We present SpaceHopper, a three-legged, small-scale robot designed for future
mobile exploration of asteroids and moons. The robot weighs 5.2kg and has a
body size of 245mm while using space-qualifiable components. Furthermore,
SpaceHopper's design and controls make it well-adapted for investigating
dynamic locomotion modes with extended flight-phases. Instead of gyroscopes or
fly-wheels, the system uses its three legs to reorient the body during flight
in preparation for landing. We control the leg motion for reorientation using
Deep Reinforcement Learning policies. In a simulation of Ceres' gravity
(0.029g), the robot can reliably jump to commanded positions up to 6m away. Our
real-world experiments show that SpaceHopper can successfully reorient to a
safe landing orientation within 9.7 degree inside a rotational gimbal and jump
in a counterweight setup in Earth's gravity. Overall, we consider SpaceHopper
an important step towards controlled jumping locomotion in low-gravity
environments.","Alexander Spiridonov, Fabio Buehler, Moriz Berclaz, Valerio Schelbert, Jorit Geurts, Elena Krasnova, Emma Steinke, Jonas Toma, Joschua Wuethrich, Recep Polat, Wim Zimmermann, Philip Arm, Nikita Rudin, Hendrik Kolvenbach, Marco Hutter",2024-03-05T10:03:21Z,2024-03-05T10:03:21Z,http://arxiv.org/abs/2403.02831v1,http://arxiv.org/pdf/2403.02831v1.pdf,all:robot arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2024
"phloSAR: a Portable, High-Flow Pressure Supply and Regulator Enabling
  Untethered Operation of Large Pneumatic Soft Robots","Pneumatic actuation benefits soft robotics by facilitating compliance,
enabling large volume change, and concentrating actuator weight away from the
end-effector. However, portability is compromised when pneumatic actuators are
tethered to cumbersome air and power supplies. While there are existing options
for portable pneumatic systems, they are limited in dynamic capabilities,
constraining their applicability to low pressure and/or small-volume soft
robots. In this work, we propose a portable, high-flow pressure supply and
regulator (phloSAR) for use in untethered, weight-constrained, dynamic soft
robot applications. PhloSAR leverages high-flow proportional valves, an
integrated pressure reservoir, and Venturi vacuum generation to achieve
portability and dynamic performance. We present a set of models that describe
the system dynamics, experimentally validate them on physical hardware, and
discuss the influence of design parameters on system operation. Lastly, we
integrate a proof-of-concept prototype with a soft robot arm mounted on an
aerial vehicle to demonstrate the system's applicability to mobile robotics.
Our system enables new opportunities in mobile soft robotics by making
untethered pneumatic supply and regulation available to a wider range of soft
robots.","Maxwell Ahlquist, Rianna Jitosho, Jiawen Bao, Allison M. Okamura",2024-03-02T04:05:36Z,2024-03-02T04:05:36Z,http://arxiv.org/abs/2403.01086v1,http://arxiv.org/pdf/2403.01086v1.pdf,all:robot arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2024
"Multiple Ways of Working with Users to Develop Physically Assistive
  Robots","Despite the growth of physically assistive robotics (PAR) research over the
last decade, nearly half of PAR user studies do not involve participants with
the target disabilities. There are several reasons for this -- recruitment
challenges, small sample sizes, and transportation logistics -- all influenced
by systemic barriers that people with disabilities face. However, it is
well-established that working with end-users results in technology that better
addresses their needs and integrates with their lived circumstances. In this
paper, we reflect on multiple approaches we have taken to working with people
with motor impairments across the design, development, and evaluation of three
PAR projects: (a) assistive feeding with a robot arm; (b) assistive
teleoperation with a mobile manipulator; and (c) shared control with a robot
arm. We discuss these approaches to working with users along three dimensions
-- individual vs. community-level insight, logistic burden on end-users vs.
researchers, and benefit to researchers vs. community -- and share
recommendations for how other PAR researchers can incorporate users into their
work.","Amal Nanavati, Max Pascher, Vinitha Ranganeni, Ethan K. Gordon, Taylor Kessler Faulkner, Siddhartha S. Srinivasa, Maya Cakmak, Patrícia Alves-Oliveira, Jens Gerken",2024-03-01T12:15:54Z,2024-03-07T07:42:22Z,http://arxiv.org/abs/2403.00489v2,http://arxiv.org/pdf/2403.00489v2.pdf,all:robot arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2024
"The GREENBOT dataset: Multimodal mobile robotic dataset for a typical
  Mediterranean greenhouse","This paper introduces an innovative dataset specifically crafted for
challenging agricultural settings (a greenhouse), where achieving precise
localization is of paramount importance. The dataset was gathered using a
mobile platform equipped with a set of sensors typically used in mobile robots,
as it was moved through all the corridors of a typical Mediterranean greenhouse
featuring tomato crop. This dataset presents a unique opportunity for
constructing detailed 3D models of plants in such indoor-like space, with
potential applications such as robotized spraying. For the first time to the
best knowledge of authors, a dataset suitable to put at test Simultaneous
Localization and Mapping (SLAM) methods is presented in a greenhouse
environment, which poses unique challenges. The suitability of the dataset for
such goal is assessed by presenting SLAM results with state-of-the-art
algorithms. The dataset is available online in
\url{https://arm.ual.es/arm-group/dataset-greenhouse-2024/}.","Fernando Cañadas-Aránega, Jose Luis Blanco-Claraco, Jose Carlos Moreno, Francisco Rodriguez",2024-02-01T09:08:29Z,2024-02-01T09:08:29Z,http://arxiv.org/abs/2402.00438v1,http://arxiv.org/pdf/2402.00438v1.pdf,all:robot arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2024
"Amplifying robotics capacities with a human touch: An immersive
  low-latency panoramic remote system","AI and robotics technologies have witnessed remarkable advancements in the
past decade, revolutionizing work patterns and opportunities in various
domains. The application of these technologies has propelled society towards an
era of symbiosis between humans and machines. To facilitate efficient
communication between humans and intelligent robots, we propose the ""Avatar""
system, an immersive low-latency panoramic human-robot interaction platform. We
have designed and tested a prototype of a rugged mobile platform integrated
with edge computing units, panoramic video capture devices, power batteries,
robot arms, and network communication equipment. Under favorable network
conditions, we achieved a low-latency high-definition panoramic visual
experience with a delay of 357ms. Operators can utilize VR headsets and
controllers for real-time immersive control of robots and devices. The system
enables remote control over vast physical distances, spanning campuses,
provinces, countries, and even continents (New York to Shenzhen). Additionally,
the system incorporates visual SLAM technology for map and trajectory
recording, providing autonomous navigation capabilities. We believe that this
intuitive system platform can enhance efficiency and situational experience in
human-robot collaboration, and with further advancements in related
technologies, it will become a versatile tool for efficient and symbiotic
cooperation between AI and humans.","Junjie Li, Kang Li, Dewei Han, Jian Xu, Zhaoyuan Ma",2024-01-07T06:55:41Z,2024-01-09T04:09:56Z,http://arxiv.org/abs/2401.03398v2,http://arxiv.org/pdf/2401.03398v2.pdf,all:robot arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2024
Safe Reinforcement Learning in a Simulated Robotic Arm,"Reinforcement learning (RL) agents need to explore their environments in
order to learn optimal policies. In many environments and tasks, safety is of
critical importance. The widespread use of simulators offers a number of
advantages, including safe exploration which will be inevitable in cases when
RL systems need to be trained directly in the physical environment (e.g. in
human-robot interaction). The popular Safety Gym library offers three mobile
agent types that can learn goal-directed tasks while considering various safety
constraints. In this paper, we extend the applicability of safe RL algorithms
by creating a customized environment with Panda robotic arm where Safety Gym
algorithms can be tested. We performed pilot experiments with the popular PPO
algorithm comparing the baseline with the constrained version and show that the
constrained version is able to learn the equally good policy while better
complying with safety constraints and taking longer training time as expected.","Luka Kovač, Igor Farkaš",2023-11-28T19:22:16Z,2024-02-28T21:04:12Z,http://arxiv.org/abs/2312.09468v2,http://arxiv.org/pdf/2312.09468v2.pdf,all:robot arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2023
"GREEMA: Proposal and Experimental Verification of Growing Robot by
  Eating Environmental MAterial for Landslide Disaster","In areas that are inaccessible to humans, such as the lunar surface and
landslide sites, there is a need for multiple autonomous mobile robot systems
that can replace human workers. In particular, at landslide sites such as river
channel blockages, robots are required to remove water and sediment from the
site as soon as possible. Conventionally, several construction machines have
been deployed to the site for civil engineering work. However, because of the
large size and weight of conventional construction equipment, it is difficult
to move multiple units of construction equipment to the site, resulting in
significant transportation costs and time. To solve such problems, this study
proposes a novel growing robot by eating environmental material called GREEMA,
which is lightweight and compact during transportation, but can function by
eating on environmental materials once it arrives at the site. GREEMA actively
takes in environmental materials such as water and sediment, uses them as its
structure, and removes them by moving itself. In this paper, we developed and
experimentally verified two types of GREEMAs. First, we developed a fin-type
swimming robot that passively takes water into its body using a water-absorbing
polymer and forms a body to express its swimming function. Second, we
constructed an arm-type robot that eats soil to increase the rigidity of its
body. We discuss the results of these two experiments from the viewpoint of
Explicit-Implicit control and describe the design theory of GREEMA.","Yusuke Tsunoda, Yuya Sato, Koichi Osuka",2023-11-02T09:16:15Z,2023-11-02T09:16:15Z,http://arxiv.org/abs/2311.01107v1,http://arxiv.org/pdf/2311.01107v1.pdf,all:robot arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2023
"Development and Characteristics of a Highly Biomimetic Robotic Shoulder
  Through Bionics-Inspired Optimization","This paper critically analyzes conventional and biomimetic robotic arms,
underscoring the trade-offs between size, motion range, and load capacity in
current biomimetic models. By delving into the human shoulder's mechanical
intelligence, particularly the glenohumeral joint's intricate features such as
its unique ball-and-socket structure and self-locking mechanism, we pinpoint
innovations that bolster both stability and mobility while maintaining
compactness. To substantiate these insights, we present a groundbreaking
biomimetic robotic glenohumeral joint that authentically mirrors human
musculoskeletal elements, from ligaments to tendons, integrating the biological
joint's mechanical intelligence. Our exhaustive simulations and tests reveal
enhanced flexibility and load capacity for the robotic joint. The advanced
robotic arm demonstrates notable capabilities, including a significant range of
motions and a 4 kg payload capacity, even exerting over 1.5 Nm torque. This
study not only confirms the human shoulder joint's mechanical innovations but
also introduces a pioneering design for a next-generation biomimetic robotic
arm, setting a new benchmark in robotic technology.","Haosen Yang, Guowu Wei, Lei Ren",2023-10-27T17:20:46Z,2023-10-27T17:20:46Z,http://arxiv.org/abs/2310.18283v1,http://arxiv.org/pdf/2310.18283v1.pdf,all:robot arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2023
"Whole-body MPC for highly redundant legged manipulators: experimental
  evaluation with a 37 DoF dual-arm quadruped","Recent progress in legged locomotion has rendered quadruped manipulators a
promising solution for performing tasks that require both mobility and
manipulation (loco-manipulation). In the real world, task specifications and/or
environment constraints may require the quadruped manipulator to be equipped
with high redundancy as well as whole-body motion coordination capabilities.
This work presents an experimental evaluation of a whole-body Model Predictive
Control (MPC) framework achieving real-time performance on a dual-arm quadruped
platform consisting of 37 actuated joints. To the best of our knowledge this is
the legged manipulator with the highest number of joints to be controlled with
real-time whole-body MPC so far. The computational efficiency of the MPC while
considering the full robot kinematics and the centroidal dynamics model builds
upon an open-source DDP-variant solver and a state-of-the-art optimal control
problem formulation. Differently from previous works on quadruped manipulators,
the MPC is directly interfaced with the low-level joint impedance controllers
without the need of designing an instantaneous whole-body controller. The
feasibility on the real hardware is showcased using the CENTAURO platform for
the challenging task of picking a heavy object from the ground. Dynamic
stepping (trotting) is also showcased for first time with this robot. The
results highlight the potential of replanning with whole-body information in a
predictive control loop.","Ioannis Dadiotis, Arturo Laurenzi, Nikos Tsagarakis",2023-10-04T15:45:16Z,2023-10-25T10:26:33Z,http://arxiv.org/abs/2310.02907v2,http://arxiv.org/pdf/2310.02907v2.pdf,all:robot arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2023
Active-Perceptive Motion Generation for Mobile Manipulation,"Mobile Manipulation (MoMa) systems incorporate the benefits of mobility and
dexterity, due to the enlarged space in which they can move and interact with
their environment. However, even when equipped with onboard sensors, e.g., an
embodied camera, extracting task-relevant visual information in unstructured
and cluttered environments, such as households, remains challenging. In this
work, we introduce an active perception pipeline for mobile manipulators to
generate motions that are informative toward manipulation tasks, such as
grasping in unknown, cluttered scenes. Our proposed approach, ActPerMoMa,
generates robot paths in a receding horizon fashion by sampling paths and
computing path-wise utilities. These utilities trade-off maximizing the visual
Information Gain (IG) for scene reconstruction and the task-oriented objective,
e.g., grasp success, by maximizing grasp reachability. We show the efficacy of
our method in simulated experiments with a dual-arm TIAGo++ MoMa robot
performing mobile grasping in cluttered scenes with obstacles. We empirically
analyze the contribution of various utilities and parameters, and compare
against representative baselines both with and without active perception
objectives. Finally, we demonstrate the transfer of our mobile grasping
strategy to the real world, indicating a promising direction for
active-perceptive MoMa.","Snehal Jauhri, Sophie Lueth, Georgia Chalvatzaki",2023-09-30T16:56:52Z,2024-03-04T12:31:46Z,http://arxiv.org/abs/2310.00433v2,http://arxiv.org/pdf/2310.00433v2.pdf,all:robot arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2023
"Safe Stabilizing Control for Polygonal Robots in Dynamic Elliptical
  Environments","This paper addresses the challenge of safe navigation for rigid-body mobile
robots in dynamic environments. We introduce an analytic approach to compute
the distance between a polygon and an ellipse, and employ it to construct a
control barrier function (CBF) for safe control synthesis. Existing CBF design
methods for mobile robot obstacle avoidance usually assume point or circular
robots, preventing their applicability to more realistic robot body geometries.
Our work enables CBF designs that capture complex robot and obstacle shapes. We
demonstrate the effectiveness of our approach in simulations highlighting
real-time obstacle avoidance in constrained and dynamic environments for both
mobile robots and multi-joint robot arms.","Kehan Long, Khoa Tran, Melvin Leok, Nikolay Atanasov",2023-09-30T06:26:12Z,2024-04-30T05:16:07Z,http://arxiv.org/abs/2310.00273v2,http://arxiv.org/pdf/2310.00273v2.pdf,all:robot arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2023
"AnyOKP: One-Shot and Instance-Aware Object Keypoint Extraction with
  Pretrained ViT","Towards flexible object-centric visual perception, we propose a one-shot
instance-aware object keypoint (OKP) extraction approach, AnyOKP, which
leverages the powerful representation ability of pretrained vision transformer
(ViT), and can obtain keypoints on multiple object instances of arbitrary
category after learning from a support image. An off-the-shelf petrained ViT is
directly deployed for generalizable and transferable feature extraction, which
is followed by training-free feature enhancement. The best-prototype pairs
(BPPs) are searched for in support and query images based on appearance
similarity, to yield instance-unaware candidate keypoints.Then, the entire
graph with all candidate keypoints as vertices are divided to sub-graphs
according to the feature distributions on the graph edges. Finally, each
sub-graph represents an object instance. AnyOKP is evaluated on real object
images collected with the cameras of a robot arm, a mobile robot, and a
surgical robot, which not only demonstrates the cross-category flexibility and
instance awareness, but also show remarkable robustness to domain shift and
viewpoint change.","Fangbo Qin, Taogang Hou, Shan Lin, Kaiyuan Wang, Michael C. Yip, Shan Yu",2023-09-15T04:05:01Z,2023-09-15T04:05:01Z,http://arxiv.org/abs/2309.08134v1,http://arxiv.org/pdf/2309.08134v1.pdf,all:robot arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2023
"Autonomous Air-Ground Vehicle Operations Optimization in Hazardous
  Environments: A Multi-Armed Bandit Approach","Hazardous environments such as chemical spills, radiological zones, and
bio-contaminated sites pose significant threats to human safety and public
infrastructure. Rapid and reliable hazard mitigation in these settings often
unsafe for humans, calling for autonomous systems that can adaptively sense and
respond to evolving risks. This paper presents a decision-making framework for
autonomous vehicle dispatch in hazardous environments with uncertain and
evolving risk levels. The system integrates a Bayesian Upper Confidence Bound
(BUCB) sensing strategy with task-specific vehicle routing problems with
profits (VRPP), enabling adaptive coordination of unmanned aerial vehicles
(UAVs) for hazard sensing and unmanned ground vehicles (UGVs) for cleaning.
Using VRPP allows selective site visits under resource constraints by assigning
each site a visit value that reflects sensing or cleaning priorities.
Site-level hazard beliefs are maintained through a time-weighted Bayesian
update. BUCB scores guide UAV routing to balance exploration and exploitation
under uncertainty, while UGV routes are optimized to maximize expected hazard
reduction under resource constraints. Simulation results demonstrate that our
framework reduces the number of dispatch cycles to resolve hazards by around
30% on average compared to baseline dispatch strategies, underscoring the value
of uncertainty-aware vehicle dispatch for reliable hazard mitigation.","Jimin Choi, Max Z. Li",2025-08-11T17:37:36Z,2025-08-11T17:37:36Z,http://arxiv.org/abs/2508.08217v1,http://arxiv.org/pdf/2508.08217v1.pdf,all:robot arm AND all:UGV AND submittedDate:[202309062236 TO 202509052236],2025
Star Formation and Magnetic Field Amplification due to Galactic Spirals,"We use global MHD galaxy simulations to investigate the effects of spiral
arms on the evolution of magnetic fields and star formation within a
self-regulated interstellar medium (ISM). The same galaxy is simulated twice:
once with self-consistent stellar spiral arms and once more with the stellar
spirals suppressed via a novel numerical approach, using the Ramses AMR code.
Spiral arms continually promote star formation, with 2.6 times higher rates in
the spiral galaxy. The higher rate is due to high gas columns gathered along
the spiral arms, rather than increasing the star formation efficiency at a
given gas column. In both cases, the magnetic field is initially amplified via
a small-scale dynamo driven by turbulence due to supernova feedback. Only the
spiral galaxy exhibits late-time, consistent field growth due to a large-scale
dynamo (e-folding time $\sim600$ Myr). This results in volume-averaged field
strengths of $\sim 1$ $\mu$G after 1 Gyr of evolution. The mean-fields tend to
align themselves with the spiral arms and are coherent up to 10 kpc scales. We
demonstrate a novel large-scale dynamo mechanism, whereby spiral-driven radial
flows enable the mean-field amplification.","Hector Robinson, James Wadsley, J. A. Sellwood, Ralph E. Pudritz",2025-06-19T18:00:08Z,2025-06-19T18:00:08Z,http://arxiv.org/abs/2506.16515v1,http://arxiv.org/pdf/2506.16515v1.pdf,all:robot arm AND all:AMR AND submittedDate:[202309062236 TO 202509052236],2025
"Asynchronous-Many-Task Systems: Challenges and Opportunities -- Scaling
  an AMR Astrophysics Code on Exascale machines using Kokkos and HPX","Dynamic and adaptive mesh refinement is pivotal in high-resolution,
multi-physics, multi-model simulations, necessitating precise physics
resolution in localized areas across expansive domains. Today's supercomputers'
extreme heterogeneity presents a significant challenge for dynamically adaptive
codes, highlighting the importance of achieving performance portability at
scale. Our research focuses on astrophysical simulations, particularly stellar
mergers, to elucidate early universe dynamics. We present Octo-Tiger,
leveraging Kokkos, HPX, and SIMD for portable performance at scale in complex,
massively parallel adaptive multi-physics simulations. Octo-Tiger supports
diverse processors, accelerators, and network backends. Experiments demonstrate
exceptional scalability across several heterogeneous supercomputers including
Perlmutter, Frontier, and Fugaku, encompassing major GPU architectures and x86,
ARM, and RISC-V CPUs. Parallel efficiency of 47.59% (110,080 cores and 6880
hybrid A100 GPUs) on a full-system run on Perlmutter (26% HPCG peak
performance) and 51.37% (using 32,768 cores and 2,048 MI250X) on Frontier are
achieved.","Gregor Daiß, Patrick Diehl, Jiakun Yan, John K. Holmen, Rahulkumar Gayatri, Christoph Junghans, Alexander Straub, Jeff R. Hammond, Dominic Marcello, Miwako Tsuji, Dirk Pflüger, Hartmut Kaiser",2024-12-20T03:11:25Z,2024-12-20T03:11:25Z,http://arxiv.org/abs/2412.15518v1,http://arxiv.org/pdf/2412.15518v1.pdf,all:robot arm AND all:AMR AND submittedDate:[202309062236 TO 202509052236],2024
AthenaK: A Performance-Portable Version of the Athena++ AMR Framework,"We describe AthenaK: a new implementation of the Athena++ block-based
adaptive mesh refinement (AMR) framework using the Kokkos programming model.
Finite volume methods for Newtonian, special relativistic (SR), and general
relativistic (GR) hydrodynamics and magnetohydrodynamics (MHD), and
GR-radiation hydrodynamics and MHD, as well as a module for evolving Lagrangian
tracer or charged test particles (e.g., cosmic rays) are implemented using the
framework. In two companion papers we describe (1) a new solver for the
Einstein equations based on the Z4c formalism and (2) a GRMHD solver in
dynamical spacetimes also implemented using the framework, enabling new
applications in numerical relativity. By adopting Kokkos, the code can be run
on virtually any hardware, including CPUs, GPUs from multiple vendors, and
emerging ARM processors. AthenaK shows excellent performance and weak scaling,
achieving over one billion cell updates per second for hydrodynamics in
three-dimensions on a single NVIDIA Grace Hopper processor and with a typical
parallel efficiency of 80% on 65536 AMD GPUs on the OLCF Frontier system. Such
performance portability enables AthenaK to leverage modern exascale computing
systems for challenging applications in astrophysical fluid dynamics, numerical
relativity, and multimessenger astrophysics.","James M. Stone, Patrick D. Mullen, Drummond Fielding, Philipp Grete, Minghao Guo, Philipp Kempski, Elias R. Most, Christopher J. White, George N. Wong",2024-09-24T12:57:19Z,2024-09-24T12:57:19Z,http://arxiv.org/abs/2409.16053v1,http://arxiv.org/pdf/2409.16053v1.pdf,all:robot arm AND all:AMR AND submittedDate:[202309062236 TO 202509052236],2024
"Angular Position Sensor Based on Anisotropic Magnetoresistive and
  Anomalous Nernst Effect","Magnetic position sensors find extensive applications in various industrial
sectors and consumer products. However, measuring angles in the full range of
0{\deg} to 360{\deg} in a wide field range using a single magnetic sensor
remains a challenge. Here, we propose a magnetic position sensor based on a
single Wheatstone bridge structure made from a single ferromagnetic layer. By
measuring the anisotropic magnetoresistance (AMR) signal from the bridge and
two sets of anomalous Nernst effect (ANE) signals from the transverse ports on
two perpendicular Wheatstone bridge arms concurrently, we show that it is
possible to achieve 0{\deg} to 360{\deg} angle detection using a single bridge
sensor. The combined use of AMR and ANE signals allows to achieve a mean angle
error in the range of 0.51{\deg} to 1.05{\deg} within a field range of 100 Oe
to 10,000 Oe.","Jiaqi Wang, Hang Xie, Yihong Wu",2024-01-30T04:26:53Z,2024-01-30T04:26:53Z,http://arxiv.org/abs/2401.16735v1,http://arxiv.org/pdf/2401.16735v1.pdf,all:robot arm AND all:AMR AND submittedDate:[202309062236 TO 202509052236],2024
"TritonZ: A Remotely Operated Underwater Rover with Manipulator Arm for
  Exploration and Rescue Operations","The increasing demand for underwater exploration and rescue operations
enforces the development of advanced wireless or semi-wireless underwater
vessels equipped with manipulator arms. This paper presents the implementation
of a semi-wireless underwater vehicle, ""TritonZ"" equipped with a manipulator
arm, tailored for effective underwater exploration and rescue operations. The
vehicle's compact design enables deployment in different submarine
surroundings, addressing the need for wireless systems capable of navigating
challenging underwater terrains. The manipulator arm can interact with the
environment, allowing the robot to perform sophisticated tasks during
exploration and rescue missions in emergency situations. TritonZ is equipped
with various sensors such as Pi-Camera, Humidity, and Temperature sensors to
send real-time environmental data. Our underwater vehicle controlled using a
customized remote controller can navigate efficiently in the water where
Pi-Camera enables live streaming of the surroundings. Motion control and video
capture are performed simultaneously using this camera. The manipulator arm is
designed to perform various tasks, similar to grasping, manipulating, and
collecting underwater objects. Experimental results shows the efficacy of the
proposed remotely operated vehicle in performing a variety of underwater
exploration and rescue tasks. Additionally, the results show that TritonZ can
maintain an average of 13.5cm/s with a minimal delay of 2-3 seconds.
Furthermore, the vehicle can sustain waves underwater by maintaining its
position as well as average velocity. The full project details and source code
can be accessed at this link: https://github.com/kawser-ahmed-byte/TritonZ","Kawser Ahmed, Mir Shahriar Fardin, Md Arif Faysal Nayem, Fahim Hafiz, Swakkhar Shatabda",2025-06-23T06:52:38Z,2025-06-27T06:11:26Z,http://arxiv.org/abs/2506.18343v2,http://arxiv.org/pdf/2506.18343v2.pdf,all:robot arm AND all:rover AND submittedDate:[202309062236 TO 202509052236],2025
"Practice Makes Perfect: A Study of Digital Twin Technology for Assembly
  and Problem-solving using Lunar Surface Telerobotics","Robotic systems that can traverse planetary or lunar surfaces to collect
environmental data and perform physical manipulation tasks, such as assembling
equipment or conducting mining operations, are envisioned to form the backbone
of future human activities in space. However, the environmental conditions in
which these robots, or ""rovers,"" operate present challenges toward achieving
fully autonomous solutions, meaning that rover missions will require some
degree of human teleoperation or supervision for the foreseeable future. As a
result, human operators require training to successfully direct rovers and
avoid costly errors or mission failures, as well as the ability to recover from
any issues that arise on the fly during mission activities. While analog
environments, such as JPL's Mars Yard, can help with such training by
simulating surface environments in the real world, access to such resources may
be rare and expensive. As an alternative or supplement to such physical
analogs, we explore the design and evaluation of a virtual reality digital twin
system to train human teleoperation of robotic rovers with mechanical arms for
space mission activities. We conducted an experiment with 24 human operators to
investigate how our digital twin system can support human teleoperation of
rovers in both pre-mission training and in real-time problem solving in a mock
lunar mission in which users directed a physical rover in the context of
deploying dipole radio antennas. We found that operators who first trained with
the digital twin showed a 28% decrease in mission completion time, an 85%
decrease in unrecoverable errors, as well as improved mental markers, including
decreased cognitive load and increased situation awareness.","Xavier O'Keefe, Katy McCutchan, Alexis Muniz, Jack Burns, Daniel Szafir",2025-05-19T20:40:52Z,2025-05-19T20:40:52Z,http://arxiv.org/abs/2505.13722v1,http://arxiv.org/pdf/2505.13722v1.pdf,all:robot arm AND all:rover AND submittedDate:[202309062236 TO 202509052236],2025
"Scalable and low-cost remote lab platforms: Teaching industrial robotics
  using open-source tools and understanding its social implications","With recent advancements in industrial robots, educating students in new
technologies and preparing them for the future is imperative. However, access
to industrial robots for teaching poses challenges, such as the high cost of
acquiring these robots, the safety of the operator and the robot, and
complicated training material. This paper proposes two low-cost platforms built
using open-source tools like Robot Operating System (ROS) and its latest
version ROS 2 to help students learn and test algorithms on remotely connected
industrial robots. Universal Robotics (UR5) arm and a custom mobile rover were
deployed in different life-size testbeds, a greenhouse, and a warehouse to
create an Autonomous Agricultural Harvester System (AAHS) and an Autonomous
Warehouse Management System (AWMS). These platforms were deployed for a period
of 7 months and were tested for their efficacy with 1,433 and 1,312 students,
respectively. The hardware used in AAHS and AWMS was controlled remotely for
160 and 355 hours, respectively, by students over a period of 3 months.","Amit Kumar, Jaison Jose, Archit Jain, Siddharth Kulkarni, Kavi Arya",2024-12-19T20:03:13Z,2024-12-19T20:03:13Z,http://arxiv.org/abs/2412.15369v1,http://arxiv.org/pdf/2412.15369v1.pdf,all:robot arm AND all:rover AND submittedDate:[202309062236 TO 202509052236],2024
Analytical Solution for Inverse Kinematics,"This paper introduces a closed-form analytical solution for the inverse
kinematics (IK) of a 6 Degrees of Freedom (DOF) serial robotic manipulator arm,
configured with six revolute joints and utilized within the Lunar Exploration
Rover System (LERS). As a critical asset for conducting precise operations in
the demanding lunar environment, this robotic arm relies on the IK solution to
determine joint parameters required for precise end-effector positioning,
essential for tasks such as sample collection, infrastructure assembly, and
equipment deployment. By applying geometric principles, the proposed method
offers a highly efficient and accurate approach to solving the IK problem,
significantly reducing computational demands compared to traditional numerical
methods. This advancement not only enhances real-time operational capabilities
but is also optimized for space robotics, where precision and speed are
critical. Additionally, the paper explores the integration of the LERS robotic
system, underscoring the importance of this work in supporting autonomous lunar
exploration within the ARTEMIS program and future missions","Serdar Kalaycioglu, Anton de Ruiter, Ethan Fung, Harrison Zhang, Haipeng Xie",2024-10-29T22:53:11Z,2024-10-29T22:53:11Z,http://arxiv.org/abs/2410.22582v1,http://arxiv.org/pdf/2410.22582v1.pdf,all:robot arm AND all:rover AND submittedDate:[202309062236 TO 202509052236],2024
Adaptive sampling with PIXL on the Mars Perseverance rover,"Planetary rovers can use onboard data analysis to adapt their measurement
plan on the fly, improving the science value of data collected between commands
from Earth. This paper describes the implementation of an adaptive sampling
algorithm used by PIXL, the X-ray fluorescence spectrometer of the Mars 2020
Perseverance rover. PIXL is deployed using the rover arm to measure X-ray
spectra of rocks with a scan density of several thousand points over an area of
typically 5 x 7 mm. The adaptive sampling algorithm is programmed to recognize
points of interest and to increase the signal-to-noise ratio at those locations
by performing longer integrations. Two approaches are used to formulate the
sampling rules based on past quantification data: 1) Expressions that isolate
particular regions within a ternary compositional diagram, and 2) Machine
learning rules that threshold for a high weight percent of particular
compounds. The design of the rulesets are outlined and the performance of the
algorithm is quantified using measurements from the surface of Mars. To our
knowledge, PIXL's adaptive sampling represents the first autonomous
decision-making based on real-time compositional analysis by a spacecraft on
the surface of another planet.","Peter R. Lawson, Tanya V. Kizovski, Michael M. Tice, Benton C. Clark III, Scott J. VanBommel, David R. Thompson, Lawrence A. Wade, Robert W. Denise, Christopher M. Heirwegh, W. Timothy Elam, Mariek E. Schmidt, Yang Liu, Abigail C. Allwood, Martin S. Gilbert, Benjamin J. Bornstein",2024-05-23T11:57:02Z,2024-05-23T11:57:02Z,http://arxiv.org/abs/2405.14471v1,http://arxiv.org/pdf/2405.14471v1.pdf,all:robot arm AND all:rover AND submittedDate:[202309062236 TO 202509052236],2024
"Pre-Flight Calibration of PIXL for X-ray Fluorescence Elemental
  Quantification","The Planetary Instrument for X-ray Lithochemistry (PIXL) is a rasterable
focused-beam X-ray fluorescence (XRF) spectrometer mounted on the arm of
National Aeronautics and Space Administration's (NASA) Mars 2020 Perseverance
rover. To ensure that PIXL would be capable of performing accurate in-flight
compositional analysis of martian targets, in situ, an elemental calibration
was performed pre-flight on the PIXL flight instrument in a simulated martian
environment. The details of this calibration, and implications for measuring
unknown materials on Mars are the subjects of this paper. The major goals of
this calibration were both to align the spectrometer to perform accurate
elemental analysis and, to derive a matrix of uncertainties that are applied to
XRF measurements of all elements in unknown materials. A small set of pure
element and pure compound targets and geologically relevant reference materials
were measured with the flight hardware in a simulated martian environment.
Elemental calibration and quantifications were carried out using PIXL's XRF
quantification software (PIQUANT). Uncertainties generated were implemented
into the PIQUANT software version employed by the PIXL's data visualization
software (PIXLISE). We outline in this work, a list of factors that impact
micro-XRF accuracy, the methodology and steps involved in the calibration,
details on the fabrication of the uncertainty matrix, instructions on the use
and interpretations of the uncertainties applied to unknowns and an assessment
on the limitations and areas open to future improvement as part of subsequent
calibration efforts.","Christopher M. Heirwegh, William Timothy Elam, Yang Liu, Anusheela Das, Christopher Hummel, Bret Naylor, Lawrence A. Wade, Abigail C. Allwood, Joel A. Hurowitz, Les G. Armstrong, Naomi Bacop, Lauren P. O'Neil, Kimberly P. Sinclair, Michael E. Sondheim, Robert W. Denise, Peter R. Lawson, Rogelio Rosas, Jonathan H. Kawamura, Mitchell H. Au, Amarit Kitiyakara, Marc C. Foote, Raul A. Romero, Mark S. Anderson, George R. Rossman, Benton C. Clark III",2024-02-02T16:38:34Z,2024-02-02T16:38:34Z,http://arxiv.org/abs/2402.01544v1,http://arxiv.org/pdf/2402.01544v1.pdf,all:robot arm AND all:rover AND submittedDate:[202309062236 TO 202509052236],2024
"3GPP-Compliant Radar Cross Section Characterization of Indoor Factory
  Targets","The following paper presents a systematic 3rd Generation Partnership Project
(3GPP)-compliant characterization of radar cross section (RCS) for indoor
factory (InF) objects, including small and mid-sized unmanned aerial vehicles
(UAVs), robotic arms, and automated guided vehicles (AGVs). Through
measurements in the 25-28 GHz range, we validate the 3GPP standardized
log-normal distribution model for RCS for above-mentioned target objects. The
3GPP-complaint RCS parameters obtained for the small-sized UAV are in close
agreement (<1 dB deviation) with 3GPP agreed values. The mid-sized UAVs exhibit
higher reflectivity compared to the small-sized UAV due to enhanced specular
components attributed to material and lithium-ion battery packs. The robotic
arm exhibits dynamic RCS behavior due to mechanical articulation, whereas UAVs
show clear size-dependent reflectivity patterns in AGVs. Our findings provide
empirical validation for RCS characterization for integrated sensing and
communication channel modeling in InF environments.","Ali Waqar Azim, Ahmad Bazzi, Roberto Bomfin, Marwa Chafii",2025-05-13T17:15:10Z,2025-05-13T17:15:10Z,http://arxiv.org/abs/2505.08754v1,http://arxiv.org/pdf/2505.08754v1.pdf,all:robot arm AND all:AGV AND submittedDate:[202309062236 TO 202509052236],2025
Object-Reconstruction-Aware Whole-body Control of Mobile Manipulators,"Object reconstruction and inspection tasks play a crucial role in various
robotics applications. Identifying paths that reveal the most unknown areas of
the object becomes paramount in this context, as it directly affects
efficiency, and this problem is known as the view path planning problem.
Current methods often use sampling-based path planning techniques, evaluating
potential views along the path to enhance reconstruction performance. However,
these methods are computationally expensive as they require evaluating several
candidate views on the path. To this end, we propose a computationally
efficient solution that relies on calculating a focus point in the most
informative (unknown) region and having the robot maintain this point in the
camera field of view along the path. We incorporated this strategy into the
whole-body control of a mobile manipulator employing a visibility constraint
without the need for an additional path planner. We conducted comprehensive and
realistic simulations using a large dataset of 114 diverse objects of varying
sizes from 57 categories to compare our method with a sampling-based planning
strategy using Bayesian data analysis. Furthermore, we performed real-world
experiments with an 8-DoF mobile manipulator to demonstrate the proposed
method's performance in practice. Our results suggest that there is no
significant difference in object coverage and entropy. In contrast, our method
is approximately nine times faster than the baseline sampling-based method in
terms of the average time the robot spends between views.","Fatih Dursun, Bruno Vilhena Adorno, Simon Watson, Wei Pan",2025-09-04T10:52:27Z,2025-09-04T10:52:27Z,http://arxiv.org/abs/2509.04094v1,http://arxiv.org/pdf/2509.04094v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
Learning Multi-Stage Pick-and-Place with a Legged Mobile Manipulator,"Quadruped-based mobile manipulation presents significant challenges in
robotics due to the diversity of required skills, the extended task horizon,
and partial observability. After presenting a multi-stage pick-and-place task
as a succinct yet sufficiently rich setup that captures key desiderata for
quadruped-based mobile manipulation, we propose an approach that can train a
visuo-motor policy entirely in simulation, and achieve nearly 80\% success in
the real world. The policy efficiently performs search, approach, grasp,
transport, and drop into actions, with emerged behaviors such as re-grasping
and task chaining. We conduct an extensive set of real-world experiments with
ablation studies highlighting key techniques for efficient training and
effective sim-to-real transfer. Additional experiments demonstrate deployment
across a variety of indoor and outdoor environments. Demo videos and additional
resources are available on the project page:
https://horizonrobotics.github.io/gail/SLIM.","Haichao Zhang, Haonan Yu, Le Zhao, Andrew Choi, Qinxun Bai, Yiqing Yang, Wei Xu",2025-09-04T03:36:07Z,2025-09-04T03:36:07Z,http://arxiv.org/abs/2509.03859v1,http://arxiv.org/pdf/2509.03859v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
Articulated Object Estimation in the Wild,"Understanding the 3D motion of articulated objects is essential in robotic
scene understanding, mobile manipulation, and motion planning. Prior methods
for articulation estimation have primarily focused on controlled settings,
assuming either fixed camera viewpoints or direct observations of various
object states, which tend to fail in more realistic unconstrained environments.
In contrast, humans effortlessly infer articulation by watching others
manipulate objects. Inspired by this, we introduce ArtiPoint, a novel
estimation framework that can infer articulated object models under dynamic
camera motion and partial observability. By combining deep point tracking with
a factor graph optimization framework, ArtiPoint robustly estimates articulated
part trajectories and articulation axes directly from raw RGB-D videos. To
foster future research in this domain, we introduce Arti4D, the first
ego-centric in-the-wild dataset that captures articulated object interactions
at a scene level, accompanied by articulation labels and ground-truth camera
poses. We benchmark ArtiPoint against a range of classical and learning-based
baselines, demonstrating its superior performance on Arti4D. We make code and
Arti4D publicly available at https://artipoint.cs.uni-freiburg.de.","Abdelrhman Werby, Martin Büchner, Adrian Röfer, Chenguang Huang, Wolfram Burgard, Abhinav Valada",2025-09-01T18:34:17Z,2025-09-01T18:34:17Z,http://arxiv.org/abs/2509.01708v1,http://arxiv.org/pdf/2509.01708v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"A Framework for Task and Motion Planning based on Expanding AND/OR
  Graphs","Robot autonomy in space environments presents unique challenges, including
high perception and motion uncertainty, strict kinematic constraints, and
limited opportunities for human intervention. Therefore, Task and Motion
Planning (TMP) may be critical for autonomous servicing, surface operations, or
even in-orbit missions, just to name a few, as it models tasks as discrete
action sequencing integrated with continuous motion feasibility assessments. In
this paper, we introduce a TMP framework based on expanding AND/OR graphs,
referred to as TMP-EAOG, and demonstrate its adaptability to different
scenarios. TMP-EAOG encodes task-level abstractions within an AND/OR graph,
which expands iteratively as the plan is executed, and performs in-the-loop
motion planning assessments to ascertain their feasibility. As a consequence,
TMP-EAOG is characterised by the desirable properties of (i) robustness to a
certain degree of uncertainty, because AND/OR graph expansion can accommodate
for unpredictable information about the robot environment, (ii) controlled
autonomy, since an AND/OR graph can be validated by human experts, and (iii)
bounded flexibility, in that unexpected events, including the assessment of
unfeasible motions, can lead to different courses of action as alternative
paths in the AND/OR graph. We evaluate TMP-EAOG on two benchmark domains. We
use a simulated mobile manipulator as a proxy for space-grade autonomous
robots. Our evaluation shows that TMP-EAOG can deal with a wide range of
challenges in the benchmarks.","Fulvio Mastrogiovanni, Antony Thomas",2025-08-30T02:28:25Z,2025-08-30T02:28:25Z,http://arxiv.org/abs/2509.00317v1,http://arxiv.org/pdf/2509.00317v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Language-Enhanced Mobile Manipulation for Efficient Object Search in
  Indoor Environments","Enabling robots to efficiently search for and identify objects in complex,
unstructured environments is critical for diverse applications ranging from
household assistance to industrial automation. However, traditional scene
representations typically capture only static semantics and lack interpretable
contextual reasoning, limiting their ability to guide object search in
completely unfamiliar settings. To address this challenge, we propose a
language-enhanced hierarchical navigation framework that tightly integrates
semantic perception and spatial reasoning. Our method, Goal-Oriented
Dynamically Heuristic-Guided Hierarchical Search (GODHS), leverages large
language models (LLMs) to infer scene semantics and guide the search process
through a multi-level decision hierarchy. Reliability in reasoning is achieved
through the use of structured prompts and logical constraints applied at each
stage of the hierarchy. For the specific challenges of mobile manipulation, we
introduce a heuristic-based motion planner that combines polar angle sorting
with distance prioritization to efficiently generate exploration paths.
Comprehensive evaluations in Isaac Sim demonstrate the feasibility of our
framework, showing that GODHS can locate target objects with higher search
efficiency compared to conventional, non-semantic search strategies. Website
and Video are available at: https://drapandiger.github.io/GODHS","Liding Zhang, Zeqi Li, Kuanqi Cai, Qian Huang, Zhenshan Bing, Alois Knoll",2025-08-28T15:27:35Z,2025-08-28T15:27:35Z,http://arxiv.org/abs/2508.20899v1,http://arxiv.org/pdf/2508.20899v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Genetic Informed Trees (GIT*): Path Planning via Reinforced Genetic
  Programming Heuristics","Optimal path planning involves finding a feasible state sequence between a
start and a goal that optimizes an objective. This process relies on heuristic
functions to guide the search direction. While a robust function can improve
search efficiency and solution quality, current methods often overlook
available environmental data and simplify the function structure due to the
complexity of information relationships. This study introduces Genetic Informed
Trees (GIT*), which improves upon Effort Informed Trees (EIT*) by integrating a
wider array of environmental data, such as repulsive forces from obstacles and
the dynamic importance of vertices, to refine heuristic functions for better
guidance. Furthermore, we integrated reinforced genetic programming (RGP),
which combines genetic programming with reward system feedback to mutate
genotype-generative heuristic functions for GIT*. RGP leverages a multitude of
data types, thereby improving computational efficiency and solution quality
within a set timeframe. Comparative analyses demonstrate that GIT* surpasses
existing single-query, sampling-based planners in problems ranging from R^4 to
R^16 and was tested on a real-world mobile manipulation task. A video
showcasing our experimental results is available at
https://youtu.be/URjXbc_BiYg","Liding Zhang, Kuanqi Cai, Zhenshan Bing, Chaoqun Wang, Alois Knoll",2025-08-28T15:02:02Z,2025-08-28T15:02:02Z,http://arxiv.org/abs/2508.20871v1,http://arxiv.org/pdf/2508.20871v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data
  for Mobile Dexterous Manipulation","Leveraging human motion data to impart robots with versatile manipulation
skills has emerged as a promising paradigm in robotic manipulation.
Nevertheless, translating multi-source human hand motions into feasible robot
behaviors remains challenging, particularly for robots equipped with
multi-fingered dexterous hands characterized by complex, high-dimensional
action spaces. Moreover, existing approaches often struggle to produce policies
capable of adapting to diverse environmental conditions. In this paper, we
introduce HERMES, a human-to-robot learning framework for mobile bimanual
dexterous manipulation. First, HERMES formulates a unified reinforcement
learning approach capable of seamlessly transforming heterogeneous human hand
motions from multiple sources into physically plausible robotic behaviors.
Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depth
image-based sim2real transfer method for improved generalization to real-world
scenarios. Furthermore, to enable autonomous operation in varied and
unstructured environments, we augment the navigation foundation model with a
closed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise
alignment of visual goals and effectively bridging autonomous navigation and
dexterous manipulation. Extensive experimental results demonstrate that HERMES
consistently exhibits generalizable behaviors across diverse, in-the-wild
scenarios, successfully performing numerous complex mobile bimanual dexterous
manipulation tasks. Project Page:https://gemcollector.github.io/HERMES/.","Zhecheng Yuan, Tianming Wei, Langzhe Gu, Pu Hua, Tianhai Liang, Yuanpei Chen, Huazhe Xu",2025-08-27T17:53:46Z,2025-08-31T06:47:56Z,http://arxiv.org/abs/2508.20085v3,http://arxiv.org/pdf/2508.20085v3.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Elliptical K-Nearest Neighbors -- Path Optimization via Coulomb's Law
  and Invalid Vertices in C-space Obstacles","Path planning has long been an important and active research area in
robotics. To address challenges in high-dimensional motion planning, this study
introduces the Force Direction Informed Trees (FDIT*), a sampling-based planner
designed to enhance speed and cost-effectiveness in pathfinding. FDIT* builds
upon the state-of-the-art informed sampling planner, the Effort Informed Trees
(EIT*), by capitalizing on often-overlooked information in invalid vertices. It
incorporates principles of physical force, particularly Coulomb's law. This
approach proposes the elliptical $k$-nearest neighbors search method, enabling
fast convergence navigation and avoiding high solution cost or infeasible paths
by exploring more problem-specific search-worthy areas. It demonstrates
benefits in search efficiency and cost reduction, particularly in confined,
high-dimensional environments. It can be viewed as an extension of nearest
neighbors search techniques. Fusing invalid vertex data with physical dynamics
facilitates force-direction-based search regions, resulting in an improved
convergence rate to the optimum. FDIT* outperforms existing single-query,
sampling-based planners on the tested problems in R^4 to R^16 and has been
demonstrated on a real-world mobile manipulation task.","Liding Zhang, Zhenshan Bing, Yu Zhang, Kuanqi Cai, Lingyun Chen, Fan Wu, Sami Haddadin, Alois Knoll",2025-08-27T10:57:50Z,2025-08-27T10:57:50Z,http://arxiv.org/abs/2508.19771v1,http://arxiv.org/pdf/2508.19771v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and
  Manipulation","Event cameras offer microsecond latency, high dynamic range, and low power
consumption, making them ideal for real-time robotic perception under
challenging conditions such as motion blur, occlusion, and illumination
changes. However, despite their advantages, synthetic event-based vision
remains largely unexplored in mainstream robotics simulators. This lack of
simulation setup hinders the evaluation of event-driven approaches for robotic
manipulation and navigation tasks. This work presents an open-source,
user-friendly v2e robotics operating system (ROS) package for Gazebo simulation
that enables seamless event stream generation from RGB camera feeds. The
package is used to investigate event-based robotic policies (ERP) for real-time
navigation and manipulation. Two representative scenarios are evaluated: (1)
object following with a mobile robot and (2) object detection and grasping with
a robotic manipulator. Transformer-based ERPs are trained by behavior cloning
and compared to RGB-based counterparts under various operating conditions.
Experimental results show that event-guided policies consistently deliver
competitive advantages. The results highlight the potential of event-driven
perception to improve real-time robotic navigation and manipulation, providing
a foundation for broader integration of event cameras into robotic policy
learning. The GitHub repo for the dataset and code:
https://eventbasedvision.github.io/SEBVS/","Krishna Vinod, Prithvi Jai Ramesh, Pavan Kumar B N, Bharatesh Chakravarthi",2025-08-25T04:14:04Z,2025-08-25T04:14:04Z,http://arxiv.org/abs/2508.17643v1,http://arxiv.org/pdf/2508.17643v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
M3DMap: Object-aware Multimodal 3D Mapping for Dynamic Environments,"3D mapping in dynamic environments poses a challenge for modern researchers
in robotics and autonomous transportation. There are no universal
representations for dynamic 3D scenes that incorporate multimodal data such as
images, point clouds, and text. This article takes a step toward solving this
problem. It proposes a taxonomy of methods for constructing multimodal 3D maps,
classifying contemporary approaches based on scene types and representations,
learning methods, and practical applications. Using this taxonomy, a brief
structured analysis of recent methods is provided. The article also describes
an original modular method called M3DMap, designed for object-aware
construction of multimodal 3D maps for both static and dynamic scenes. It
consists of several interconnected components: a neural multimodal object
segmentation and tracking module; an odometry estimation module, including
trainable algorithms; a module for 3D map construction and updating with
various implementations depending on the desired scene representation; and a
multimodal data retrieval module. The article highlights original
implementations of these modules and their advantages in solving various
practical tasks, from 3D object grounding to mobile manipulation. Additionally,
it presents theoretical propositions demonstrating the positive effect of using
multimodal data and modern foundational models in 3D mapping methods. Details
of the taxonomy and method implementation are available at
https://yuddim.github.io/M3DMap.",Dmitry Yudin,2025-08-23T14:45:48Z,2025-08-23T14:45:48Z,http://arxiv.org/abs/2508.17044v1,http://arxiv.org/pdf/2508.17044v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"SafeSpace: An Integrated Web Application for Digital Safety and
  Emotional Well-being","In the digital era, individuals are increasingly exposed to online harms such
as toxicity, manipulation, and grooming, which often pose emotional and safety
risks. Existing systems for detecting abusive content or issuing safety alerts
operate in isolation and rarely combine digital safety with emotional
well-being. In this paper, we present SafeSpace, a unified web application that
integrates three modules: (1) toxicity detection in chats and screenshots using
NLP models and Google's Perspective API, (2) a configurable safety ping system
that issues emergency alerts with the user's live location (longitude and
latitude) via SMTP-based emails when check-ins are missed or SOS alerts are
manually triggered, and (3) a reflective questionnaire that evaluates
relationship health and emotional resilience. The system employs Firebase for
alert management and a modular architecture designed for usability, privacy,
and scalability. The experimental evaluation shows 93% precision in toxicity
detection, 100% reliability in safety alerts under emulator tests, and 92%
alignment between automated and manual questionnaire scoring. SafeSpace,
implemented as a web application, demonstrates the feasibility of integrating
detection, protection, and reflection within a single platform, with future
deployment envisioned as a mobile application for broader accessibility.","Kayenat Fatmi, Mohammad Abbas",2025-08-22T16:07:29Z,2025-08-22T16:07:29Z,http://arxiv.org/abs/2508.16488v1,http://arxiv.org/pdf/2508.16488v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Mind and Motion Aligned: A Joint Evaluation IsaacSim Benchmark for Task
  Planning and Low-Level Policies in Mobile Manipulation","Benchmarks are crucial for evaluating progress in robotics and embodied AI.
However, a significant gap exists between benchmarks designed for high-level
language instruction following, which often assume perfect low-level execution,
and those for low-level robot control, which rely on simple, one-step commands.
This disconnect prevents a comprehensive evaluation of integrated systems where
both task planning and physical execution are critical. To address this, we
propose Kitchen-R, a novel benchmark that unifies the evaluation of task
planning and low-level control within a simulated kitchen environment. Built as
a digital twin using the Isaac Sim simulator and featuring more than 500
complex language instructions, Kitchen-R supports a mobile manipulator robot.
We provide baseline methods for our benchmark, including a task-planning
strategy based on a vision-language model and a low-level control policy based
on diffusion policy. We also provide a trajectory collection system. Our
benchmark offers a flexible framework for three evaluation modes: independent
assessment of the planning module, independent assessment of the control
policy, and, crucially, an integrated evaluation of the whole system. Kitchen-R
bridges a key gap in embodied AI research, enabling more holistic and realistic
benchmarking of language-guided robotic agents.","Nikita Kachaev, Andrei Spiridonov, Andrey Gorodetsky, Kirill Muravyev, Nikita Oskolkov, Aditya Narendra, Vlad Shakhuro, Dmitry Makarov, Aleksandr I. Panov, Polina Fedotova, Alexey K. Kovalev",2025-08-21T15:48:51Z,2025-08-21T15:48:51Z,http://arxiv.org/abs/2508.15663v1,http://arxiv.org/pdf/2508.15663v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Manipulate-to-Navigate: Reinforcement Learning with Visual Affordances
  and Manipulability Priors","Mobile manipulation in dynamic environments is challenging due to movable
obstacles blocking the robot's path. Traditional methods, which treat
navigation and manipulation as separate tasks, often fail in such
'manipulate-to-navigate' scenarios, as obstacles must be removed before
navigation. In these cases, active interaction with the environment is required
to clear obstacles while ensuring sufficient space for movement. To address the
manipulate-to-navigate problem, we propose a reinforcement learning-based
approach for learning manipulation actions that facilitate subsequent
navigation. Our method combines manipulability priors to focus the robot on
high manipulability body positions with affordance maps for selecting
high-quality manipulation actions. By focusing on feasible and meaningful
actions, our approach reduces unnecessary exploration and allows the robot to
learn manipulation strategies more effectively. We present two new
manipulate-to-navigate simulation tasks called Reach and Door with the Boston
Dynamics Spot robot. The first task tests whether the robot can select a good
hand position in the target area such that the robot base can move effectively
forward while keeping the end effector position fixed. The second task requires
the robot to move a door aside in order to clear the navigation path. Both of
these tasks need first manipulation and then navigating the base forward.
Results show that our method allows a robot to effectively interact with and
traverse dynamic environments. Finally, we transfer the learned policy to a
real Boston Dynamics Spot robot, which successfully performs the Reach task.","Yuying Zhang, Joni Pajarinen",2025-08-18T17:58:57Z,2025-08-18T17:58:57Z,http://arxiv.org/abs/2508.13151v1,http://arxiv.org/pdf/2508.13151v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Handows: A Palm-Based Interactive Multi-Window Management System in
  Virtual Reality","Window management in virtual reality (VR) remains a challenging task due to
the spatial complexity and physical demands of current interaction methods. We
introduce Handows, a palm-based interface that enables direct manipulation of
spatial windows through familiar smartphone-inspired gestures on the user's
non-dominant hand. Combining ergonomic layout design with body-centric input
and passive haptics, Handows supports four core operations: window selection,
closure, positioning, and scaling. We evaluate Handows in a user study (N=15)
against two common VR techniques (virtual hand and controller) across these
core window operations. Results show that Handows significantly reduces
physical effort and head movement while improving task efficiency and
interaction precision. A follow-up case study (N=8) demonstrates Handows'
usability in realistic multitasking scenarios, highlighting user-adapted
workflows and spontaneous layout strategies. Our findings suggest the potential
of embedding mobile-inspired metaphors into proprioceptive body-centric
interfaces to support low-effort and spatially coherent interaction in VR.","Jindu Wang, Ke Zhou, Haoyu Ren, Per Ola Kristensson, Xiang Li",2025-08-13T03:54:29Z,2025-08-13T03:54:29Z,http://arxiv.org/abs/2508.09469v1,http://arxiv.org/pdf/2508.09469v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"AgentWorld: An Interactive Simulation Platform for Scene Construction
  and Mobile Robotic Manipulation","We introduce AgentWorld, an interactive simulation platform for developing
household mobile manipulation capabilities. Our platform combines automated
scene construction that encompasses layout generation, semantic asset
placement, visual material configuration, and physics simulation, with a
dual-mode teleoperation system supporting both wheeled bases and humanoid
locomotion policies for data collection. The resulting AgentWorld Dataset
captures diverse tasks ranging from primitive actions (pick-and-place,
push-pull, etc.) to multistage activities (serve drinks, heat up food, etc.)
across living rooms, bedrooms, and kitchens. Through extensive benchmarking of
imitation learning methods including behavior cloning, action chunking
transformers, diffusion policies, and vision-language-action models, we
demonstrate the dataset's effectiveness for sim-to-real transfer. The
integrated system provides a comprehensive solution for scalable robotic skill
acquisition in complex home environments, bridging the gap between
simulation-based training and real-world deployment. The code, datasets will be
available at https://yizhengzhang1.github.io/agent_world/","Yizheng Zhang, Zhenjun Yu, Jiaxin Lai, Cewu Lu, Lei Han",2025-08-11T08:56:19Z,2025-08-13T09:09:33Z,http://arxiv.org/abs/2508.07770v2,http://arxiv.org/pdf/2508.07770v2.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
Learning Causal Structure Distributions for Robust Planning,"Structural causal models describe how the components of a robotic system
interact. They provide both structural and functional information about the
relationships that are present in the system. The structural information
outlines the variables among which there is interaction. The functional
information describes how such interactions work, via equations or learned
models. In this paper we find that learning the functional relationships while
accounting for the uncertainty about the structural information leads to more
robust dynamics models which improves downstream planning, while using
significantly lower computational resources. This in contrast with common
model-learning methods that ignore the causal structure and fail to leverage
the sparsity of interactions in robotic systems. We achieve this by estimating
a causal structure distribution that is used to sample causal graphs that
inform the latent-space representations in an encoder-multidecoder
probabilistic model. We show that our model can be used to learn the dynamics
of a robot, which together with a sampling-based planner can be used to perform
new tasks in novel environments, provided an objective function for the new
requirement is available. We validate our method using manipulators and mobile
robots in both simulation and the real-world. Additionally, we validate the
learned dynamics' adaptability and increased robustness to corrupted inputs and
changes in the environment, which is highly desirable in challenging real-world
robotics scenarios. Video: https://youtu.be/X6k5t7OOnNc.","Alejandro Murillo-Gonzalez, Junhong Xu, Lantao Liu",2025-08-08T22:43:17Z,2025-08-08T22:43:17Z,http://arxiv.org/abs/2508.06742v1,http://arxiv.org/pdf/2508.06742v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Surrogate-Enhanced Modeling and Adaptive Modular Control of All-Electric
  Heavy-Duty Robotic Manipulators","This paper presents a unified system-level modeling and control framework for
an all-electric heavy-duty robotic manipulator (HDRM) driven by
electromechanical linear actuators (EMLAs). A surrogate-enhanced actuator
model, combining integrated electromechanical dynamics with a neural network
trained on a dedicated testbed, is integrated into an extended virtual
decomposition control (VDC) architecture augmented by a natural adaptation law.
The derived analytical HDRM model supports a hierarchical control structure
that seamlessly maps high-level force and velocity objectives to real-time
actuator commands, accompanied by a Lyapunov-based stability proof. In
multi-domain simulations of both cubic and a custom planar triangular
trajectory, the proposed adaptive modular controller achieves sub-centimeter
Cartesian tracking accuracy. Experimental validation of the same 1-DoF platform
under realistic load emulation confirms the efficacy of the proposed control
strategy. These findings demonstrate that a surrogate-enhanced EMLA model
embedded in the VDC approach can enable modular, real-time control of an
all-electric HDRM, supporting its deployment in next-generation mobile working
machines.","Amir Hossein Barjini, Mohammad Bahari, Mahdi Hejrati, Jouni Mattila",2025-08-08T13:40:37Z,2025-08-08T13:40:37Z,http://arxiv.org/abs/2508.06313v1,http://arxiv.org/pdf/2508.06313v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Finite Length Effects and Coulomb Interaction in Ge Quantum Well-Based
  Josephson Junctions Probed with Microwave Spectroscopy","Proximitized Ge quantum wells have emerged as a novel platform for studying
Andreev bound states (ABSs), due to their expected strong spin-orbit
interaction and high mobility. Here, we used microwave spectroscopy techniques
to investigate ABSs in Josephson junctions (JJs) realized in proximitized Ge
quantum wells. Spectroscopic signatures observed in a 350 nm junction indicated
the presence of multiple ABSs, and were reproduced with a model including
finite-length effects. The ABS spectra measured for a $1.2~\mu$m junction were
explained by a model including three ABSs in two conduction channels and finite
Coulomb interaction. Our work highlights the importance of interactions in JJs
and serves as a basis for understanding and manipulating ABSs in Ge-based
hybrid devices.","S. C. ten Kate, D. C. Ohnmacht, M. Coraiola, T. Antonelli, S. Paredes, F. J. Schupp, M. Hinderling, S. W. Bedell, W. Belzig, J. C. Cuevas, A. E. Svetogorov, F. Nichele, D. Sabonis",2025-08-08T09:52:01Z,2025-08-08T09:52:01Z,http://arxiv.org/abs/2508.06180v1,http://arxiv.org/pdf/2508.06180v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Manip4Care: Robotic Manipulation of Human Limbs for Solving Assistive
  Tasks","Enabling robots to grasp and reposition human limbs can significantly enhance
their ability to provide assistive care to individuals with severe mobility
impairments, particularly in tasks such as robot-assisted bed bathing and
dressing. However, existing assistive robotics solutions often assume that the
human remains static or quasi-static, limiting their effectiveness. To address
this issue, we present Manip4Care, a modular simulation pipeline that enables
robotic manipulators to grasp and reposition human limbs effectively. Our
approach features a physics simulator equipped with built-in techniques for
grasping and repositioning while considering biomechanical and collision
avoidance constraints. Our grasping method employs antipodal sampling with
force closure to grasp limbs, and our repositioning system utilizes the Model
Predictive Path Integral (MPPI) and vector-field-based control method to
generate motion trajectories under collision avoidance and biomechanical
constraints. We evaluate this approach across various limb manipulation tasks
in both supine and sitting positions and compare outcomes for different age
groups with differing shoulder joint limits. Additionally, we demonstrate our
approach for limb manipulation using a real-world mannequin and further
showcase its effectiveness in bed bathing tasks.","Yubin Koh, Ahmed H. Qureshi",2025-08-04T17:41:58Z,2025-08-04T17:41:58Z,http://arxiv.org/abs/2508.02649v1,http://arxiv.org/pdf/2508.02649v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
On-Device Diffusion Transformer Policy for Efficient Robot Manipulation,"Diffusion Policies have significantly advanced robotic manipulation tasks via
imitation learning, but their application on resource-constrained mobile
platforms remains challenging due to computational inefficiency and extensive
memory footprint. In this paper, we propose LightDP, a novel framework
specifically designed to accelerate Diffusion Policies for real-time deployment
on mobile devices. LightDP addresses the computational bottleneck through two
core strategies: network compression of the denoising modules and reduction of
the required sampling steps. We first conduct an extensive computational
analysis on existing Diffusion Policy architectures, identifying the denoising
network as the primary contributor to latency. To overcome performance
degradation typically associated with conventional pruning methods, we
introduce a unified pruning and retraining pipeline, optimizing the model's
post-pruning recoverability explicitly. Furthermore, we combine pruning
techniques with consistency distillation to effectively reduce sampling steps
while maintaining action prediction accuracy. Experimental evaluations on the
standard datasets, \ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that
LightDP achieves real-time action prediction on mobile devices with competitive
performance, marking an important step toward practical deployment of
diffusion-based policies in resource-limited environments. Extensive real-world
experiments also show the proposed LightDP can achieve performance comparable
to state-of-the-art Diffusion Policies.","Yiming Wu, Huan Wang, Zhenghao Chen, Jianxin Pang, Dong Xu",2025-08-01T15:14:39Z,2025-08-01T15:14:39Z,http://arxiv.org/abs/2508.00697v1,http://arxiv.org/pdf/2508.00697v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation,"The rapid advancement of Vision-Language-Action models has created an urgent
need for large-scale, high-quality robot demonstration datasets. Although
teleoperation is the predominant method for data collection, current approaches
suffer from limited scalability, complex setup procedures, and suboptimal data
quality. This paper presents XRoboToolkit, a cross-platform framework for
extended reality based robot teleoperation built on the OpenXR standard. The
system features low-latency stereoscopic visual feedback, optimization-based
inverse kinematics, and support for diverse tracking modalities including head,
controller, hand, and auxiliary motion trackers. XRoboToolkit's modular
architecture enables seamless integration across robotic platforms and
simulation environments, spanning precision manipulators, mobile robots, and
dexterous hands. We demonstrate the framework's effectiveness through precision
manipulation tasks and validate data quality by training VLA models that
exhibit robust autonomous performance.","Zhigen Zhao, Liuchuan Yu, Ke Jing, Ning Yang",2025-07-31T18:45:13Z,2025-07-31T18:45:13Z,http://arxiv.org/abs/2508.00097v1,http://arxiv.org/pdf/2508.00097v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"MoDeSuite: Robot Learning Task Suite for Benchmarking Mobile
  Manipulation with Deformable Objects","Mobile manipulation is a critical capability for robots operating in diverse,
real-world environments. However, manipulating deformable objects and materials
remains a major challenge for existing robot learning algorithms. While various
benchmarks have been proposed to evaluate manipulation strategies with rigid
objects, there is still a notable lack of standardized benchmarks that address
mobile manipulation tasks involving deformable objects.
  To address this gap, we introduce MoDeSuite, the first Mobile Manipulation
Deformable Object task suite, designed specifically for robot learning.
MoDeSuite consists of eight distinct mobile manipulation tasks covering both
elastic objects and deformable objects, each presenting a unique challenge
inspired by real-world robot applications. Success in these tasks requires
effective collaboration between the robot's base and manipulator, as well as
the ability to exploit the deformability of the objects. To evaluate and
demonstrate the use of the proposed benchmark, we train two state-of-the-art
reinforcement learning algorithms and two imitation learning algorithms,
highlighting the difficulties encountered and showing their performance in
simulation. Furthermore, we demonstrate the practical relevance of the suite by
deploying the trained policies directly into the real world with the Spot
robot, showcasing the potential for sim-to-real transfer. We expect that
MoDeSuite will open a novel research domain in mobile manipulation involving
deformable objects. Find more details, code, and videos at
https://sites.google.com/view/modesuite/home.","Yuying Zhang, Kevin Sebastian Luck, Francesco Verdoja, Ville Kyrki, Joni Pajarinen",2025-07-29T13:33:43Z,2025-07-29T13:33:43Z,http://arxiv.org/abs/2507.21796v1,http://arxiv.org/pdf/2507.21796v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
Learning Physical Interaction Skills from Human Demonstrations,"Learning physical interaction skills, such as dancing, handshaking, or
sparring, remains a fundamental challenge for agents operating in human
environments, particularly when the agent's morphology differs significantly
from that of the demonstrator. Existing approaches often rely on handcrafted
objectives or morphological similarity, limiting their capacity for
generalization. Here, we introduce a framework that enables agents with diverse
embodiments to learn wholebbody interaction behaviors directly from human
demonstrations. The framework extracts a compact, transferable representation
of interaction dynamics, called the Embedded Interaction Graph (EIG), which
captures key spatiotemporal relationships between the interacting agents. This
graph is then used as an imitation objective to train control policies in
physics-based simulations, allowing the agent to generate motions that are both
semantically meaningful and physically feasible. We demonstrate BuddyImitation
on multiple agents, such as humans, quadrupedal robots with manipulators, or
mobile manipulators and various interaction scenarios, including sparring,
handshaking, rock-paper-scissors, or dancing. Our results demonstrate a
promising path toward coordinated behaviors across morphologically distinct
characters via cross embodiment interaction learning.","Tianyu Li, Hengbo Ma, Sehoon Ha, Kwonjoon Lee",2025-07-28T00:25:45Z,2025-08-03T18:32:05Z,http://arxiv.org/abs/2507.20445v2,http://arxiv.org/pdf/2507.20445v2.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Detecting Visual Information Manipulation Attacks in Augmented Reality:
  A Multimodal Semantic Reasoning Approach","The virtual content in augmented reality (AR) can introduce misleading or
harmful information, leading to semantic misunderstandings or user errors. In
this work, we focus on visual information manipulation (VIM) attacks in AR,
where virtual content changes the meaning of real-world scenes in subtle but
impactful ways. We introduce a taxonomy that categorizes these attacks into
three formats: character, phrase, and pattern manipulation, and three purposes:
information replacement, information obfuscation, and extra wrong information.
Based on the taxonomy, we construct a dataset, AR-VIM, which consists of 452
raw-AR video pairs spanning 202 different scenes, each simulating a real-world
AR scenario. To detect the attacks in the dataset, we propose a multimodal
semantic reasoning framework, VIM-Sense. It combines the language and visual
understanding capabilities of vision-language models (VLMs) with optical
character recognition (OCR)-based textual analysis. VIM-Sense achieves an
attack detection accuracy of 88.94% on AR-VIM, consistently outperforming
vision-only and text-only baselines. The system achieves an average attack
detection latency of 7.07 seconds in a simulated video processing framework and
7.17 seconds in a real-world evaluation conducted on a mobile Android AR
application.","Yanming Xiu, Maria Gorlatova",2025-07-27T17:04:50Z,2025-09-02T15:23:42Z,http://arxiv.org/abs/2507.20356v4,http://arxiv.org/pdf/2507.20356v4.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Design and Dimensional Optimization of Legged Structures for
  Construction Robots","Faced with complex and unstructured construction environments, wheeled and
tracked robots exhibit significant limitations in terrain adaptability and
flexibility, making it difficult to meet the requirements of autonomous
operation. Inspired by ants in nature, this paper proposes a leg configuration
design and optimization method tailored for construction scenarios, aiming to
enhance the autonomous mobility of construction robots. This paper analyzes the
full operational motion performance of the leg during both swing and stance
phases. First, based on kinematic modeling and multi-dimensional workspace
analysis, the concept of an ""improved workspace"" is introduced, and graphical
methods are used to optimize the leg dimensions during the swing phase.
Furthermore, a new concept of ""average manipulability"" is introduced based on
the velocity Jacobian matrix, and numerical solutions are applied to obtain the
leg segment ratio that maximizes manipulability. To overcome the difficulties
associated with traditional analytical methods, virtual prototype simulations
are conducted in ADAMS to explore the relationship between the robot body's
optimal flexibility and leg segment proportions. In summary, the leg segment
proportions with the best comprehensive motion performance are obtained. This
study presents the first multi-dimensional quantitative evaluation framework
for leg motion performance tailored for construction environments, providing a
structural design foundation for legged construction robots to achieve
autonomous mobility in complex terrains.","Xiao Liu, Xianlong Yang, Weijun Wang, Wei Feng",2025-07-22T08:10:12Z,2025-07-22T08:10:12Z,http://arxiv.org/abs/2507.16328v1,http://arxiv.org/pdf/2507.16328v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Tidal-Like Concept Drift in RIS-Covered Buildings: When Programmable
  Wireless Environments Meet Human Behaviors","Indoor mobile networks handle the majority of data traffic, with their
performance limited by building materials and structures. However, building
designs have historically not prioritized wireless performance. Prior to the
advent of reconfigurable intelligent surfaces (RIS), the industry passively
adapted to wireless propagation challenges within buildings. Inspired by RIS's
successes in outdoor networks, we propose embedding RIS into building
structures to manipulate and enhance building wireless performance
comprehensively. Nonetheless, the ubiquitous mobility of users introduces
complex dynamics to the channels of RIS-covered buildings. A deep understanding
of indoor human behavior patterns is essential for achieving wireless-friendly
building design. This article is the first to systematically examine the tidal
evolution phenomena emerging in the channels of RIS-covered buildings driven by
complex human behaviors. We demonstrate that a universal channel model is
unattainable and focus on analyzing the challenges faced by advanced deep
learning-based prediction and control strategies, including high-order Markov
dependencies, concept drift, and generalization issues caused by human-induced
disturbances. Possible solutions for orchestrating the coexistence of
RIS-covered buildings and crowd mobility are also laid out.","Zi-Yang Wu, Muhammad Ismail, Jiliang Zhang, Jie Zhang",2025-07-20T09:19:26Z,2025-07-20T09:19:26Z,http://arxiv.org/abs/2507.14876v1,http://arxiv.org/pdf/2507.14876v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"LLM Agent-Based Simulation of Student Activities and Mental Health Using
  Smartphone Sensing Data","Students' mental well-being is vital for academic success, with activities
such as studying, socializing, and sleeping playing a role. Current mobile
sensing data highlight this intricate link using statistical and machine
learning analyses. We propose a novel LLM agent-based simulation framework to
model student activities and mental health using the StudentLife Dataset. Each
LLM agent was initialized with personality questionnaires and guided by
smartphone sensing data throughout the simulated semester. These agents predict
individual behaviors, provide self-reported mental health data via ecological
momentary assessments (EMAs), and complete follow-up personality
questionnaires. To ensure accuracy, we investigated various prompting
techniques, memory systems, and activity-based mental state management
strategies that dynamically update an agent's mental state based on their daily
activities. This simulation goes beyond simply replicating existing data. This
allows us to explore new scenarios that are not present in the original
dataset, such as peer influence through agent-to-agent interactions and the
impact of social media. Furthermore, we can conduct intervention studies by
manipulating activity patterns via sensing signals and personality traits using
questionnaire responses. This provides valuable insights into the behavioral
changes that could enhance student well-being. The framework also facilitates
hypothetical interviews with LLM agents, offering deeper insights into their
mental health. This study showcases the power of LLM-driven behavioral modeling
with sensing data, opening new avenues for understanding and supporting student
mental health.","Wayupuk Sommuang, Kun Kerdthaisong, Pasin Buakhaw, Aslan B. Wong, Nutchanon Yongsatianchot",2025-07-17T03:30:11Z,2025-08-08T12:56:11Z,http://arxiv.org/abs/2508.02679v2,http://arxiv.org/pdf/2508.02679v2.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
Learning to Predict Mobile Robot Stability in Off-Road Environments,"Navigating in off-road environments for wheeled mobile robots is challenging
due to dynamic and rugged terrain. Traditional physics-based stability metrics,
such as Static Stability Margin (SSM) or Zero Moment Point (ZMP) require
knowledge of contact forces, terrain geometry, and the robot's precise
center-of-mass that are difficult to measure accurately in real-world field
conditions. In this work, we propose a learning-based approach to estimate
robot platform stability directly from proprioceptive data using a lightweight
neural network, IMUnet. Our method enables data-driven inference of robot
stability without requiring an explicit terrain model or force sensing.
  We also develop a novel vision-based ArUco tracking method to compute a
scalar score to quantify robot platform stability called C3 score. The score
captures image-space perturbations over time as a proxy for physical
instability and is used as a training signal for the neural network based
model. As a pilot study, we evaluate our approach on data collected across
multiple terrain types and speeds and demonstrate generalization to previously
unseen conditions. These initial results highlight the potential of using IMU
and robot velocity as inputs to estimate platform stability. The proposed
method finds application in gating robot tasks such as precision actuation and
sensing, especially for mobile manipulation tasks in agricultural and space
applications. Our learning method also provides a supervision mechanism for
perception based traversability estimation and planning.","Nathaniel Rose, Arif Ahmed, Emanuel Gutierrez-Cornejo, Parikshit Maini",2025-07-17T02:24:35Z,2025-07-17T02:24:35Z,http://arxiv.org/abs/2507.12731v1,http://arxiv.org/pdf/2507.12731v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Object-Centric Mobile Manipulation through SAM2-Guided Perception and
  Imitation Learning","Imitation learning for mobile manipulation is a key challenge in the field of
robotic manipulation. However, current mobile manipulation frameworks typically
decouple navigation and manipulation, executing manipulation only after
reaching a certain location. This can lead to performance degradation when
navigation is imprecise, especially due to misalignment in approach angles. To
enable a mobile manipulator to perform the same task from diverse orientations,
an essential capability for building general-purpose robotic models, we propose
an object-centric method based on SAM2, a foundation model towards solving
promptable visual segmentation in images, which incorporates manipulation
orientation information into our model. Our approach enables consistent
understanding of the same task from different orientations. We deploy the model
on a custom-built mobile manipulator and evaluate it on a pick-and-place task
under varied orientation angles. Compared to Action Chunking Transformer, our
model maintains superior generalization when trained with demonstrations from
varied approach angles. This work significantly enhances the generalization and
robustness of imitation learning-based mobile manipulation systems.","Wang Zhicheng, Satoshi Yagi, Satoshi Yamamori, Jun Morimoto",2025-07-15T01:26:59Z,2025-07-15T01:26:59Z,http://arxiv.org/abs/2507.10899v1,http://arxiv.org/pdf/2507.10899v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Incentive-Aware Dynamic Resource Allocation under Long-Term Cost
  Constraints","Motivated by applications such as cloud platforms allocating GPUs to users or
governments deploying mobile health units across competing regions, we study
the dynamic allocation of a reusable resource to strategic agents with private
valuations. Our objective is to simultaneously (i) maximize social welfare,
(ii) satisfy multi-dimensional long-term cost constraints, and (iii)
incentivize truthful reporting. We begin by numerically evaluating primal-dual
methods widely used in constrained online optimization and find them to be
highly fragile in strategic settings -- agents can easily manipulate their
reports to distort future dual updates for future gain.
  To address this vulnerability, we develop an incentive-aware framework that
makes primal-dual methods robust to strategic behavior. Our design combines
epoch-based lazy updates -- where dual variables remain fixed within each epoch
-- with randomized exploration rounds that extract approximately truthful
signals for learning. Leveraging carefully designed online learning subroutines
that can be of independent interest for dual updates, our mechanism achieves
$\tilde{\mathcal{O}}(\sqrt{T})$ social welfare regret, satisfies all cost
constraints, and ensures incentive alignment. This matches the performance of
non-strategic allocation approaches while being robust to strategic agents.","Yan Dai, Negin Golrezaei, Patrick Jaillet",2025-07-13T03:18:02Z,2025-07-13T03:18:02Z,http://arxiv.org/abs/2507.09473v1,http://arxiv.org/pdf/2507.09473v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
Anyon-trions in atomically thin semiconductor heterostructures,"The study of anyons in topologically ordered quantum systems has mainly
relied on edge-state interferometry. However, realizing controlled braiding of
anyons necessitates the ability to detect and manipulate individual anyons
within the bulk. Here, we propose and theoretically investigate a first step
toward this goal by demonstrating that a long-lived, optically generated
interlayer exciton can bind to a quasihole in a fractional quantum Hall state,
forming a composite excitation we term an anyon-trion. Using exact
diagonalization, we show that mobile anyon-trions possess a binding energy of
approximately 0.5 meV, whereas static anyon-trions exhibit a binding energy of
about 0.9 meV, that is linearly proportional to the quasiholes fractional
charge. An experimental realization based on photoluminescence from localized
interlayer excitons in a quantum twisting microscope setup should allow for a
direct optical observation of anyon-trions.","Nader Mostaan, Nathan Goldman, Ataç İmamoğlu, Fabian Grusdt",2025-07-11T18:00:05Z,2025-07-20T11:48:26Z,http://arxiv.org/abs/2507.08933v2,http://arxiv.org/pdf/2507.08933v2.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
Self-Wearing Adaptive Garments via Soft Robotic Unfurling,"Robotic dressing assistance has the potential to improve the quality of life
for individuals with limited mobility. Existing solutions predominantly rely on
rigid robotic manipulators, which have challenges in handling deformable
garments and ensuring safe physical interaction with the human body. Prior
robotic dressing methods require excessive operation times, complex control
strategies, and constrained user postures, limiting their practicality and
adaptability. This paper proposes a novel soft robotic dressing system, the
Self-Wearing Adaptive Garment (SWAG), which uses an unfurling and growth
mechanism to facilitate autonomous dressing. Unlike traditional approaches,the
SWAG conforms to the human body through an unfurling based deployment method,
eliminating skin-garment friction and enabling a safer and more efficient
dressing process. We present the working principles of the SWAG, introduce its
design and fabrication, and demonstrate its performance in dressing assistance.
The proposed system demonstrates effective garment application across various
garment configurations, presenting a promising alternative to conventional
robotic dressing assistance.","Nam Gyun Kim, William E. Heap, Yimeng Qin, Elvy B. Yao, Jee-Hwan Ryu, Allison M. Okamura",2025-07-09T18:47:41Z,2025-07-09T18:47:41Z,http://arxiv.org/abs/2507.07221v1,http://arxiv.org/pdf/2507.07221v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
Entanglement switching via mobility edges in a quasiperiodic chain,"We propose quasiperiodic chains with tunable mobility edge physics, as a
promising platform for engineering long-range quantum entanglement. Using the
generalized Aubry-Andr\'e model, we show that the mobility edges play a key
role in manipulating long-range indirect interactions in these systems. Near
the mobility edge, critical states exhibit unexpectedly strong correlations
between sites that share similar local structures, regardless of their spatial
separation. Remarkably, by tuning the mobility edge across the Fermi level, one
can induce both adiabatic transport and abrupt switching of entanglement
between distant sites. These results highlight the potential of aperiodic
structures for controlling nonlocal quantum correlations, opening new avenues
for entanglement-based applications in quasiperiodic systems.","YouYoung Joung, Junmo Jeon, SungBin Lee",2025-07-08T18:00:07Z,2025-07-08T18:00:07Z,http://arxiv.org/abs/2507.06305v1,http://arxiv.org/pdf/2507.06305v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"iThermTroj: Exploiting Intermittent Thermal Trojans in Multi-Processor
  System-on-Chips","Thermal Trojan attacks present a pressing concern for the security and
reliability of System-on-Chips (SoCs), especially in mobile applications. The
situation becomes more complicated when such attacks are more evasive and
operate sporadically to stay hidden from detection mechanisms. In this paper,
we introduce Intermittent Thermal Trojans (iThermTroj) that exploit the chips'
thermal information in a random time-triggered manner. According to our
experiments, iThermTroj attack can easily bypass available threshold-based
thermal Trojan detection solutions. We investigate SoC vulnerabilities to
variations of iThermTroj through an in-depth analysis of Trojan activation and
duration scenarios. We also propose a set of tiny Machine Learning classifiers
for run-time anomaly detection to protect SoCs against such intermittent
thermal Trojan attacks. Compared to existing methods, our approach improves the
attack detection rate by 29.4\%, 17.2\%, and 14.3\% in scenarios where
iThermTroj manipulates up to 80\%, 60\%, and 40\% of SoC's thermal data,
respectively. Additionally, our method increases the full protection resolution
to 0.8 degrees Celsius, meaning that any temperature manipulations exceeding
$\pm 0.8$ degrees will be detected with 100\% accuracy.","Mehdi Elahi, Mohamed R. Elshamy, Abdel-Hameed Badawy, Ahmad Patooghy",2025-07-08T01:24:28Z,2025-07-08T01:24:28Z,http://arxiv.org/abs/2507.05576v1,http://arxiv.org/pdf/2507.05576v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Hijacking JARVIS: Benchmarking Mobile GUI Agents against Unprivileged
  Third Parties","Mobile GUI agents are designed to autonomously execute diverse device-control
tasks by interpreting and interacting with mobile screens. Despite notable
advancements, their resilience in real-world scenarios where screen content may
be partially manipulated by untrustworthy third parties remains largely
unexplored. Owing to their black-box and autonomous nature, these agents are
vulnerable to manipulations that could compromise user devices. In this work,
we present the first systematic investigation into the vulnerabilities of
mobile GUI agents. We introduce a scalable attack simulation framework
AgentHazard, which enables flexible and targeted modifications of screen
content within existing applications. Leveraging this framework, we develop a
comprehensive benchmark suite comprising both a dynamic task execution
environment and a static dataset of vision-language-action tuples, totaling
over 3,000 attack scenarios. The dynamic environment encompasses 58
reproducible tasks in an emulator with various types of hazardous UI content,
while the static dataset is constructed from 210 screenshots collected from 14
popular commercial apps. Importantly, our content modifications are designed to
be feasible for unprivileged third parties. We evaluate 7 widely-used mobile
GUI agents and 5 common backbone models using our benchmark. Our findings
reveal that all examined agents are significantly influenced by misleading
third-party content (with an average misleading rate of 28.8% in human-crafted
attack scenarios) and that their vulnerabilities are closely linked to the
employed perception modalities and backbone LLMs. Furthermore, we assess
training-based mitigation strategies, highlighting both the challenges and
opportunities for enhancing the robustness of mobile GUI agents. Our code and
data will be released at https://agenthazard.github.io.","Guohong Liu, Jialei Ye, Jiacheng Liu, Yuanchun Li, Wei Liu, Pengzhi Gao, Jian Luan, Yunxin Liu",2025-07-06T03:31:36Z,2025-07-06T03:31:36Z,http://arxiv.org/abs/2507.04227v1,http://arxiv.org/pdf/2507.04227v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Trajectory Optimization for Differential Drive Mobile Manipulators via
  Topological Paths Search and Arc Length-Yaw Parameterization","We present an efficient hierarchical motion planning pipeline for
differential drive mobile manipulators. Our approach first searches for
multiple collisionfree and topologically distinct paths for the mobile base to
extract the space in which optimal solutions may exist. Further sampling and
optimization are then conducted in parallel to explore feasible whole-body
trajectories. For trajectory optimization, we employ polynomial trajectories
and arc length-yaw parameterization, enabling efficient handling of the
nonholonomic dynamics while ensuring optimality.","Long Xu, Choilam Wong, Mengke Zhang, Junxiao Lin, Fei Gao",2025-07-03T16:21:43Z,2025-07-03T16:21:43Z,http://arxiv.org/abs/2507.02761v1,http://arxiv.org/pdf/2507.02761v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"AC-DiT: Adaptive Coordination Diffusion Transformer for Mobile
  Manipulation","Recently, mobile manipulation has attracted increasing attention for enabling
language-conditioned robotic control in household tasks. However, existing
methods still face challenges in coordinating mobile base and manipulator,
primarily due to two limitations. On the one hand, they fail to explicitly
model the influence of the mobile base on manipulator control, which easily
leads to error accumulation under high degrees of freedom. On the other hand,
they treat the entire mobile manipulation process with the same visual
observation modality (e.g., either all 2D or all 3D), overlooking the distinct
multimodal perception requirements at different stages during mobile
manipulation. To address this, we propose the Adaptive Coordination Diffusion
Transformer (AC-DiT), which enhances mobile base and manipulator coordination
for end-to-end mobile manipulation. First, since the motion of the mobile base
directly influences the manipulator's actions, we introduce a mobility-to-body
conditioning mechanism that guides the model to first extract base motion
representations, which are then used as context prior for predicting whole-body
actions. This enables whole-body control that accounts for the potential impact
of the mobile base's motion. Second, to meet the perception requirements at
different stages of mobile manipulation, we design a perception-aware
multimodal conditioning strategy that dynamically adjusts the fusion weights
between various 2D visual images and 3D point clouds, yielding visual features
tailored to the current perceptual needs. This allows the model to, for
example, adaptively rely more on 2D inputs when semantic information is crucial
for action prediction, while placing greater emphasis on 3D geometric
information when precise spatial understanding is required. We validate AC-DiT
through extensive experiments on both simulated and real-world mobile
manipulation tasks.","Sixiang Chen, Jiaming Liu, Siyuan Qian, Han Jiang, Lily Li, Renrui Zhang, Zhuoyang Liu, Chenyang Gu, Chengkai Hou, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang",2025-07-02T17:59:54Z,2025-07-05T04:00:38Z,http://arxiv.org/abs/2507.01961v3,http://arxiv.org/pdf/2507.01961v3.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
Ark: An Open-source Python-based Framework for Robot Learning,"Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics
Challenges to the first humanoid-robot kickboxing tournament-yet commercial
autonomy still lags behind progress in machine learning. A major bottleneck is
software: current robot stacks demand steep learning curves, low-level C/C++
expertise, fragmented tooling, and intricate hardware integration, in stark
contrast to the Python-centric, well-documented ecosystems that propelled
modern AI. We introduce ARK, an open-source, Python-first robotics framework
designed to close that gap. ARK presents a Gym-style environment interface that
allows users to collect data, preprocess it, and train policies using
state-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy)
while seamlessly toggling between high-fidelity simulation and physical robots.
A lightweight client-server architecture provides networked
publisher-subscriber communication, and optional C/C++ bindings ensure
real-time performance when needed. ARK ships with reusable modules for control,
SLAM, motion planning, system identification, and visualization, along with
native ROS interoperability. Comprehensive documentation and case studies-from
manipulation to mobile navigation-demonstrate rapid prototyping, effortless
hardware swapping, and end-to-end pipelines that rival the convenience of
mainstream machine-learning workflows. By unifying robotics and AI practices
under a common Python umbrella, ARK lowers entry barriers and accelerates
research and commercial deployment of autonomous robots.","Magnus Dierking, Christopher E. Mower, Sarthak Das, Huang Helong, Jiacheng Qiu, Cody Reading, Wei Chen, Huidong Liang, Huang Guowei, Jan Peters, Quan Xingyue, Jun Wang, Haitham Bou-Ammar",2025-06-24T20:23:39Z,2025-07-14T17:46:29Z,http://arxiv.org/abs/2506.21628v2,http://arxiv.org/pdf/2506.21628v2.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Can Movable Antenna-enabled Micro-Mobility Replace UAV-enabled
  Macro-Mobility? A Physical Layer Security Perspective","This paper investigates the potential of movable antenna (MA)-enabled
micro-mobility to replace UAV-enabled macro-mobility for enhancing physical
layer security (PLS) in air-to-ground communications. While UAV trajectory
optimization offers high flexibility and Line-of-Sight (LoS) advantages, it
suffers from significant energy consumption, latency, and complex trajectory
optimization. Conversely, MA technology provides fine-grained spatial
reconfiguration (antenna positioning within a confined area) with ultra-low
energy overhead and millisecond-scale response, enabling real-time channel
manipulation and covert beam steering. To systematically compare these
paradigms, we establish a dual-scale mobility framework where a UAV-mounted
uniform linear array (ULA) serves as a base station transmitting confidential
information to a legitimate user (Bob) in the presence of an eavesdropper
(Eve). We formulate non-convex average secrecy rate (ASR) maximization problems
for both schemes: 1) MA-based micro-mobility: Jointly optimizing antenna
positions and beamforming (BF) vectors under positioning constraints; 2)
UAV-based macro-mobility: Jointly optimizing the UAV's trajectory and BF
vectors under kinematic constraints. Extensive simulations reveal distinct
operational regimes: MA micro-mobility demonstrates significant ASR advantages
in low-transmit-power scenarios or under antenna constraints due to its
energy-efficient spatial control. Conversely, UAV macro-mobility excels under
resource-sufficient conditions (higher power, larger antenna arrays) by
leveraging global mobility for optimal positioning. The findings highlight the
complementary strengths of both approaches, suggesting hybrid micro-macro
mobility as a promising direction for balancing security, energy efficiency,
and deployment complexity in future wireless networks.","Kaixuan Li, Kan Yu, Dingyou Ma, Yujia Zhao, Xiaowu Liu, Qixun Zhang, ZHiyong Feng",2025-06-24T09:37:03Z,2025-06-24T09:37:03Z,http://arxiv.org/abs/2506.19456v1,http://arxiv.org/pdf/2506.19456v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Efficient Navigation Among Movable Obstacles using a Mobile Manipulator
  via Hierarchical Policy Learning","We propose a hierarchical reinforcement learning (HRL) framework for
efficient Navigation Among Movable Obstacles (NAMO) using a mobile manipulator.
Our approach combines interaction-based obstacle property estimation with
structured pushing strategies, facilitating the dynamic manipulation of
unforeseen obstacles while adhering to a pre-planned global path. The
high-level policy generates pushing commands that consider environmental
constraints and path-tracking objectives, while the low-level policy precisely
and stably executes these commands through coordinated whole-body movements.
Comprehensive simulation-based experiments demonstrate improvements in
performing NAMO tasks, including higher success rates, shortened traversed path
length, and reduced goal-reaching times, compared to baselines. Additionally,
ablation studies assess the efficacy of each component, while a qualitative
analysis further validates the accuracy and reliability of the real-time
obstacle property estimation.","Taegeun Yang, Jiwoo Hwang, Jeil Jeong, Minsung Yoon, Sung-Eui Yoon",2025-06-18T11:49:57Z,2025-06-18T11:49:57Z,http://arxiv.org/abs/2506.15380v1,http://arxiv.org/pdf/2506.15380v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Socially-aware Object Transportation by a Mobile Manipulator in Static
  Planar Environments with Obstacles","Socially-aware robotic navigation is essential in environments where humans
and robots coexist, ensuring both safety and comfort. However, most existing
approaches have been primarily developed for mobile robots, leaving a
significant gap in research that addresses the unique challenges posed by
mobile manipulators. In this paper, we tackle the challenge of navigating a
robotic mobile manipulator, carrying a non-negligible load, within a static
human-populated environment while adhering to social norms. Our goal is to
develop a method that enables the robot to simultaneously manipulate an object
and navigate between locations in a socially-aware manner. We propose an
approach based on the Risk-RRT* framework that enables the coordinated
actuation of both the mobile base and manipulator. This approach ensures
collision-free navigation while adhering to human social preferences. We
compared our approach in a simulated environment to socially-aware mobile-only
methods applied to a mobile manipulator. The results highlight the necessity
for mobile manipulator-specific techniques, with our method outperforming
mobile-only approaches. Our method enabled the robot to navigate, transport an
object, avoid collisions, and minimize social discomfort effectively.","Caio C. G. Ribeiro, Leonardo R. D. Paes, Douglas G. Macharet",2025-06-16T19:45:30Z,2025-06-16T19:45:30Z,http://arxiv.org/abs/2506.13953v1,http://arxiv.org/pdf/2506.13953v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Intelligent Metasurface-Enabled Integrated Sensing and Communication:
  Unified Framework and Key Technologies","As the demand for ubiquitous connectivity and high-precision environmental
awareness grows, integrated sensing and communication (ISAC) has emerged as a
key technology for sixth-generation (6G) wireless networks. Intelligent
metasurfaces (IMs) have also been widely adopted in ISAC scenarios due to their
efficient, programmable control over electromagnetic waves. This provides a
versatile solution that meets the dual-function requirements of next-generation
networks. Although reconfigurable intelligent surfaces (RISs) have been
extensively studied for manipulating the propagation channel between base and
mobile stations, the full potential of IMs in ISAC transceiver design remains
under-explored. Against this backdrop, this article explores emerging
IM-enabled transceiver designs for ISAC systems. It begins with an overview of
representative IM architectures, their unique principles, and their inherent
advantages in EM wave manipulation. Next, a unified ISAC framework is
established to systematically model the design and derivation of diverse
IM-enabled transceiver structures. This lays the foundation for performance
optimization, trade-offs, and analysis. The paper then discusses several
critical technologies for IM-enabled ISAC transceivers, including dedicated
channel modeling, effective channel estimation, tailored beamforming
strategies, and dual-functional waveform design.","Shunyu Li, Tianqi Mao, Guangyao Liu, Fan Zhang, Ruiqi Liu, Meng Hua, Zhen Gao, Qingqing Wu, George K. Karagiannidis",2025-06-16T17:23:27Z,2025-06-16T17:23:27Z,http://arxiv.org/abs/2506.13713v1,http://arxiv.org/pdf/2506.13713v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
Deceptive Path Planning: A Bayesian Game Approach,"This paper investigates how an autonomous agent can transmit information
through its motion in an adversarial setting. We consider scenarios where an
agent must reach its goal while deceiving an intelligent observer about its
destination. We model this interaction as a dynamic Bayesian game between a
mobile Attacker with a privately known goal and a Defender who infers the
Attacker's intent to allocate defensive resources effectively. We use Perfect
Bayesian Nash Equilibrium (PBNE) as our solution concept and propose a
computationally efficient approach to find it. In the resulting equilibrium,
the Defender employs a simple Markovian strategy, while the Attacker
strategically balances deception and goal efficiency by stochastically mixing
shortest and non-shortest paths to manipulate the Defender's beliefs. Numerical
experiments demonstrate the advantages of our PBNE-based strategies over
existing methods based on one-sided optimization.","Violetta Rostobaya, James Berneburg, Yue Guan, Michael Dorothy, Daigo Shishika",2025-06-16T16:15:25Z,2025-06-16T16:15:25Z,http://arxiv.org/abs/2506.13650v1,http://arxiv.org/pdf/2506.13650v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Towards Efficient Occupancy Mapping via Gaussian Process Latent Field
  Shaping","Occupancy mapping has been a key enabler of mobile robotics. Originally based
on a discrete grid representation, occupancy mapping has evolved towards
continuous representations that can predict the occupancy status at any
location and account for occupancy correlations between neighbouring areas.
Gaussian Process (GP) approaches treat this task as a binary classification
problem using both observations of occupied and free space. Conceptually, a GP
latent field is passed through a logistic function to obtain the output class
without actually manipulating the GP latent field. In this work, we propose to
act directly on the latent function to efficiently integrate free space
information as a prior based on the shape of the sensor's field-of-view. A
major difference with existing methods is the change in the classification
problem, as we distinguish between free and unknown space. The `occupied' area
is the infinitesimally thin location where the class transitions from free to
unknown. We demonstrate in simulated environments that our approach is sound
and leads to competitive reconstruction accuracy.","Cedric Le Gentil, Cedric Pradalier, Timothy D. Barfoot",2025-06-16T16:04:54Z,2025-06-16T16:04:54Z,http://arxiv.org/abs/2506.13640v1,http://arxiv.org/pdf/2506.13640v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"ROS-related Robotic Systems Development with V-model-based Application
  of MeROS Metamodel","Systems built on the Robot Operating System (ROS) are increasingly easy to
assemble, yet hard to govern and reliably coordinate. Beyond the sheer number
of subsystems involved, the difficulty stems from their diversity and
interaction depth. In this paper, we use a compact heterogeneous robotic system
(HeROS), combining mobile and manipulation capabilities, as a demonstration
vehicle under dynamically changing tasks. Notably, all its subsystems are
powered by ROS.
  The use of compatible interfaces and other ROS integration capabilities
simplifies the construction of such systems. However, this only addresses part
of the complexity: the semantic coherence and structural traceability are even
more important for precise coordination and call for deliberate engineering
methods. The Model-Based Systems Engineering (MBSE) discipline, which emerged
from the experience of complexity management in large-scale engineering
domains, offers the methodological foundations needed.
  Despite their strengths in complementary aspects of robotics systems
engineering, the lack of a unified approach to integrate ROS and MBSE hinders
the full potential of these tools. Motivated by the anticipated impact of such
a synergy in robotics practice, we propose a structured methodology based on
MeROS - a SysML metamodel created specifically to put the ROS-based systems
into the focus of the MBSE workflow. As its methodological backbone, we adapt
the well-known V-model to this context, illustrating how complex robotic
systems can be designed with traceability and validation capabilities embedded
into their lifecycle using practices familiar to engineering teams.","Tomasz Winiarski, Jan Kaniuka, Daniel Giełdowski, Jakub Ostrysz, Krystian Radlak, Dmytro Kushnir",2025-06-10T11:44:00Z,2025-08-22T17:50:24Z,http://arxiv.org/abs/2506.08706v2,http://arxiv.org/pdf/2506.08706v2.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Re4MPC: Reactive Nonlinear MPC for Multi-model Motion Planning via Deep
  Reinforcement Learning","Traditional motion planning methods for robots with many degrees-of-freedom,
such as mobile manipulators, are often computationally prohibitive for
real-world settings. In this paper, we propose a novel multi-model motion
planning pipeline, termed Re4MPC, which computes trajectories using Nonlinear
Model Predictive Control (NMPC). Re4MPC generates trajectories in a
computationally efficient manner by reactively selecting the model, cost, and
constraints of the NMPC problem depending on the complexity of the task and
robot state. The policy for this reactive decision-making is learned via a Deep
Reinforcement Learning (DRL) framework. We introduce a mathematical formulation
to integrate NMPC into this DRL framework. To validate our methodology and
design choices, we evaluate DRL training and test outcomes in a physics-based
simulation involving a mobile manipulator. Experimental results demonstrate
that Re4MPC is more computationally efficient and achieves higher success rates
in reaching end-effector goals than the NMPC baseline, which computes
whole-body trajectories without our learning mechanism.","Neşet Ünver Akmandor, Sarvesh Prajapati, Mark Zolotas, Taşkın Padır",2025-06-10T01:58:32Z,2025-06-10T01:58:32Z,http://arxiv.org/abs/2506.08344v1,http://arxiv.org/pdf/2506.08344v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data
  Synthesis","The rapid progress of navigation, manipulation, and vision models has made
mobile manipulators capable in many specialized tasks. However, the open-world
mobile manipulation (OWMM) task remains a challenge due to the need for
generalization to open-ended instructions and environments, as well as the
systematic complexity to integrate high-level decision making with low-level
robot control based on both global scene understanding and current agent state.
To address this complexity, we propose a novel multi-modal agent architecture
that maintains multi-view scene frames and agent states for decision-making and
controls the robot by function calling. A second challenge is the hallucination
from domain shift. To enhance the agent performance, we further introduce an
agentic data synthesis pipeline for the OWMM task to adapt the VLM model to our
task domain with instruction fine-tuning. We highlight our fine-tuned OWMM-VLM
as the first dedicated foundation model for mobile manipulators with global
scene understanding, robot state tracking, and multi-modal action generation in
a unified model. Through experiments, we demonstrate that our model achieves
SOTA performance compared to other foundation models including GPT-4o and
strong zero-shot generalization in real world. The project page is at
https://github.com/HHYHRHY/OWMM-Agent","Junting Chen, Haotian Liang, Lingxiao Du, Weiyun Wang, Mengkang Hu, Yao Mu, Wenhai Wang, Jifeng Dai, Ping Luo, Wenqi Shao, Lin Shao",2025-06-04T17:57:44Z,2025-06-21T07:48:50Z,http://arxiv.org/abs/2506.04217v2,http://arxiv.org/pdf/2506.04217v2.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"UniConFlow: A Unified Constrained Generalization Framework for Certified
  Motion Planning with Flow Matching Models","Generative models have become increasingly powerful tools for robot motion
generation, enabling flexible and multimodal trajectory generation across
various tasks. Yet, most existing approaches remain limited in handling
multiple types of constraints, such as collision avoidance and dynamic
consistency, which are often treated separately or only partially considered.
This paper proposes UniConFlow, a unified flow matching (FM) based framework
for trajectory generation that systematically incorporates both equality and
inequality constraints. UniConFlow introduces a novel prescribed-time zeroing
function to enhance flexibility during the inference process, allowing the
model to adapt to varying task requirements. To ensure constraint satisfaction,
particularly with respect to obstacle avoidance, admissible action range, and
kinodynamic consistency, the guidance inputs to the FM model are derived
through a quadratic programming formulation, which enables constraint-aware
generation without requiring retraining or auxiliary controllers. We conduct
mobile navigation and high-dimensional manipulation tasks, demonstrating
improved safety and feasibility compared to state-of-the-art constrained
generative planners. Project page is available at https://uniconflow.github.io.","Zewen Yang, Xiaobing Dai, Dian Yu, Qianru Li, Yu Li, Valentin Le Mesle",2025-06-03T14:48:04Z,2025-06-03T14:48:04Z,http://arxiv.org/abs/2506.02955v1,http://arxiv.org/pdf/2506.02955v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
Mobi-$π$: Mobilizing Your Robot Learning Policy,"Learned visuomotor policies are capable of performing increasingly complex
manipulation tasks. However, most of these policies are trained on data
collected from limited robot positions and camera viewpoints. This leads to
poor generalization to novel robot positions, which limits the use of these
policies on mobile platforms, especially for precise tasks like pressing
buttons or turning faucets. In this work, we formulate the policy mobilization
problem: find a mobile robot base pose in a novel environment that is in
distribution with respect to a manipulation policy trained on a limited set of
camera viewpoints. Compared to retraining the policy itself to be more robust
to unseen robot base pose initializations, policy mobilization decouples
navigation from manipulation and thus does not require additional
demonstrations. Crucially, this problem formulation complements existing
efforts to improve manipulation policy robustness to novel viewpoints and
remains compatible with them. To study policy mobilization, we introduce the
Mobi-$\pi$ framework, which includes: (1) metrics that quantify the difficulty
of mobilizing a given policy, (2) a suite of simulated mobile manipulation
tasks based on RoboCasa to evaluate policy mobilization, (3) visualization
tools for analysis, and (4) several baseline methods. We also propose a novel
approach that bridges navigation and manipulation by optimizing the robot's
base pose to align with an in-distribution base pose for a learned policy. Our
approach utilizes 3D Gaussian Splatting for novel view synthesis, a score
function to evaluate pose suitability, and sampling-based optimization to
identify optimal robot poses. We show that our approach outperforms baselines
in both simulation and real-world environments, demonstrating its effectiveness
for policy mobilization.","Jingyun Yang, Isabella Huang, Brandon Vu, Max Bajracharya, Rika Antonova, Jeannette Bohg",2025-05-29T17:27:54Z,2025-05-29T17:27:54Z,http://arxiv.org/abs/2505.23692v1,http://arxiv.org/pdf/2505.23692v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"ManiTaskGen: A Comprehensive Task Generator for Benchmarking and
  Improving Vision-Language Agents on Embodied Decision-Making","Building embodied agents capable of accomplishing arbitrary tasks is a core
objective towards achieving embodied artificial general intelligence (E-AGI).
While recent work has advanced such general robot policies, their training and
evaluation are often limited to tasks within specific scenes, involving
restricted instructions and scenarios. Existing benchmarks also typically rely
on manual annotation of limited tasks in a few scenes. We argue that exploring
the full spectrum of feasible tasks within any given scene is crucial, as they
provide both extensive benchmarks for evaluation and valuable resources for
agent improvement. Towards this end, we introduce ManiTaskGen, a novel system
that automatically generates comprehensive, diverse, feasible mobile
manipulation tasks for any given scene. The generated tasks encompass both
process-based, specific instructions (e.g., ""move object from X to Y"") and
outcome-based, abstract instructions (e.g., ""clear the table""). We apply
ManiTaskGen to both simulated and real-world scenes, demonstrating the validity
and diversity of the generated tasks. We then leverage these tasks to
automatically construct benchmarks, thoroughly evaluating the embodied
decision-making capabilities of agents built upon existing vision-language
models (VLMs). Furthermore, we propose a simple yet effective method that
utilizes ManiTaskGen tasks to enhance embodied decision-making. Overall, this
work presents a universal task generation framework for arbitrary scenes,
facilitating both benchmarking and improvement of embodied decision-making
agents.","Liu Dai, Haina Wang, Weikang Wan, Hao Su",2025-05-27T05:14:50Z,2025-07-29T02:51:37Z,http://arxiv.org/abs/2505.20726v2,http://arxiv.org/pdf/2505.20726v2.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Sketch Interface for Teleoperation of Mobile Manipulator to Enable
  Intuitive and Intended Operation: A Proof of Concept","Recent advancements in robotics have underscored the need for effective
collaboration between humans and robots. Traditional interfaces often struggle
to balance robot autonomy with human oversight, limiting their practical
application in complex tasks like mobile manipulation. This study aims to
develop an intuitive interface that enables a mobile manipulator to
autonomously interpret user-provided sketches, enhancing user experience while
minimizing burden. We implemented a web-based application utilizing machine
learning algorithms to process sketches, making the interface accessible on
mobile devices for use anytime, anywhere, by anyone. In the first validation,
we examined natural sketches drawn by users for 27 selected manipulation and
navigation tasks, gaining insights into trends related to sketch instructions.
The second validation involved comparative experiments with five grasping
tasks, showing that the sketch interface reduces workload and enhances
intuitiveness compared to conventional axis control interfaces. These findings
suggest that the proposed sketch interface improves the efficiency of mobile
manipulators and opens new avenues for integrating intuitive human-robot
collaboration in various applications.","Yuka Iwanaga, Masayoshi Tsuchinaga, Kosei Tanada, Yuji Nakamura, Takemitsu Mori, Takashi Yamamoto",2025-05-20T04:56:14Z,2025-05-21T04:30:19Z,http://arxiv.org/abs/2505.13931v2,http://arxiv.org/pdf/2505.13931v2.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Granular Loco-Manipulation: Repositioning Rocks Through Strategic Sand
  Avalanche","Legged robots have the potential to leverage obstacles to climb steep sand
slopes. However, efficiently repositioning these obstacles to desired locations
is challenging. Here we present DiffusiveGRAIN, a learning-based method that
enables a multi-legged robot to strategically induce localized sand avalanches
during locomotion and indirectly manipulate obstacles. We conducted 375 trials,
systematically varying obstacle spacing, robot orientation, and leg actions in
75 of them. Results show that the movement of closely-spaced obstacles exhibits
significant interference, requiring joint modeling. In addition, different
multi-leg excavation actions could cause distinct robot state changes,
necessitating integrated planning of manipulation and locomotion. To address
these challenges, DiffusiveGRAIN includes a diffusion-based environment
predictor to capture multi-obstacle movements under granular flow interferences
and a robot state predictor to estimate changes in robot state from multi-leg
action patterns. Deployment experiments (90 trials) demonstrate that by
integrating the environment and robot state predictors, the robot can
autonomously plan its movements based on loco-manipulation goals, successfully
shifting closely located rocks to desired locations in over 65% of trials. Our
study showcases the potential for a locomoting robot to strategically
manipulate obstacles to achieve improved mobility on challenging terrains.","Haodi Hu, Yue Wu, Feifei Qian, Daniel Seita",2025-05-19T10:17:03Z,2025-05-19T10:17:03Z,http://arxiv.org/abs/2505.12934v1,http://arxiv.org/pdf/2505.12934v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
Guardian Positioning System (GPS) for Location Based Services,"Location-based service (LBS) applications proliferate and support
transportation, entertainment, and more. Modern mobile platforms, with
smartphones being a prominent example, rely on terrestrial and satellite
infrastructures (e.g., global navigation satellite system (GNSS) and
crowdsourced Wi-Fi, Bluetooth, cellular, and IP databases) for correct
positioning. However, they are vulnerable to attacks that manipulate positions
to control and undermine LBS functionality -- thus enabling the scamming of
users or services. Our work reveals that GNSS spoofing attacks succeed even
though smartphones have multiple sources of positioning information. Moreover,
that Wi-Fi spoofing attacks with GNSS jamming are surprisingly effective. More
concerning is the evidence that sophisticated, coordinated spoofing attacks are
highly effective. Attacks can target GNSS in combination with other positioning
methods, thus defenses that assume that only GNSS is under attack cannot be
effective. More so, resilient GNSS receivers and special-purpose antennas are
not feasible on smartphones. To address this gap, we propose an extended
receiver autonomous integrity monitoring (RAIM) framework that leverages the
readily available, redundant, often so-called opportunistic positioning
information on off-the-shelf platforms. We jointly use onboard sensors,
terrestrial infrastructures, and GNSS. We show that our extended RAIM framework
improves resilience against location spoofing, e.g., achieving a detection
accuracy improvement of up to 24-58% compared to the state-of-the-art
algorithms and location providers; detecting attacks within 5 seconds, with a
low false positive rate.","Wenjie Liu, Panos Papadimitratos",2025-05-14T19:06:13Z,2025-05-14T19:06:13Z,http://arxiv.org/abs/2505.09743v1,http://arxiv.org/pdf/2505.09743v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Self-Supervised Federated GNSS Spoofing Detection with Opportunistic
  Data","Global navigation satellite systems (GNSS) are vulnerable to spoofing
attacks, with adversarial signals manipulating the location or time information
of receivers, potentially causing severe disruptions. The task of discerning
the spoofing signals from benign ones is naturally relevant for machine
learning, thus recent interest in applying it for detection. While deep
learning-based methods are promising, they require extensive labeled datasets,
consume significant computational resources, and raise privacy concerns due to
the sensitive nature of position data. This is why this paper proposes a
self-supervised federated learning framework for GNSS spoofing detection. It
consists of a cloud server and local mobile platforms. Each mobile platform
employs a self-supervised anomaly detector using long short-term memory (LSTM)
networks. Labels for training are generated locally through a
spoofing-deviation prediction algorithm, ensuring privacy. Local models are
trained independently, and only their parameters are uploaded to the cloud
server, which aggregates them into a global model using FedAvg. The updated
global model is then distributed back to the mobile platforms and trained
iteratively. The evaluation shows that our self-supervised federated learning
framework outperforms position-based and deep learning-based methods in
detecting spoofing attacks while preserving data privacy.","Wenjie Liu, Panos Papadimitratos",2025-05-09T16:22:44Z,2025-05-09T16:22:44Z,http://arxiv.org/abs/2505.06171v1,http://arxiv.org/pdf/2505.06171v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"MORE: Mobile Manipulation Rearrangement Through Grounded Language
  Reasoning","Autonomous long-horizon mobile manipulation encompasses a multitude of
challenges, including scene dynamics, unexplored areas, and error recovery.
Recent works have leveraged foundation models for scene-level robotic reasoning
and planning. However, the performance of these methods degrades when dealing
with a large number of objects and large-scale environments. To address these
limitations, we propose MORE, a novel approach for enhancing the capabilities
of language models to solve zero-shot mobile manipulation planning for
rearrangement tasks. MORE leverages scene graphs to represent environments,
incorporates instance differentiation, and introduces an active filtering
scheme that extracts task-relevant subgraphs of object and region instances.
These steps yield a bounded planning problem, effectively mitigating
hallucinations and improving reliability. Additionally, we introduce several
enhancements that enable planning across both indoor and outdoor environments.
We evaluate MORE on 81 diverse rearrangement tasks from the BEHAVIOR-1K
benchmark, where it becomes the first approach to successfully solve a
significant share of the benchmark, outperforming recent foundation model-based
approaches. Furthermore, we demonstrate the capabilities of our approach in
several complex real-world tasks, mimicking everyday activities. We make the
code publicly available at https://more-model.cs.uni-freiburg.de.","Mohammad Mohammadi, Daniel Honerkamp, Martin Büchner, Matteo Cassinelli, Tim Welschehold, Fabien Despinoy, Igor Gilitschenski, Abhinav Valada",2025-05-05T21:26:03Z,2025-05-05T21:26:03Z,http://arxiv.org/abs/2505.03035v1,http://arxiv.org/pdf/2505.03035v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"J-PARSE: Jacobian-based Projection Algorithm for Resolving Singularities
  Effectively in Inverse Kinematic Control of Serial Manipulators","J-PARSE is a method for smooth first-order inverse kinematic control of a
serial manipulator near kinematic singularities. The commanded end-effector
velocity is interpreted component-wise, according to the available mobility in
each dimension of the task space. First, a substitute ""Safety"" Jacobian matrix
is created, keeping the aspect ratio of the manipulability ellipsoid above a
threshold value. The desired motion is then projected onto non-singular and
singular directions, and the latter projection scaled down by a factor informed
by the threshold value. A right-inverse of the non-singular Safety Jacobian is
applied to the modified command. In the absence of joint limits and collisions,
this ensures smooth transition into and out of low-rank poses, guaranteeing
asymptotic stability for target poses within the workspace, and stability for
those outside. Velocity control with J-PARSE is benchmarked against the
Least-Squares and Damped Least-Squares inversions of the Jacobian, and shows
high accuracy in reaching and leaving singular target poses. By expanding the
available workspace of manipulators, the method finds applications in servoing,
teleoperation, and learning. Videos and code are available at
https://jparse-manip.github.io/.","Shivani Guptasarma, Matthew Strong, Honghao Zhen, Monroe Kennedy III",2025-05-01T04:58:50Z,2025-07-18T22:46:57Z,http://arxiv.org/abs/2505.00306v3,http://arxiv.org/pdf/2505.00306v3.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"NMPC-based Unified Posture Manipulation and Thrust Vectoring for Agile
  and Fault-Tolerant Flight of a Morphing Aerial Robot","This thesis presents a unified control framework for agile and fault-tolerant
flight of the Multi-Modal Mobility Morphobot (M4) in aerial mode. The M4 robot
is capable of transitioning between ground and aerial locomotion. The
articulated legs enable more dynamic maneuvers than a standard quadrotor
platform. A nonlinear model predictive control (NMPC) approach is developed to
simultaneously plan posture manipulation and thrust vectoring actions, allowing
the robot to execute sharp turns and dynamic flight trajectories. The framework
integrates an agile and fault-tolerant control logic that enables precise
tracking under aggressive maneuvers while compensating for actuator failures,
ensuring continued operation without significant performance degradation.
Simulation results validate the effectiveness of the proposed method,
demonstrating accurate trajectory tracking and robust recovery from faults,
contributing to resilient autonomous flight in complex environments.",Shashwat Pandya,2025-04-29T00:33:46Z,2025-04-29T00:33:46Z,http://arxiv.org/abs/2504.20326v1,http://arxiv.org/pdf/2504.20326v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
Immersive Teleoperation Framework for Locomanipulation Tasks,"Recent advancements in robotic loco-manipulation have leveraged Virtual
Reality (VR) to enhance the precision and immersiveness of teleoperation
systems, significantly outperforming traditional methods reliant on 2D camera
feeds and joystick controls. Despite these advancements, challenges remain,
particularly concerning user experience across different setups. This paper
introduces a novel VR-based teleoperation framework designed for a robotic
manipulator integrated onto a mobile platform. Central to our approach is the
application of Gaussian splatting, a technique that abstracts the manipulable
scene into a VR environment, thereby enabling more intuitive and immersive
interactions. Users can navigate and manipulate within the virtual scene as if
interacting with a real robot, enhancing both the engagement and efficacy of
teleoperation tasks. An extensive user study validates our approach,
demonstrating significant usability and efficiency improvements. Two-thirds
(66%) of participants completed tasks faster, achieving an average time
reduction of 43%. Additionally, 93% preferred the Gaussian Splat interface
overall, with unanimous (100%) recommendations for future use, highlighting
improvements in precision, responsiveness, and situational awareness. Finally,
we demonstrate the effectiveness of our framework through real-world
experiments in two distinct application scenarios, showcasing the practical
capabilities and versatility of the Splat-based VR interface.","Takuya Boehringer, Jonathan Embley-Riches, Karim Hammoud, Valerio Modugno, Dimitrios Kanoulas",2025-04-21T17:00:31Z,2025-04-21T17:00:31Z,http://arxiv.org/abs/2504.15229v1,http://arxiv.org/pdf/2504.15229v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Multi-Sensor Fusion-Based Mobile Manipulator Remote Control for
  Intelligent Smart Home Assistance","This paper proposes a wearable-controlled mobile manipulator system for
intelligent smart home assistance, integrating MEMS capacitive microphones, IMU
sensors, vibration motors, and pressure feedback to enhance human-robot
interaction. The wearable device captures forearm muscle activity and converts
it into real-time control signals for mobile manipulation. The wearable device
achieves an offline classification accuracy of 88.33\%\ across six distinct
movement-force classes for hand gestures by using a CNN-LSTM model, while
real-world experiments involving five participants yield a practical accuracy
of 83.33\%\ with an average system response time of 1.2 seconds. In Human-Robot
synergy in navigation and grasping tasks, the robot achieved a 98\%\ task
success rate with an average trajectory deviation of only 3.6 cm. Finally, the
wearable-controlled mobile manipulator system achieved a 93.3\%\ gripping
success rate, a transfer success of 95.6\%\, and a full-task success rate of
91.1\%\ during object grasping and transfer tests, in which a total of 9
object-texture combinations were evaluated. These three experiments' results
validate the effectiveness of MEMS-based wearable sensing combined with
multi-sensor fusion for reliable and intuitive control of assistive robots in
smart home scenarios.","Xiao Jin, Bo Xiao, Huijiang Wang, Wendong Wang, Zhenhua Yu",2025-04-17T22:33:38Z,2025-04-17T22:33:38Z,http://arxiv.org/abs/2504.13370v1,http://arxiv.org/pdf/2504.13370v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
B*: Efficient and Optimal Base Placement for Fixed-Base Manipulators,"B* is a novel optimization framework that addresses a critical challenge in
fixed-base manipulator robotics: optimal base placement. Current methods rely
on pre-computed kinematics databases generated through sampling to search for
solutions. However, they face an inherent trade-off between solution optimality
and computational efficiency when determining sampling resolution. To address
these limitations, B* unifies multiple objectives without database dependence.
The framework employs a two-layer hierarchical approach. The outer layer
systematically manages terminal constraints through progressive tightening,
particularly for base mobility, enabling feasible initialization and broad
solution exploration. The inner layer addresses non-convexities in each
outer-layer subproblem through sequential local linearization, converting the
original problem into tractable sequential linear programming (SLP). Testing
across multiple robot platforms demonstrates B*'s effectiveness. The framework
achieves solution optimality five orders of magnitude better than
sampling-based approaches while maintaining perfect success rates and reduced
computational overhead. Operating directly in configuration space, B* enables
simultaneous path planning with customizable optimization criteria. B* serves
as a crucial initialization tool that bridges the gap between theoretical
motion planning and practical deployment, where feasible trajectory existence
is fundamental.","Zihang Zhao, Leiyao Cui, Sirui Xie, Saiyao Zhang, Zhi Han, Lecheng Ruan, Yixin Zhu",2025-04-17T07:48:50Z,2025-08-22T12:38:13Z,http://arxiv.org/abs/2504.12719v2,http://arxiv.org/pdf/2504.12719v2.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Real-Time Shape Estimation of Tensegrity Structures Using Strut
  Inclination Angles","Tensegrity structures are becoming widely used in robotics, such as
continuously bending soft manipulators and mobile robots to explore unknown and
uneven environments dynamically. Estimating their shape, which is the
foundation of their state, is essential for establishing control. However,
on-board sensor-based shape estimation remains difficult despite its
importance, because tensegrity structures lack well-defined joints, which makes
it challenging to use conventional angle sensors such as potentiometers or
encoders for shape estimation. To our knowledge, no existing work has
successfully achieved shape estimation using only onboard sensors such as
Inertial Measurement Units (IMUs). This study addresses this issue by proposing
a novel approach that uses energy minimization to estimate the shape. We
validated our method through experiments on a simple Class 1 tensegrity
structure, and the results show that the proposed algorithm can estimate the
real-time shape of the structure using onboard sensors, even in the presence of
external disturbances.","Tufail Ahmad Bhat, Yuhei Yoshimitsu, Kazuki Wada, Shuhei Ikemoto",2025-04-16T08:44:06Z,2025-04-16T08:44:06Z,http://arxiv.org/abs/2504.11868v1,http://arxiv.org/pdf/2504.11868v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Leveraging Passive Compliance of Soft Robotics for Physical Human-Robot
  Collaborative Manipulation","This work represents an initial benchmark of a large-scale soft robot
performing physical, collaborative manipulation of a long, extended object with
a human partner. The robot consists of a pneumatically-actuated, three-link
continuum soft manipulator mounted to an omni-directional mobile base. The
system level configuration of the robot and design of the collaborative
manipulation (co-manipulation) study are presented. The initial results, both
quantitative and qualitative, are directly compared to previous similar
human-human co-manipulation studies. These initial results show promise in the
ability for large-scale soft robots to perform comparably to human partners
acting as non-visual followers in a co-manipulation task. Furthermore, these
results challenge traditional soft robot strength limitations and indicate
potential for applications requiring strength and adaptability.","Dallin L. Cordon, Shaden Moss, Marc Killpack, John L. Salmon",2025-04-11T00:54:10Z,2025-04-11T00:54:10Z,http://arxiv.org/abs/2504.08184v1,http://arxiv.org/pdf/2504.08184v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Energy Efficient Planning for Repetitive Heterogeneous Tasks in
  Precision Agriculture","Robotic weed removal in precision agriculture introduces a repetitive
heterogeneous task planning (RHTP) challenge for a mobile manipulator. RHTP has
two unique characteristics: 1) an observe-first-and-manipulate-later (OFML)
temporal constraint that forces a unique ordering of two different tasks for
each target and 2) energy savings from efficient task collocation to minimize
unnecessary movements. RHTP can be framed as a stochastic renewal process.
According to the Renewal Reward Theorem, the expected energy usage per task
cycle is the long-run average. Traditional task and motion planning focuses on
feasibility rather than optimality due to the unknown object and obstacle
position prior to execution. However, the known target/obstacle distribution in
precision agriculture allows minimizing the expected energy usage. For each
instance in this renewal process, we first compute task space partition, a
novel data structure that computes all possibilities of task multiplexing and
its probabilities with robot reachability. Then we propose a region-based
set-coverage problem to formulate the RHTP as a mixed-integer nonlinear
programming. We have implemented and solved RHTP using Branch-and-Bound solver.
Compared to a baseline in simulations based on real field data, the results
suggest a significant improvement in path length, number of robot stops,
overall energy usage, and number of replans.","Shuangyu Xie, Ken Goldberg, Dezhen Song",2025-04-04T21:09:17Z,2025-04-04T21:09:17Z,http://arxiv.org/abs/2504.03938v1,http://arxiv.org/pdf/2504.03938v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Seeing is Believing: Belief-Space Planning with Foundation Models as
  Uncertainty Estimators","Generalizable robotic mobile manipulation in open-world environments poses
significant challenges due to long horizons, complex goals, and partial
observability. A promising approach to address these challenges involves
planning with a library of parameterized skills, where a task planner sequences
these skills to achieve goals specified in structured languages, such as
logical expressions over symbolic facts. While vision-language models (VLMs)
can be used to ground these expressions, they often assume full observability,
leading to suboptimal behavior when the agent lacks sufficient information to
evaluate facts with certainty. This paper introduces a novel framework that
leverages VLMs as a perception module to estimate uncertainty and facilitate
symbolic grounding. Our approach constructs a symbolic belief representation
and uses a belief-space planner to generate uncertainty-aware plans that
incorporate strategic information gathering. This enables the agent to
effectively reason about partial observability and property uncertainty. We
demonstrate our system on a range of challenging real-world tasks that require
reasoning in partially observable environments. Simulated evaluations show that
our approach outperforms both vanilla VLM-based end-to-end planning or
VLM-based state estimation baselines by planning for and executing strategic
information gathering. This work highlights the potential of VLMs to construct
belief-space symbolic scene representations, enabling downstream tasks such as
uncertainty-aware planning.","Linfeng Zhao, Willie McClinton, Aidan Curtis, Nishanth Kumar, Tom Silver, Leslie Pack Kaelbling, Lawson L. S. Wong",2025-04-04T07:48:53Z,2025-04-04T07:48:53Z,http://arxiv.org/abs/2504.03245v1,http://arxiv.org/pdf/2504.03245v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"NMPC-based Unified Posture Manipulation and Thrust Vectoring for Fault
  Recovery","Multi-rotors face significant risks, as actuator failures at high altitudes
can easily result in a crash and the robot's destruction. Therefore, rapid
fault recovery in the event of an actuator failure is necessary for the
fault-tolerant and safe operation of unmanned aerial robots. In this work, we
present a fault recovery approach based on the unification of posture
manipulation and thrust vectoring. The key contributions of this work are: 1)
Derivation of two flight dynamics models (high-fidelity and reduced-order) that
capture posture control and thrust vectoring. 2) Design of a controller based
on Nonlinear Model Predictive Control (NMPC) and demonstration of fault
recovery in simulation using a high-fidelity model of the Multi-Modal Mobility
Morphobot (M4) in Simscape.","Adarsh Salagame, Shashwat Pandya, Ioannis Mandralis, Eric Sihite, Alireza Ramezani, Morteza Gharib",2025-03-24T03:12:02Z,2025-03-24T03:12:02Z,http://arxiv.org/abs/2503.18307v1,http://arxiv.org/pdf/2503.18307v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
Two-qubit logic and teleportation with mobile spin qubits in silicon,"The scalability and power of quantum computing architectures depend
critically on high-fidelity operations and robust and flexible qubit
connectivity. In this respect, mobile qubits are particularly attractive as
they enable dynamic and reconfigurable qubit arrays. This approach allows
quantum processors to adapt their connectivity patterns during operation,
implement different quantum error correction codes on the same hardware, and
optimize resource utilization through dedicated functional zones for specific
operations like measurement or entanglement generation. Such flexibility also
relieves architectural constraints, as recently demonstrated in atomic systems
based on trapped ions and neutral atoms manipulated with optical tweezers. In
solid-state platforms, highly coherent shuttling of electron spins was recently
reported. A key outstanding question is whether it may be possible to perform
quantum gates directly on the mobile spins. In this work, we demonstrate
two-qubit operations between two electron spins carried towards each other in
separate traveling potential minima in a semiconductor device. We find that the
interaction strength is highly tunable by their spatial separation, achieving
an average two-qubit gate fidelity of about 99\%. Additionally, we implement
conditional post-selected quantum state teleportation between spatially
separated qubits with an average gate fidelity of 87\%, showcasing the
potential of mobile spin qubits for non-local quantum information processing.
We expect that operations on mobile qubits will become a universal feature of
future large-scale semiconductor quantum processors.","Yuta Matsumoto, Maxim De Smet, Larysa Tryputen, Sander L. de Snoo, Sergey V. Amitonov, Amir Sammak, Maximilian Rimbach-Russ, Giordano Scappucci, Lieven M. K. Vandersypen",2025-03-19T17:17:31Z,2025-03-19T17:17:31Z,http://arxiv.org/abs/2503.15434v1,http://arxiv.org/pdf/2503.15434v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Dexterous Control of an 11-DOF Redundant Robot for CT-Guided Needle
  Insertion With Task-Oriented Weighted Policies","Computed tomography (CT)-guided needle biopsies are critical for diagnosing a
range of conditions, including lung cancer, but present challenges such as
limited in-bore space, prolonged procedure times, and radiation exposure.
Robotic assistance offers a promising solution by improving needle trajectory
accuracy, reducing radiation exposure, and enabling real-time adjustments. In
our previous work, we introduced a redundant robotic platform designed for
dexterous needle insertion within the confined CT bore. However, its limited
base mobility restricts flexible deployment in clinical settings. In this
study, we present an improved 11-degree-of-freedom (DOF) robotic system that
integrates a 6-DOF robotic base with a 5-DOF cable-driven end-effector,
significantly enhancing workspace flexibility and precision. With the
hyper-redundant degrees of freedom, we introduce a weighted inverse kinematics
controller with a two-stage priority scheme for large-scale movement and fine
in-bore adjustments, along with a null-space control strategy to optimize
dexterity. We validate our system through both simulation and real-world
experiments, demonstrating superior tracking accuracy and enhanced
manipulability in CT-guided procedures. The study provides a strong case for
hyper-redundancy and null-space control formulations for robot-assisted needle
biopsy scenarios.","Peihan Zhang, Florian Richter, Ishan Duriseti, Albert Hsiao, Sean Tutton, Alexander Norbash, Michael Yip",2025-03-18T21:46:22Z,2025-05-29T18:58:27Z,http://arxiv.org/abs/2503.14753v2,http://arxiv.org/pdf/2503.14753v2.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"ShieldUp!: Inoculating Users Against Online Scams Using A Game Based
  Intervention","Online scams are a growing threat in India, impacting millions and causing
substantial financial losses year over year. This white paper presents
ShieldUp!, a novel mobile game prototype designed to inoculate users against
common online scams by leveraging the principles of psychological inoculation
theory. ShieldUp! exposes users to weakened versions of manipulation tactics
frequently used by scammers, and teaches them to recognize and pre-emptively
refute these techniques. A randomized controlled trial (RCT) with 3,000
participants in India was conducted to evaluate the game's efficacy in helping
users better identify scams scenarios. Participants were assigned to one of
three groups: the ShieldUp! group (play time: 15 min), a general scam awareness
group (watching videos and reading tips for 10-15 min), and a control group
(plays ""Chrome Dino"", an unrelated game, for 10 minutes). Scam discernment
ability was measured using a newly developed Scam Discernment Ability Test
(SDAT-10) before the intervention, immediately after, and at a 21-day
follow-up. Results indicated that participants who played ShieldUp! showed a
significant improvement in their ability to identify scams compared to both
control groups, and this improvement was maintained at follow-up. Importantly,
while both interventions initially led users to to show increased skepticism
towards even genuine online offers (NOT Scam scenarios), this effect dissipated
after 21 days, suggesting no long-term negative impact on user trust. This
study demonstrates the potential of game-based inoculation as a scalable and
effective scam prevention strategy, offering valuable insights for product
design, policy interventions, and future research, including the need for
longitudinal studies and cross-cultural adaptations.","Abhishek Roy, Narsi G, Sujata Mukherjee",2025-03-16T03:40:23Z,2025-03-16T03:40:23Z,http://arxiv.org/abs/2503.12341v1,http://arxiv.org/pdf/2503.12341v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Instruction-Augmented Long-Horizon Planning: Embedding Grounding
  Mechanisms in Embodied Mobile Manipulation","Enabling humanoid robots to perform long-horizon mobile manipulation planning
in real-world environments based on embodied perception and comprehension
abilities has been a longstanding challenge. With the recent rise of large
language models (LLMs), there has been a notable increase in the development of
LLM-based planners. These approaches either utilize human-provided textual
representations of the real world or heavily depend on prompt engineering to
extract such representations, lacking the capability to quantitatively
understand the environment, such as determining the feasibility of manipulating
objects. To address these limitations, we present the Instruction-Augmented
Long-Horizon Planning (IALP) system, a novel framework that employs LLMs to
generate feasible and optimal actions based on real-time sensor feedback,
including grounded knowledge of the environment, in a closed-loop interaction.
Distinct from prior works, our approach augments user instructions into PDDL
problems by leveraging both the abstract reasoning capabilities of LLMs and
grounding mechanisms. By conducting various real-world long-horizon tasks, each
consisting of seven distinct manipulatory skills, our results demonstrate that
the IALP system can efficiently solve these tasks with an average success rate
exceeding 80%. Our proposed method can operate as a high-level planner,
equipping robots with substantial autonomy in unstructured environments through
the utilization of multi-modal sensor inputs.","Fangyuan Wang, Shipeng Lyu, Peng Zhou, Anqing Duan, Guodong Guo, David Navarro-Alarcon",2025-03-11T06:37:33Z,2025-03-11T06:37:33Z,http://arxiv.org/abs/2503.08084v1,http://arxiv.org/pdf/2503.08084v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
Planning and Control for Deformable Linear Object Manipulation,"Manipulating a deformable linear object (DLO) such as wire, cable, and rope
is a common yet challenging task due to their high degrees of freedom and
complex deformation behaviors, especially in an environment with obstacles.
Existing local control methods are efficient but prone to failure in complex
scenarios, while precise global planners are computationally intensive and
difficult to deploy. This paper presents an efficient, easy-to-deploy framework
for collision-free DLO manipulation using mobile manipulators. We demonstrate
the effectiveness of leveraging standard planning tools for high-dimensional
DLO manipulation without requiring custom planners or extensive data-driven
models. Our approach combines an off-the-shelf global planner with a real-time
local controller. The global planner approximates the DLO as a series of rigid
links connected by spherical joints, enabling rapid path planning without the
need for problem-specific planners or large datasets. The local controller
employs control barrier functions (CBFs) to enforce safety constraints,
maintain the DLO integrity, prevent overstress, and handle obstacle avoidance.
It compensates for modeling inaccuracies by using a state-of-the-art
position-based dynamics technique that approximates physical properties like
Young's and shear moduli. We validate our framework through extensive
simulations and real-world demonstrations. In complex obstacle
scenarios-including tent pole transport, corridor navigation, and tasks
requiring varied stiffness-our method achieves a 100% success rate over
thousands of trials, with significantly reduced planning times compared to
state-of-the-art techniques. Real-world experiments include transportation of a
tent pole and a rope using mobile manipulators. We share our ROS-based
implementation to facilitate adoption in various applications.","Burak Aksoy, John Wen",2025-03-06T01:44:36Z,2025-03-06T01:44:36Z,http://arxiv.org/abs/2503.04007v1,http://arxiv.org/pdf/2503.04007v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"EVLoc: Event-based Visual Localization in LiDAR Maps via Event-Depth
  Registration","Event cameras are bio-inspired sensors with some notable features, including
high dynamic range and low latency, which makes them exceptionally suitable for
perception in challenging scenarios such as high-speed motion and extreme
lighting conditions. In this paper, we explore their potential for localization
within pre-existing LiDAR maps, a critical task for applications that require
precise navigation and mobile manipulation. Our framework follows a paradigm
based on the refinement of an initial pose. Specifically, we first project
LiDAR points into 2D space based on a rough initial pose to obtain depth maps,
and then employ an optical flow estimation network to align events with LiDAR
points in 2D space, followed by camera pose estimation using a PnP solver. To
enhance geometric consistency between these two inherently different
modalities, we develop a novel frame-based event representation that improves
structural clarity. Additionally, given the varying degrees of bias observed in
the ground truth poses, we design a module that predicts an auxiliary variable
as a regularization term to mitigate the impact of this bias on network
convergence. Experimental results on several public datasets demonstrate the
effectiveness of our proposed method. To facilitate future research, both the
code and the pre-trained models are made available online.","Kuangyi Chen, Jun Zhang, Friedrich Fraundorfer",2025-02-28T20:27:49Z,2025-02-28T20:27:49Z,http://arxiv.org/abs/2503.00167v1,http://arxiv.org/pdf/2503.00167v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"CSubBT: A Self-Adjusting Execution Framework for Mobile Manipulation
  System","With the advancements in modern intelligent technologies, mobile robots
equipped with manipulators are increasingly operating in unstructured
environments. These robots can plan sequences of actions for long-horizon tasks
based on perceived information. However, in practice, the planned actions often
fail due to discrepancies between the perceptual information used for planning
and the actual conditions. In this paper, we introduce the {\itshape
Conditional Subtree} (CSubBT), a general self-adjusting execution framework for
mobile manipulation tasks based on Behavior Trees (BTs). CSubBT decomposes
symbolic action into sub-actions and uses BTs to control their execution,
addressing any potential anomalies during the process. CSubBT treats common
anomalies as constraint non-satisfaction problems and continuously guides the
robot in performing tasks by sampling new action parameters in the constraint
space when anomalies are detected. We demonstrate the robustness of our
framework through extensive manipulation experiments on different platforms,
both in simulation and real-world settings.","Huihui Guo, Huizhang Luo, Huilong Pi, Mingxing Duan, Kenli Li, Chubo Liu",2025-02-28T06:46:10Z,2025-02-28T06:46:10Z,http://arxiv.org/abs/2502.20771v1,http://arxiv.org/pdf/2502.20771v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Evaluating the Robustness of Multimodal Agents Against Active
  Environmental Injection Attacks","As researchers continue to optimize AI agents for more effective task
execution within operating systems, they often overlook a critical security
concern: the ability of these agents to detect ""impostors"" within their
environment. Through an analysis of the agents' operational context, we
identify a significant threat-attackers can disguise malicious attacks as
environmental elements, injecting active disturbances into the agents'
execution processes to manipulate their decision-making. We define this novel
threat as the Active Environment Injection Attack (AEIA). Focusing on the
interaction mechanisms of the Android OS, we conduct a risk assessment of AEIA
and identify two critical security vulnerabilities: (1) Adversarial content
injection in multimodal interaction interfaces, where attackers embed
adversarial instructions within environmental elements to mislead agent
decision-making; and (2) Reasoning gap vulnerabilities in the agent's task
execution process, which increase susceptibility to AEIA attacks during
reasoning. To evaluate the impact of these vulnerabilities, we propose AEIA-MN,
an attack scheme that exploits interaction vulnerabilities in mobile operating
systems to assess the robustness of MLLM-based agents. Experimental results
show that even advanced MLLMs are highly vulnerable to this attack, achieving a
maximum attack success rate of 93% on the AndroidWorld benchmark by combining
two vulnerabilities.","Yurun Chen, Xavier Hu, Keting Yin, Juncheng Li, Shengyu Zhang",2025-02-18T17:01:28Z,2025-08-06T07:04:43Z,http://arxiv.org/abs/2502.13053v3,http://arxiv.org/pdf/2502.13053v3.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"RobotIQ: Empowering Mobile Robots with Human-Level Planning for
  Real-World Execution","This paper introduces RobotIQ, a framework that empowers mobile robots with
human-level planning capabilities, enabling seamless communication via natural
language instructions through any Large Language Model. The proposed framework
is designed in the ROS architecture and aims to bridge the gap between humans
and robots, enabling robots to comprehend and execute user-expressed text or
voice commands. Our research encompasses a wide spectrum of robotic tasks,
ranging from fundamental logical, mathematical, and learning reasoning for
transferring knowledge in domains like navigation, manipulation, and object
localization, enabling the application of learned behaviors from simulated
environments to real-world operations. All encapsulated within a modular
crafted robot library suite of API-wise control functions, RobotIQ offers a
fully functional AI-ROS-based toolset that allows researchers to design and
develop their own robotic actions tailored to specific applications and robot
configurations. The effectiveness of the proposed system was tested and
validated both in simulated and real-world experiments focusing on a home
service scenario that included an assistive application designed for elderly
people. RobotIQ with an open-source, easy-to-use, and adaptable robotic library
suite for any robot can be found at https://github.com/emmarapt/RobotIQ.","Emmanuel K. Raptis, Athanasios Ch. Kapoutsis, Elias B. Kosmatopoulos",2025-02-18T13:49:28Z,2025-02-18T13:49:28Z,http://arxiv.org/abs/2502.12862v1,http://arxiv.org/pdf/2502.12862v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"A Machine Learning Approach to Sensor Substitution from Tactile Sensing
  to Visual Perception for Non-Prehensile Manipulation","Mobile manipulators are increasingly deployed in complex environments,
requiring diverse sensors to perceive and interact with their surroundings.
However, equipping every robot with every possible sensor is often impractical
due to cost and physical constraints. A critical challenge arises when robots
with differing sensor capabilities need to collaborate or perform similar
tasks. For example, consider a scenario where a mobile manipulator equipped
with high-resolution tactile skin is skilled at non-prehensile manipulation
tasks like pushing. If this robot needs to be replaced or augmented by a robot
lacking such tactile sensing, the learned manipulation policies become
inapplicable. This paper addresses the problem of sensor substitution in
non-prehensile manipulation. We propose a novel machine learning-based
framework that enables a robot with a limited sensor set (e.g., LiDAR or RGB-D)
to effectively perform tasks previously reliant on a richer sensor suite (e.g.,
tactile skin). Our approach learns a mapping between the available sensor data
and the information provided by the substituted sensor, effectively
synthesizing the missing sensory input. Specifically, we demonstrate the
efficacy of our framework by training a model to substitute tactile skin data
for the task of non-prehensile pushing using a mobile manipulator. We show that
a manipulator equipped only with LiDAR or RGB-D can, after training, achieve
comparable and sometimes even better pushing performance to a mobile base
utilizing direct tactile feedback.","Idil Ozdamar, Doganay Sirintuna, Arash Ajoudani",2025-02-13T11:15:37Z,2025-06-09T09:53:34Z,http://arxiv.org/abs/2502.09180v3,http://arxiv.org/pdf/2502.09180v3.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
Bilevel Learning for Bilevel Planning,"A robot that learns from demonstrations should not just imitate what it sees
-- it should understand the high-level concepts that are being demonstrated and
generalize them to new tasks. Bilevel planning is a hierarchical model-based
approach where predicates (relational state abstractions) can be leveraged to
achieve compositional generalization. However, previous bilevel planning
approaches depend on predicates that are either hand-engineered or restricted
to very simple forms, limiting their scalability to sophisticated,
high-dimensional state spaces. To address this limitation, we present IVNTR,
the first bilevel planning approach capable of learning neural predicates
directly from demonstrations. Our key innovation is a neuro-symbolic bilevel
learning framework that mirrors the structure of bilevel planning. In IVNTR,
symbolic learning of the predicate ""effects"" and neural learning of the
predicate ""functions"" alternate, with each providing guidance for the other. We
evaluate IVNTR in six diverse robot planning domains, demonstrating its
effectiveness in abstracting various continuous and high-dimensional states.
While most existing approaches struggle to generalize (with <35% success rate),
our IVNTR achieves an average of 77% success rate on unseen tasks.
Additionally, we showcase IVNTR on a mobile manipulator, where it learns to
perform real-world mobile manipulation tasks and generalizes to unseen test
scenarios that feature new objects, new states, and longer task horizons. Our
findings underscore the promise of learning and planning with abstractions as a
path towards high-level generalization.","Bowen Li, Tom Silver, Sebastian Scherer, Alexander Gray",2025-02-12T18:59:56Z,2025-05-11T15:19:44Z,http://arxiv.org/abs/2502.08697v3,http://arxiv.org/pdf/2502.08697v3.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
Implicit Communication in Human-Robot Collaborative Transport,"We focus on human-robot collaborative transport, in which a robot and a user
collaboratively move an object to a goal pose. In the absence of explicit
communication, this problem is challenging because it demands tight implicit
coordination between two heterogeneous agents, who have very different sensing,
actuation, and reasoning capabilities. Our key insight is that the two agents
can coordinate fluently by encoding subtle, communicative signals into actions
that affect the state of the transported object. To this end, we design an
inference mechanism that probabilistically maps observations of joint actions
executed by the two agents to a set of joint strategies of workspace traversal.
Based on this mechanism, we define a cost representing the human's uncertainty
over the unfolding traversal strategy and introduce it into a model predictive
controller that balances between uncertainty minimization and efficiency
maximization. We deploy our framework on a mobile manipulator (Hello Robot
Stretch) and evaluate it in a within-subjects lab study (N=24). We show that
our framework enables greater team performance and empowers the robot to be
perceived as a significantly more fluent and competent partner compared to
baselines lacking a communicative mechanism.","Elvin Yang, Christoforos Mavrogiannis",2025-02-05T16:39:26Z,2025-02-05T16:39:26Z,http://arxiv.org/abs/2502.03346v1,http://arxiv.org/pdf/2502.03346v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
Dueling QR Codes: The Hyding of Dr. Jeckyl,"The paper presents a novel technique for encoding dual messages within
standard Quick Response (QR) codes through precise half-pixel module splitting.
This work challenges fundamental assumptions about deterministic decoding in
the ISO/IEC 18004:2015 standard while maintaining complete compatibility with
existing QR infrastructure. The proposed two-dimensional barcode attack enables
angle-dependent message selection while maintaining compatibility with
unmodified QR readers and the 100 million US mobile users who use their phone's
built-in scanners. Unlike previous approaches that rely on nested codes,
watermarking, or error correction exploitation, our method achieves true
one-to-many mapping by manipulating the physical sampling process built into
the QR standard. By preserving critical function patterns while bifurcating
data modules, we create automated codes that produce different but valid
readings based on camera viewing angle. Experimental results demonstrate
successful implementation across multiple use cases, including simple message
text pairs, complex URLs (nsa.gov/nasa.gov), and security test patterns for
malware and spam detectors (EICAR/GTUBE). Our technique achieves reliable
dual-message decoding using standard QR readers at module scales of 9-11
pixels, with successful angle-dependent reading demonstrated across vertical,
horizontal, and diagonal orientations. The method's success suggests potential
applications beyond QR code phishing ('quishing') including two-factor
authentication, anti-counterfeiting, and information density optimization. The
half-pixel technique may offer future avenues for similar implementations in
other 2D barcode formats such as Data Matrix and Aztec Code.","David Noever, Forrest McKee",2025-02-04T23:09:21Z,2025-02-04T23:09:21Z,http://arxiv.org/abs/2503.13458v1,http://arxiv.org/pdf/2503.13458v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Embrace Collisions: Humanoid Shadowing for Deployable Contact-Agnostics
  Motions","Previous humanoid robot research works treat the robot as a bipedal mobile
manipulation platform, where only the feet and hands contact the environment.
However, we humans use all body parts to interact with the world, e.g., we sit
in chairs, get up from the ground, or roll on the floor. Contacting the
environment using body parts other than feet and hands brings significant
challenges in both model-predictive control and reinforcement learning-based
methods. An unpredictable contact sequence makes it almost impossible for
model-predictive control to plan ahead in real time. The success of the
zero-shot sim-to-real reinforcement learning method for humanoids heavily
depends on the acceleration of GPU-based rigid-body physical simulator and
simplification of the collision detection. Lacking extreme torso movement of
the humanoid research makes all other components non-trivial to design, such as
termination conditions, motion commands and reward designs. To address these
potential challenges, we propose a general humanoid motion framework that takes
discrete motion commands and controls the robot's motor action in real time.
Using a GPU-accelerated rigid-body simulator, we train a humanoid whole-body
control policy that follows the high-level motion command in the real world in
real time, even with stochastic contacts and extremely large robot base
rotation and not-so-feasible motion command. More details at
https://project-instinct.github.io","Ziwen Zhuang, Hang Zhao",2025-02-03T15:57:54Z,2025-02-03T15:57:54Z,http://arxiv.org/abs/2502.01465v1,http://arxiv.org/pdf/2502.01465v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Mobile Manipulation Instruction Generation from Multiple Images with
  Automatic Metric Enhancement","We consider the problem of generating free-form mobile manipulation
instructions based on a target object image and receptacle image. Conventional
image captioning models are not able to generate appropriate instructions
because their architectures are typically optimized for single-image. In this
study, we propose a model that handles both the target object and receptacle to
generate free-form instruction sentences for mobile manipulation tasks.
Moreover, we introduce a novel training method that effectively incorporates
the scores from both learning-based and n-gram based automatic evaluation
metrics as rewards. This method enables the model to learn the co-occurrence
relationships between words and appropriate paraphrases. Results demonstrate
that our proposed method outperforms baseline methods including representative
multimodal large language models on standard automatic evaluation metrics.
Moreover, physical experiments reveal that using our method to augment data on
language instructions improves the performance of an existing multimodal
language understanding model for mobile manipulation.","Kei Katsumata, Motonari Kambara, Daichi Yashima, Ryosuke Korekata, Komei Sugiura",2025-01-28T15:39:07Z,2025-01-28T15:39:07Z,http://arxiv.org/abs/2501.17022v1,http://arxiv.org/pdf/2501.17022v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Efficient Phishing URL Detection Using Graph-based Machine Learning and
  Loopy Belief Propagation","The proliferation of mobile devices and online interactions have been
threatened by different cyberattacks, where phishing attacks and malicious
Uniform Resource Locators (URLs) pose significant risks to user security.
Traditional phishing URL detection methods primarily rely on URL string-based
features, which attackers often manipulate to evade detection. To address these
limitations, we propose a novel graph-based machine learning model for phishing
URL detection, integrating both URL structure and network-level features such
as IP addresses and authoritative name servers. Our approach leverages Loopy
Belief Propagation (LBP) with an enhanced convergence strategy to enable
effective message passing and stable classification in the presence of complex
graph structures. Additionally, we introduce a refined edge potential mechanism
that dynamically adapts based on entity similarity and label relationships to
further improve classification accuracy. Comprehensive experiments on
real-world datasets demonstrate our model's effectiveness by achieving F1 score
of up to 98.77\%. This robust and reproducible method advances phishing
detection capabilities, offering enhanced reliability and valuable insights in
the field of cybersecurity.","Wenye Guo, Qun Wang, Hao Yue, Haijian Sun, Rose Qingyang Hu",2025-01-12T19:49:00Z,2025-01-12T19:49:00Z,http://arxiv.org/abs/2501.06912v1,http://arxiv.org/pdf/2501.06912v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"FRESHR-GSI: A Generalized Safety Model and Evaluation Framework for
  Mobile Robots in Multi-Human Environments","Human safety is critical in applications involving close human-robot
interactions (HRI) and is a key aspect of physical compatibility between humans
and robots. While measures of human safety in HRI exist, these mainly target
industrial settings involving robotic manipulators. Less attention has been
paid to settings where mobile robots and humans share the space. This paper
introduces a new robot-centered directional framework of human safety. It is
particularly useful for evaluating mobile robots as they operate in
environments populated by multiple humans. The framework integrates several key
metrics, such as each human's relative distance, speed, and orientation. The
core novelty lies in the framework's flexibility to accommodate different
application requirements while allowing for both the robot-centered and
external observer points of view. We instantiate the framework by using RGB-D
based vision integrated with a deep learning-based human detection pipeline to
yield a generalized safety index (GSI) that instantaneously assesses human
safety. We evaluate GSI's capability of producing appropriate, robust, and
fine-grained safety measures in real-world experimental scenarios and compare
its performance with extant safety models.","Pranav Pandey, Ramviyas Parasuraman, Prashant Doshi",2025-01-07T01:51:12Z,2025-01-07T01:51:12Z,http://arxiv.org/abs/2501.03467v1,http://arxiv.org/pdf/2501.03467v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"Sim-to-Real Transfer for Mobile Robots with Reinforcement Learning: from
  NVIDIA Isaac Sim to Gazebo and Real ROS 2 Robots","Unprecedented agility and dexterous manipulation have been demonstrated with
controllers based on deep reinforcement learning (RL), with a significant
impact on legged and humanoid robots. Modern tooling and simulation platforms,
such as NVIDIA Isaac Sim, have been enabling such advances. This article
focuses on demonstrating the applications of Isaac in local planning and
obstacle avoidance as one of the most fundamental ways in which a mobile robot
interacts with its environments. Although there is extensive research on
proprioception-based RL policies, the article highlights less standardized and
reproducible approaches to exteroception. At the same time, the article aims to
provide a base framework for end-to-end local navigation policies and how a
custom robot can be trained in such simulation environment. We benchmark
end-to-end policies with the state-of-the-art Nav2, navigation stack in Robot
Operating System (ROS). We also cover the sim-to-real transfer process by
demonstrating zero-shot transferability of policies trained in the Isaac
simulator to real-world robots. This is further evidenced by the tests with
different simulated robots, which show the generalization of the learned
policy. Finally, the benchmarks demonstrate comparable performance to Nav2,
opening the door to quick deployment of state-of-the-art end-to-end local
planners for custom robot platforms, but importantly furthering the
possibilities by expanding the state and action spaces or task definitions for
more complex missions. Overall, with this article we introduce the most
important steps, and aspects to consider, in deploying RL policies for local
path planning and obstacle avoidance with Isaac Sim training, Gazebo testing,
and ROS 2 for real-time inference in real robots. The code is available at
https://github.com/sahars93/RL-Navigation.","Sahar Salimpour, Jorge Peña-Queralta, Diego Paez-Granados, Jukka Heikkonen, Tomi Westerlund",2025-01-06T10:26:16Z,2025-01-06T10:26:16Z,http://arxiv.org/abs/2501.02902v1,http://arxiv.org/pdf/2501.02902v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"AAM-SEALS: Developing Aerial-Aquatic Manipulators in SEa, Air, and Land
  Simulator","Current mobile manipulators and high-fidelity simulators lack the ability to
seamlessly operate and simulate across integrated environments spanning sea,
air, and land. To address this gap, we introduce Aerial-Aquatic Manipulators
(AAMs) in SEa, Air, and Land Simulator (SEALS), a comprehensive and
photorealistic simulator designed for AAMs to operate and learn in these
diverse environments. The development of AAM-SEALS tackles several significant
challenges, including the creation of integrated controllers for flying,
swimming, and manipulation, and the high-fidelity simulation of aerial dynamics
and hydrodynamics leveraging particle-based hydrodynamics. Our evaluation
demonstrates smooth operation and photorealistic transitions across air, water,
and their interfaces. We quantitatively validate the fidelity of particle-based
hydrodynamics by comparing position-tracking errors across real-world and
simulated systems. AAM-SEALS benefits a broad range of robotics communities,
including robot learning, aerial robotics, underwater robotics, mobile
manipulation, and robotic simulators. We will open-source our code and data to
foster the advancement of research in these fields. The overview video is
available at https://youtu.be/MbqIIrYvR78. Visit our project website at
https://aam-seals.umd.edu for more details.","William Yang, Karthikeya Kona, Yashveer Jain, Tomer Atzili, Abhinav Bhamidipati, Xiaomin Lin, Yantian Zha",2024-12-27T17:13:14Z,2025-02-17T17:51:08Z,http://arxiv.org/abs/2412.19744v4,http://arxiv.org/pdf/2412.19744v4.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Open-Vocabulary Mobile Manipulation Based on Double Relaxed Contrastive
  Learning with Dense Labeling","Growing labor shortages are increasing the demand for domestic service robots
(DSRs) to assist in various settings. In this study, we develop a DSR that
transports everyday objects to specified pieces of furniture based on
open-vocabulary instructions. Our approach focuses on retrieving images of
target objects and receptacles from pre-collected images of indoor
environments. For example, given an instruction ""Please get the right red towel
hanging on the metal towel rack and put it in the white washing machine on the
left,"" the DSR is expected to carry the red towel to the washing machine based
on the retrieved images. This is challenging because the correct images should
be retrieved from thousands of collected images, which may include many images
of similar towels and appliances. To address this, we propose RelaX-Former,
which learns diverse and robust representations from among positive, unlabeled
positive, and negative samples. We evaluated RelaX-Former on a dataset
containing real-world indoor images and human annotated instructions including
complex referring expressions. The experimental results demonstrate that
RelaX-Former outperformed existing baseline models across standard image
retrieval metrics. Moreover, we performed physical experiments using a DSR to
evaluate the performance of our approach in a zero-shot transfer setting. The
experiments involved the DSR to carry objects to specific receptacles based on
open-vocabulary instructions, achieving an overall success rate of 75%.","Daichi Yashima, Ryosuke Korekata, Komei Sugiura",2024-12-21T10:40:56Z,2024-12-24T07:56:48Z,http://arxiv.org/abs/2412.16576v2,http://arxiv.org/pdf/2412.16576v2.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"InfiniteWorld: A Unified Scalable Simulation Framework for General
  Visual-Language Robot Interaction","Realizing scaling laws in embodied AI has become a focus. However, previous
work has been scattered across diverse simulation platforms, with assets and
models lacking unified interfaces, which has led to inefficiencies in research.
To address this, we introduce InfiniteWorld, a unified and scalable simulator
for general vision-language robot interaction built on Nvidia Isaac Sim.
InfiniteWorld encompasses a comprehensive set of physics asset construction
methods and generalized free robot interaction benchmarks. Specifically, we
first built a unified and scalable simulation framework for embodied learning
that integrates a series of improvements in generation-driven 3D asset
construction, Real2Sim, automated annotation framework, and unified 3D asset
processing. This framework provides a unified and scalable platform for robot
interaction and learning. In addition, to simulate realistic robot interaction,
we build four new general benchmarks, including scene graph collaborative
exploration and open-world social mobile manipulation. The former is often
overlooked as an important task for robots to explore the environment and build
scene knowledge, while the latter simulates robot interaction tasks with
different levels of knowledge agents based on the former. They can more
comprehensively evaluate the embodied agent's capabilities in environmental
understanding, task planning and execution, and intelligent interaction. We
hope that this work can provide the community with a systematic asset
interface, alleviate the dilemma of the lack of high-quality assets, and
provide a more comprehensive evaluation of robot interactions.","Pengzhen Ren, Min Li, Zhen Luo, Xinshuai Song, Ziwei Chen, Weijia Liufu, Yixuan Yang, Hao Zheng, Rongtao Xu, Zitong Huang, Tongsheng Ding, Luyang Xie, Kaidong Zhang, Changfei Fu, Yang Liu, Liang Lin, Feng Zheng, Xiaodan Liang",2024-12-08T02:59:04Z,2024-12-08T02:59:04Z,http://arxiv.org/abs/2412.05789v1,http://arxiv.org/pdf/2412.05789v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
Level Up or Game Over: Exploring How Dark Patterns Shape Mobile Games,"This study explores the prevalence of dark patterns in mobile games that
exploit players through temporal, monetary, social, and psychological means.
Recognizing the ethical concerns and potential harm surrounding these
manipulative strategies, we analyze user-generated data of 1496 games to
identify relationships between the deployment of dark patterns within ""dark""
and ""healthy"" games. Our findings reveal that dark patterns are not only
widespread in games typically seen as problematic but are also present in games
that may be perceived as benign. This research contributes needed quantitative
support to the broader understanding of dark patterns in games. With an
emphasis on ethical design, our study highlights current problems of revenue
models that can be particularly harmful to vulnerable populations. To this end,
we discuss the relevance of community-based approaches to surface harmful
design and the necessity for collaboration among players/users and
practitioners to promote healthier gaming experiences.","Sam Niknejad, Thomas Mildner, Nima Zargham, Susanne Putze, Rainer Malaka",2024-12-06T13:45:27Z,2024-12-06T13:45:27Z,http://arxiv.org/abs/2412.05039v1,http://arxiv.org/pdf/2412.05039v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
Fast LiDAR Data Generation with Rectified Flows,"Building LiDAR generative models holds promise as powerful data priors for
restoration, scene manipulation, and scalable simulation in autonomous mobile
robots. In recent years, approaches using diffusion models have emerged,
significantly improving training stability and generation quality. Despite
their success, diffusion models require numerous iterations of running neural
networks to generate high-quality samples, making the increasing computational
cost a potential barrier for robotics applications. To address this challenge,
this paper presents R2Flow, a fast and high-fidelity generative model for LiDAR
data. Our method is based on rectified flows that learn straight trajectories,
simulating data generation with significantly fewer sampling steps compared to
diffusion models. We also propose an efficient Transformer-based model
architecture for processing the image representation of LiDAR range and
reflectance measurements. Our experiments on unconditional LiDAR data
generation using the KITTI-360 dataset demonstrate the effectiveness of our
approach in terms of both efficiency and quality.","Kazuto Nakashima, Xiaowen Liu, Tomoya Miyawaki, Yumi Iwashita, Ryo Kurazume",2024-12-03T08:10:53Z,2025-03-08T08:39:59Z,http://arxiv.org/abs/2412.02241v2,http://arxiv.org/pdf/2412.02241v2.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Lost & Found: Tracking Changes from Egocentric Observations in 3D
  Dynamic Scene Graphs","Recent approaches have successfully focused on the segmentation of static
reconstructions, thereby equipping downstream applications with semantic 3D
understanding. However, the world in which we live is dynamic, characterized by
numerous interactions between the environment and humans or robotic agents.
Static semantic maps are unable to capture this information, and the naive
solution of rescanning the environment after every change is both costly and
ineffective in tracking e.g. objects being stored away in drawers. With Lost &
Found we present an approach that addresses this limitation. Based solely on
egocentric recordings with corresponding hand position and camera pose
estimates, we are able to track the 6DoF poses of the moving object within the
detected interaction interval. These changes are applied online to a
transformable scene graph that captures object-level relations. Compared to
state-of-the-art object pose trackers, our approach is more reliable in
handling the challenging egocentric viewpoint and the lack of depth
information. It outperforms the second-best approach by 34% and 56% for
translational and orientational error, respectively, and produces visibly
smoother 6DoF object trajectories. In addition, we illustrate how the acquired
interaction information in the dynamic scene graph can be employed in the
context of robotic applications that would otherwise be unfeasible: We show how
our method allows to command a mobile manipulator through teach & repeat, and
how information about prior interaction allows a mobile manipulator to retrieve
an object hidden in a drawer. Code, videos and corresponding data are
accessible at https://behretj.github.io/LostAndFound.","Tjark Behrens, René Zurbrügg, Marc Pollefeys, Zuria Bauer, Hermann Blum",2024-11-28T14:05:07Z,2025-03-11T09:49:37Z,http://arxiv.org/abs/2411.19162v2,http://arxiv.org/pdf/2411.19162v2.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
From Exploration to Revelation: Detecting Dark Patterns in Mobile Apps,"Mobile apps are essential in daily life, yet they often employ dark patterns,
such as visual tricks to highlight certain options or linguistic tactics to nag
users into making purchases, to manipulate user behavior. Current research
mainly uses manual methods to detect dark patterns, a process that is
time-consuming and struggles to keep pace with continually updating and
emerging apps. While some studies targeted at automated detection, they are
constrained to static patterns and still necessitate manual app exploration. To
bridge these gaps, we present AppRay, an innovative system that seamlessly
blends task-oriented app exploration with automated dark pattern detection,
reducing manual efforts. Our approach consists of two steps: First, we harness
the commonsense knowledge of large language models for targeted app
exploration, supplemented by traditional random exploration to capture a
broader range of UI states. Second, we developed a static and dynamic dark
pattern detector powered by a contrastive learning-based multi-label classifier
and a rule-based refiner to perform detection. We contributed two datasets,
AppRay-Dark and AppRay-Light, with 2,185 unique deceptive patterns (including
149 dynamic instances) across 18 types from 876 UIs and 871 benign UIs. These
datasets cover both static and dynamic dark patterns while preserving UI
relationships. Experimental results confirm that AppRay can efficiently explore
the app and identify a wide range of dark patterns with great performance.","Jieshan Chen, Zhen Wang, Jiamou Sun, Wenbo Zou, Zhenchang Xing, Qinghua Lu, Qing Huang, Xiwei Xu",2024-11-27T06:39:35Z,2024-11-27T06:39:35Z,http://arxiv.org/abs/2411.18084v1,http://arxiv.org/pdf/2411.18084v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Dynamic Logistic Ensembles with Recursive Probability and Automatic
  Subset Splitting for Enhanced Binary Classification","This paper presents a novel approach to binary classification using dynamic
logistic ensemble models. The proposed method addresses the challenges posed by
datasets containing inherent internal clusters that lack explicit feature-based
separations. By extending traditional logistic regression, we develop an
algorithm that automatically partitions the dataset into multiple subsets,
constructing an ensemble of logistic models to enhance classification accuracy.
A key innovation in this work is the recursive probability calculation, derived
through algebraic manipulation and mathematical induction, which enables
scalable and efficient model construction. Compared to traditional ensemble
methods such as Bagging and Boosting, our approach maintains interpretability
while offering competitive performance. Furthermore, we systematically employ
maximum likelihood and cost functions to facilitate the analytical derivation
of recursive gradients as functions of ensemble depth. The effectiveness of the
proposed approach is validated on a custom dataset created by introducing noise
and shifting data to simulate group structures, resulting in significant
performance improvements with layers. Implemented in Python, this work balances
computational efficiency with theoretical rigor, providing a robust and
interpretable solution for complex classification tasks with broad implications
for machine learning applications. Code at
https://github.com/ensemble-art/Dynamic-Logistic-Ensembles","Mohammad Zubair Khan, David Li",2024-11-27T00:22:55Z,2024-11-27T00:22:55Z,http://arxiv.org/abs/2411.18649v1,http://arxiv.org/pdf/2411.18649v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
Self-Supervised Learning of Grasping Arbitrary Objects On-the-Move,"Mobile grasping enhances manipulation efficiency by utilizing robots'
mobility. This study aims to enable a commercial off-the-shelf robot for mobile
grasping, requiring precise timing and pose adjustments. Self-supervised
learning can develop a generalizable policy to adjust the robot's velocity and
determine grasp position and orientation based on the target object's shape and
pose. Due to mobile grasping's complexity, action primitivization and
step-by-step learning are crucial to avoid data sparsity in learning from trial
and error. This study simplifies mobile grasping into two grasp action
primitives and a moving action primitive, which can be operated with limited
degrees of freedom for the manipulator. This study introduces three fully
convolutional neural network (FCN) models to predict static grasp primitive,
dynamic grasp primitive, and residual moving velocity error from visual inputs.
A two-stage grasp learning approach facilitates seamless FCN model learning.
The ablation study demonstrated that the proposed method achieved the highest
grasping accuracy and pick-and-place efficiency. Furthermore, randomizing
object shapes and environments in the simulation effectively achieved
generalizable mobile grasping.","Takuya Kiyokawa, Eiki Nagata, Yoshihisa Tsurumine, Yuhwan Kwon, Takamitsu Matsubara",2024-11-15T02:59:16Z,2024-11-15T02:59:16Z,http://arxiv.org/abs/2411.09904v1,http://arxiv.org/pdf/2411.09904v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
Experience-based Subproblem Planning for Multi-Robot Motion Planning,"Multi-robot systems enhance efficiency and productivity across various
applications, from manufacturing to surveillance. While single-robot motion
planning has improved by using databases of prior solutions, extending this
approach to multi-robot motion planning (MRMP) presents challenges due to the
increased complexity and diversity of tasks and configurations. Recent discrete
methods have attempted to address this by focusing on relevant
lower-dimensional subproblems, but they are inadequate for complex scenarios
like those involving manipulator robots. To overcome this, we propose a novel
approach that %leverages experience-based planning by constructs and utilizes
databases of solutions for smaller sub-problems. By focusing on interactions
between fewer robots, our method reduces the need for exhaustive database
growth, allowing for efficient handling of more complex MRMP scenarios. We
validate our approach with experiments involving both mobile and manipulator
robots, demonstrating significant improvements over existing methods in
scalability and planning efficiency. Our contributions include a rapidly
constructed database for low-dimensional MRMP problems, a framework for
applying these solutions to larger problems, and experimental validation with
up to 32 mobile and 16 manipulator robots.","Irving Solis, James Motes, Mike Qin, Marco Morales, Nancy M. Amato",2024-11-13T18:30:28Z,2024-11-13T18:30:28Z,http://arxiv.org/abs/2411.08851v1,http://arxiv.org/pdf/2411.08851v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Robust Nonprehensile Object Transportation with Uncertain Inertial
  Parameters","We consider the nonprehensile object transportation task known as the
waiter's problem - in which a robot must move an object on a tray from one
location to another - when the transported object has uncertain inertial
parameters. In contrast to existing approaches that completely ignore
uncertainty in the inertia matrix or which only consider small parameter
errors, we are interested in pushing the limits of the amount of inertial
parameter uncertainty that can be handled. We first show how constraints that
are robust to inertial parameter uncertainty can be incorporated into an
optimization-based motion planning framework to transport objects while moving
quickly. Next, we develop necessary conditions for the inertial parameters to
be realizable on a bounding shape based on moment relaxations, allowing us to
verify whether a trajectory will violate the constraints for any realizable
inertial parameters. Finally, we demonstrate our approach on a mobile
manipulator in simulations and real hardware experiments: our proposed robust
constraints consistently successfully transport a 56 cm tall object with
substantial inertial parameter uncertainty in the real world, while the
baseline approaches drop the object while transporting it.","Adam Heins, Angela P. Schoellig",2024-11-11T15:55:42Z,2025-03-31T15:25:40Z,http://arxiv.org/abs/2411.07079v3,http://arxiv.org/pdf/2411.07079v3.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"SuperQ-GRASP: Superquadrics-based Grasp Pose Estimation on Larger
  Objects for Mobile-Manipulation","Grasp planning and estimation have been a longstanding research problem in
robotics, with two main approaches to find graspable poses on the objects: 1)
geometric approach, which relies on 3D models of objects and the gripper to
estimate valid grasp poses, and 2) data-driven, learning-based approach, with
models trained to identify grasp poses from raw sensor observations. The latter
assumes comprehensive geometric coverage during the training phase. However,
the data-driven approach is typically biased toward tabletop scenarios and
struggle to generalize to out-of-distribution scenarios with larger objects
(e.g. chair). Additionally, raw sensor data (e.g. RGB-D data) from a single
view of these larger objects is often incomplete and necessitates additional
observations. In this paper, we take a geometric approach, leveraging
advancements in object modeling (e.g. NeRF) to build an implicit model by
taking RGB images from views around the target object. This model enables the
extraction of explicit mesh model while also capturing the visual appearance
from novel viewpoints that is useful for perception tasks like object detection
and pose estimation. We further decompose the NeRF-reconstructed 3D mesh into
superquadrics (SQs) -- parametric geometric primitives, each mapped to a set of
precomputed grasp poses, allowing grasp composition on the target object based
on these primitives. Our proposed pipeline overcomes the problems: a) noisy
depth and incomplete view of the object, with a modeling step, and b)
generalization to objects of any size. For more qualitative results, refer to
the supplementary video and webpage https://bit.ly/3ZrOanU","Xun Tu, Karthik Desingh",2024-11-07T03:00:12Z,2025-04-10T01:26:16Z,http://arxiv.org/abs/2411.04386v3,http://arxiv.org/pdf/2411.04386v3.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"LiquiRIS: A Major Step Towards Fast Beam Switching in Liquid
  Crystal-based RISs","Reconfigurable intelligent surfaces (RISs) offer enhanced control over
propagation through phase and amplitude manipulation but face practical
challenges like cost and power usage, especially at high frequencies. This is
specifically a major problem at high frequencies (Ka- and V-band) where the
high cost of semiconductor components (i.e., diodes, varactors, MEMSs) can make
RISs prohibitively costly. In recent years, it is shown that liquid crystals
(LCs) are low-cost and low-energy alternative which can address the
aforementioned challenges but at the cost of lower response time. In LiquiRIS,
we enable leveraging LC-based RIS in mobile networks. Specifically, we devise
techniques that minimize the beam switching time of LC-based RIS by tapping
into the physical properties of LCs and the underlying mathematical principles
of beamforming. We achieve this by modeling and optimizing the beamforming
vector to account for the rotation characteristics of LC molecules to reduce
their transition time from one state to another. In addition to prototyping the
proposed system, we show via extensive experimental analysis that LiquiRIS
substantially reduces the response time (up to 70.80%) of liquid crystal
surface (LCS).","Luis F. Abanto-Leon, Robin Neuder, Waqar Ahmed, Alejandro Jimenez Saez, Vahid Jamali, Arash Asadi",2024-10-28T20:21:23Z,2024-10-28T20:21:23Z,http://arxiv.org/abs/2410.21506v1,http://arxiv.org/pdf/2410.21506v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Configuração e operação da plataforma Clearpath Husky A200 e
  manipulador Cobot UR5 2-finger gripper","This article presents initial configuration work and use of the robotic
platform and manipulator in question. The development of the ideal
configuration for using this robot serves as a guide for new users and also
validates its functionality for use in projects. Husky is a large payload
capacity and power systems robotics development platform that accommodates a
wide variety of payloads, customized to meet research needs. Together with the
Cobot UR5 Manipulator attached to its base, it expands the application area of
its capacity in projects. Advances in robots and mobile manipulators have
revolutionized industries by automating tasks that previously required human
intervention. These innovations alone increase productivity but also reduce
operating costs, which makes the company more competitive in an evolving global
market. Therefore, this article investigates the functionalities of this robot
to validate its execution in robotics projects.","Sodre Hiago, Barcelona Sebastian, Sandin Vincent, Moraes Pablo, Peters Christopher, da Silva Angél, Flores Gabriela, Mazondo Ahilen, Fernández Santiago, Assunção Nathalie, de Vargas Bruna, Grando Ricardo, Kelbouscas André",2024-10-22T22:00:58Z,2024-10-28T01:19:35Z,http://arxiv.org/abs/2410.17453v3,http://arxiv.org/pdf/2410.17453v3.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Lie Theory Based Optimization for Unified State Planning of Mobile
  Manipulators","Mobile manipulators are finding use in numerous practical applications. The
current issues with mobile manipulation are the large state space owing to the
mobile base and the challenge of modeling high degree of freedom systems. It is
critical to devise fast and accurate algorithms that generate smooth motion
plans for such mobile manipulators. Existing techniques attempt to solve this
problem but focus on separating the motion of the base and manipulator. We
propose an approach using Lie theory to find the inverse kinematic constraints
by converting the kinematic model, created using screw coordinates, between its
Lie group and vector representation. An optimization function is devised to
solve for the desired joint states of the entire mobile manipulator. This
allows the motion of the mobile base and manipulator to be planned and applied
in unison resulting in a smooth and accurate motion plan. The performance of
the proposed state planner is validated on simulated mobile manipulators in an
analytical experiment. Our solver is available with further derivations and
results at https://github.com/peleito/slithers.","William Smith, Siddharth Singh, Julia Rudy, Yuxiang Guan",2024-10-20T16:39:15Z,2024-10-20T16:39:15Z,http://arxiv.org/abs/2410.15443v1,http://arxiv.org/pdf/2410.15443v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"BestMan: A Modular Mobile Manipulator Platform for Embodied AI with
  Unified Simulation-Hardware APIs","Embodied Artificial Intelligence (Embodied AI) emphasizes agents' ability to
perceive, understand, and act in physical environments. Simulation platforms
play a crucial role in advancing this field by enabling the validation and
optimization of algorithms. However, existing platforms face challenges such as
multilevel technical integration complexity, insufficient modularity, interface
heterogeneity, and adaptation to diverse hardware. We present BestMan, a
simulation platform based on PyBullet, designed to address these issues.
BestMan introduces an integrated multilevel skill chain for seamless
coordination across perception, planning, and control; a highly modular
architecture for flexible algorithm integration; unified interfaces for smooth
simulation-to-reality transfer; and a hardware-agnostic approach for adapting
to various mobile manipulator configurations. These features collectively
simplify development and enhance platform expandability, making BestMan a
valuable tool for Embodied AI research.","Kui Yang, Nieqing Cao, Yan Ding, Chao Chen",2024-10-17T10:09:44Z,2024-10-17T10:09:44Z,http://arxiv.org/abs/2410.13407v1,http://arxiv.org/pdf/2410.13407v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Dynamic Open-Vocabulary 3D Scene Graphs for Long-term Language-Guided
  Mobile Manipulation","Enabling mobile robots to perform long-term tasks in dynamic real-world
environments is a formidable challenge, especially when the environment changes
frequently due to human-robot interactions or the robot's own actions.
Traditional methods typically assume static scenes, which limits their
applicability in the continuously changing real world. To overcome these
limitations, we present DovSG, a novel mobile manipulation framework that
leverages dynamic open-vocabulary 3D scene graphs and a language-guided task
planning module for long-term task execution. DovSG takes RGB-D sequences as
input and utilizes vision-language models (VLMs) for object detection to obtain
high-level object semantic features. Based on the segmented objects, a
structured 3D scene graph is generated for low-level spatial relationships.
Furthermore, an efficient mechanism for locally updating the scene graph,
allows the robot to adjust parts of the graph dynamically during interactions
without the need for full scene reconstruction. This mechanism is particularly
valuable in dynamic environments, enabling the robot to continually adapt to
scene changes and effectively support the execution of long-term tasks. We
validated our system in real-world environments with varying degrees of manual
modifications, demonstrating its effectiveness and superior performance in
long-term tasks. Our project page is available at:
https://bjhyzj.github.io/dovsg-web.","Zhijie Yan, Shufei Li, Zuoxu Wang, Lixiu Wu, Han Wang, Jun Zhu, Lijiang Chen, Jihong Liu",2024-10-15T18:52:22Z,2025-03-19T03:24:54Z,http://arxiv.org/abs/2410.11989v6,http://arxiv.org/pdf/2410.11989v6.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"M2Diffuser: Diffusion-based Trajectory Optimization for Mobile
  Manipulation in 3D Scenes","Recent advances in diffusion models have opened new avenues for research into
embodied AI agents and robotics. Despite significant achievements in complex
robotic locomotion and skills, mobile manipulation-a capability that requires
the coordination of navigation and manipulation-remains a challenge for
generative AI techniques. This is primarily due to the high-dimensional action
space, extended motion trajectories, and interactions with the surrounding
environment. In this paper, we introduce M2Diffuser, a diffusion-based,
scene-conditioned generative model that directly generates coordinated and
efficient whole-body motion trajectories for mobile manipulation based on
robot-centric 3D scans. M2Diffuser first learns trajectory-level distributions
from mobile manipulation trajectories provided by an expert planner. Crucially,
it incorporates an optimization module that can flexibly accommodate physical
constraints and task objectives, modeled as cost and energy functions, during
the inference process. This enables the reduction of physical violations and
execution errors at each denoising step in a fully differentiable manner.
Through benchmarking on three types of mobile manipulation tasks across over 20
scenes, we demonstrate that M2Diffuser outperforms state-of-the-art neural
planners and successfully transfers the generated trajectories to a real-world
robot. Our evaluations underscore the potential of generative AI to enhance the
generalization of traditional planning and learning-based robotic methods,
while also highlighting the critical role of enforcing physical constraints for
safe and robust execution.","Sixu Yan, Zeyu Zhang, Muzhi Han, Zaijin Wang, Qi Xie, Zhitian Li, Zhehan Li, Hangxin Liu, Xinggang Wang, Song-Chun Zhu",2024-10-15T08:49:35Z,2024-10-15T08:49:35Z,http://arxiv.org/abs/2410.11402v1,http://arxiv.org/pdf/2410.11402v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
Refusal-Trained LLMs Are Easily Jailbroken As Browser Agents,"For safety reasons, large language models (LLMs) are trained to refuse
harmful user instructions, such as assisting dangerous activities. We study an
open question in this work: does the desired safety refusal, typically enforced
in chat contexts, generalize to non-chat and agentic use cases? Unlike
chatbots, LLM agents equipped with general-purpose tools, such as web browsers
and mobile devices, can directly influence the real world, making it even more
crucial to refuse harmful instructions. In this work, we primarily focus on
red-teaming browser agents, LLMs that manipulate information via web browsers.
To this end, we introduce Browser Agent Red teaming Toolkit (BrowserART), a
comprehensive test suite designed specifically for red-teaming browser agents.
BrowserART is consist of 100 diverse browser-related harmful behaviors
(including original behaviors and ones sourced from HarmBench [Mazeika et al.,
2024] and AirBench 2024 [Zeng et al., 2024b]) across both synthetic and real
websites. Our empirical study on state-of-the-art browser agents reveals that,
while the backbone LLM refuses harmful instructions as a chatbot, the
corresponding agent does not. Moreover, attack methods designed to jailbreak
refusal-trained LLMs in the chat settings transfer effectively to browser
agents. With human rewrites, GPT-4o and o1-preview-based browser agents
attempted 98 and 63 harmful behaviors (out of 100), respectively. We publicly
release BrowserART and call on LLM developers, policymakers, and agent
developers to collaborate on improving agent safety","Priyanshu Kumar, Elaine Lau, Saranya Vijayakumar, Tu Trinh, Scale Red Team, Elaine Chang, Vaughn Robinson, Sean Hendryx, Shuyan Zhou, Matt Fredrikson, Summer Yue, Zifan Wang",2024-10-11T06:54:12Z,2024-10-21T18:06:58Z,http://arxiv.org/abs/2410.13886v2,http://arxiv.org/pdf/2410.13886v2.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
DTactive: A Vision-Based Tactile Sensor with Active Surface,"The development of vision-based tactile sensors has significantly enhanced
robots' perception and manipulation capabilities, especially for tasks
requiring contact-rich interactions with objects. In this work, we present
DTactive, a novel vision-based tactile sensor with active surfaces. DTactive
inherits and modifies the tactile 3D shape reconstruction method of DTact while
integrating a mechanical transmission mechanism that facilitates the mobility
of its surface. Thanks to this design, the sensor is capable of simultaneously
performing tactile perception and in-hand manipulation with surface movement.
Leveraging the high-resolution tactile images from the sensor and the magnetic
encoder data from the transmission mechanism, we propose a learning-based
method to enable precise angular trajectory control during in-hand
manipulation. In our experiments, we successfully achieved accurate rolling
manipulation within the range of [ -180{\deg},180{\deg} ] on various objects,
with the root mean square error between the desired and actual angular
trajectories being less than 12{\deg} on nine trained objects and less than
19{\deg} on three novel objects. The results demonstrate the potential of
DTactive for in-hand object manipulation in terms of effectiveness, robustness
and precision.","Jikai Xu, Lei Wu, Changyi Lin, Ding Zhao, Huazhe Xu",2024-10-10T19:53:16Z,2024-10-10T19:53:16Z,http://arxiv.org/abs/2410.08337v1,http://arxiv.org/pdf/2410.08337v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"MotionAura: Generating High-Quality and Motion Consistent Videos using
  Discrete Diffusion","The spatio-temporal complexity of video data presents significant challenges
in tasks such as compression, generation, and inpainting. We present four key
contributions to address the challenges of spatiotemporal video processing.
First, we introduce the 3D Mobile Inverted Vector-Quantization Variational
Autoencoder (3D-MBQ-VAE), which combines Variational Autoencoders (VAEs) with
masked token modeling to enhance spatiotemporal video compression. The model
achieves superior temporal consistency and state-of-the-art (SOTA)
reconstruction quality by employing a novel training strategy with full frame
masking. Second, we present MotionAura, a text-to-video generation framework
that utilizes vector-quantized diffusion models to discretize the latent space
and capture complex motion dynamics, producing temporally coherent videos
aligned with text prompts. Third, we propose a spectral transformer-based
denoising network that processes video data in the frequency domain using the
Fourier Transform. This method effectively captures global context and
long-range dependencies for high-quality video generation and denoising.
Lastly, we introduce a downstream task of Sketch Guided Video Inpainting. This
task leverages Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning.
Our models achieve SOTA performance on a range of benchmarks. Our work offers
robust frameworks for spatiotemporal modeling and user-driven video content
manipulation. We will release the code, datasets, and models in open-source.","Onkar Susladkar, Jishu Sen Gupta, Chirag Sehgal, Sparsh Mittal, Rekha Singhal",2024-10-10T07:07:56Z,2025-03-11T05:19:31Z,http://arxiv.org/abs/2410.07659v2,http://arxiv.org/pdf/2410.07659v2.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
Combining Planning and Diffusion for Mobility with Unknown Dynamics,"Manipulation of large objects over long horizons (such as carts in a
warehouse) is an essential skill for deployable robotic systems. Large objects
require mobile manipulation which involves simultaneous manipulation,
navigation, and movement with the object in tow. In many real-world situations,
object dynamics are incredibly complex, such as the interaction of an office
chair (with a rotating base and five caster wheels) and the ground. We present
a hierarchical algorithm for long-horizon robot manipulation problems in which
the dynamics are partially unknown. We observe that diffusion-based behavior
cloning is highly effective for short-horizon problems with unknown dynamics,
so we decompose the problem into an abstract high-level, obstacle-aware
motion-planning problem that produces a waypoint sequence. We use a
short-horizon, relative-motion diffusion policy to achieve the waypoints in
sequence. We train mobile manipulation policies on a Spot robot that has to
push and pull an office chair. Our hierarchical manipulation policy performs
consistently better, especially when the horizon increases, compared to a
diffusion policy trained on long-horizon demonstrations or motion planning
assuming a rigidly-attached object (success rate of 8 (versus 0 and 5
respectively) out of 10 runs). Importantly, our learned policy generalizes to
new layouts, grasps, chairs, and flooring that induces more friction, without
any further training, showing promise for other complex mobile manipulation
problems. Project Page: https://yravan.github.io/plannerorderedpolicy/","Yajvan Ravan, Zhutian Yang, Tao Chen, Tomás Lozano-Pérez, Leslie Pack Kaelbling",2024-10-09T14:12:28Z,2024-10-09T14:12:28Z,http://arxiv.org/abs/2410.06911v1,http://arxiv.org/pdf/2410.06911v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Unique ID based Trust Scheme for Improved IoV Wireless Sensor Network
  Security Against Power Controlled Sybil Attacks","Wireless sensor networks (WSN) are widely used in vehicular networks to
support Vehicle-to-Everything (V2X) communications. Wireless sensors in
vehicular networks support sensing and monitoring of various environmental
factors and vehicle movement, which can help to enhance traffic management,
road safety, and transportation efficiency. However, WSNs face security
challenges due to their distributed nature and resource limited modules. In
Sybil attacks, attackers create multiple fake identities to disrupt network
operations (e.g., denial-of-service (DoS)), which is one of the major security
concerns in WSNs. Defensive techniques have been proposed, which recently
include a received signal strength indicator (RSSI) profiling scheme that
improves the performance and is not affected by internal forgeable information.
However, even this new RSSI based robust detection scheme was found to be
vulnerable when Sybil attackers are mobile or intentionally manipulate their
radio transmission power in addition to their device address. In this paper, a
unique identification based trust path routing scheme (UITrust) is proposed,
which uses the device's physically invariable unique identifiers and routing
path trust level estimations to avoid power-controlled Sybil attacks, where the
simulation results show the proposed scheme can provide a significant
improvement compared to existing schemes.","Jae-Dong Kim, Dabin Kim, Minseok Ko, Jong-Moon Chung",2024-10-05T07:20:55Z,2025-04-16T01:24:07Z,http://arxiv.org/abs/2410.04063v2,http://arxiv.org/pdf/2410.04063v2.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
Towards a Self-rescuing System for UAVs Under GNSS Attack,"There has been substantial growth in the UAV market along with an expansion
in their applications. However, the successful execution of a UAV mission is
very often dependent on the use of a GNSS. Unfortunately, the vulnerability of
GNSS signals, due to their lack of encryption and authentication, poses a
significant cybersecurity issue. This vulnerability makes various attacks,
particularly the ""GNSS spoofing attack,"" and ""GNSS jamming attack"" easily
executable. Generally speaking, during this attack, the drone is manipulated
into altering its path, usually resulting in an immediate forced landing or
crash. As far as we know, we are the first to propose a lightweight-solution
that enable a drone to autonomously rescue itself, assuming it is under GNSS
attack and the GNSS is no longer available, and return safely to its initial
takeoff position, thereby preventing any potential crashes. During the flight,
wind plays a critical role as it can instantaneously alter the drone's
position. To solve this problem, we have devised a highly effective 2-phases
solution: (i) Forward Phase, for monitoring and recording the forward journey,
and (ii) Backward Phase, that generates a backward route, based on the Forward
Phase and wind presence. The final solution ensures strong performance in
consistently returning the drone to the original position, even in wind
situations, while maintaining a very fast computation time.","Giulio Rigoni, Nicola Scremin, Mauro Conti",2024-10-03T12:39:54Z,2024-10-03T12:39:54Z,http://arxiv.org/abs/2410.02442v1,http://arxiv.org/pdf/2410.02442v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
RoTip: A Finger-Shaped Tactile Sensor with Active Rotation Capability,"In recent years, advancements in optical tactile sensor technology have
primarily centred on enhancing sensing precision and expanding the range of
sensing modalities. To meet the requirements for more skilful manipulation,
there should be a movement towards making tactile sensors more dynamic. In this
paper, we introduce RoTip, a novel vision-based tactile sensor that is uniquely
designed with an independently controlled joint and the capability to sense
contact over its entire surface. The rotational capability of the sensor is
particularly crucial for manipulating everyday objects, especially thin and
flexible ones, as it enables the sensor to mobilize while in contact with the
object's surface. The manipulation experiments demonstrate the ability of our
proposed RoTip to manipulate rigid and flexible objects, and the full-finger
tactile feedback and active rotation capabilities have the potential to explore
more complex and precise manipulation tasks.","Xuyang Zhang, Jiaqi Jiang, Shan Luo",2024-10-01T21:28:30Z,2025-02-01T11:15:23Z,http://arxiv.org/abs/2410.01085v2,http://arxiv.org/pdf/2410.01085v2.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Pre-Chirp-Domain Index Modulation for Full-Diversity Affine Frequency
  Division Multiplexing towards 6G","Affine frequency division multiplexing (AFDM), tailored as a superior
multicarrier technique utilizing chirp signals for high-mobility
communications, is envisioned as a promising candidate for the sixth-generation
(6G) wireless network. AFDM is based on the discrete affine Fourier transform
(DAFT) with two adjustable parameters of the chirp signals, termed as the
pre-chirp and post-chirp parameters, respectively. We show that the pre-chirp
counterpart can be flexibly manipulated for additional degree-of-freedom (DoF).
Therefore, this paper proposes a novel AFDM scheme with the pre-chirp index
modulation (PIM) philosophy (AFDM-PIM), which can implicitly convey extra
information bits through dynamic pre-chirp parameter assignment, thus enhancing
both spectral and energy efficiency. Specifically, we first demonstrate that
the subcarrier orthogonality is still maintained by applying distinct pre-chirp
parameters to various subcarriers in the AFDM modulation process. Inspired by
this property, each AFDM subcarrier is constituted with a unique pre-chirp
signal according to the incoming bits. By such arrangement, extra binary bits
can be embedded into the index patterns of pre-chirp parameter assignment
without additional energy consumption. For performance analysis, we derive the
asymptotically tight upper bounds on the average bit error rates (BERs) of the
proposed schemes with maximum-likelihood (ML) detection, and validate that the
proposed AFDM-PIM can achieve the optimal diversity order under doubly
dispersive channels. Based on the derivations, we further propose an optimal
pre-chirp alphabet design to enhance the BER performance via intelligent
optimization algorithms. Simulations demonstrate that the proposed AFDM-PIM
outperforms the classical benchmarks under doubly dispersive channel.","Guangyao Liu, Tianqi Mao, Zhenyu Xiao, Miaowen Wen, Ruiqi Liu, Jingjing Zhao, Ertugrul Basar, Zhaocheng Wang, Sheng Chen",2024-10-01T01:24:40Z,2025-04-23T14:06:32Z,http://arxiv.org/abs/2410.00313v4,http://arxiv.org/pdf/2410.00313v4.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Helpful DoggyBot: Open-World Object Fetching using Legged Robots and
  Vision-Language Models","Learning-based methods have achieved strong performance for quadrupedal
locomotion. However, several challenges prevent quadrupeds from learning
helpful indoor skills that require interaction with environments and humans:
lack of end-effectors for manipulation, limited semantic understanding using
only simulation data, and low traversability and reachability in indoor
environments. We present a system for quadrupedal mobile manipulation in indoor
environments. It uses a front-mounted gripper for object manipulation, a
low-level controller trained in simulation using egocentric depth for agile
skills like climbing and whole-body tilting, and pre-trained vision-language
models (VLMs) with a third-person fisheye and an egocentric RGB camera for
semantic understanding and command generation. We evaluate our system in two
unseen environments without any real-world data collection or training. Our
system can zero-shot generalize to these environments and complete tasks, like
following user's commands to fetch a randomly placed stuff toy after climbing
over a queen-sized bed, with a 60% success rate. Project website:
https://helpful-doggybot.github.io/","Qi Wu, Zipeng Fu, Xuxin Cheng, Xiaolong Wang, Chelsea Finn",2024-09-30T20:58:38Z,2024-09-30T20:58:38Z,http://arxiv.org/abs/2410.00231v1,http://arxiv.org/pdf/2410.00231v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Urban Anomalies: A Simulated Human Mobility Dataset with Injected
  Anomalies","Human mobility anomaly detection based on location is essential in areas such
as public health, safety, welfare, and urban planning. Developing models and
approaches for location-based anomaly detection requires a comprehensive
dataset. However, privacy concerns and the absence of ground truth hinder the
availability of publicly available datasets. With this paper, we provide
extensive simulated human mobility datasets featuring various anomaly types
created using an existing Urban Patterns of Life Simulation. To create these
datasets, we inject changes in the logic of individual agents to change their
behavior. Specifically, we create four of anomalous agent behavior by (1)
changing the agents' appetite (causing agents to have meals more frequently),
(2) changing their group of interest (causing agents to interact with different
agents from another group). (3) changing their social place selection (causing
agents to visit different recreational places) and (4) changing their work
schedule (causing agents to skip work), For each type of anomaly, we use three
degrees of behavioral change to tune the difficulty of detecting the anomalous
agents. To select agents to inject anomalous behavior into, we employ three
methods: (1) Random selection using a centralized manipulation mechanism, (2)
Spread based selection using an infectious disease model, and (3) through
exposure of agents to a specific location. All datasets are split into normal
and anomalous phases. The normal phase, which can be used for training models
of normalcy, exhibits no anomalous behavior. The anomalous phase, which can be
used for testing for anomalous detection algorithm, includes ground truth
labels that indicate, for each five-minute simulation step, which agents are
anomalous at that time. Datasets are generated using the maps (roads and
buildings) for Atlanta and Berlin, having 1k agents in each simulation.","Hossein Amiri, Ruochen Kong, Andreas Zufle",2024-09-28T20:44:54Z,2024-10-11T20:18:54Z,http://arxiv.org/abs/2410.01844v2,http://arxiv.org/pdf/2410.01844v2.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
RAIL: Reachability-Aided Imitation Learning for Safe Policy Execution,"Imitation learning (IL) has shown great success in learning complex robot
manipulation tasks. However, there remains a need for practical safety methods
to justify widespread deployment. In particular, it is important to certify
that a system obeys hard constraints on unsafe behavior in settings when it is
unacceptable to design a tradeoff between performance and safety via tuning the
policy (i.e. soft constraints). This leads to the question, how does enforcing
hard constraints impact the performance (meaning safely completing tasks) of an
IL policy? To answer this question, this paper builds a reachability-based
safety filter to enforce hard constraints on IL, which we call
Reachability-Aided Imitation Learning (RAIL). Through evaluations with
state-of-the-art IL policies in mobile robots and manipulation tasks, we make
two key findings. First, the highest-performing policies are sometimes only so
because they frequently violate constraints, and significantly lose performance
under hard constraints. Second, surprisingly, hard constraints on the
lower-performing policies can occasionally increase their ability to perform
tasks safely. Finally, hardware evaluation confirms the method can operate in
real time.","Wonsuhk Jung, Dennis Anthony, Utkarsh A. Mishra, Nadun Ranawaka Arachchige, Matthew Bronars, Danfei Xu, Shreyas Kousik",2024-09-28T00:18:12Z,2024-09-28T00:18:12Z,http://arxiv.org/abs/2409.19190v1,http://arxiv.org/pdf/2409.19190v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"ReloPush: Multi-object Rearrangement in Confined Spaces with a
  Nonholonomic Mobile Robot Pusher","We focus on push-based multi-object rearrangement planning using a
nonholonomically constrained mobile robot. The simultaneous geometric,
kinematic, and physics constraints make this problem especially challenging.
Prior work on rearrangement planning often relaxes some of these constraints by
assuming dexterous hardware, prehensile manipulation, or sparsely occupied
workspaces. Our key insight is that by capturing these constraints into a
unified representation, we could empower a constrained robot to tackle
difficult problem instances by modifying the environment in its favor. To this
end, we introduce a Push-Traversability graph, whose vertices represent poses
that the robot can push objects from, and edges represent optimal,
kinematically feasible, and stable transitions between them. Based on this
graph, we develop ReloPush, a graph-based planning framework that takes as
input a complex multi-object rearrangement task and breaks it down into a
sequence of single-object pushing tasks. We evaluate ReloPush across a series
of challenging scenarios, involving the rearrangement of densely cluttered
workspaces with up to nine objects, using a 1/10-scale robot racecar. ReloPush
exhibits orders of magnitude faster runtimes and significantly more robust
execution in the real world, evidenced in lower execution times and fewer
losses of object contact, compared to two baselines lacking our proposed graph
structure.","Jeeho Ahn, Christoforos Mavrogiannis",2024-09-26T19:21:06Z,2025-03-12T16:15:03Z,http://arxiv.org/abs/2409.18231v2,http://arxiv.org/pdf/2409.18231v2.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
KinScene: Model-Based Mobile Manipulation of Articulated Scenes,"Sequentially interacting with articulated objects is crucial for a mobile
manipulator to operate effectively in everyday environments. To enable
long-horizon tasks involving articulated objects, this study explores building
scene-level articulation models for indoor scenes through autonomous
exploration. While previous research has studied mobile manipulation with
articulated objects by considering object kinematic constraints, it primarily
focuses on individual-object scenarios and lacks extension to a scene-level
context for task-level planning. To manipulate multiple object parts
sequentially, the robot needs to reason about the resultant motion of each part
and anticipate its impact on future actions. We introduce KinScene, a
full-stack approach for long-horizon manipulation tasks with articulated
objects. The robot maps the scene, detects and physically interacts with
articulated objects, collects observations, and infers the articulation
properties. For sequential tasks, the robot plans a feasible series of object
interactions based on the inferred articulation model. We demonstrate that our
approach repeatably constructs accurate scene-level kinematic and geometric
models, enabling long-horizon mobile manipulation in a real-world scene. Code
and additional results are available at
https://chengchunhsu.github.io/KinScene/","Cheng-Chun Hsu, Ben Abbatematteo, Zhenyu Jiang, Yuke Zhu, Roberto Martín-Martín, Joydeep Biswas",2024-09-24T21:48:14Z,2024-09-28T19:49:49Z,http://arxiv.org/abs/2409.16473v2,http://arxiv.org/pdf/2409.16473v2.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"MHRC: Closed-loop Decentralized Multi-Heterogeneous Robot Collaboration
  with Large Language Models","The integration of large language models (LLMs) with robotics has
significantly advanced robots' abilities in perception, cognition, and task
planning. The use of natural language interfaces offers a unified approach for
expressing the capability differences of heterogeneous robots, facilitating
communication between them, and enabling seamless task allocation and
collaboration. Currently, the utilization of LLMs to achieve decentralized
multi-heterogeneous robot collaborative tasks remains an under-explored area of
research. In this paper, we introduce a novel framework that utilizes LLMs to
achieve decentralized collaboration among multiple heterogeneous robots. Our
framework supports three robot categories, mobile robots, manipulation robots,
and mobile manipulation robots, working together to complete tasks such as
exploration, transportation, and organization. We developed a rich set of
textual feedback mechanisms and chain-of-thought (CoT) prompts to enhance task
planning efficiency and overall system performance. The mobile manipulation
robot can adjust its base position flexibly, ensuring optimal conditions for
grasping tasks. The manipulation robot can comprehend task requirements, seek
assistance when necessary, and handle objects appropriately. Meanwhile, the
mobile robot can explore the environment extensively, map object locations, and
communicate this information to the mobile manipulation robot, thus improving
task execution efficiency. We evaluated the framework using PyBullet, creating
scenarios with three different room layouts and three distinct operational
tasks. We tested various LLM models and conducted ablation studies to assess
the contributions of different modules. The experimental results confirm the
effectiveness and necessity of our proposed framework.","Wenhao Yu, Jie Peng, Yueliang Ying, Sai Li, Jianmin Ji, Yanyong Zhang",2024-09-24T12:29:44Z,2024-09-25T18:51:25Z,http://arxiv.org/abs/2409.16030v2,http://arxiv.org/pdf/2409.16030v2.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"In the Wild Ungraspable Object Picking with Bimanual Nonprehensile
  Manipulation","Picking diverse objects in the real world is a fundamental robotics skill.
However, many objects in such settings are bulky, heavy, or irregularly shaped,
making them ungraspable by conventional end effectors like suction grippers and
parallel jaw grippers (PJGs). In this paper, we expand the range of pickable
items without hardware modifications using bimanual nonprehensile manipulation.
We focus on a grocery shopping scenario, where a bimanual mobile manipulator
equipped with a suction gripper and a PJG is tasked with retrieving ungraspable
items from tightly packed grocery shelves. From visual observations, our method
first identifies optimal grasp points based on force closure and friction
constraints. If the grasp points are occluded, a series of nonprehensile
nudging motions are performed to clear the obstruction. A bimanual grasp
utilizing contacts on the side of the end effectors is then executed to grasp
the target item. In our replica grocery store, we achieved a 90% success rate
over 102 trials in uncluttered scenes, and a 67% success rate over 45 trials in
cluttered scenes. We also deployed our system to a real-world grocery store and
successfully picked previously unseen items. Our results highlight the
potential of bimanual nonprehensile manipulation for in-the-wild robotic
picking tasks. A video summarizing this work can be found at
youtu.be/g0hOrDuK8jM","Albert Wu, Dan Kruse",2024-09-23T18:43:31Z,2024-09-23T18:43:31Z,http://arxiv.org/abs/2409.15465v1,http://arxiv.org/pdf/2409.15465v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
Whole-Body Teleoperation for Mobile Manipulation at Zero Added Cost,"Demonstration data plays a key role in learning complex behaviors and
training robotic foundation models. While effective control interfaces exist
for static manipulators, data collection remains cumbersome and time intensive
for mobile manipulators due to their large number of degrees of freedom. While
specialized hardware, avatars, or motion tracking can enable whole-body
control, these approaches are either expensive, robot-specific, or suffer from
the embodiment mismatch between robot and human demonstrator. In this work, we
present MoMa-Teleop, a novel teleoperation method that infers end-effector
motions from existing interfaces and delegates the base motions to a previously
developed reinforcement learning agent, leaving the operator to focus fully on
the task-relevant end-effector motions. This enables whole-body teleoperation
of mobile manipulators with no additional hardware or setup costs via standard
interfaces such as joysticks or hand guidance. Moreover, the operator is not
bound to a tracked workspace and can move freely with the robot over spatially
extended tasks. We demonstrate that our approach results in a significant
reduction in task completion time across a variety of robots and tasks. As the
generated data covers diverse whole-body motions without embodiment mismatch,
it enables efficient imitation learning. By focusing on task-specific
end-effector motions, our approach learns skills that transfer to unseen
settings, such as new obstacles or changed object positions, from as little as
five demonstrations. We make code and videos available at
https://moma-teleop.cs.uni-freiburg.de.","Daniel Honerkamp, Harsh Mahesheka, Jan Ole von Hartz, Tim Welschehold, Abhinav Valada",2024-09-23T15:09:45Z,2025-02-10T10:50:14Z,http://arxiv.org/abs/2409.15095v2,http://arxiv.org/pdf/2409.15095v2.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Safe Expeditious Whole-Body Control of Mobile Manipulators for Collision
  Avoidance","Whole-body reactive obstacle avoidance for mobile manipulators (MM) remains
an open research problem. Control Barrier Functions (CBF), combined with
Quadratic Programming (QP), have become a popular approach for reactive control
with safety guarantees. However, traditional CBF methods often face issues such
as pseudo-equilibrium problems (PEP) and are ineffective in handling dynamic
obstacles. To overcome these challenges, we introduce the Adaptive Cyclic
Inequality (ACI) method. ACI takes into account both the obstacle's velocity
and the robot's nominal control to define a directional safety constraint. When
added to the CBF-QP, ACI helps avoid PEP and enables reliable collision
avoidance in dynamic environments. We validate our approach on a MM that
includes a low-dimensional mobile base and a high-dimensional manipulator,
demonstrating the generality of the framework. In addition, we integrate a
simple yet effective method for avoiding self-collisions, allowing the robot
enabling comprehensive whole-body collision-free operation. Extensive benchmark
comparisons and experiments demonstrate that our method performs well in
unknown and dynamic scenarios, including difficult tasks like avoiding sticks
swung by humans and rapidly thrown objects.","Bingjie Chen, Yancong Wei, Rihao Liu, Chenxi Han, Houde Liu, Chongkun Xia, Liang Han, Bin Liang",2024-09-23T07:46:19Z,2025-07-28T12:26:21Z,http://arxiv.org/abs/2409.14775v3,http://arxiv.org/pdf/2409.14775v3.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Admittance Control-based Floating Base Reaction Mitigation for Limbed
  Climbing Robots","Reaction force-aware control is essential for legged climbing robots to
ensure a safer and more stable operation. This becomes particularly crucial
when navigating steep terrain or operating in microgravity environments, where
excessive reaction forces may result in the loss of foot contact with the
ground, leading to potential falls or floating over in microgravity.
Furthermore, such robots are often tasked with manipulation activities,
exposing them to external forces in addition to those generated during
locomotion. To effectively handle such disturbances while maintaining precise
motion trajectory tracking, we propose a novel control scheme based on
position-based impedance control, also known as admittance control. We
validated this control method through simulation-based case studies by
intentionally introducing continuous and impact interference forces to simulate
scenarios such as object manipulation or obstacle collisions. The results
demonstrated a significant reduction in both the reaction force and joint
torque when employing the proposed method.","Masazumi Imai, Kentaro Uno, Kazuya Yoshida",2024-09-20T05:10:25Z,2024-09-20T05:10:25Z,http://arxiv.org/abs/2409.13218v1,http://arxiv.org/pdf/2409.13218v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"System-Level Efficient Performance of EMLA-Driven Heavy-Duty
  Manipulators via Bilevel Optimization Framework with a Leader--Follower
  Scenario","The global push for sustainability and energy efficiency is driving
significant advancements across various industries, including the development
of electrified solutions for heavy-duty mobile manipulators (HDMMs).
Electromechanical linear actuators (EMLAs), powered by permanent magnet
synchronous motors, present an all-electric alternative to traditional internal
combustion engine (ICE)-powered hydraulic actuators, offering a promising path
toward an eco-friendly future for HDMMs. However, the limited operational range
of electrified HDMMs, closely tied to battery capacity, highlights the need to
fully exploit the potential of EMLAs that driving the manipulators. This goal
is contingent upon a deep understanding of the harmonious interplay between
EMLA mechanisms and the dynamic behavior of heavy-duty manipulators. To this
end, this paper introduces a bilevel multi-objective optimization framework,
conceptualizing the EMLA-actuated manipulator of an electrified HDMM as a
leader--follower scenario. At the leader level, the optimization algorithm
maximizes EMLA efficiency by considering electrical and mechanical constraints,
while the follower level optimizes manipulator motion through a trajectory
reference generator that adheres to manipulator limits. This optimization
approach ensures that the system operates with a synergistic trade-off between
the most efficient operating region of the actuation system, achieving a total
efficiency of 70.3\%, and high manipulator performance. Furthermore, to
complement this framework and ensure precise tracking of the generated optimal
trajectories, a robust, adaptive, subsystem-based control strategy is developed
with accurate control and exponential stability. The proposed methodologies are
validated on a three-degrees-of-freedom manipulator, demonstrating significant
efficiency improvements while maintaining high-performance operation.","Mohammad Bahari, Alvaro Paz, Mehdi Heydari Shahna, Jouni Mattila",2024-09-18T10:04:30Z,2024-09-18T10:04:30Z,http://arxiv.org/abs/2409.11849v1,http://arxiv.org/pdf/2409.11849v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"DeMoBot: Deformable Mobile Manipulation with Vision-based Sub-goal
  Retrieval","Imitation learning (IL) algorithms typically distil experience into
parametric behavior policies to mimic expert demonstrations. With limited
experience previous methods often struggle and cannot accurately align the
current state with expert demonstrations, particularly in tasks that are
characterised by partial observations or dynamic object deformations. We
consider imitation learning in deformable mobile manipulation with an
ego-centric limited field of view and introduce a novel IL approach called
DeMoBot that directly retrieves observations from demonstrations. DeMoBot
utilizes vision foundation models to identify relevant expert data based on
visual similarity and matches the current trajectory with demonstrated
trajectories using trajectory similarity and forward reachability constraints
to select suitable sub-goals. A goal-conditioned motion generation policy shall
guide the robot to the sub-goal until the task is completed. We evaluate
DeMoBot using a Spot robot in several simulated and real-world settings,
demonstrating its effectiveness and generalizability. DeMoBot outperforms
baselines with only 20 demonstrations, attaining high success rates in gap
covering (85% simulation, 80% real-world) and table uncovering (87.5%
simulation, 70% real-world), while showing promise in complex tasks like
curtain opening (47.5% simulation, 35% real-world). Additional details are
available at: https://sites.google.com/view/demobot-fewshot/home","Yuying Zhang, Wenyan Yang, Guhan Sivasubramanian, Joni Pajarinen",2024-08-28T16:33:21Z,2024-12-18T10:05:46Z,http://arxiv.org/abs/2408.15919v2,http://arxiv.org/pdf/2408.15919v2.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
PhysPart: Physically Plausible Part Completion for Interactable Objects,"Interactable objects are ubiquitous in our daily lives. Recent advances in 3D
generative models make it possible to automate the modeling of these objects,
benefiting a range of applications from 3D printing to the creation of robot
simulation environments. However, while significant progress has been made in
modeling 3D shapes and appearances, modeling object physics, particularly for
interactable objects, remains challenging due to the physical constraints
imposed by inter-part motions. In this paper, we tackle the problem of
physically plausible part completion for interactable objects, aiming to
generate 3D parts that not only fit precisely into the object but also allow
smooth part motions. To this end, we propose a diffusion-based part generation
model that utilizes geometric conditioning through classifier-free guidance and
formulates physical constraints as a set of stability and mobility losses to
guide the sampling process. Additionally, we demonstrate the generation of
dependent parts, paving the way toward sequential part generation for objects
with complex part-whole hierarchies. Experimentally, we introduce a new metric
for measuring physical plausibility based on motion success rates. Our model
outperforms existing baselines over shape and physical metrics, especially
those that do not adequately model physical constraints. We also demonstrate
our applications in 3D printing, robot manipulation, and sequential part
generation, showing our strength in realistic tasks with the demand for high
physical plausibility.","Rundong Luo, Haoran Geng, Congyue Deng, Puhao Li, Zan Wang, Baoxiong Jia, Leonidas Guibas, Siyuan Huang",2024-08-25T04:56:09Z,2025-02-04T02:39:54Z,http://arxiv.org/abs/2408.13724v3,http://arxiv.org/pdf/2408.13724v3.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Tunable interfacial Rashba spin-orbit coupling in asymmetric
  Al$_x$In$_{1-x}$Sb/InSb/CdTe quantum well heterostructures","The manipulation of Rashba-type spin-orbit coupling (SOC) in molecular beam
epitaxy-grown Al$_x$In$_{1-x}$Sb/InSb/CdTe quantum well heterostructures is
reported. The effective band bending provides robust two-dimensional quantum
confinement, while the unidirectional built-in electric field from the
asymmetric hetero-interfaces results in pronounced Rashba SOC strength. By
tuning the Al concentration in the top Al$_x$In$_{1-x}$Sb barrier layer, the
optimal structure with $x = 0.15$ shows the largest Rashba coefficient of 0.23
eV-Angstrom. and the highest low-temperature electron mobility of 4400
cm$^2$/Vs . Quantitative investigations of the weak anti-localization effect
further confirm the dominant D'yakonov-Perel (DP) spin relaxation mechanism
during charge-to-spin conversion. These findings highlight the significance of
quantum well engineering in shaping magneto-resistance responses, and narrow
bandgap semiconductor-based heterostructures may offer a reliable platform for
energy-efficient spintronic applications.","Hanzhi Ruan, Zhenghang Zhi, Yuyang Wu, Jiuming Liu, Puyang Huang, Shan Yao, Xinqi Liu, Chenjia Tang, Qi Yao, Lu Sun, Yifan Zhang, Yujie Xiao, Renchao Che, Xufeng Kou",2024-08-19T14:25:01Z,2024-08-19T14:25:01Z,http://arxiv.org/abs/2408.10032v1,http://arxiv.org/pdf/2408.10032v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Autonomous Behavior Planning For Humanoid Loco-manipulation Through
  Grounded Language Model","Enabling humanoid robots to perform autonomously loco-manipulation in
unstructured environments is crucial and highly challenging for achieving
embodied intelligence. This involves robots being able to plan their actions
and behaviors in long-horizon tasks while using multi-modality to perceive
deviations between task execution and high-level planning. Recently, large
language models (LLMs) have demonstrated powerful planning and reasoning
capabilities for comprehension and processing of semantic information through
robot control tasks, as well as the usability of analytical judgment and
decision-making for multi-modal inputs. To leverage the power of LLMs towards
humanoid loco-manipulation, we propose a novel language-model based framework
that enables robots to autonomously plan behaviors and low-level execution
under given textual instructions, while observing and correcting failures that
may occur during task execution. To systematically evaluate this framework in
grounding LLMs, we created the robot 'action' and 'sensing' behavior library
for task planning, and conducted mobile manipulation tasks and experiments in
both simulated and real environments using the CENTAURO robot, and verified the
effectiveness and application of this approach in robotic tasks with autonomous
behavioral planning.","Jin Wang, Arturo Laurenzi, Nikos Tsagarakis",2024-08-15T17:33:32Z,2024-08-15T17:33:32Z,http://arxiv.org/abs/2408.08282v1,http://arxiv.org/pdf/2408.08282v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"DM2RM: Dual-Mode Multimodal Ranking for Target Objects and Receptacles
  Based on Open-Vocabulary Instructions","In this study, we aim to develop a domestic service robot (DSR) that, guided
by open-vocabulary instructions, can carry everyday objects to the specified
pieces of furniture. Few existing methods handle mobile manipulation tasks with
open-vocabulary instructions in the image retrieval setting, and most do not
identify both the target objects and the receptacles. We propose the Dual-Mode
Multimodal Ranking model (DM2RM), which enables images of both the target
objects and receptacles to be retrieved using a single model based on
multimodal foundation models. We introduce a switching mechanism that leverages
a mode token and phrase identification via a large language model to switch the
embedding space based on the prediction target. To evaluate the DM2RM, we
construct a novel dataset including real-world images collected from hundreds
of building-scale environments and crowd-sourced instructions with referring
expressions. The evaluation results show that the proposed DM2RM outperforms
previous approaches in terms of standard metrics in image retrieval settings.
Furthermore, we demonstrate the application of the DM2RM on a standardized
real-world DSR platform including fetch-and-carry actions, where it achieves a
task success rate of 82% despite the zero-shot transfer setting. Demonstration
videos, code, and more materials are available at
https://kkrr10.github.io/dm2rm/.","Ryosuke Korekata, Kanta Kaneda, Shunya Nagashima, Yuto Imai, Komei Sugiura",2024-08-15T03:34:02Z,2024-08-15T03:34:02Z,http://arxiv.org/abs/2408.07910v1,http://arxiv.org/pdf/2408.07910v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Lateral Mn5Ge3 spin-valve in contact with a high-mobility Ge
  two-dimensional hole gas","Ge two-dimensional hole gases in strained modulation-doped quantum-wells
represent a promising material platform for future spintronic applications due
to their excellent spin transport properties and the theoretical possibility of
efficient spin manipulation. Due to the continuous development of epitaxial
growth recipes extreme high hole mobilities and low effective masses can be
achieved, promising an efficient spin transport. Furthermore, the Ge
two-dimensional hole gas (2DHG) can be integrated in the well-established
industrial complementary metal-oxide-semiconductor (CMOS) devices technology.
However, efficient electrical spin injection into a Ge 2DHG - a prerequisite
for the realization of spintronic devices - has not yet been demonstrated. In
this work, we report the fabrication and low-temperature magnetoresistance
measurements of a laterally structured Mn5Ge3/Ge 2DHG/ Mn5Ge3 device. The
ferromagnetic Mn5Ge3 contacts are grown directly into the Ge quantum well by
means of an interdiffusion process with a spacing of approximately 130 nm. We
observe a magnetoresistance signal for temperatures below 13 K possibly arising
from successful spin injection. The results represent a step forward toward the
realization of CMOS compatible spintronic devices based on a 2DHG.","David Weißhaupt, Christoph Sürgers, Dominik Bloos, Hannes Simon Funk, Michael Oehme, Gerda Fischer, Markus Andreas Schubert, Christian Wenger, Joris van Slageren, Inga Anita Fischer, Jörg Schulze",2024-08-14T09:40:13Z,2024-08-14T09:40:13Z,http://arxiv.org/abs/2408.07412v1,http://arxiv.org/pdf/2408.07412v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Plant Robots: Harnessing Growth Actuation of Plants for Locomotion and
  Object Manipulation","Plants display physical displacements during their growth due to
photosynthesis, which converts light into chemical energy. This can be
interpreted as plants acting as actuators with a built-in power source. This
paper presents a method to create plant robots that move and perform tasks by
harnessing the actuation output of plants: displacement and force generated
from the growing process. As the target plant, radish sprouts are employed, and
their displacement and force are characterized, followed by the calculation of
power and energy densities. Based on the characterization, two different plant
robots are designed and fabricated: a rotational robot and a gripper. The
former demonstrates ground locomotion, achieving a travel distance of 14.6 mm
with an average speed of 0.8 mm/h. The latter demonstrates the picking and
placing of an object with a 0.1-g mass by the light-controlled open-close
motion of plant fingers. A good agreement between the experimental and model
values is observed in the specific data of the mobile robot, suggesting that
obtaining the actuation characteristics of plants can enable the design and
prediction of behavior in plant robots. These results pave the way for the
realization of novel types of environmentally friendly and sustainable robots.","Kazuya Murakami, Misao Sato, Momoki Kubota, Jun Shintake",2024-07-23T04:06:23Z,2024-09-26T00:47:25Z,http://arxiv.org/abs/2407.16162v3,http://arxiv.org/pdf/2407.16162v3.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"DISCO: Embodied Navigation and Interaction via Differentiable Scene
  Semantics and Dual-level Control","Building a general-purpose intelligent home-assistant agent skilled in
diverse tasks by human commands is a long-term blueprint of embodied AI
research, which poses requirements on task planning, environment modeling, and
object interaction. In this work, we study primitive mobile manipulations for
embodied agents, i.e. how to navigate and interact based on an instructed
verb-noun pair. We propose DISCO, which features non-trivial advancements in
contextualized scene modeling and efficient controls. In particular, DISCO
incorporates differentiable scene representations of rich semantics in object
and affordance, which is dynamically learned on the fly and facilitates
navigation planning. Besides, we propose dual-level coarse-to-fine action
controls leveraging both global and local cues to accomplish mobile
manipulation tasks efficiently. DISCO easily integrates into embodied tasks
such as embodied instruction following. To validate our approach, we take the
ALFRED benchmark of large-scale long-horizon vision-language navigation and
interaction tasks as a test bed. In extensive experiments, we make
comprehensive evaluations and demonstrate that DISCO outperforms the art by a
sizable +8.6% success rate margin in unseen scenes, even without step-by-step
instructions. Our code is publicly released at
https://github.com/AllenXuuu/DISCO.","Xinyu Xu, Shengcheng Luo, Yanchao Yang, Yong-Lu Li, Cewu Lu",2024-07-20T05:39:28Z,2024-07-20T05:39:28Z,http://arxiv.org/abs/2407.14758v1,http://arxiv.org/pdf/2407.14758v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
Anticipatory Task and Motion Planning,"We consider a sequential task and motion planning (tamp) setting in which a
robot is assigned continuous-space rearrangement-style tasks one-at-a-time in
an environment that persists between each. Lacking advance knowledge of future
tasks, existing (myopic) planning strategies unwittingly introduce side effects
that impede completion of subsequent tasks: e.g., by blocking future access or
manipulation. We present anticipatory task and motion planning, in which
estimates of expected future cost from a learned model inform selection of
plans generated by a model-based tamp planner so as to avoid such side effects,
choosing configurations of the environment that both complete the task and
minimize overall cost. Simulated multi-task deployments in
navigation-among-movable-obstacles and cabinet-loading domains yield
improvements of 32.7% and 16.7% average per-task cost respectively. When given
time in advance to prepare the environment, our learning-augmented planning
approach yields improvements of 83.1% and 22.3%. Both showcase the value of our
approach. Finally, we also demonstrate anticipatory tamp on a real-world Fetch
mobile manipulator.","Roshan Dhakal, Duc M. Nguyen, Tom Silver, Xuesu Xiao, Gregory J. Stein",2024-07-18T16:59:33Z,2024-07-18T16:59:33Z,http://arxiv.org/abs/2407.13694v1,http://arxiv.org/pdf/2407.13694v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Affordance Perception by a Knowledge-Guided Vision-Language Model with
  Efficient Error Correction","Mobile robot platforms will increasingly be tasked with activities that
involve grasping and manipulating objects in open world environments.
Affordance understanding provides a robot with means to realise its goals and
execute its tasks, e.g. to achieve autonomous navigation in unknown buildings
where it has to find doors and ways to open these. In order to get actionable
suggestions, robots need to be able to distinguish subtle differences between
objects, as they may result in different action sequences: doorknobs require
grasp and twist, while handlebars require grasp and push. In this paper, we
improve affordance perception for a robot in an open-world setting. Our
contribution is threefold: (1) We provide an affordance representation with
precise, actionable affordances; (2) We connect this knowledge base to a
foundational vision-language models (VLM) and prompt the VLM for a wider
variety of new and unseen objects; (3) We apply a human-in-the-loop for
corrections on the output of the VLM. The mix of affordance representation,
image detection and a human-in-the-loop is effective for a robot to search for
objects to achieve its goals. We have demonstrated this in a scenario of
finding various doors and the many different ways to open them.","Gertjan Burghouts, Marianne Schaaphok, Michael van Bekkum, Wouter Meijer, Fieke Hillerström, Jelle van Mil",2024-07-18T10:24:22Z,2024-07-18T10:24:22Z,http://arxiv.org/abs/2407.13368v1,http://arxiv.org/pdf/2407.13368v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
Visual-tactile manipulation to collect household waste in outdoor,"This work presents a perception system applied to robotic manipulation, that
is able to assist in navigation, household waste classification and collection
in outdoor environments. This system is made up of optical tactile sensors,
RGBD cameras and a LiDAR. These sensors are integrated on a mobile platform
with a robot manipulator and a robotic gripper. Our system is divided in three
software modules, two of them are vision-based and the last one is
tactile-based. The vision-based modules use CNNs to localize and recognize
solid household waste, together with the grasping points estimation. The
tactile-based module, which also uses CNNs and image processing, adjusts the
gripper opening to control the grasping from touch data. Our proposal achieves
localization errors around 6 %, a recognition accuracy of 98% and ensures the
grasping stability the 91% of the attempts. The sum of runtimes of the three
modules is less than 750 ms.","Julio Castaño-Amorós, Ignacio de Loyola Páez-Ubieta, Pablo Gil, Santiago Timoteo Puente",2024-07-15T10:31:57Z,2024-07-15T10:31:57Z,http://arxiv.org/abs/2407.10606v1,http://arxiv.org/pdf/2407.10606v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Model Predictive Control For Mobile Manipulators Based On Neural
  Dynamics(Extended version)","This article focuses on the trajectory tracking problem of mobile
manipulators (MMs). Firstly, we construct a position and orientation model
predictive tracking control (POMPTC) scheme for mobile manipulators. The
proposed POMPTC scheme can simultaneously minimize the tracking error, joint
velocity, and joint acceleration. Moreover, it can achieve synchronous control
for the position and orientation of the end-effector. Secondly, a finite-time
convergent neural dynamics (FTCND) model is constructed to find the optimal
solution of the POMPTC scheme. Then, based on the proposed POMPTC scheme, a
non-singular fast terminal sliding model (NFTSM) control method is presented,
which considers the disturbances caused by the base motion on the manipulator
at the dynamic level. It can achieve finite-time tracking performance and
improve the anti-disturbances ability. Finally, simulation and experiments show
that the proposed control method has the advantages of strong robustness, fast
convergence, and high control accuracy.","Tao Su, Shiqi Zheng",2024-07-11T07:17:23Z,2024-07-11T07:17:23Z,http://arxiv.org/abs/2407.08234v1,http://arxiv.org/pdf/2407.08234v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Revolutionizing Battery Disassembly: The Design and Implementation of a
  Battery Disassembly Autonomous Mobile Manipulator Robot(BEAM-1)","The efficient disassembly of end-of-life electric vehicle batteries(EOL-EVBs)
is crucial for green manufacturing and sustainable development. The current
pre-programmed disassembly conducted by the Autonomous Mobile Manipulator
Robot(AMMR) struggles to meet the disassembly requirements in dynamic
environments, complex scenarios, and unstructured processes. In this paper, we
propose a Battery Disassembly AMMR(BEAM-1) system based on NeuralSymbolic AI.
It detects the environmental state by leveraging a combination of multi-sensors
and neural predicates and then translates this information into a
quasi-symbolic space. In real-time, it identifies the optimal sequence of
action primitives through LLM-heuristic tree search, ensuring high-precision
execution of these primitives. Additionally, it employs positional speculative
sampling using intuitive networks and achieves the disassembly of various bolt
types with a meticulously designed end-effector. Importantly, BEAM-1 is a
continuously learning embodied intelligence system capable of subjective
reasoning like a human, and possessing intuition. A large number of real scene
experiments have proved that it can autonomously perceive, decide, and execute
to complete the continuous disassembly of bolts in multiple, multi-category,
and complex situations, with a success rate of 98.78%. This research attempts
to use NeuroSymbolic AI to give robots real autonomous reasoning, planning, and
learning capabilities. BEAM-1 realizes the revolution of battery disassembly.
Its framework can be easily ported to any robotic system to realize different
application scenarios, which provides a ground-breaking idea for the design and
implementation of future embodied intelligent robotic systems.","Yanlong Peng, Zhigang Wang, Yisheng Zhang, Shengmin Zhang, Nan Cai, Fan Wu, Ming Chen",2024-07-09T06:44:20Z,2024-07-09T06:44:20Z,http://arxiv.org/abs/2407.06590v1,http://arxiv.org/pdf/2407.06590v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"HiLMa-Res: A General Hierarchical Framework via Residual RL for
  Combining Quadrupedal Locomotion and Manipulation","This work presents HiLMa-Res, a hierarchical framework leveraging
reinforcement learning to tackle manipulation tasks while performing continuous
locomotion using quadrupedal robots. Unlike most previous efforts that focus on
solving a specific task, HiLMa-Res is designed to be general for various
loco-manipulation tasks that require quadrupedal robots to maintain sustained
mobility. The novel design of this framework tackles the challenges of
integrating continuous locomotion control and manipulation using legs. It
develops an operational space locomotion controller that can track arbitrary
robot end-effector (toe) trajectories while walking at different velocities.
This controller is designed to be general to different downstream tasks, and
therefore, can be utilized in high-level manipulation planning policy to
address specific tasks. To demonstrate the versatility of this framework, we
utilize HiLMa-Res to tackle several challenging loco-manipulation tasks using a
quadrupedal robot in the real world. These tasks span from leveraging
state-based policy to vision-based policy, from training purely from the
simulation data to learning from real-world data. In these tasks, HiLMa-Res
shows better performance than other methods.","Xiaoyu Huang, Qiayuan Liao, Yiming Ni, Zhongyu Li, Laura Smith, Sergey Levine, Xue Bin Peng, Koushil Sreenath",2024-07-09T06:31:54Z,2024-07-09T06:31:54Z,http://arxiv.org/abs/2407.06584v1,http://arxiv.org/pdf/2407.06584v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Toward Precise Robotic Weed Flaming Using a Mobile Manipulator with a
  Flamethrower","Robotic weed flaming is a new and environmentally friendly approach to weed
removal in the agricultural field. Using a mobile manipulator equipped with a
flamethrower, we design a new system and algorithm to enable effective weed
flaming, which requires robotic manipulation with a soft and deformable end
effector, as the thermal coverage of the flame is affected by dynamic or
unknown environmental factors such as gravity, wind, atmospheric pressure, fuel
tank pressure, and pose of the nozzle. System development includes overall
design, hardware integration, and software pipeline. To enable precise weed
removal, the greatest challenge is to detect and predict dynamic flame coverage
in real time before motion planning, which is quite different from a
conventional rigid gripper in grasping or a spray gun in painting. Based on the
images from two onboard infrared cameras and the pose information of the
flamethrower nozzle on a mobile manipulator, we propose a new dynamic flame
coverage model. The flame model uses a center-arc curve with a Gaussian
cross-section model to describe the flame coverage in real time. The
experiments have demonstrated the working system and shown that our model and
algorithm can achieve a mean average precision (mAP) of more than 76\% in the
reprojected images during online prediction.","Di Wang, Chengsong Hu, Shuangyu Xie, Joe Johnson, Hojun Ji, Yingtao Jiang, Muthukumar Bagavathiannan, Dezhen Song",2024-07-06T02:43:12Z,2024-07-06T02:43:12Z,http://arxiv.org/abs/2407.04929v1,http://arxiv.org/pdf/2407.04929v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"EquiBot: SIM(3)-Equivariant Diffusion Policy for Generalizable and Data
  Efficient Learning","Building effective imitation learning methods that enable robots to learn
from limited data and still generalize across diverse real-world environments
is a long-standing problem in robot learning. We propose Equibot, a robust,
data-efficient, and generalizable approach for robot manipulation task
learning. Our approach combines SIM(3)-equivariant neural network architectures
with diffusion models. This ensures that our learned policies are invariant to
changes in scale, rotation, and translation, enhancing their applicability to
unseen environments while retaining the benefits of diffusion-based policy
learning such as multi-modality and robustness. We show on a suite of 6
simulation tasks that our proposed method reduces the data requirements and
improves generalization to novel scenarios. In the real world, with 10
variations of 6 mobile manipulation tasks, we show that our method can easily
generalize to novel objects and scenes after learning from just 5 minutes of
human demonstrations in each task.","Jingyun Yang, Zi-ang Cao, Congyue Deng, Rika Antonova, Shuran Song, Jeannette Bohg",2024-07-01T17:09:43Z,2024-10-29T17:49:41Z,http://arxiv.org/abs/2407.01479v2,http://arxiv.org/pdf/2407.01479v2.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Open-vocabulary Mobile Manipulation in Unseen Dynamic Environments with
  3D Semantic Maps","Open-Vocabulary Mobile Manipulation (OVMM) is a crucial capability for
autonomous robots, especially when faced with the challenges posed by unknown
and dynamic environments. This task requires robots to explore and build a
semantic understanding of their surroundings, generate feasible plans to
achieve manipulation goals, adapt to environmental changes, and comprehend
natural language instructions from humans. To address these challenges, we
propose a novel framework that leverages the zero-shot detection and grounded
recognition capabilities of pretraining visual-language models (VLMs) combined
with dense 3D entity reconstruction to build 3D semantic maps. Additionally, we
utilize large language models (LLMs) for spatial region abstraction and online
planning, incorporating human instructions and spatial semantic context. We
have built a 10-DoF mobile manipulation robotic platform JSR-1 and demonstrated
in real-world robot experiments that our proposed framework can effectively
capture spatial semantics and process natural language user instructions for
zero-shot OVMM tasks under dynamic environment settings, with an overall
navigation and task success rate of 80.95% and 73.33% over 105 episodes, and
better SFT and SPL by 157.18% and 19.53% respectively compared to the baseline.
Furthermore, the framework is capable of replanning towards the next most
probable candidate location based on the spatial semantic context derived from
the 3D semantic map when initial plans fail, keeping an average success rate of
76.67%.","Dicong Qiu, Wenzong Ma, Zhenfu Pan, Hui Xiong, Junwei Liang",2024-06-26T07:06:42Z,2024-06-26T07:06:42Z,http://arxiv.org/abs/2406.18115v1,http://arxiv.org/pdf/2406.18115v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Robust Dynamic Control Barrier Function Based Trajectory Planning for
  Mobile Manipulator","High-dimensional robot dynamic trajectory planning poses many challenges for
traditional planning algorithms. Existing planning methods suffer from issues
such as long computation times, limited capacity to address intricate obstacle
models, and lack of consideration for external disturbances and measurement
inaccuracies in these high-dimensional systems. To tackle these challenges,
this paper proposes a novel trajectory planning approach that combines Dynamic
Control Barrier Function (DCBF) with a disturbance observer to create a Robust
Dynamic Control Barrier Function (RDCBF) planner. This approach successfully
plans trajectories in environments with complex dynamic obstacles while
accounting for external disturbances and measurement uncertainties, ensuring
system safety and enabling precise obstacle avoidance. Experimental results on
a mobile manipulator demonstrate outstanding performance of the proposed
approach.","Lihao Xu, Xiaogang Xiong, Bai Yang, Yunjiang Lou",2024-06-22T10:24:48Z,2024-06-22T10:24:48Z,http://arxiv.org/abs/2406.15806v1,http://arxiv.org/pdf/2406.15806v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Beyond Near-Field: Far-Field Location Division Multiple Access in
  Downlink MIMO Systems","Exploring channel dimensions has been the driving force behind breakthroughs
in successive generations of mobile communication systems. In 5G, space
division multiple access (SDMA) leveraging massive MIMO has been crucial in
enhancing system capacity through spatial differentiation of users. However,
SDMA can only finely distinguish users at adjacent angles in ultra-dense
networks by extremely large-scale antenna arrays. For a long time, most
research has focused on the angle domain of the space, overlooking the
potential of the distance domain. Near-field location division multiple access
(LDMA) was proposed based on the beam-focusing effect yielded by near-field
spherical propagation model, partitioning channel resources by both angle and
distance. To achieve a similar idea in the far-field region, this paper
introduces a far-field LDMA scheme for wideband systems based on orthogonal
frequency division multiplexing (OFDM). Benefiting from frequency diverse
arrays (FDA), it becomes possible to manipulate beams in the distance domain.
Combined with OFDM, the inherent cyclic prefix ensures a complete OFDM symbol
can be received without losing distance information, while the matched filter
of OFDM helps eliminate the time-variance of FDA steering vectors. Theoretical
and simulation results show that LDMA can fully exploit the additional degrees
of freedom in the distance domain to significantly improve spectral efficiency,
especially in narrow sector multiple access (MA) scenarios. Moreover, LDMA can
maintain independence between array elements even in single-path channels,
making it stand out in MA schemes at millimeter-wave and higher frequency
bands.","Haoyan Liu, Caijian Jie, Min Yang, Chengguang Li",2024-06-18T13:28:21Z,2025-01-30T08:01:07Z,http://arxiv.org/abs/2406.12596v3,http://arxiv.org/pdf/2406.12596v3.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Online Pareto-Optimal Decision-Making for Complex Tasks using Active
  Inference","When a robot autonomously performs a complex task, it frequently must balance
competing objectives while maintaining safety. This becomes more difficult in
uncertain environments with stochastic outcomes. Enhancing transparency in the
robot's behavior and aligning with user preferences are also crucial. This
paper introduces a novel framework for multi-objective reinforcement learning
that ensures safe task execution, optimizes trade-offs between objectives, and
adheres to user preferences. The framework has two main layers: a
multi-objective task planner and a high-level selector. The planning layer
generates a set of optimal trade-off plans that guarantee satisfaction of a
temporal logic task. The selector uses active inference to decide which
generated plan best complies with user preferences and aids learning. Operating
iteratively, the framework updates a parameterized learning model based on
collected data. Case studies and benchmarks on both manipulation and mobile
robots show that our framework outperforms other methods and (i) learns
multiple optimal trade-offs, (ii) adheres to a user preference, and (iii)
allows the user to adjust the balance between (i) and (ii).","Peter Amorese, Shohei Wakayama, Nisar Ahmed, Morteza Lahijanian",2024-06-17T18:03:45Z,2024-06-17T18:03:45Z,http://arxiv.org/abs/2406.11984v1,http://arxiv.org/pdf/2406.11984v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Rethinking Waveform for 6G: Harnessing Delay-Doppler Alignment
  Modulation","Waveform design has served as a cornerstone for each generation of mobile
communication systems. The future sixth-generation (6G) mobile communication
networks are expected to employ larger-scale antenna arrays and exploit
higher-frequency bands for further boosting data transmission rate and
providing ubiquitous wireless sensing. This brings new opportunities and
challenges for 6G waveform design. In this article, by leveraging the super
spatial resolution of large antenna arrays and the multi-path spatial sparsity
of highfrequency wireless channels, we introduce a new approach for waveform
design based on the recently proposed delay-Doppler alignment modulation
(DDAM). In particular, DDAM makes a paradigm shift of waveform design from the
conventional manner of tolerating channel delay and Doppler spreads to actively
manipulating them. First, we review the fundamental constraints and performance
limitations of orthogonal frequency division multiplexing (OFDM) and introduce
new opportunities for 6G waveform design. Next, the motivations and basic
principles of DDAM are presented, followed by its various extensions to
different wireless system setups. Finally, the main design considerations for
DDAM are discussed and the new opportunities for future research are
highlighted.","Zhiqiang Xiao, Xianda Liu, Yong Zeng, J. Andrew Zhang, Shi Jin, Rui Zhang",2024-06-13T14:51:04Z,2024-06-13T14:51:04Z,http://arxiv.org/abs/2406.09190v1,http://arxiv.org/pdf/2406.09190v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"BaSeNet: A Learning-based Mobile Manipulator Base Pose Sequence Planning
  for Pickup Tasks","In many applications, a mobile manipulator robot is required to grasp a set
of objects distributed in space. This may not be feasible from a single base
pose and the robot must plan the sequence of base poses for grasping all
objects, minimizing the total navigation and grasping time. This is a
Combinatorial Optimization problem that can be solved using exact methods,
which provide optimal solutions but are computationally expensive, or
approximate methods, which offer computationally efficient but sub-optimal
solutions. Recent studies have shown that learning-based methods can solve
Combinatorial Optimization problems, providing near-optimal and computationally
efficient solutions.
  In this work, we present BASENET - a learning-based approach to plan the
sequence of base poses for the robot to grasp all the objects in the scene. We
propose a Reinforcement Learning based solution that learns the base poses for
grasping individual objects and the sequence in which the objects should be
grasped to minimize the total navigation and grasping costs using Layered
Learning. As the problem has a varying number of states and actions, we
represent states and actions as a graph and use Graph Neural Networks for
learning. We show that the proposed method can produce comparable solutions to
exact and approximate methods with significantly less computation time.","Lakshadeep Naik, Sinan Kalkan, Sune L. Sørensen, Mikkel B. Kjærgaard, Norbert Krüger",2024-06-12T21:31:32Z,2024-06-12T21:31:32Z,http://arxiv.org/abs/2406.08653v1,http://arxiv.org/pdf/2406.08653v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
Open-Vocabulary Part-Based Grasping,"Many robotic applications require to grasp objects not arbitrarily but at a
very specific object part. This is especially important for manipulation tasks
beyond simple pick-and-place scenarios or in robot-human interactions, such as
object handovers. We propose AnyPart, a practical system that combines
open-vocabulary object detection, open-vocabulary part segmentation and 6DOF
grasp pose prediction to infer a grasp pose on a specific part of an object in
800 milliseconds. We contribute two new datasets for the task of
open-vocabulary part-based grasping, a hand-segmented dataset containing 1014
object-part segmentations, and a dataset of real-world scenarios gathered
during our robot trials for individual objects and table-clearing tasks. We
evaluate AnyPart on a mobile manipulator robot using a set of 28 common
household objects over 360 grasping trials. AnyPart is capable of producing
successful grasps 69.52 %, when ignoring robot-based grasp failures, AnyPart
predicts a grasp location on the correct part 88.57 % of the time.","Tjeard van Oort, Dimity Miller, Will N. Browne, Nicolas Marticorena, Jesse Haviland, Niko Suenderhauf",2024-06-10T01:06:01Z,2024-06-10T01:06:01Z,http://arxiv.org/abs/2406.05951v1,http://arxiv.org/pdf/2406.05951v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Distributed Motion Control of Multiple Mobile Manipulators for Reducing
  Interaction Wrench in Object Manipulation","In real-world cooperative manipulation of objects, multiple mobile
manipulator systems may suffer from disturbances and asynchrony, leading to
excessive interaction wrenches and potentially causing object damage or
emergency stops. Existing methods often rely on torque control and dynamic
models, which are uncommon in many industrial robots and settings.
Additionally, dynamic models often neglect joint friction forces and are not
accurate. These methods are challenging to implement and validate in physical
systems. To address the problems, this paper presents a novel distributed
motion control approach aimed at reducing these unnecessary interaction
wrenches. The control law is only based on local information and joint velocity
control to enhance practical applicability. The communication delays within the
distributed architecture are considered. The stability of the control law is
rigorously proven by the Lyapunov theorem. In the simulations, the
effectiveness is shown, and the impact of communication graph connectivity and
communication delays has been studied. A comparison with other methods shows
the advantages of the proposed control law in terms of convergence speed and
robustness. Finally, the control law has been validated in physical
experiments. It does not require dynamic modeling or torque control, and thus
is more user-friendly for physical robots.","Wenhang Liu, Meng Ren, Kun Song, Gaoming Chen, Michael Yu Wang, Zhenhua Xiong",2024-06-09T02:12:06Z,2025-04-07T09:12:48Z,http://arxiv.org/abs/2406.05613v4,http://arxiv.org/pdf/2406.05613v4.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Huygens-Fresnel Model Based Position-Aided Phase Configuration for 1-Bit
  RIS Assisted Wireless Communication","Reconfigurable intelligent surface (RIS), composed of nearly passive
elements, is regarded as one of the potential paradigms to support
multi-gigabit data in real-time. However, in traditional CSI (channel state
information) driven frame, the training overhead of channel estimation greatly
increases as the number of RIS elements increases to intelligently manipulate
the reflected signals. To conveniently use the reflected signal without complex
CSI feedback, in this paper we propose a position-aided phase configuration
scheme based on the property of Fresnel zone. In particular, we design the
impedance based discrete RIS elements with joint absorption mode and reflection
mode considering the fabrication complexities, which integrated the property of
the Fresnel zone to resist the impact of position error. Then, with joint
absorption and 1-bit reflection mode elements, we develop the two-step
position-aided ON/OFF states judgement (TPOSJ) scheme and the frame structure
to control the ON/OFF state of RIS, followed by analyzing the impacts of
mobility and position error on our proposed scheme. Also, we derive the
Helmholtz-Kirchhoff integral theorem based power flow. Simulations show that
the proposed scheme can manipulate the ON/OFF state intelligently without
complex CSI, thus verifying the practical application of our proposed scheme.","Xiao Zheng, Wenchi Cheng, Jiangzhou Wang",2024-06-05T07:18:56Z,2024-06-05T07:18:56Z,http://arxiv.org/abs/2406.03011v1,http://arxiv.org/pdf/2406.03011v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Dropout MPC: An Ensemble Neural MPC Approach for Systems with Learned
  Dynamics","Neural networks are lately more and more often being used in the context of
data-driven control, as an approximate model of the true system dynamics. Model
Predictive Control (MPC) adopts this practise leading to neural MPC strategies.
This raises a question of whether the trained neural network has converged and
generalized in a way that the learned model encapsulates an accurate
approximation of the true dynamic model of the system, thus making it a
reliable choice for model-based control, especially for disturbed and uncertain
systems. To tackle that, we propose Dropout MPC, a novel sampling-based
ensemble neural MPC algorithm that employs the Monte-Carlo dropout technique on
the learned system model. The closed loop is based on an ensemble of predictive
controllers, that are used simultaneously at each time-step for trajectory
optimization. Each member of the ensemble influences the control input, based
on a weighted voting scheme, thus by employing different realizations of the
learned system dynamics, neural control becomes more reliable by design. An
additional strength of the method is that it offers by design a way to estimate
future uncertainty, leading to cautious control. While the method aims in
general at uncertain systems with complex dynamics, where models derived from
first principles are hard to infer, to showcase the application we utilize data
gathered in the laboratory from a real mobile manipulator and employ the
proposed algorithm for the navigation of the robot in simulation.","Spyridon Syntakas, Kostas Vlachos",2024-06-04T17:15:25Z,2024-06-04T17:15:25Z,http://arxiv.org/abs/2406.02497v1,http://arxiv.org/pdf/2406.02497v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"A Multipurpose Interface for Close- and Far-Proximity Control of Mobile
  Collaborative Robots","This letter introduces an innovative visuo-haptic interface to control Mobile
Collaborative Robots (MCR). Thanks to a passive detachable mechanism, the
interface can be attached/detached from a robot, offering two control modes:
local control (attached) and teleoperation (detached). These modes are
integrated with a robot whole-body controller and presented in a unified close-
and far-proximity control framework for MCR. The earlier introduction of the
haptic component in this interface enabled users to execute intricate
loco-manipulation tasks via admittance-type control, effectively decoupling
task dynamics and enhancing human capabilities. In contrast, this ongoing work
proposes a novel design that integrates a visual component. This design
utilizes Visual-Inertial Odometry (VIO) for teleoperation, estimating the
interface's pose through stereo cameras and an Inertial Measurement Unit (IMU).
The estimated pose serves as the reference for the robot's end-effector in
teleoperation mode. Hence, the interface offers complete flexibility and
adaptability, enabling any user to operate an MCR seamlessly without needing
expert knowledge. In this letter, we primarily focus on the new visual feature,
and first present a performance evaluation of different VIO-based methods for
teleoperation. Next, the interface's usability is analyzed in a home-care
application and compared to an alternative designed by a commercial MoCap
system. Results show comparable performance in terms of accuracy, completion
time, and usability. Nevertheless, the proposed interface is low-cost, poses
minimal wearability constraints, and can be used anywhere and anytime without
needing external devices or additional equipment, offering a versatile and
accessible solution for teleoperation.","Hamidreza Raei, Juan M. Gandarias, Elena De Momi, Pietro Balatti, Arash Ajoudani",2024-06-04T10:00:46Z,2024-06-04T10:00:46Z,http://arxiv.org/abs/2406.02171v1,http://arxiv.org/pdf/2406.02171v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Vision-based Manipulation from Single Human Video with Open-World Object
  Graphs","This work presents an object-centric approach to learning vision-based
manipulation skills from human videos. We investigate the problem of robot
manipulation via imitation in the open-world setting, where a robot learns to
manipulate novel objects from a single video demonstration. We introduce ORION,
an algorithm that tackles the problem by extracting an object-centric
manipulation plan from a single RGB or RGB-D video and deriving a policy that
conditions on the extracted plan. Our method enables the robot to learn from
videos captured by daily mobile devices and to generalize the policies to
deployment environments with varying visual backgrounds, camera angles, spatial
layouts, and novel object instances. We systematically evaluate our method on
both short-horizon and long-horizon tasks, using RGB-D and RGB-only
demonstration videos. Across varied tasks and demonstration types (RGB-D /
RGB), we observe an average success rate of 74.4%, demonstrating the efficacy
of ORION in learning from a single human video in the open world. Additional
materials can be found on our project website:
https://ut-austin-rpl.github.io/ORION-release.","Yifeng Zhu, Arisrei Lim, Peter Stone, Yuke Zhu",2024-05-30T17:56:54Z,2025-09-04T08:23:37Z,http://arxiv.org/abs/2405.20321v2,http://arxiv.org/pdf/2405.20321v2.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
Planning Robot Placement for Object Grasping,"When performing manipulation-based activities such as picking objects, a
mobile robot needs to position its base at a location that supports successful
execution. To address this problem, prominent approaches typically rely on
costly grasp planners to provide grasp poses for a target object, which are
then are then analysed to identify the best robot placements for achieving each
grasp pose. In this paper, we propose instead to first find robot placements
that would not result in collision with the environment and from where picking
up the object is feasible, then evaluate them to find the best placement
candidate. Our approach takes into account the robot's reachability, as well as
RGB-D images and occupancy grid maps of the environment for identifying
suitable robot poses. The proposed algorithm is embedded in a service robotic
workflow, in which a person points to select the target object for grasping. We
evaluate our approach with a series of grasping experiments, against an
existing baseline implementation that sends the robot to a fixed navigation
goal. The experimental results show how the approach allows the robot to grasp
the target object from locations that are very challenging to the baseline
implementation.","Manish Saini, Melvin Paul Jacob, Minh Nguyen, Nico Hochgeschwender",2024-05-26T20:57:32Z,2024-05-26T20:57:32Z,http://arxiv.org/abs/2405.16692v1,http://arxiv.org/pdf/2405.16692v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"An Adaptive Framework for Manipulator Skill Reproduction in Dynamic
  Environments","Robot skill learning and execution in uncertain and dynamic environments is a
challenging task. This paper proposes an adaptive framework that combines
Learning from Demonstration (LfD), environment state prediction, and high-level
decision making. Proactive adaptation prevents the need for reactive
adaptation, which lags behind changes in the environment rather than
anticipating them. We propose a novel LfD representation, Elastic-Laplacian
Trajectory Editing (ELTE), which continuously adapts the trajectory shape to
predictions of future states. Then, a high-level reactive system using an
Unscented Kalman Filter (UKF) and Hidden Markov Model (HMM) prevents unsafe
execution in the current state of the dynamic environment based on a discrete
set of decisions. We first validate our LfD representation in simulation, then
experimentally assess the entire framework using a legged mobile manipulator in
36 real-world scenarios. We show the effectiveness of the proposed framework
under different dynamic changes in the environment. Our results show that the
proposed framework produces robust and stable adaptive behaviors.","Ryan Donald, Brendan Hertel, Stephen Misenti, Yan Gu, Reza Azadeh",2024-05-24T17:01:12Z,2024-05-24T17:01:12Z,http://arxiv.org/abs/2405.15711v1,http://arxiv.org/pdf/2405.15711v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
Dynamic Planning for Sequential Whole-body Mobile Manipulation,"The dynamic Sequential Mobile Manipulation Planning (SMMP) framework is
essential for the safe and robust operation of mobile manipulators in dynamic
environments. Previous research has primarily focused on either motion-level or
task-level dynamic planning, with limitations in handling state changes that
have long-term effects or in generating responsive motions for diverse tasks,
respectively. This paper presents a holistic dynamic planning framework that
extends the Virtual Kinematic Chain (VKC)-based SMMP method, automating dynamic
long-term task planning and reactive whole-body motion generation for SMMP
problems. The framework consists of an online task planning module designed to
respond to environment changes with long-term effects, a VKC-based whole-body
motion planning module for manipulating both rigid and articulated objects,
alongside a reactive Model Predictive Control (MPC) module for obstacle
avoidance during execution. Simulations and real-world experiments validate the
framework, demonstrating its efficacy and validity across sequential mobile
manipulation tasks, even in scenarios involving human interference.","Zhitian Li, Yida Niu, Yao Su, Hangxin Liu, Ziyuan Jiao",2024-05-24T09:23:51Z,2024-06-21T03:19:07Z,http://arxiv.org/abs/2405.15377v3,http://arxiv.org/pdf/2405.15377v3.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"R-NeRF: Neural Radiance Fields for Modeling RIS-enabled Wireless
  Environments","Recently, ray tracing has gained renewed interest with the advent of
Reflective Intelligent Surfaces (RIS) technology, a key enabler of 6G wireless
communications due to its capability of intelligent manipulation of
electromagnetic waves. However, accurately modeling RIS-enabled wireless
environments poses significant challenges due to the complex variations caused
by various environmental factors and the mobility of RISs. In this paper, we
propose a novel modeling approach using Neural Radiance Fields (NeRF) to
characterize the dynamics of electromagnetic fields in such environments. Our
method utilizes NeRF-based ray tracing to intuitively capture and visualize the
complex dynamics of signal propagation, effectively modeling the complete
signal pathways from the transmitter to the RIS, and from the RIS to the
receiver. This two-stage process accurately characterizes multiple complex
transmission paths, enhancing our understanding of signal behavior in
real-world scenarios. Our approach predicts the signal field for any specified
RIS placement and receiver location, facilitating efficient RIS deployment.
Experimental evaluations using both simulated and real-world data validate the
significant benefits of our methodology.","Huiying Yang, Zihan Jin, Chenhao Wu, Rujing Xiong, Robert Caiming Qiu, Zenan Ling",2024-05-19T13:11:48Z,2024-11-06T08:07:50Z,http://arxiv.org/abs/2405.11541v2,http://arxiv.org/pdf/2405.11541v2.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
Neural Randomized Planning for Whole Body Robot Motion,"Robot motion planning has made vast advances over the past decades, but the
challenge remains: robot mobile manipulators struggle to plan long-range
whole-body motion in common household environments in real time, because of
high-dimensional robot configuration space and complex environment geometry. To
tackle the challenge, this paper proposes Neural Randomized Planner (NRP),
which combines a global sampling-based motion planning (SBMP) algorithm and a
local neural sampler. Intuitively, NRP uses the search structure inside the
global planner to stitch together learned local sampling distributions to form
a global sampling distribution adaptively. It benefits from both learning and
planning. Locally, it tackles high dimensionality by learning to sample in
promising regions from data, with a rich neural network representation.
Globally, it composes the local sampling distributions through planning and
exploits local geometric similarity to scale up to complex environments.
Experiments both in simulation and on a real robot show \NRP yields superior
performance compared to some of the best classical and learning-enhanced SBMP
algorithms. Further, despite being trained in simulation, NRP demonstrates
zero-shot transfer to a real robot operating in novel household environments,
without any fine-tuning or manual adaptation.","Yunfan Lu, Yuchen Ma, David Hsu, Panpan Cai",2024-05-18T15:26:41Z,2024-08-12T12:57:43Z,http://arxiv.org/abs/2405.11317v2,http://arxiv.org/pdf/2405.11317v2.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"DEMO: RTKiller -- manipulation of GNSS RTK rovers by reference base
  spoofing","Global Navigation Satellite Systems (GNSS) provide global positioning and
timing. Multiple receivers with known reference positions (stations) can assist
mobile receivers (rovers) in obtaining GNSS corrections and achieve
centimeter-level accuracy on consumer devices. However, GNSS spoofing and
jamming, nowadays achievable with off-the-shelf devices, are serious threats to
the integrity and robustness of public correction networks. In this demo, we
show how manipulation of the Position Navigation and Timing (PNT) solution at
the reference station is reflected in the loss of baseline fix or degraded
accuracy at the rover. Real Time Kinematics (RTK) corrections are valuable but
fundamentally vulnerable: attacking the reference stations can harm all
receivers (rovers) that rely on the targeted reference station.","Marco Spanghero, Panos Papadimitratos",2024-05-17T17:07:51Z,2024-05-17T17:07:51Z,http://arxiv.org/abs/2406.07565v1,http://arxiv.org/pdf/2406.07565v1.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Magnetization dynamics in skyrmions due to high-speed carrier injections
  from Dirac half-metals","Recent developments in the magnetization dynamics in spin textures,
particularly skyrmions, offer promising new directions for magnetic storage
technologies and spintronics. Skyrmions, characterized by their topological
protection and efficient mobility at low current density, are increasingly
recognized for their potential applications in next-generation logic and memory
devices. This study investigates the dynamics of skyrmion magnetization,
focusing on the manipulation of their topological states as a basis for bitwise
data storage through a modified Landau-Lifshitz-Gilbert equation (LLG). We
introduce spin-polarized electrons from a topological ferromagnet that induce
an electric dipole moment that interacts with the electric gauge field within
the skyrmion domain. This interaction creates an effective magnetic field that
results in a torque that can dynamically change the topological state of the
skyrmion. In particular, we show that these torques can selectively destroy and
create skyrmions, effectively writing and erasing bits, highlighting the
potential of using controlled electron injection for robust and scalable
skyrmion-based data storage solutions.","Satadeep Bhattacharjee, Seung-Cheol Lee",2024-05-14T11:40:56Z,2024-08-20T06:37:52Z,http://arxiv.org/abs/2405.08516v3,http://arxiv.org/pdf/2405.08516v3.pdf,all:manipulator AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
EMMA: Scaling Mobile Manipulation via Egocentric Human Data,"Scaling mobile manipulation imitation learning is bottlenecked by expensive
mobile robot teleoperation. We present Egocentric Mobile MAnipulation (EMMA),
an end-to-end framework training mobile manipulation policies from human mobile
manipulation data with static robot data, sidestepping mobile teleoperation. To
accomplish this, we co-train human full-body motion data with static robot
data. In our experiments across three real-world tasks, EMMA demonstrates
comparable performance to baselines trained on teleoperated mobile robot data
(Mobile ALOHA), achieving higher or equivalent task performance in full task
success. We find that EMMA is able to generalize to new spatial configurations
and scenes, and we observe positive performance scaling as we increase the
hours of human data, opening new avenues for scalable robotic learning in
real-world environments. Details of this project can be found at
https://ego-moma.github.io/.","Lawrence Y. Zhu, Pranav Kuppili, Ryan Punamiya, Patcharapong Aphiwetsa, Dhruv Patel, Simar Kareer, Sehoon Ha, Danfei Xu",2025-09-04T17:59:10Z,2025-09-04T17:59:10Z,http://arxiv.org/abs/2509.04443v1,http://arxiv.org/pdf/2509.04443v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"Fail2Progress: Learning from Real-World Robot Failures with Stein
  Variational Inference","Skill effect models for long-horizon manipulation tasks are prone to failures
in conditions not covered by training data distributions. Therefore, enabling
robots to reason about and learn from failures is necessary. We investigate the
problem of efficiently generating a dataset targeted to observed failures.
After fine-tuning a skill effect model on this dataset, we evaluate the extent
to which the model can recover from failures and minimize future failures. We
propose Fail2Progress, an approach that leverages Stein variational inference
to generate multiple simulation environments in parallel, enabling efficient
data sample generation similar to observed failures. Our method is capable of
handling several challenging mobile manipulation tasks, including transporting
multiple objects, organizing a constrained shelf, and tabletop organization.
Through large-scale simulation and real-world experiments, we demonstrate that
our approach excels at learning from failures across different numbers of
objects. Furthermore, we show that Fail2Progress outperforms several baselines.","Yixuan Huang, Novella Alvina, Mohanraj Devendran Shanthi, Tucker Hermans",2025-09-01T20:00:56Z,2025-09-01T20:00:56Z,http://arxiv.org/abs/2509.01746v1,http://arxiv.org/pdf/2509.01746v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
Galaxea Open-World Dataset and G0 Dual-System VLA Model,"We present Galaxea Open-World Dataset, a large-scale, diverse collection of
robot behaviors recorded in authentic human living and working environments.
All demonstrations are gathered using a consistent robotic embodiment, paired
with precise subtask-level language annotations to facilitate both training and
evaluation. Building on this dataset, we introduce G0, a dual-system framework
that couples a Vision-Language Model (VLM) for multimodal planning with a
Vision-Language-Action (VLA) model for fine-grained execution. G0 is trained
using a three-stage curriculum: cross-embodiment pre-training,
single-embodiment pre-training, and task-specific post-training. A
comprehensive benchmark spanning tabletop manipulation, few-shot learning, and
long-horizon mobile manipulation, demonstrates the effectiveness of our
approach. In particular, we find that the single-embodiment pre-training stage,
together with the Galaxea Open-World Dataset, plays a critical role in
achieving strong performance.","Tao Jiang, Tianyuan Yuan, Yicheng Liu, Chenhao Lu, Jianning Cui, Xiao Liu, Shuiqi Cheng, Jiyang Gao, Huazhe Xu, Hang Zhao",2025-08-30T18:04:19Z,2025-08-30T18:04:19Z,http://arxiv.org/abs/2509.00576v1,http://arxiv.org/pdf/2509.00576v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
Autonomous Mobile Plant Watering Robot : A Kinematic Approach,"Plants need regular and the appropriate amount of watering to thrive and
survive. While agricultural robots exist that can spray water on plants and
crops such as the , they are expensive and have limited mobility and/or
functionality. We introduce a novel autonomous mobile plant watering robot that
uses a 6 degree of freedom (DOF) manipulator, connected to a 4 wheel drive
alloy chassis, to be able to hold a garden hose, recognize and detect plants,
and to water them with the appropriate amount of water by being able to insert
a soil humidity/moisture sensor into the soil. The robot uses Jetson Nano and
Arduino microcontroller and real sense camera to perform computer vision to
detect plants using real-time YOLOv5 with the Pl@ntNet-300K dataset. The robot
uses LIDAR for object and collision avoideance and does not need to move on a
pre-defined path and can keep track of which plants it has watered. We provide
the Denavit-Hartenberg (DH) Table, forward kinematics, differential driving
kinematics, and inverse kinematics along with simulation and experiment results",Justin London,2025-08-12T03:41:33Z,2025-08-12T03:41:33Z,http://arxiv.org/abs/2508.08607v1,http://arxiv.org/pdf/2508.08607v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for
  Long-Horizon Tasks","Language-guided long-horizon mobile manipulation has long been a grand
challenge in embodied semantic reasoning, generalizable manipulation, and
adaptive locomotion. Three fundamental limitations hinder progress: First,
although large language models have improved spatial reasoning and task
planning through semantic priors, existing implementations remain confined to
tabletop scenarios, failing to address the constrained perception and limited
actuation ranges of mobile platforms. Second, current manipulation strategies
exhibit insufficient generalization when confronted with the diverse object
configurations encountered in open-world environments. Third, while crucial for
practical deployment, the dual requirement of maintaining high platform
maneuverability alongside precise end-effector control in unstructured settings
remains understudied.
  In this work, we present ODYSSEY, a unified mobile manipulation framework for
agile quadruped robots equipped with manipulators, which seamlessly integrates
high-level task planning with low-level whole-body control. To address the
challenge of egocentric perception in language-conditioned tasks, we introduce
a hierarchical planner powered by a vision-language model, enabling
long-horizon instruction decomposition and precise action execution. At the
control level, our novel whole-body policy achieves robust coordination across
challenging terrains. We further present the first benchmark for long-horizon
mobile manipulation, evaluating diverse indoor and outdoor scenarios. Through
successful sim-to-real transfer, we demonstrate the system's generalization and
robustness in real-world deployments, underscoring the practicality of legged
manipulators in unstructured environments. Our work advances the feasibility of
generalized robotic assistants capable of complex, dynamic tasks. Our project
page: https://kaijwang.github.io/odyssey.github.io/","Kaijun Wang, Liqin Lu, Mingyu Liu, Jianuo Jiang, Zeju Li, Bolin Zhang, Wancai Zheng, Xinyi Yu, Hao Chen, Chunhua Shen",2025-08-11T17:54:31Z,2025-08-11T17:54:31Z,http://arxiv.org/abs/2508.08240v1,http://arxiv.org/pdf/2508.08240v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"Whole-Body Coordination for Dynamic Object Grasping with Legged
  Manipulators","Quadrupedal robots with manipulators offer strong mobility and adaptability
for grasping in unstructured, dynamic environments through coordinated
whole-body control. However, existing research has predominantly focused on
static-object grasping, neglecting the challenges posed by dynamic targets and
thus limiting applicability in dynamic scenarios such as logistics sorting and
human-robot collaboration. To address this, we introduce DQ-Bench, a new
benchmark that systematically evaluates dynamic grasping across varying object
motions, velocities, heights, object types, and terrain complexities, along
with comprehensive evaluation metrics. Building upon this benchmark, we propose
DQ-Net, a compact teacher-student framework designed to infer grasp
configurations from limited perceptual cues. During training, the teacher
network leverages privileged information to holistically model both the static
geometric properties and dynamic motion characteristics of the target, and
integrates a grasp fusion module to deliver robust guidance for motion
planning. Concurrently, we design a lightweight student network that performs
dual-viewpoint temporal modeling using only the target mask, depth map, and
proprioceptive state, enabling closed-loop action outputs without reliance on
privileged data. Extensive experiments on DQ-Bench demonstrate that DQ-Net
achieves robust dynamic objects grasping across multiple task settings,
substantially outperforming baseline methods in both success rate and
responsiveness.","Qiwei Liang, Boyang Cai, Rongyi He, Hui Li, Tao Teng, Haihan Duan, Changxin Huang, Runhao Zeng",2025-08-10T09:21:00Z,2025-08-10T09:21:00Z,http://arxiv.org/abs/2508.08328v1,http://arxiv.org/pdf/2508.08328v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
Real-Time 3D Vision-Language Embedding Mapping,"A metric-accurate semantic 3D representation is essential for many robotic
tasks. This work proposes a simple, yet powerful, way to integrate the 2D
embeddings of a Vision-Language Model in a metric-accurate 3D representation at
real-time. We combine a local embedding masking strategy, for a more distinct
embedding distribution, with a confidence-weighted 3D integration for more
reliable 3D embeddings. The resulting metric-accurate embedding representation
is task-agnostic and can represent semantic concepts on a global multi-room, as
well as on a local object-level. This enables a variety of interactive robotic
applications that require the localisation of objects-of-interest via natural
language. We evaluate our approach on a variety of real-world sequences and
demonstrate that these strategies achieve a more accurate object-of-interest
localisation while improving the runtime performance in order to meet our
real-time constraints. We further demonstrate the versatility of our approach
in a variety of interactive handheld, mobile robotics and manipulation tasks,
requiring only raw image data.","Christian Rauch, Björn Ellensohn, Linus Nwankwo, Vedant Dave, Elmar Rueckert",2025-08-08T13:11:54Z,2025-08-08T13:11:54Z,http://arxiv.org/abs/2508.06291v1,http://arxiv.org/pdf/2508.06291v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"Language-Conditioned Open-Vocabulary Mobile Manipulation with Pretrained
  Models","Open-vocabulary mobile manipulation (OVMM) that involves the handling of
novel and unseen objects across different workspaces remains a significant
challenge for real-world robotic applications. In this paper, we propose a
novel Language-conditioned Open-Vocabulary Mobile Manipulation framework, named
LOVMM, incorporating the large language model (LLM) and vision-language model
(VLM) to tackle various mobile manipulation tasks in household environments.
Our approach is capable of solving various OVMM tasks with free-form natural
language instructions (e.g. ""toss the food boxes on the office room desk to the
trash bin in the corner"", and ""pack the bottles from the bed to the box in the
guestroom""). Extensive experiments simulated in complex household environments
show strong zero-shot generalization and multi-task learning abilities of
LOVMM. Moreover, our approach can also generalize to multiple tabletop
manipulation tasks and achieve better success rates compared to other
state-of-the-art methods.","Shen Tan, Dong Zhou, Xiangyu Shao, Junqiao Wang, Guanghui Sun",2025-07-23T10:23:15Z,2025-07-23T10:23:15Z,http://arxiv.org/abs/2507.17379v1,http://arxiv.org/pdf/2507.17379v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"Mobile Manipulation with Active Inference for Long-Horizon Rearrangement
  Tasks","Despite growing interest in active inference for robotic control, its
application to complex, long-horizon tasks remains untested. We address this
gap by introducing a fully hierarchical active inference architecture for
goal-directed behavior in realistic robotic settings. Our model combines a
high-level active inference model that selects among discrete skills realized
via a whole-body active inference controller. This unified approach enables
flexible skill composition, online adaptability, and recovery from task
failures without requiring offline training. Evaluated on the Habitat Benchmark
for mobile manipulation, our method outperforms state-of-the-art baselines
across the three long-horizon tasks, demonstrating for the first time that
active inference can scale to the complexity of modern robotics benchmarks.","Corrado Pezzato, Ozan Çatal, Toon Van de Maele, Riddhi J. Pitliya, Tim Verbelen",2025-07-23T09:08:21Z,2025-07-23T09:08:21Z,http://arxiv.org/abs/2507.17338v1,http://arxiv.org/pdf/2507.17338v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
GR-3 Technical Report,"We report our recent progress towards building generalist robot policies, the
development of GR-3. GR-3 is a large-scale vision-language-action (VLA) model.
It showcases exceptional capabilities in generalizing to novel objects,
environments, and instructions involving abstract concepts. Furthermore, it can
be efficiently fine-tuned with minimal human trajectory data, enabling rapid
and cost-effective adaptation to new settings. GR-3 also excels in handling
long-horizon and dexterous tasks, including those requiring bi-manual
manipulation and mobile movement, showcasing robust and reliable performance.
These capabilities are achieved through a multi-faceted training recipe that
includes co-training with web-scale vision-language data, efficient fine-tuning
from human trajectory data collected via VR devices, and effective imitation
learning with robot trajectory data. In addition, we introduce ByteMini, a
versatile bi-manual mobile robot designed with exceptional flexibility and
reliability, capable of accomplishing a wide range of tasks when integrated
with GR-3. Through extensive real-world experiments, we show GR-3 surpasses the
state-of-the-art baseline method, $\pi_0$, on a wide variety of challenging
tasks. We hope GR-3 can serve as a step towards building generalist robots
capable of assisting humans in daily life.","Chilam Cheang, Sijin Chen, Zhongren Cui, Yingdong Hu, Liqun Huang, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Xiao Ma, Hao Niu, Wenxuan Ou, Wanli Peng, Zeyu Ren, Haixin Shi, Jiawen Tian, Hongtao Wu, Xin Xiao, Yuyang Xiao, Jiafeng Xu, Yichu Yang",2025-07-21T10:54:13Z,2025-07-22T15:04:37Z,http://arxiv.org/abs/2507.15493v2,http://arxiv.org/pdf/2507.15493v2.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"Design of a Modular Mobile Inspection and Maintenance Robot for an
  Orbital Servicing Hub","The use of autonomous robots in space is an essential part of the ""New Space""
commercial ecosystem of assembly and re-use of space hardware components in
Earth orbit and beyond. The STARFAB project aims to create a ground
demonstration of an orbital automated warehouse as a hub for sustainable
commercial operations and servicing. A critical part of this fully-autonomous
robotic facility will be the capability to monitor, inspect, and assess the
condition of both the components stored in the warehouse, and the STARFAB
facility itself. This paper introduces ongoing work on the STARFAB Mobile
Inspection Module (MIM). The MIM uses Standard Interconnects (SI) so that it
can be carried by Walking Manipulators (WM) as an independently-mobile robot,
and multiple MIMs can be stored and retrieved as needed for operations on
STARFAB. The MIM carries high-resolution cameras, a 3D profilometer, and a
thermal imaging sensor, with the capability to add other modular sensors. A
grasping tool and torque wrench are stored within the modular body for use by
an attached WM for maintenance operations. Implementation and testing is still
ongoing at the time of writing. This paper details the concept of operations
for the MIM as an on-orbit autonomous inspection and maintenance system, the
mechanical and electronic design of the MIM, and the sensors package used for
non-destructive testing.","Tianyuan Wang, Mark A Post, Mathieu Deremetz",2025-07-18T16:40:58Z,2025-07-18T16:40:58Z,http://arxiv.org/abs/2507.14059v1,http://arxiv.org/pdf/2507.14059v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
EdgeVLA: Efficient Vision-Language-Action Models,"Vision-Language Models (VLMs) have emerged as a promising approach to address
the data scarcity challenge in robotics, enabling the development of
generalizable visuomotor control policies. While models like OpenVLA showcase
the potential of this paradigm, deploying large-scale VLMs on
resource-constrained mobile manipulation systems remains a significant hurdle.
This paper introduces Edge VLA (EVLA), a novel approach designed to
significantly enhance the inference speed of Vision-Language-Action (VLA)
models. EVLA maintains the representational power of these models while
enabling real-time performance on edge devices. We achieve this through two key
innovations: 1) Eliminating the autoregressive requirement for end-effector
position prediction, leading to a 7x speedup in inference, and 2) Leveraging
the efficiency of Small Language Models (SLMs), demonstrating comparable
training performance to larger models with significantly reduced computational
demands. Our early results demonstrate that EVLA achieves comparable training
characteristics to OpenVLA while offering substantial gains in inference speed
and memory efficiency. We release our model checkpoints and training
\href{https://github.com/kscalelabs/evla }{codebase} to foster further
research.","Paweł Budzianowski, Wesley Maa, Matthew Freed, Jingxiang Mo, Winston Hsiao, Aaron Xie, Tomasz Młoduchowski, Viraj Tipnis, Benjamin Bolte",2025-07-18T16:15:09Z,2025-07-18T16:15:09Z,http://arxiv.org/abs/2507.14049v1,http://arxiv.org/pdf/2507.14049v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"Acting and Planning with Hierarchical Operational Models on a Mobile
  Robot: A Study with RAE+UPOM","Robotic task execution faces challenges due to the inconsistency between
symbolic planner models and the rich control structures actually running on the
robot. In this paper, we present the first physical deployment of an integrated
actor-planner system that shares hierarchical operational models for both
acting and planning, interleaving the Reactive Acting Engine (RAE) with an
anytime UCT-like Monte Carlo planner (UPOM). We implement RAE+UPOM on a mobile
manipulator in a real-world deployment for an object collection task. Our
experiments demonstrate robust task execution under action failures and sensor
noise, and provide empirical insights into the interleaved acting-and-planning
decision making process.","Oscar Lima, Marc Vinci, Sunandita Patra, Sebastian Stock, Joachim Hertzberg, Martin Atzmueller, Malik Ghallab, Dana Nau, Paolo Traverso",2025-07-15T14:20:26Z,2025-07-15T14:20:26Z,http://arxiv.org/abs/2507.11345v1,http://arxiv.org/pdf/2507.11345v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"Probabilistic Human Intent Prediction for Mobile Manipulation: An
  Evaluation with Human-Inspired Constraints","Accurate inference of human intent enables human-robot collaboration without
constraining human control or causing conflicts between humans and robots. We
present GUIDER (Global User Intent Dual-phase Estimation for Robots), a
probabilistic framework that enables a robot to estimate the intent of human
operators. GUIDER maintains two coupled belief layers, one tracking navigation
goals and the other manipulation goals. In the Navigation phase, a Synergy Map
blends controller velocity with an occupancy grid to rank interaction areas.
Upon arrival at a goal, an autonomous multi-view scan builds a local 3D cloud.
The Manipulation phase combines U2Net saliency, FastSAM instance saliency, and
three geometric grasp-feasibility tests, with an end-effector kinematics-aware
update rule that evolves object probabilities in real-time. GUIDER can
recognize areas and objects of intent without predefined goals. We evaluated
GUIDER on 25 trials (five participants x five task variants) in Isaac Sim, and
compared it with two baselines, one for navigation and one for manipulation.
Across the 25 trials, GUIDER achieved a median stability of 93-100% during
navigation, compared with 60-100% for the BOIR baseline, with an improvement of
39.5% in a redirection scenario (T5). During manipulation, stability reached
94-100% (versus 69-100% for Trajectron), with a 31.4% difference in a
redirection task (T3). In geometry-constrained trials (manipulation), GUIDER
recognized the object intent three times earlier than Trajectron (median
remaining time to confident prediction 23.6 s vs 7.8 s). These results validate
our dual-phase framework and show improvements in intent inference in both
phases of mobile manipulation tasks.","Cesar Alan Contreras, Manolis Chiou, Alireza Rastegarpanah, Michal Szulik, Rustam Stolkin",2025-07-14T10:21:27Z,2025-07-14T10:21:27Z,http://arxiv.org/abs/2507.10131v1,http://arxiv.org/pdf/2507.10131v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
Safe Bimanual Teleoperation with Language-Guided Collision Avoidance,"Teleoperating precise bimanual manipulations in cluttered environments is
challenging for operators, who often struggle with limited spatial perception
and difficulty estimating distances between target objects, the robot's body,
obstacles, and the surrounding environment. To address these challenges, local
robot perception and control should assist the operator during teleoperation.
In this work, we introduce a safe teleoperation system that enhances operator
control by preventing collisions in cluttered environments through the
combination of immersive VR control and voice-activated collision avoidance.
Using HTC Vive controllers, operators directly control a bimanual mobile
manipulator, while spoken commands such as ""avoid the yellow tool"" trigger
visual grounding and segmentation to build 3D obstacle meshes. These meshes are
integrated into a whole-body controller to actively prevent collisions during
teleoperation. Experiments in static, cluttered scenes demonstrate that our
system significantly improves operational safety without compromising task
efficiency.","Dionis Totsila, Clemente Donoso, Enrico Mingo Hoffman, Jean-Baptiste Mouret, Serena Ivaldi",2025-07-07T09:12:02Z,2025-07-07T09:12:02Z,http://arxiv.org/abs/2507.04791v1,http://arxiv.org/pdf/2507.04791v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"SafeMimic: Towards Safe and Autonomous Human-to-Robot Imitation for
  Mobile Manipulation","For robots to become efficient helpers in the home, they must learn to
perform new mobile manipulation tasks simply by watching humans perform them.
Learning from a single video demonstration from a human is challenging as the
robot needs to first extract from the demo what needs to be done and how,
translate the strategy from a third to a first-person perspective, and then
adapt it to be successful with its own morphology. Furthermore, to mitigate the
dependency on costly human monitoring, this learning process should be
performed in a safe and autonomous manner. We present SafeMimic, a framework to
learn new mobile manipulation skills safely and autonomously from a single
third-person human video. Given an initial human video demonstration of a
multi-step mobile manipulation task, SafeMimic first parses the video into
segments, inferring both the semantic changes caused and the motions the human
executed to achieve them and translating them to an egocentric reference. Then,
it adapts the behavior to the robot's own morphology by sampling candidate
actions around the human ones, and verifying them for safety before execution
in a receding horizon fashion using an ensemble of safety Q-functions trained
in simulation. When safe forward progression is not possible, SafeMimic
backtracks to previous states and attempts a different sequence of actions,
adapting both the trajectory and the grasping modes when required for its
morphology. As a result, SafeMimic yields a strategy that succeeds in the
demonstrated behavior and learns task-specific actions that reduce exploration
in future attempts. Our experiments show that our method allows robots to
safely and efficiently learn multi-step mobile manipulation behaviors from a
single human demonstration, from different users, and in different
environments, with improvements over state-of-the-art baselines across seven
tasks","Arpit Bahety, Arnav Balaji, Ben Abbatematteo, Roberto Martín-Martín",2025-06-18T19:55:10Z,2025-06-18T19:55:10Z,http://arxiv.org/abs/2506.15847v1,http://arxiv.org/pdf/2506.15847v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"Casper: Inferring Diverse Intents for Assistive Teleoperation with
  Vision Language Models","Assistive teleoperation, where control is shared between a human and a robot,
enables efficient and intuitive human-robot collaboration in diverse and
unstructured environments. A central challenge in real-world assistive
teleoperation is for the robot to infer a wide range of human intentions from
user control inputs and to assist users with correct actions. Existing methods
are either confined to simple, predefined scenarios or restricted to
task-specific data distributions at training, limiting their support for
real-world assistance. We introduce Casper, an assistive teleoperation system
that leverages commonsense knowledge embedded in pre-trained visual language
models (VLMs) for real-time intent inference and flexible skill execution.
Casper incorporates an open-world perception module for a generalized
understanding of novel objects and scenes, a VLM-powered intent inference
mechanism that leverages commonsense reasoning to interpret snippets of
teleoperated user input, and a skill library that expands the scope of prior
assistive teleoperation systems to support diverse, long-horizon mobile
manipulation tasks. Extensive empirical evaluation, including human studies and
system ablations, demonstrates that Casper improves task performance, reduces
human cognitive load, and achieves higher user satisfaction than direct
teleoperation and assistive teleoperation baselines. More information is
available at https://ut-austin-rpl.github.io/casper/","Huihan Liu, Rutav Shah, Shuijing Liu, Jack Pittenger, Mingyo Seo, Yuchen Cui, Yonatan Bisk, Roberto Martín-Martín, Yuke Zhu",2025-06-17T17:06:43Z,2025-07-04T20:27:52Z,http://arxiv.org/abs/2506.14727v2,http://arxiv.org/pdf/2506.14727v2.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"Robotic System for Chemical Experiment Automation with Dual
  Demonstration of End-effector and Jig Operations","While robotic automation has demonstrated remarkable performance, such as
executing hundreds of experiments continuously over several days, it is
challenging to design a program that synchronizes the robot's movements with
the experimental jigs to conduct an experiment. We propose a concept that
enables the automation of experiments by utilizing dual demonstrations of robot
motions and jig operations by chemists in an experimental environment
constructed to be controlled by a robot. To verify this concept, we developed a
chemical-experiment-automation system consisting of jigs to assist the robot in
experiments, a motion-demonstration interface, a jig-control interface, and a
mobile manipulator. We validate the concept through polymer-synthesis
experiments, focusing on critical liquid-handling tasks such as pipetting and
dilution. The experimental results indicate high reproducibility of the
demonstrated motions and robust task-success rates. This comprehensive concept
not only simplifies the robot programming process for chemists but also
provides a flexible and efficient solution to accommodate a wide range of
experimental conditions, contributing significantly to the field of chemical
experiment automation.","Hikaru Sasaki, Naoto Komeno, Takumi Hachimine, Kei Takahashi, Yu-ya Ohnishi, Tetsunori Sugawara, Araki Wakiuchi, Miho Hatanaka, Tomoyuki Miyao, Hiroharu Ajiro, Mikiya Fujii, Takamitsu Matsubara",2025-06-13T01:08:08Z,2025-06-13T01:08:08Z,http://arxiv.org/abs/2506.11384v1,http://arxiv.org/pdf/2506.11384v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
Multimodal Spatial Language Maps for Robot Navigation and Manipulation,"Grounding language to a navigating agent's observations can leverage
pretrained multimodal foundation models to match perceptions to object or event
descriptions. However, previous approaches remain disconnected from environment
mapping, lack the spatial precision of geometric maps, or neglect additional
modality information beyond vision. To address this, we propose multimodal
spatial language maps as a spatial map representation that fuses pretrained
multimodal features with a 3D reconstruction of the environment. We build these
maps autonomously using standard exploration. We present two instances of our
maps, which are visual-language maps (VLMaps) and their extension to
audio-visual-language maps (AVLMaps) obtained by adding audio information. When
combined with large language models (LLMs), VLMaps can (i) translate natural
language commands into open-vocabulary spatial goals (e.g., ""in between the
sofa and TV"") directly localized in the map, and (ii) be shared across
different robot embodiments to generate tailored obstacle maps on demand.
Building upon the capabilities above, AVLMaps extend VLMaps by introducing a
unified 3D spatial representation integrating audio, visual, and language cues
through the fusion of features from pretrained multimodal foundation models.
This enables robots to ground multimodal goal queries (e.g., text, images, or
audio snippets) to spatial locations for navigation. Additionally, the
incorporation of diverse sensory inputs significantly enhances goal
disambiguation in ambiguous environments. Experiments in simulation and
real-world settings demonstrate that our multimodal spatial language maps
enable zero-shot spatial and multimodal goal navigation and improve recall by
50% in ambiguous scenarios. These capabilities extend to mobile robots and
tabletop manipulators, supporting navigation and interaction guided by visual,
audio, and spatial cues.","Chenguang Huang, Oier Mees, Andy Zeng, Wolfram Burgard",2025-06-07T17:02:13Z,2025-06-07T17:02:13Z,http://arxiv.org/abs/2506.06862v1,http://arxiv.org/pdf/2506.06862v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"SLAC: Simulation-Pretrained Latent Action Space for Whole-Body
  Real-World RL","Building capable household and industrial robots requires mastering the
control of versatile, high-degree-of-freedom (DoF) systems such as mobile
manipulators. While reinforcement learning (RL) holds promise for autonomously
acquiring robot control policies, scaling it to high-DoF embodiments remains
challenging. Direct RL in the real world demands both safe exploration and high
sample efficiency, which are difficult to achieve in practice. Sim-to-real RL,
on the other hand, is often brittle due to the reality gap. This paper
introduces SLAC, a method that renders real-world RL feasible for complex
embodiments by leveraging a low-fidelity simulator to pretrain a task-agnostic
latent action space. SLAC trains this latent action space via a customized
unsupervised skill discovery method designed to promote temporal abstraction,
disentanglement, and safety, thereby facilitating efficient downstream
learning. Once a latent action space is learned, SLAC uses it as the action
interface for a novel off-policy RL algorithm to autonomously learn downstream
tasks through real-world interactions. We evaluate SLAC against existing
methods on a suite of bimanual mobile manipulation tasks, where it achieves
state-of-the-art performance. Notably, SLAC learns contact-rich whole-body
tasks in under an hour of real-world interactions, without relying on any
demonstrations or hand-crafted behavior priors. More information and robot
videos at robo-rl.github.io","Jiaheng Hu, Peter Stone, Roberto Martín-Martín",2025-06-04T16:41:55Z,2025-08-16T02:41:09Z,http://arxiv.org/abs/2506.04147v4,http://arxiv.org/pdf/2506.04147v4.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"Variational Adaptive Noise and Dropout towards Stable Recurrent Neural
  Networks","This paper proposes a novel stable learning theory for recurrent neural
networks (RNNs), so-called variational adaptive noise and dropout (VAND). As
stabilizing factors for RNNs, noise and dropout on the internal state of RNNs
have been separately confirmed in previous studies. We reinterpret the
optimization problem of RNNs as variational inference, showing that noise and
dropout can be derived simultaneously by transforming the explicit
regularization term arising in the optimization problem into implicit
regularization. Their scale and ratio can also be adjusted appropriately to
optimize the main objective of RNNs, respectively. In an imitation learning
scenario with a mobile manipulator, only VAND is able to imitate sequential and
periodic behaviors as instructed. https://youtu.be/UOho3Xr6A2w","Taisuke Kobayashi, Shingo Murata",2025-06-02T06:13:22Z,2025-06-02T06:13:22Z,http://arxiv.org/abs/2506.01350v1,http://arxiv.org/pdf/2506.01350v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"Hi-Dyna Graph: Hierarchical Dynamic Scene Graph for Robotic Autonomy in
  Human-Centric Environments","Autonomous operation of service robotics in human-centric scenes remains
challenging due to the need for understanding of changing environments and
context-aware decision-making. While existing approaches like topological maps
offer efficient spatial priors, they fail to model transient object
relationships, whereas dense neural representations (e.g., NeRF) incur
prohibitive computational costs. Inspired by the hierarchical scene
representation and video scene graph generation works, we propose Hi-Dyna
Graph, a hierarchical dynamic scene graph architecture that integrates
persistent global layouts with localized dynamic semantics for embodied robotic
autonomy. Our framework constructs a global topological graph from posed RGB-D
inputs, encoding room-scale connectivity and large static objects (e.g.,
furniture), while environmental and egocentric cameras populate dynamic
subgraphs with object position relations and human-object interaction patterns.
A hybrid architecture is conducted by anchoring these subgraphs to the global
topology using semantic and spatial constraints, enabling seamless updates as
the environment evolves. An agent powered by large language models (LLMs) is
employed to interpret the unified graph, infer latent task triggers, and
generate executable instructions grounded in robotic affordances. We conduct
complex experiments to demonstrate Hi-Dyna Grap's superior scene representation
effectiveness. Real-world deployments validate the system's practicality with a
mobile manipulator: robotics autonomously complete complex tasks with no
further training or complex rewarding in a dynamic scene as cafeteria
assistant. See https://anonymous.4open.science/r/Hi-Dyna-Graph-B326 for video
demonstration and more details.","Jiawei Hou, Xiangyang Xue, Taiping Zeng",2025-05-30T03:35:29Z,2025-05-30T03:35:29Z,http://arxiv.org/abs/2506.00083v1,http://arxiv.org/pdf/2506.00083v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"LabUtopia: High-Fidelity Simulation and Hierarchical Benchmark for
  Scientific Embodied Agents","Scientific embodied agents play a crucial role in modern laboratories by
automating complex experimental workflows. Compared to typical household
environments, laboratory settings impose significantly higher demands on
perception of physical-chemical transformations and long-horizon planning,
making them an ideal testbed for advancing embodied intelligence. However, its
development has been long hampered by the lack of suitable simulator and
benchmarks. In this paper, we address this gap by introducing LabUtopia, a
comprehensive simulation and benchmarking suite designed to facilitate the
development of generalizable, reasoning-capable embodied agents in laboratory
settings. Specifically, it integrates i) LabSim, a high-fidelity simulator
supporting multi-physics and chemically meaningful interactions; ii) LabScene,
a scalable procedural generator for diverse scientific scenes; and iii)
LabBench, a hierarchical benchmark spanning five levels of complexity from
atomic actions to long-horizon mobile manipulation. LabUtopia supports 30
distinct tasks and includes more than 200 scene and instrument assets, enabling
large-scale training and principled evaluation in high-complexity environments.
We demonstrate that LabUtopia offers a powerful platform for advancing the
integration of perception, planning, and control in scientific-purpose agents
and provides a rigorous testbed for exploring the practical capabilities and
generalization limits of embodied intelligence in future research.","Rui Li, Zixuan Hu, Wenxi Qu, Jinouwen Zhang, Zhenfei Yin, Sha Zhang, Xuantuo Huang, Hanqing Wang, Tai Wang, Jiangmiao Pang, Wanli Ouyang, Lei Bai, Wangmeng Zuo, Ling-Yu Duan, Dongzhan Zhou, Shixiang Tang",2025-05-28T17:50:53Z,2025-05-28T17:50:53Z,http://arxiv.org/abs/2505.22634v1,http://arxiv.org/pdf/2505.22634v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
Mobile Manipulation Planning for Tabletop Rearrangement,"Efficient tabletop rearrangement planning seeks to find high-quality
solutions while minimizing total cost. However, the task is challenging due to
object dependencies and limited buffer space for temporary placements. The
complexity increases for mobile robots, which must navigate around the table
with restricted access. A*-based methods yield high-quality solutions, but
struggle to scale as the number of objects increases. Monte Carlo Tree Search
(MCTS) has been introduced as an anytime algorithm, but its convergence speed
to high-quality solutions remains slow. Previous work~\cite{strap2024}
accelerated convergence but required the robot to move to the closest position
to the object for each pick and place operation, leading to inefficiencies. To
address these limitations, we extend the planner by introducing a more
efficient strategy for mobile robots. Instead of selecting the nearest
available location for each action, our approach allows multiple operations
(e.g., pick-and-place) from a single standing position, reducing unnecessary
movement. Additionally, we incorporate state re-exploration to further improve
plan quality. Experimental results show that our planner outperforms existing
planners both in terms of solution quality and planning time.","Jiaming Hu, Jiawei Wang, Henrik I Christensen",2025-05-24T15:10:14Z,2025-05-24T15:10:14Z,http://arxiv.org/abs/2505.18732v1,http://arxiv.org/pdf/2505.18732v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
Bench-NPIN: Benchmarking Non-prehensile Interactive Navigation,"Mobile robots are increasingly deployed in unstructured environments where
obstacles and objects are movable. Navigation in such environments is known as
interactive navigation, where task completion requires not only avoiding
obstacles but also strategic interactions with movable objects. Non-prehensile
interactive navigation focuses on non-grasping interaction strategies, such as
pushing, rather than relying on prehensile manipulation. Despite a growing body
of research in this field, most solutions are evaluated using case-specific
setups, limiting reproducibility and cross-comparison. In this paper, we
present Bench-NPIN, the first comprehensive benchmark for non-prehensile
interactive navigation. Bench-NPIN includes multiple components: 1) a
comprehensive range of simulated environments for non-prehensile interactive
navigation tasks, including navigating a maze with movable obstacles,
autonomous ship navigation in icy waters, box delivery, and area clearing, each
with varying levels of complexity; 2) a set of evaluation metrics that capture
unique aspects of interactive navigation, such as efficiency, interaction
effort, and partial task completion; and 3) demonstrations using Bench-NPIN to
evaluate example implementations of established baselines across environments.
Bench-NPIN is an open-source Python library with a modular design. The code,
documentation, and trained models can be found at
https://github.com/IvanIZ/BenchNPIN.","Ninghan Zhong, Steven Caro, Avraiem Iskandar, Megnath Ramesh, Stephen L. Smith",2025-05-17T16:54:18Z,2025-05-17T16:54:18Z,http://arxiv.org/abs/2505.12084v1,http://arxiv.org/pdf/2505.12084v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
Learning Rock Pushability on Rough Planetary Terrain,"In the context of mobile navigation in unstructured environments, the
predominant approach entails the avoidance of obstacles. The prevailing path
planning algorithms are contingent upon deviating from the intended path for an
indefinite duration and returning to the closest point on the route after the
obstacle is left behind spatially. However, avoiding an obstacle on a path that
will be used repeatedly by multiple agents can hinder long-term efficiency and
lead to a lasting reliance on an active path planning system. In this study, we
propose an alternative approach to mobile navigation in unstructured
environments by leveraging the manipulation capabilities of a robotic
manipulator mounted on top of a mobile robot. Our proposed framework integrates
exteroceptive and proprioceptive feedback to assess the push affordance of
obstacles, facilitating their repositioning rather than avoidance. While our
preliminary visual estimation takes into account the characteristics of both
the obstacle and the surface it relies on, the push affordance estimation
module exploits the force feedback obtained by interacting with the obstacle
via a robotic manipulator as the guidance signal. The objective of our
navigation approach is to enhance the efficiency of routes utilized by multiple
agents over extended periods by reducing the overall time spent by a fleet in
environments where autonomous infrastructure development is imperative, such as
lunar or Martian surfaces.","Tuba Girgin, Emre Girgin, Cagri Kilic",2025-05-14T22:23:30Z,2025-06-05T15:00:47Z,http://arxiv.org/abs/2505.09833v2,http://arxiv.org/pdf/2505.09833v2.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
DexWild: Dexterous Human Interactions for In-the-Wild Robot Policies,"Large-scale, diverse robot datasets have emerged as a promising path toward
enabling dexterous manipulation policies to generalize to novel environments,
but acquiring such datasets presents many challenges. While teleoperation
provides high-fidelity datasets, its high cost limits its scalability. Instead,
what if people could use their own hands, just as they do in everyday life, to
collect data? In DexWild, a diverse team of data collectors uses their hands to
collect hours of interactions across a multitude of environments and objects.
To record this data, we create DexWild-System, a low-cost, mobile, and
easy-to-use device. The DexWild learning framework co-trains on both human and
robot demonstrations, leading to improved performance compared to training on
each dataset individually. This combination results in robust robot policies
capable of generalizing to novel environments, tasks, and embodiments with
minimal additional robot-specific data. Experimental results demonstrate that
DexWild significantly improves performance, achieving a 68.5% success rate in
unseen environments-nearly four times higher than policies trained with robot
data only-and offering 5.8x better cross-embodiment generalization. Video
results, codebases, and instructions at https://dexwild.github.io","Tony Tao, Mohan Kumar Srirama, Jason Jingzhou Liu, Kenneth Shaw, Deepak Pathak",2025-05-12T17:59:05Z,2025-05-12T17:59:05Z,http://arxiv.org/abs/2505.07813v1,http://arxiv.org/pdf/2505.07813v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"Cooperative Assembly with Autonomous Mobile Manipulators in an
  Underwater Scenario","[...] Specifically, the problem addressed is an assembly one known as the
peg-in-hole task. In this case, two autonomous manipulators must carry
cooperatively (at kinematic level) a peg and must insert it into an hole fixed
in the environment. Even if the peg-in-hole is a well-known problem, there are
no specific studies related to the use of two different autonomous
manipulators, especially in underwater scenarios. Among all the possible
investigations towards the problem, this work focuses mainly on the kinematic
control of the robots. The methods used are part of the Task Priority Inverse
Kinematics (TPIK) approach, with a cooperation scheme that permits to exchange
as less information as possible between the agents (that is really important
being water a big impediment for communication). A force-torque sensor is
exploited at kinematic level to help the insertion phase. The results show how
the TPIK and the chosen cooperation scheme can be used for the stated problem.
The simulated experiments done consider little errors in the hole's pose, that
still permit to insert the peg but with a lot of frictions and possible stucks.
It is shown how can be possible to improve (thanks to the data provided by the
force-torque sensor) the insertion phase performed by the two manipulators in
presence of these errors. [...]",Davide Torielli,2025-05-12T11:03:50Z,2025-05-12T11:03:50Z,http://arxiv.org/abs/2505.07441v1,http://arxiv.org/pdf/2505.07441v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
The First WARA Robotics Mobile Manipulation Challenge -- Lessons Learned,"The first WARA Robotics Mobile Manipulation Challenge, held in December 2024
at ABB Corporate Research in V\""aster{\aa}s, Sweden, addressed the automation
of task-intensive and repetitive manual labor in laboratory environments -
specifically the transport and cleaning of glassware. Designed in collaboration
with AstraZeneca, the challenge invited academic teams to develop autonomous
robotic systems capable of navigating human-populated lab spaces and performing
complex manipulation tasks, such as loading items into industrial dishwashers.
This paper presents an overview of the challenge setup, its industrial
motivation, and the four distinct approaches proposed by the participating
teams. We summarize lessons learned from this edition and propose improvements
in design to enable a more effective second iteration to take place in 2025.
The initiative bridges an important gap in effective academia-industry
collaboration within the domain of autonomous mobile manipulation systems by
promoting the development and deployment of applied robotic solutions in
real-world laboratory contexts.","David Cáceres Domínguez, Marco Iannotta, Abhishek Kashyap, Shuo Sun, Yuxuan Yang, Christian Cella, Matteo Colombo, Martina Pelosi, Giuseppe F. Preziosa, Alessandra Tafuro, Isacco Zappa, Finn Busch, Yifei Dong, Alberta Longhini, Haofei Lu, Rafael I. Cabral Muchacho, Jonathan Styrud, Sebastiano Fregnan, Marko Guberina, Zheng Jia, Graziano Carriero, Sofia Lindqvist, Silvio Di Castro, Matteo Iovino",2025-05-11T09:36:07Z,2025-05-11T09:36:07Z,http://arxiv.org/abs/2505.06919v1,http://arxiv.org/pdf/2505.06919v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"An Efficient Method for Accurate Pose Estimation and Error Correction of
  Cuboidal Objects","The proposed system outlined in this paper is a solution to a use case that
requires the autonomous picking of cuboidal objects from an organized or
unorganized pile with high precision. This paper presents an efficient method
for precise pose estimation of cuboid-shaped objects, which aims to reduce
errors in target pose in a time-efficient manner. Typical pose estimation
methods like global point cloud registrations are prone to minor pose errors
for which local registration algorithms are generally used to improve pose
accuracy. However, due to the execution time overhead and uncertainty in the
error of the final achieved pose, an alternate, linear time approach is
proposed for pose error estimation and correction. This paper presents an
overview of the solution followed by a detailed description of individual
modules of the proposed algorithm.","Utsav Rai, Hardik Mehta, Vismay Vakharia, Aditya Choudhary, Amit Parmar, Rolif Lima, Kaushik Das",2025-05-08T05:43:31Z,2025-05-08T05:43:31Z,http://arxiv.org/abs/2505.04962v1,http://arxiv.org/pdf/2505.04962v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"WLTCL: Wide Field-of-View 3-D LiDAR Truck Compartment Automatic
  Localization System","As an essential component of logistics automation, the automated loading
system is becoming a critical technology for enhancing operational efficiency
and safety. Precise automatic positioning of the truck compartment, which
serves as the loading area, is the primary step in automated loading. However,
existing methods have difficulty adapting to truck compartments of various
sizes, do not establish a unified coordinate system for LiDAR and mobile
manipulators, and often exhibit reliability issues in cluttered environments.
To address these limitations, our study focuses on achieving precise automatic
positioning of key points in large, medium, and small fence-style truck
compartments in cluttered scenarios. We propose an innovative wide
field-of-view 3-D LiDAR vehicle compartment automatic localization system. For
vehicles of various sizes, this system leverages the LiDAR to generate
high-density point clouds within an extensive field-of-view range. By
incorporating parking area constraints, our vehicle point cloud segmentation
method more effectively segments vehicle point clouds within the scene. Our
compartment key point positioning algorithm utilizes the geometric features of
the compartments to accurately locate the corner points, providing stackable
spatial regions. Extensive experiments on our collected data and public
datasets demonstrate that this system offers reliable positioning accuracy and
reduced computational resource consumption, leading to its application and
promotion in relevant fields.","Guodong Sun, Mingjing Li, Dingjie Liu, Mingxuan Liu, Bo Wu, Yang Zhang",2025-04-26T09:35:47Z,2025-04-26T09:35:47Z,http://arxiv.org/abs/2504.18870v1,http://arxiv.org/pdf/2504.18870v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"STDArm: Transferring Visuomotor Policies From Static Data Training to
  Dynamic Robot Manipulation","Recent advances in mobile robotic platforms like quadruped robots and drones
have spurred a demand for deploying visuomotor policies in increasingly dynamic
environments. However, the collection of high-quality training data, the impact
of platform motion and processing delays, and limited onboard computing
resources pose significant barriers to existing solutions. In this work, we
present STDArm, a system that directly transfers policies trained under static
conditions to dynamic platforms without extensive modifications.
  The core of STDArm is a real-time action correction framework consisting of:
(1) an action manager to boost control frequency and maintain temporal
consistency, (2) a stabilizer with a lightweight prediction network to
compensate for motion disturbances, and (3) an online latency estimation module
for calibrating system parameters. In this way, STDArm achieves
centimeter-level precision in mobile manipulation tasks.
  We conduct comprehensive evaluations of the proposed STDArm on two types of
robotic arms, four types of mobile platforms, and three tasks. Experimental
results indicate that the STDArm enables real-time compensation for platform
motion disturbances while preserving the original policy's manipulation
capabilities, achieving centimeter-level operational precision during robot
motion.","Yifan Duan, Heng Li, Yilong Wu, Wenhao Yu, Xinran Zhang, Yedong Shen, Jianmin Ji, Yanyong Zhang",2025-04-26T04:13:14Z,2025-04-26T04:13:14Z,http://arxiv.org/abs/2504.18792v1,http://arxiv.org/pdf/2504.18792v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"Beyond Task and Motion Planning: Hierarchical Robot Planning with
  General-Purpose Policies","Task and motion planning is a well-established approach for solving
long-horizon robot planning problems. However, traditional methods assume that
each task-level robot action, or skill, can be reduced to kinematic motion
planning. In this work, we address the challenge of planning with both
kinematic skills and closed-loop motor controllers that go beyond kinematic
considerations. We propose a novel method that integrates these controllers
into motion planning using Composable Interaction Primitives (CIPs), enabling
the use of diverse, non-composable pre-learned skills in hierarchical robot
planning. Toward validating our Task and Skill Planning (TASP) approach, we
describe ongoing robot experiments in real-world scenarios designed to
demonstrate how CIPs can allow a mobile manipulator robot to effectively
combine motion planning with general-purpose skills to accomplish complex
tasks.","Benned Hedegaard, Ziyi Yang, Yichen Wei, Ahmed Jaafar, Stefanie Tellex, George Konidaris, Naman Shah",2025-04-24T19:22:50Z,2025-04-24T19:22:50Z,http://arxiv.org/abs/2504.17901v1,http://arxiv.org/pdf/2504.17901v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"Practical Insights on Grasp Strategies for Mobile Manipulation in the
  Wild","Mobile manipulation robots are continuously advancing, with their grasping
capabilities rapidly progressing. However, there are still significant gaps
preventing state-of-the-art mobile manipulators from widespread real-world
deployments, including their ability to reliably grasp items in unstructured
environments. To help bridge this gap, we developed SHOPPER, a mobile
manipulation robot platform designed to push the boundaries of reliable and
generalizable grasp strategies. We develop these grasp strategies and deploy
them in a real-world grocery store -- an exceptionally challenging setting
chosen for its vast diversity of manipulable items, fixtures, and layouts. In
this work, we present our detailed approach to designing general grasp
strategies towards picking any item in a real grocery store. Additionally, we
provide an in-depth analysis of our latest real-world field test, discussing
key findings related to fundamental failure modes over hundreds of distinct
pick attempts. Through our detailed analysis, we aim to offer valuable
practical insights and identify key grasping challenges, which can guide the
robotics community towards pressing open problems in the field.","Isabella Huang, Richard Cheng, Sangwoon Kim, Dan Kruse, Carolyn Matl, Lukas Kaul, JC Hancock, Shanmuga Harikumar, Mark Tjersland, James Borders, Dan Helmick",2025-04-16T22:17:32Z,2025-04-16T22:17:32Z,http://arxiv.org/abs/2504.12512v1,http://arxiv.org/pdf/2504.12512v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
Robust Visual Servoing under Human Supervision for Assembly Tasks,"We propose a framework enabling mobile manipulators to reliably complete
pick-and-place tasks for assembling structures from construction blocks. The
picking uses an eye-in-hand visual servoing controller for object tracking with
Control Barrier Functions (CBFs) to ensure fiducial markers in the blocks
remain visible. An additional robot with an eye-to-hand setup ensures precise
placement, critical for structural stability. We integrate human-in-the-loop
capabilities for flexibility and fault correction and analyze robustness to
camera pose errors, proposing adapted barrier functions to handle them. Lastly,
experiments validate the framework on 6-DoF mobile arms.","Victor Nan Fernandez-Ayala, Jorge Silva, Meng Guo, Dimos V. Dimarogonas",2025-04-16T21:57:52Z,2025-04-16T21:57:52Z,http://arxiv.org/abs/2504.12506v1,http://arxiv.org/pdf/2504.12506v1.pdf,all:manipulator AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"Tactical Decision for Multi-UGV Confrontation with a Vision-Language
  Model-Based Commander","In multiple unmanned ground vehicle confrontations, autonomously evolving
multi-agent tactical decisions from situational awareness remain a significant
challenge. Traditional handcraft rule-based methods become vulnerable in the
complicated and transient battlefield environment, and current reinforcement
learning methods mainly focus on action manipulation instead of strategic
decisions due to lack of interpretability. Here, we propose a vision-language
model-based commander to address the issue of intelligent
perception-to-decision reasoning in autonomous confrontations. Our method
integrates a vision language model for scene understanding and a lightweight
large language model for strategic reasoning, achieving unified perception and
decision within a shared semantic space, with strong adaptability and
interpretability. Unlike rule-based search and reinforcement learning methods,
the combination of the two modules establishes a full-chain process, reflecting
the cognitive process of human commanders. Simulation and ablation experiments
validate that the proposed approach achieves a win rate of over 80% compared
with baseline models.","Li Wang, Qizhen Wu, Lei Chen",2025-07-15T08:22:37Z,2025-07-15T08:22:37Z,http://arxiv.org/abs/2507.11079v1,http://arxiv.org/pdf/2507.11079v1.pdf,all:manipulator AND all:UGV AND submittedDate:[202309062236 TO 202509052236],2025
"Design and Evaluation of a UGV-Based Robotic Platform for Precision Soil
  Moisture Remote Sensing","This extended abstract presents the design and evaluation of AgriOne, an
automated unmanned ground vehicle (UGV) platform for high precision sensing of
soil moisture in large agricultural fields. The developed robotic system is
equipped with a volumetric water content (VWC) sensor mounted on a robotic
manipulator and utilizes a surface-aware data collection framework to ensure
accurate measurements in heterogeneous terrains. The framework identifies and
removes invalid data points where the sensor fails to penetrate the soil,
ensuring data reliability. Multiple field experiments were conducted to
validate the platform's performance, while the obtained results demonstrate the
efficacy of the AgriOne robot in real-time data acquisition, reducing the need
for permanent sensors and labor-intensive methods.","Ilektra Tsimpidi, Ilias Tevetzidis, Vidya Sumathy, George Nikolakopoulos",2025-04-25T11:52:06Z,2025-04-25T11:52:06Z,http://arxiv.org/abs/2504.18284v1,http://arxiv.org/pdf/2504.18284v1.pdf,all:manipulator AND all:UGV AND submittedDate:[202309062236 TO 202509052236],2025
Jailbreaking LLM-Controlled Robots,"The recent introduction of large language models (LLMs) has revolutionized
the field of robotics by enabling contextual reasoning and intuitive
human-robot interaction in domains as varied as manipulation, locomotion, and
self-driving vehicles. When viewed as a stand-alone technology, LLMs are known
to be vulnerable to jailbreaking attacks, wherein malicious prompters elicit
harmful text by bypassing LLM safety guardrails. To assess the risks of
deploying LLMs in robotics, in this paper, we introduce RoboPAIR, the first
algorithm designed to jailbreak LLM-controlled robots. Unlike existing, textual
attacks on LLM chatbots, RoboPAIR elicits harmful physical actions from
LLM-controlled robots, a phenomenon we experimentally demonstrate in three
scenarios: (i) a white-box setting, wherein the attacker has full access to the
NVIDIA Dolphins self-driving LLM, (ii) a gray-box setting, wherein the attacker
has partial access to a Clearpath Robotics Jackal UGV robot equipped with a
GPT-4o planner, and (iii) a black-box setting, wherein the attacker has only
query access to the GPT-3.5-integrated Unitree Robotics Go2 robot dog. In each
scenario and across three new datasets of harmful robotic actions, we
demonstrate that RoboPAIR, as well as several static baselines, finds
jailbreaks quickly and effectively, often achieving 100% attack success rates.
Our results reveal, for the first time, that the risks of jailbroken LLMs
extend far beyond text generation, given the distinct possibility that
jailbroken robots could cause physical damage in the real world. Indeed, our
results on the Unitree Go2 represent the first successful jailbreak of a
deployed commercial robotic system. Addressing this emerging vulnerability is
critical for ensuring the safe deployment of LLMs in robotics. Additional media
is available at: https://robopair.org","Alexander Robey, Zachary Ravichandran, Vijay Kumar, Hamed Hassani, George J. Pappas",2024-10-17T15:55:36Z,2024-11-09T20:00:07Z,http://arxiv.org/abs/2410.13691v2,http://arxiv.org/pdf/2410.13691v2.pdf,all:manipulator AND all:UGV AND submittedDate:[202309062236 TO 202509052236],2024
"GraphAttack: Exploiting Representational Blindspots in LLM Safety
  Mechanisms","Large Language Models (LLMs) have been equipped with safety mechanisms to
prevent harmful outputs, but these guardrails can often be bypassed through
""jailbreak"" prompts. This paper introduces a novel graph-based approach to
systematically generate jailbreak prompts through semantic transformations. We
represent malicious prompts as nodes in a graph structure with edges denoting
different transformations, leveraging Abstract Meaning Representation (AMR) and
Resource Description Framework (RDF) to parse user goals into semantic
components that can be manipulated to evade safety filters. We demonstrate a
particularly effective exploitation vector by instructing LLMs to generate code
that realizes the intent described in these semantic graphs, achieving success
rates of up to 87% against leading commercial LLMs. Our analysis reveals that
contextual framing and abstraction are particularly effective at circumventing
safety measures, highlighting critical gaps in current safety alignment
techniques that focus primarily on surface-level patterns. These findings
provide insights for developing more robust safeguards against structured
semantic attacks. Our research contributes both a theoretical framework and
practical methodology for systematically stress-testing LLM safety mechanisms.","Sinan He, An Wang",2025-04-17T16:09:12Z,2025-04-17T16:09:12Z,http://arxiv.org/abs/2504.13052v1,http://arxiv.org/pdf/2504.13052v1.pdf,all:manipulator AND all:AMR AND submittedDate:[202309062236 TO 202509052236],2025
"Nonzero RMS Magnetoresistance Yielding Control Space Partition of CrTe2
  Monolayer","The study of magnetic phenomena in low-dimensional systems has largely
explored after the discovery of two-dimensional (2D) magnetic materials, such
as CrI3 and Cr2Ge2Te6 in 2017. These materials presents intrinsic magnetic
order, overcoming the limitations predicted by the Mermin-Wagner theorem, due
to magnetic crystalline anisotropy energy. Among these, CrTe2, a van der Waals
2D magnet, has gather significant interest due to its in-plane anisotropic
magnetoresistance (AMR) and high Curie temperature. This study investigates the
magnetic field-regulated resistance of CrTe2 monolayers in the context of
spintronics applications. Utilizing the zigzag-ordered parameters obtained from
prior simulations, we examine how external magnetic fields influence resistance
states and control the ON/OFF state of nano-devices. The analysis demonstrates
that specific magnetic field configurations, particularly those in the form of
(0, 0, Bz), which is out-of-plane directed field, gives a non-zero root mean
square resistance, indicating a functional ON state. This provides a novel
method for magnetically controlled current regulation in spintronic devices.
The experimental results also reveal an interesting spin-flop transition in
CrTe2 under a z-directed magnetic field, leading to y-directional
magnetization. This phenomenon, combined with the material's robust magnetic
properties, positions CrTe2 as a promising candidate for next-generation memory
and logic devices. By advancing the understanding of magnetic field
manipulation in 2D magnetic materials, this research opens new pathways in the
development of energy-efficient spintronics technology.","Chee Kian Yap, Arun Kumar Singh",2025-03-27T05:32:09Z,2025-03-27T05:32:09Z,http://arxiv.org/abs/2503.22750v1,http://arxiv.org/pdf/2503.22750v1.pdf,all:manipulator AND all:AMR AND submittedDate:[202309062236 TO 202509052236],2025
Firewalls to Secure Dynamic LLM Agentic Networks,"LLM agents will likely communicate on behalf of users with other
entity-representing agents on tasks involving long-horizon plans with
interdependent goals. Current work neglects these agentic networks and their
challenges. We identify required properties for agent communication:
proactivity, adaptability, privacy (sharing only task-necessary information),
and security (preserving integrity and utility against selfish entities). After
demonstrating communication vulnerabilities, we propose a practical design and
protocol inspired by network security principles. Our framework automatically
derives task-specific rules from prior conversations to build firewalls. These
firewalls construct a closed language that is completely controlled by the
developer. They transform any personal data to the allowed degree of
permissibility entailed by the task. Both operations are completely quarantined
from external attackers, disabling the potential for prompt injections,
jailbreaks, or manipulation. By incorporating rules learned from their previous
mistakes, agents rewrite their instructions and self-correct during
communication. Evaluations on diverse attacks demonstrate our framework
significantly reduces privacy and security vulnerabilities while allowing
adaptability.","Sahar Abdelnabi, Amr Gomaa, Eugene Bagdasarian, Per Ola Kristensson, Reza Shokri",2025-02-03T21:00:14Z,2025-05-26T12:24:15Z,http://arxiv.org/abs/2502.01822v5,http://arxiv.org/pdf/2502.01822v5.pdf,all:manipulator AND all:AMR AND submittedDate:[202309062236 TO 202509052236],2025
Aim My Robot: Precision Local Navigation to Any Object,"Existing navigation systems mostly consider ""success"" when the robot reaches
within 1m radius to a goal. This precision is insufficient for emerging
applications where the robot needs to be positioned precisely relative to an
object for downstream tasks, such as docking, inspection, and manipulation. To
this end, we design and implement Aim-My-Robot (AMR), a local navigation system
that enables a robot to reach any object in its vicinity at the desired
relative pose, with centimeter-level precision. AMR achieves high precision and
robustness by leveraging multi-modal perception, precise action prediction, and
is trained on large-scale photorealistic data generated in simulation. AMR
shows strong sim2real transfer and can adapt to different robot kinematics and
unseen objects with little to no fine-tuning.","Xiangyun Meng, Xuning Yang, Sanghun Jung, Fabio Ramos, Srid Sadhan Jujjavarapu, Sanjoy Paul, Dieter Fox",2024-11-22T07:21:30Z,2024-12-27T08:02:59Z,http://arxiv.org/abs/2411.14770v2,http://arxiv.org/pdf/2411.14770v2.pdf,all:manipulator AND all:AMR AND submittedDate:[202309062236 TO 202509052236],2024
"A generic approach for reactive stateful mitigation of application
  failures in distributed robotics systems deployed with Kubernetes","Offloading computationally expensive algorithms to the edge or even cloud
offers an attractive option to tackle limitations regarding on-board
computational and energy resources of robotic systems. In cloud-native
applications deployed with the container management system Kubernetes (K8s),
one key problem is ensuring resilience against various types of failures.
However, complex robotic systems interacting with the physical world pose a
very specific set of challenges and requirements that are not yet covered by
failure mitigation approaches from the cloud-native domain. In this paper, we
therefore propose a novel approach for robotic system monitoring and stateful,
reactive failure mitigation for distributed robotic systems deployed using
Kubernetes (K8s) and the Robot Operating System (ROS2). By employing the
generic substrate of Behaviour Trees, our approach can be applied to any
robotic workload and supports arbitrarily complex monitoring and failure
mitigation strategies. We demonstrate the effectiveness and
application-agnosticism of our approach on two example applications, namely
Autonomous Mobile Robot (AMR) navigation and robotic manipulation in a
simulated environment.","Florian Mirus, Frederik Pasch, Nikhil Singhal, Kay-Ulrich Scholl",2024-10-24T15:17:09Z,2024-11-04T10:59:17Z,http://arxiv.org/abs/2410.18825v2,http://arxiv.org/pdf/2410.18825v2.pdf,all:manipulator AND all:AMR AND submittedDate:[202309062236 TO 202509052236],2024
Compact LED-Based Displacement Sensing for Robot Fingers,"In this paper, we introduce a sensor designed for integration in robot
fingers, where it can provide information on the displacements induced by
external contact. Our sensor uses LEDs to sense the displacement between two
plates connected by a transparent elastomer; when a force is applied to the
finger, the elastomer displaces and the LED signals change. We show that using
LEDs as both light emitters an receivers in this context provides high
sensitivity, allowing such an emitter and receiver pairs to detect very small
displacements. We characterize the standalone performance of the sensor by
testing the ability of a supervised learning model to predict complete force
and torque data from its raw signals, and obtain a mean error between 0.05 and
0.07 N across the three directions of force applied to the finger. Our method
allows for finger-size packaging with no amplification electronics, low cost
manufacturing, easy integration into a complete hand, and high overload shear
forces and bending torques, suggesting future applicability to complete
manipulation tasks.","Amr El-Azizi, Sharfin Islam, Pedro Piacenza, Kai Jiang, Ioannis Kymissis, Matei Ciocarlie",2024-10-04T14:53:45Z,2025-08-06T16:39:36Z,http://arxiv.org/abs/2410.03481v3,http://arxiv.org/pdf/2410.03481v3.pdf,all:manipulator AND all:AMR AND submittedDate:[202309062236 TO 202509052236],2024
"Unmasking Covert Intrusions: Detection of Fault-Masking Cyberattacks on
  Differential Protection Systems","Line Current Differential Relays (LCDRs) are high-speed relays progressively
used to protect critical transmission lines. However, LCDRs are vulnerable to
cyberattacks. Fault-Masking Attacks (FMAs) are stealthy cyberattacks performed
by manipulating the remote measurements of the targeted LCDR to disguise faults
on the protected line. Hence, they remain undetected by this LCDR. In this
paper, we propose a two-module framework to detect FMAs. The first module is a
Mismatch Index (MI) developed from the protected transmission line's equivalent
physical model. The MI is triggered only if there is a significant mismatch in
the LCDR's local and remote measurements while the LCDR itself is untriggered,
which indicates an FMA. After the MI is triggered, the second module, a neural
network-based classifier, promptly confirms that the triggering event is a
physical fault that lies on the line protected by the LCDR before declaring the
occurrence of an FMA. The proposed framework is tested using the IEEE 39-bus
benchmark system. Our simulation results confirm that the proposed framework
can accurately detect FMAs on LCDRs and is not affected by normal system
disturbances, variations, or measurement noise. Our experimental results using
OPAL-RT's real-time simulator confirm the proposed solution's real-time
performance capability.","Ahmad Mohammad Saber, Amr Youssef, Davor Svetinovic, Hatem Zeineldin, Ehab F. El-Saadany",2024-09-06T12:47:15Z,2024-09-06T12:47:15Z,http://arxiv.org/abs/2409.04242v1,http://arxiv.org/pdf/2409.04242v1.pdf,all:manipulator AND all:AMR AND submittedDate:[202309062236 TO 202509052236],2024
Targeted Deep Learning System Boundary Testing,"Evaluating the behavioral boundaries of deep learning (DL) systems is crucial
for understanding their reliability across diverse, unseen inputs. Existing
solutions fall short as they rely on untargeted random, model- or latent-based
perturbations, due to difficulties in generating controlled input variations.
In this work, we introduce Mimicry, a novel black-box test generator for
fine-grained, targeted exploration of DL system boundaries. Mimicry performs
boundary testing by leveraging the probabilistic nature of DL outputs to
identify promising directions for exploration. It uses style-based GANs to
disentangle input representations into content and style components, enabling
controlled feature mixing to approximate the decision boundary. We evaluated
Mimicry's effectiveness in generating boundary inputs for five widely used DL
image classification systems of increasing complexity, comparing it to two
baseline approaches. Our results show that Mimicry consistently identifies
inputs closer to the decision boundary. It generates semantically meaningful
boundary test cases that reveal new functional (mis)behaviors, while the
baselines produce mainly corrupted or invalid inputs. Thanks to its enhanced
control over latent space manipulations, Mimicry remains effective as dataset
complexity increases, maintaining competitive diversity and higher validity
rates, confirmed by human assessors.","Oliver Weißl, Amr Abdellatif, Xingcheng Chen, Giorgi Merabishvili, Vincenzo Riccio, Severin Kacianka, Andrea Stocco",2024-08-12T16:14:55Z,2025-05-11T13:33:33Z,http://arxiv.org/abs/2408.06258v2,http://arxiv.org/pdf/2408.06258v2.pdf,all:manipulator AND all:AMR AND submittedDate:[202309062236 TO 202509052236],2024
Terahertz antiferromagnetic dynamics induced by ultrafast spin currents,"Insulating antiferromagnets are anticipated as the main protagonists of
ultrafast spintronics, with their intrinsic terahertz dynamics and their
abililty to transport spin information over long distances. However, direct
transfer of spin angular momentum to an antiferromagnetic insulator at
picosecond time scales remains to be demonstrated. Here, studying the ultrafast
behaviour of ferromagnetic metal/antiferromagnetic insulator bilayers, we
evidence the generation of coherent excitations in the antiferromagnet combined
with a modulation of the demagnetization behavior of the ferromagnet. This
confirms that magnetic information can indeed be propagated into
antiferromagnetic spin waves at picosecond timescales, thereby opening an
avenue towards ultrafast manipulation of magnetic information.","Sanjay René, Artem Levchuk, Amr Abdelsamie, Zixin Li, Pauline Dufour, Arthur Chaudron, Florian Godel, Jean-Baptiste Moussy, Karim Bouzehouane, Stéphane Fusil, Vincent Garcia, Michel Viret, Jean-Yves Chauleau",2024-07-20T07:37:15Z,2024-07-20T07:37:15Z,http://arxiv.org/abs/2407.14787v1,http://arxiv.org/pdf/2407.14787v1.pdf,all:manipulator AND all:AMR AND submittedDate:[202309062236 TO 202509052236],2024
Compositional Reversible Computation,"Reversible computing is motivated by both pragmatic and foundational
considerations arising from a variety of disciplines. We take a particular path
through the development of reversible computation, emphasizing compositional
reversible computation. We start from a historical perspective, by reviewing
those approaches that developed reversible extensions of lambda-calculi, Turing
machines, and communicating process calculi. These approaches share a common
challenge: computations made reversible in this way do not naturally compose
locally.
  We then turn our attention to computational models that eschew the detour via
existing irreversible models. Building on an original analysis by Landauer, the
insights of Bennett, Fredkin, and Toffoli introduced a fresh approach to
reversible computing in which reversibility is elevated to the status of the
main design principle. These initial models are expressed using low-level bit
manipulations, however.
  Abstracting from the low-level of the Bennett-Fredkin-Toffoli models and
pursuing more intrinsic, typed, and algebraic models, naturally leads to rig
categories as the canonical model for compositional reversible programming. The
categorical model reveals connections to type isomorphisms, symmetries,
permutations, groups, and univalent universes. This, in turn, paves the way for
extensions to reversible programming based on monads and arrows. These
extensions are shown to recover conventional irreversible programming, a
variety of reversible computational effects, and more interestingly both pure
(measurement-free) and measurement-based quantum programming.","Jacques Carette, Chris Heunen, Robin Kaarsgaard, Amr Sabry",2024-05-31T14:29:36Z,2024-05-31T14:29:36Z,http://arxiv.org/abs/2405.20842v1,http://arxiv.org/pdf/2405.20842v1.pdf,all:manipulator AND all:AMR AND submittedDate:[202309062236 TO 202509052236],2024
"Orbital origin of fourfold anisotropic magnetoresistance in Dirac
  materials","Fourfold anisotropic magnetoresistance (AMR) have been widely observed in
quantum materials, but the underlying mechanisms remain poorly understood. Here
we find, in a variety of three-dimensional Dirac materials that can be
unifiedly described by the massive Dirac equation, the intrinsic orbital
magnetic moment of electrons vary synchronously with the magnetic field and
give rise to a {\pi} periodic correction to its velocity, further leading to
unusual fourfold AMR, dubbed orbital fourfold AMR. Our theory not only explains
the observation of fourfold AMR in bismuth but also uncovers the nature of the
dominant fourfold AMR in thin films of antiferromagnetic topological insulator
MnBi2Te4, which arises from the near cancellation of the twofold AMR from the
surface states and bulk states due to distinct spin-momentum lockings. Our work
provides a new mechanism for creation and manipulation of orbital fourfold AMR
in both conventional conductors and various topological insulators.","Daifeng Tu, Can Wang, Jianhui Zhou",2024-02-02T14:56:56Z,2025-07-21T23:21:14Z,http://arxiv.org/abs/2402.01470v2,http://arxiv.org/pdf/2402.01470v2.pdf,all:manipulator AND all:AMR AND submittedDate:[202309062236 TO 202509052236],2024
"AERIAL-CORE: AI-Powered Aerial Robots for Inspection and Maintenance of
  Electrical Power Infrastructures","Large-scale infrastructures are prone to deterioration due to age,
environmental influences, and heavy usage. Ensuring their safety through
regular inspections and maintenance is crucial to prevent incidents that can
significantly affect public safety and the environment. This is especially
pertinent in the context of electrical power networks, which, while essential
for energy provision, can also be sources of forest fires. Intelligent drones
have the potential to revolutionize inspection and maintenance, eliminating the
risks for human operators, increasing productivity, reducing inspection time,
and improving data collection quality. However, most of the current methods and
technologies in aerial robotics have been trialed primarily in indoor testbeds
or outdoor settings under strictly controlled conditions, always within the
line of sight of human operators. Additionally, these methods and technologies
have typically been evaluated in isolation, lacking comprehensive integration.
This paper introduces the first autonomous system that combines various
innovative aerial robots. This system is designed for extended-range
inspections beyond the visual line of sight, features aerial manipulators for
maintenance tasks, and includes support mechanisms for human operators working
at elevated heights. The paper further discusses the successful validation of
this system on numerous electrical power lines, with aerial robots executing
flights over 10 kilometers away from their ground control stations.","Anibal Ollero, Alejandro Suarez, Christos Papaioannidis, Ioannis Pitas, Juan M. Marredo, Viet Duong, Emad Ebeid, Vit Kratky, Martin Saska, Chloe Hanoune, Amr Afifi, Antonio Franchi, Charalampos Vourtsis, Dario Floreano, Goran Vasiljevic, Stjepan Bogdan, Alvaro Caballero, Fabio Ruggiero, Vincenzo Lippiello, Carlos Matilla, Giovanni Cioffi, Davide Scaramuzza, Jose R. Martinez-de-Dios, Begona C. Arrue, Carlos Martin, Krzysztof Zurad, Carlos Gaitan, Jacob Rodriguez, Antonio Munoz, Antidio Viguria",2024-01-04T16:34:27Z,2024-01-04T16:34:27Z,http://arxiv.org/abs/2401.02343v1,http://arxiv.org/pdf/2401.02343v1.pdf,all:manipulator AND all:AMR AND submittedDate:[202309062236 TO 202509052236],2024
"Cooperation, Competition, and Maliciousness: LLM-Stakeholders
  Interactive Negotiation","There is an growing interest in using Large Language Models (LLMs) in
multi-agent systems to tackle interactive real-world tasks that require
effective collaboration and assessing complex situations. Yet, we still have a
limited understanding of LLMs' communication and decision-making abilities in
multi-agent setups. The fundamental task of negotiation spans many key features
of communication, such as cooperation, competition, and manipulation
potentials. Thus, we propose using scorable negotiation to evaluate LLMs. We
create a testbed of complex multi-agent, multi-issue, and semantically rich
negotiation games. To reach an agreement, agents must have strong arithmetic,
inference, exploration, and planning capabilities while integrating them in a
dynamic and multi-turn setup. We propose multiple metrics to rigorously
quantify agents' performance and alignment with the assigned role. We provide
procedures to create new games and increase games' difficulty to have an
evolving benchmark. Importantly, we evaluate critical safety aspects such as
the interaction dynamics between agents influenced by greedy and adversarial
players. Our benchmark is highly challenging; GPT-3.5 and small models mostly
fail, and GPT-4 and SoTA large models (e.g., Llama-3 70b) still underperform.","Sahar Abdelnabi, Amr Gomaa, Sarath Sivaprasad, Lea Schönherr, Mario Fritz",2023-09-29T13:33:06Z,2024-06-10T14:43:34Z,http://arxiv.org/abs/2309.17234v2,http://arxiv.org/pdf/2309.17234v2.pdf,all:manipulator AND all:AMR AND submittedDate:[202309062236 TO 202509052236],2023
"Learning-Based Detection of Malicious Volt-VAr Control Parameters in
  Smart Inverters","Distributed Volt-Var Control (VVC) is a widely used control mode of smart
inverters. However, necessary VVC curve parameters are remotely communicated to
the smart inverter, which opens doors for cyberattacks. If the VVC curves of an
inverter are maliciously manipulated, the attacked inverter's reactive power
injection will oscillate, causing undesirable voltage oscillations to manifest
in the distribution system, which, in turn, threatens the system's stability.
In contrast with previous works that proposed methods to mitigate the
oscillations after they are already present in the system, this paper presents
an intrusion detection method to detect malicious VVC curves once they are
communicated to the inverter. The proposed method utilizes a Multi-Layer
Perceptron (MLP) that is trained on features extracted from only the local
measurements of the inverter. After a smart inverter is equipped with the
proposed method, any communicated VVC curve will be verified by the MLP once
received. If the curve is found to be malicious, it will be rejected, thus
preventing unwanted oscillations beforehand. Otherwise, legitimate curves will
be permitted. The performance of the proposed scheme is verified using the
9-bus Canadian urban benchmark distribution system simulated in PSCAD/EMTDC
environment. Our results show that the proposed solution can accurately detect
malicious VVC curves.","Ahmad Mohammad Saber, Amr Youssef, Davor Svetinovic, Hatem Zeineldin, Ehab El-Saadany",2023-09-19T04:11:49Z,2023-09-19T04:11:49Z,http://arxiv.org/abs/2309.10304v1,http://arxiv.org/pdf/2309.10304v1.pdf,all:manipulator AND all:AMR AND submittedDate:[202309062236 TO 202509052236],2023
"ROVER: Recursive Reasoning Over Videos with Vision-Language Models for
  Embodied Tasks","Vision-language models (VLMs) have exhibited impressive capabilities across
diverse image understanding tasks, but still struggle in settings that require
reasoning over extended sequences of camera frames from a video. This limits
their utility in embodied settings, which require reasoning over long frame
sequences from a continuous stream of visual input at each moment of a task
attempt. To address this limitation, we propose ROVER (Reasoning Over VidEo
Recursively), a framework that enables the model to recursively decompose
long-horizon video trajectories into segments corresponding to shorter subtasks
within the trajectory. In doing so, ROVER facilitates more focused and accurate
reasoning over temporally localized frame sequences without losing global
context. We evaluate ROVER, implemented using an in-context learning approach,
on diverse OpenX Embodiment videos and on a new dataset derived from RoboCasa
that consists of 543 videos showing both expert and perturbed non-expert
trajectories across 27 robotic manipulation tasks. ROVER outperforms strong
baselines across three video reasoning tasks: task progress estimation,
frame-level natural language reasoning, and video question answering. We
observe that, by reducing the number of frames the model reasons over at each
timestep, ROVER mitigates hallucinations, especially during unexpected or
non-optimal moments of a trajectory. In addition, by enabling the
implementation of a subtask-specific sliding context window, ROVER's time
complexity scales linearly with video length, an asymptotic improvement over
baselines. Demos, code, and data available at: https://rover-vlm.github.io","Philip Schroeder, Ondrej Biza, Thomas Weng, Hongyin Luo, James Glass",2025-08-03T22:33:43Z,2025-08-03T22:33:43Z,http://arxiv.org/abs/2508.01943v1,http://arxiv.org/pdf/2508.01943v1.pdf,all:manipulator AND all:rover AND submittedDate:[202309062236 TO 202509052236],2025
A smart granular intruder,"It has been recently reported that irregular objects sink irregularly when
released in a granular medium: a subtle lack of symmetry in the density or
shape of a macroscopic object may produce a large tilting and deviation from
the vertical path when released from the free surface of a granular bed. This
can be inconvenient -- even catastrophic -- in scenarios ranging from buildings
to space rovers. Here, we take advantage of the high sensitivity of granular
intruders to shape asymmetry: we introduce a granular intruder equipped with an
inflatable bladder that protrudes from the intruder's surface as an autonomous
response to an unwanted tilting. So, the intruder's symmetry is only slightly
manipulated, resulting in the rectification of the undesired tilting. Our smart
intruder is even able to rectify its settling path when perturbed by an
external element, like a vertical wall. The general concept introduced here can
be potentially expanded to real-life scenarios, such as ``smart foundations''
to mitigate the inclination of constructions on a partially fluidized soil.","Lázaro Martínez-Ortíz, Alex Rivera-Rivera, Ernesto Altshuler",2024-01-27T04:58:46Z,2024-03-20T19:40:31Z,http://arxiv.org/abs/2401.15297v2,http://arxiv.org/pdf/2401.15297v2.pdf,all:manipulator AND all:rover AND submittedDate:[202309062236 TO 202509052236],2024
"Enabling In-Situ Resources Utilisation by leveraging collaborative
  robotics and astronaut-robot interaction","Space exploration and establishing human presence on other planets demand
advanced technology and effective collaboration between robots and astronauts.
Efficient space resource utilization is also vital for extraterrestrial
settlements. The Collaborative In-Situ Resources Utilisation (CISRU) project
has developed a software suite comprising five key modules. The first module
manages multi-agent autonomy, facilitating communication between agents and
mission control. The second focuses on environment perception, employing AI
algorithms for tasks like environment segmentation and object pose estimation.
The third module ensures safe navigation, covering obstacle avoidance, social
navigation with astronauts, and cooperation among robots. The fourth module
addresses manipulation functions, including multi-tool capabilities and
tool-changer design for diverse tasks in In-Situ Resources Utilization (ISRU)
scenarios. Finally, the fifth module controls cooperative behaviour,
incorporating astronaut commands, Mixed Reality interfaces, map fusion, task
supervision, and error control. The suite was tested using an astronaut-rover
interaction dataset in a planetary environment and GMV SPoT analogue
environments. Results demonstrate the advantages of E4 autonomy and AI in space
systems, benefiting astronaut-robot collaboration. This paper details CISRU's
development, field test preparation, and analysis, highlighting its potential
to revolutionize planetary exploration through AI-powered technology.","Silvia Romero-Azpitarte, Cristina Luna, Alba Guerra, Mercedes Alonso, Pablo Romeo Manrique, Marina L. Seoane, Daniel Olayo, Almudena Moreno, Pablo Castellanos, Fernando Gandía, Gianfranco Visentin",2023-11-06T14:43:03Z,2023-11-06T14:43:03Z,http://arxiv.org/abs/2311.03146v1,http://arxiv.org/pdf/2311.03146v1.pdf,all:manipulator AND all:rover AND submittedDate:[202309062236 TO 202509052236],2023
"Learning manipulation of steep granular slopes for fast Mini Rover
  turning","Future planetary exploration missions will require reaching challenging
regions such as craters and steep slopes. Such regions are ubiquitous and
present science-rich targets potentially containing information regarding the
planet's internal structure. Steep slopes consisting of low-cohesion regolith
are prone to flow downward under small disturbances, making it very challenging
for autonomous rovers to traverse. Moreover, the navigation trajectories of
rovers are heavily limited by the terrain topology and future systems will need
to maneuver on flowable surfaces without getting trapped, allowing them to
further expand their reach and increase mission efficiency.
  In this work, we used a laboratory-scale rover robot and performed
maneuvering experiments on a steep granular slope of poppy seeds to explore the
rover's turning capabilities. The rover is capable of lifting, sweeping, and
spinning its wheels, allowing it to execute leg-like gait patterns. The
high-dimensional actuation capabilities of the rover facilitate effective
manipulation of the underlying granular surface. We used Bayesian Optimization
(BO) to gain insight into successful turning gaits in high dimensional search
space and found strategies such as differential wheel spinning and pivoting
around a single sweeping wheel. We then used these insights to further
fine-tune the turning gait, enabling the rover to turn 90 degrees at just above
4 seconds with minimal slip. Combining gait optimization and human-tuning
approaches, we found that fast turning is empowered by creating anisotropic
torques with the sweeping wheel.","Deniz Kerimoglu, Daniel Soto, Malone Lincoln Hemsley, Joseph Brunner, Sehoon Ha, Tingnan Zhang, Daniel I. Goldman",2023-10-02T15:18:39Z,2023-10-02T15:18:39Z,http://arxiv.org/abs/2310.01273v1,http://arxiv.org/pdf/2310.01273v1.pdf,all:manipulator AND all:rover AND submittedDate:[202309062236 TO 202509052236],2023
"Policy Learning for Individualized Treatment Regimes on Infinite Time
  Horizon","With the recent advancements of technology in facilitating real-time
monitoring and data collection, ""just-in-time"" interventions can be delivered
via mobile devices to achieve both real-time and long-term management and
control. Reinforcement learning formalizes such mobile interventions as a
sequence of decision rules and assigns treatment arms based on the user's
status at each decision point. In practice, real applications concern a large
number of decision points beyond the time horizon of the currently collected
data. This usually refers to reinforcement learning in the infinite horizon
setting, which becomes much more challenging. This article provides a selective
overview of some statistical methodologies on this topic. We discuss their
modeling framework, generalizability, and interpretability and provide some use
case examples. Some future research directions are discussed in the end.","Wenzhuo Zhou, Yuhan Li, Ruoqing Zhu",2023-09-23T18:59:49Z,2023-09-23T18:59:49Z,http://arxiv.org/abs/2309.13458v1,http://arxiv.org/pdf/2309.13458v1.pdf,all:robotic arm AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2023
"Demonstrating DVS: Dynamic Virtual-Real Simulation Platform for Mobile
  Robotic Tasks","With the development of embodied artificial intelligence, robotic research
has increasingly focused on complex tasks. Existing simulation platforms,
however, are often limited to idealized environments, simple task scenarios and
lack data interoperability. This restricts task decomposition and multi-task
learning. Additionally, current simulation platforms face challenges in dynamic
pedestrian modeling, scene editability, and synchronization between virtual and
real assets. These limitations hinder real world robot deployment and feedback.
To address these challenges, we propose DVS (Dynamic Virtual-Real Simulation
Platform), a platform for dynamic virtual-real synchronization in mobile
robotic tasks. DVS integrates a random pedestrian behavior modeling plugin and
large-scale, customizable indoor scenes for generating annotated training
datasets. It features an optical motion capture system, synchronizing object
poses and coordinates between virtual and real world to support dynamic task
benchmarking. Experimental validation shows that DVS supports tasks such as
pedestrian trajectory prediction, robot path planning, and robotic arm
grasping, with potential for both simulation and real world deployment. In this
way, DVS represents more than just a versatile robotic platform; it paves the
way for research in human intervention in robot execution tasks and real-time
feedback algorithms in virtual-real fusion environments. More information about
the simulation platform is available on https://immvlab.github.io/DVS/.","Zijie Zheng, Zeshun Li, Yunpeng Wang, Qinghongbing Xie, Long Zeng",2025-04-26T15:07:22Z,2025-04-26T15:07:22Z,http://arxiv.org/abs/2504.18944v1,http://arxiv.org/pdf/2504.18944v1.pdf,all:robotic arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"Vector Quantized-Elites: Unsupervised and Problem-Agnostic
  Quality-Diversity Optimization","Quality-Diversity algorithms have transformed optimization by prioritizing
the discovery of diverse, high-performing solutions over a single optimal
result. However, traditional Quality-Diversity methods, such as MAP-Elites,
rely heavily on predefined behavior descriptors and complete prior knowledge of
the task to define the behavior space grid, limiting their flexibility and
applicability. In this work, we introduce Vector Quantized-Elites (VQ-Elites),
a novel Quality-Diversity algorithm that autonomously constructs a structured
behavior space grid using unsupervised learning, eliminating the need for prior
task-specific knowledge. At the core of VQ-Elites is the integration of Vector
Quantized Variational Autoencoders, which enables the dynamic learning of
behavior descriptors and the generation of a structured, rather than
unstructured, behavior space grid -- a significant advancement over existing
unsupervised Quality-Diversity approaches. This design establishes VQ-Elites as
a flexible, robust, and task-agnostic optimization framework. To further
enhance the performance of unsupervised Quality-Diversity algorithms, we
introduce behavior space bounding and cooperation mechanisms, which
significantly improve convergence and performance, as well as the Effective
Diversity Ratio and Coverage Diversity Score, two novel metrics that quantify
the actual diversity in the unsupervised setting. We validate VQ-Elites on
robotic arm pose-reaching, mobile robot space-covering, and MiniGrid
exploration tasks. The results demonstrate its ability to efficiently generate
diverse, high-quality solutions, emphasizing its adaptability, scalability,
robustness to hyperparameters, and potential to extend Quality-Diversity
optimization to complex, previously inaccessible domains.","Constantinos Tsakonas, Konstantinos Chatzilygeroudis",2025-04-10T18:23:19Z,2025-08-07T11:56:27Z,http://arxiv.org/abs/2504.08057v2,http://arxiv.org/pdf/2504.08057v2.pdf,all:robotic arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
Volumetric Reconstruction From Partial Views for Task-Oriented Grasping,"Object affordance and volumetric information are essential in devising
effective grasping strategies under task-specific constraints. This paper
presents an approach for inferring suitable grasping strategies from limited
partial views of an object. To achieve this, a recurrent generative adversarial
network (R-GAN) was proposed by incorporating a recurrent generator with long
short-term memory (LSTM) units for it to process a variable number of depth
scans. To determine object affordances, the AffordPose knowledge dataset is
utilized as prior knowledge. Affordance retrieving is defined by the volume
similarity measured via Chamfer Distance and action similarities. A Proximal
Policy Optimization (PPO) reinforcement learning model is further implemented
to refine the retrieved grasp strategies for task-oriented grasping. The
retrieved grasp strategies were evaluated on a dual-arm mobile manipulation
robot with an overall grasping accuracy of 89% for four tasks: lift, handle
grasp, wrap grasp, and press.","Fujian Yan, Hui Li, Hongsheng He",2025-03-19T12:47:50Z,2025-03-19T12:47:50Z,http://arxiv.org/abs/2503.15167v1,http://arxiv.org/pdf/2503.15167v1.pdf,all:robotic arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"AhaRobot: A Low-Cost Open-Source Bimanual Mobile Manipulator for
  Embodied AI","Navigation and manipulation in open-world environments remain unsolved
challenges in the Embodied AI. The high cost of commercial mobile manipulation
robots significantly limits research in real-world scenes. To address this
issue, we propose AhaRobot, a low-cost and fully open-source dual-arm mobile
manipulation robot system with a hardware cost of only $1,000 (excluding
optional computational resources), which is less than 1/15 of the cost of
popular mobile robots. The AhaRobot system consists of three components: (1) a
novel low-cost hardware architecture primarily composed of off-the-shelf
components, (2) an optimized control solution to enhance operational precision
integrating dual-motor backlash control and static friction compensation, and
(3) a simple remote teleoperation method RoboPilot. We use handles to control
the dual arms and pedals for whole-body movement. The teleoperation process is
low-burden and easy to operate, much like piloting. RoboPilot is designed for
remote data collection in embodied scenarios. Experimental results demonstrate
that RoboPilot significantly enhances data collection efficiency in complex
manipulation tasks, achieving a 30% increase compared to methods using 3D mouse
and leader-follower systems. It also excels at completing extremely
long-horizon tasks in one go. Furthermore, AhaRobot can be used to learn
end-to-end policies and autonomously perform complex manipulation tasks, such
as pen insertion and cleaning up the floor. We aim to build an affordable yet
powerful platform to promote the development of embodied tasks on real devices,
advancing more robust and reliable embodied AI. All hardware and software
systems are available at https://aha-robot.github.io.","Haiqin Cui, Yifu Yuan, Yan Zheng, Jianye Hao",2025-03-13T05:34:43Z,2025-03-13T05:34:43Z,http://arxiv.org/abs/2503.10070v1,http://arxiv.org/pdf/2503.10070v1.pdf,all:robotic arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"Hi Robot: Open-Ended Instruction Following with Hierarchical
  Vision-Language-Action Models","Generalist robots that can perform a range of different tasks in open-world
settings must be able to not only reason about the steps needed to accomplish
their goals, but also process complex instructions, prompts, and even feedback
during task execution. Intricate instructions (e.g., ""Could you make me a
vegetarian sandwich?"" or ""I don't like that one"") require not just the ability
to physically perform the individual steps, but the ability to situate complex
commands and feedback in the physical world. In this work, we describe a system
that uses vision-language models in a hierarchical structure, first reasoning
over complex prompts and user feedback to deduce the most appropriate next step
to fulfill the task, and then performing that step with low-level actions. In
contrast to direct instruction following methods that can fulfill simple
commands (""pick up the cup""), our system can reason through complex prompts and
incorporate situated feedback during task execution (""that's not trash""). We
evaluate our system across three robotic platforms, including single-arm,
dual-arm, and dual-arm mobile robots, demonstrating its ability to handle tasks
such as cleaning messy tables, making sandwiches, and grocery shopping. Videos
are available at https://www.pi.website/research/hirobot","Lucy Xiaoyang Shi, Brian Ichter, Michael Equi, Liyiming Ke, Karl Pertsch, Quan Vuong, James Tanner, Anna Walling, Haohuan Wang, Niccolo Fusai, Adrian Li-Bell, Danny Driess, Lachy Groom, Sergey Levine, Chelsea Finn",2025-02-26T18:58:41Z,2025-07-15T17:44:28Z,http://arxiv.org/abs/2502.19417v2,http://arxiv.org/pdf/2502.19417v2.pdf,all:robotic arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"MITO: A Millimeter-Wave Dataset and Simulator for Non-Line-of-Sight
  Perception","The ability to observe the world is fundamental to reasoning and making
informed decisions on how to interact with the environment. However, optical
perception can often be disrupted due to common occurrences, such as
occlusions, which can pose challenges to existing vision systems. We present
MITO, the first millimeter-wave (mmWave) dataset of diverse, everyday objects,
collected using a UR5 robotic arm with two mmWave radars operating at different
frequencies and an RGB-D camera. Unlike visible light, mmWave signals can
penetrate common occlusions (e.g., cardboard boxes, fabric, plastic) but each
mmWave frame has much lower resolution than typical cameras. To capture
higher-resolution mmWave images, we leverage the robot's mobility and fuse
frames over the synthesized aperture. MITO captures over 24 million mmWave
frames and uses them to generate 550 high-resolution mmWave (synthetic
aperture) images in line-of-sight and non-light-of-sight (NLOS), as well as
RGB-D images, segmentation masks, and raw mmWave signals, taken from 76
different objects. We develop an open-source simulation tool that can be used
to generate synthetic mmWave images for any 3D triangle mesh. Finally, we
demonstrate the utility of our dataset and simulator for enabling broader NLOS
perception by developing benchmarks for NLOS segmentation and classification.","Laura Dodds, Tara Boroushaki, Cusuh Ham, Fadel Adib",2025-02-14T16:12:14Z,2025-03-11T18:31:32Z,http://arxiv.org/abs/2502.10259v3,http://arxiv.org/pdf/2502.10259v3.pdf,all:robotic arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"SLIM: Sim-to-Real Legged Instructive Manipulation via Long-Horizon
  Visuomotor Learning","We present a low-cost legged mobile manipulation system that solves
long-horizon real-world tasks, trained by reinforcement learning purely in
simulation. This system is made possible by 1) a hierarchical design of a
high-level policy for visual-mobile manipulation following task instructions,
and a low-level quadruped locomotion policy, 2) a teacher and student training
pipeline for the high level, which trains a teacher to tackle long-horizon
tasks using privileged task decomposition and target object information, and
further trains a student for visual-mobile manipulation via RL guided by the
teacher's behavior, and 3) a suite of techniques for minimizing the sim-to-real
gap.
  In contrast to many previous works that use high-end equipments, our system
demonstrates effective performance with more accessible hardware --
specifically, a Unitree Go1 quadruped, a WidowX-250S arm, and a single
wrist-mounted RGB camera -- despite the increased challenges of sim-to-real
transfer. Trained fully in simulation, a single policy autonomously solves
long-horizon tasks involving search, move to, grasp, transport, and drop into,
achieving nearly 80% real-world success. This performance is comparable to that
of expert human teleoperation on the same tasks while the robot is more
efficient, operating at about 1.5x the speed of the teleoperation. Finally, we
perform extensive ablations on key techniques for efficient RL training and
effective sim-to-real transfer, and demonstrate effective deployment across
diverse indoor and outdoor scenes under various lighting conditions.","Haichao Zhang, Haonan Yu, Le Zhao, Andrew Choi, Qinxun Bai, Break Yang, Wei Xu",2025-01-17T01:32:18Z,2025-01-29T19:58:23Z,http://arxiv.org/abs/2501.09905v4,http://arxiv.org/pdf/2501.09905v4.pdf,all:robotic arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
Kiri-Spoon: A Kirigami Utensil for Robot-Assisted Feeding,"For millions of adults with mobility limitations, eating meals is a daily
challenge. A variety of robotic systems have been developed to address this
societal need. Unfortunately, end-user adoption of robot-assisted feeding is
limited, in part because existing devices are unable to seamlessly grasp,
manipulate, and feed diverse foods. Recent works seek to address this issue by
creating new algorithms for food acquisition and bite transfer. In parallel to
these algorithmic developments, however, we hypothesize that mechanical
intelligence will make it fundamentally easier for robot arms to feed humans.
We therefore propose Kiri-Spoon, a soft utensil specifically designed for
robot-assisted feeding. Kiri-Spoon consists of a spoon-shaped kirigami
structure: when actuated, the kirigami sheet deforms into a bowl of increasing
curvature. Robot arms equipped with Kiri-Spoon can leverage the kirigami
structure to wrap-around morsels during acquisition, contain those items as the
robot moves, and then compliantly release the food into the user's mouth.
Overall, Kiri-Spoon combines the familiar and comfortable shape of a standard
spoon with the increased capabilities of soft robotic grippers. In what
follows, we first apply a stakeholder-driven design process to ensure that
Kiri-Spoon meets the needs of caregivers and users with physical disabilities.
We next characterize the dynamics of Kiri-Spoon, and derive a mechanics model
to relate actuation force to the spoon's shape. The paper concludes with three
separate experiments that evaluate (a) the mechanical advantage provided by
Kiri-Spoon, (b) the ways users with disabilities perceive our system, and (c)
how the mechanical intelligence of Kiri-Spoon complements state-of-the-art
algorithms. Our results suggest that Kiri-Spoon advances robot-assisted feeding
across diverse foods, multiple robotic platforms, and different manipulation
algorithms.","Maya Keely, Brandon Franco, Casey Grothoff, Rajat Kumar Jenamani, Tapomayukh Bhattacharjee, Dylan P. Losey, Heramb Nemlekar",2025-01-02T16:23:59Z,2025-01-02T16:23:59Z,http://arxiv.org/abs/2501.01323v1,http://arxiv.org/pdf/2501.01323v1.pdf,all:robotic arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
Bimanual Dexterity for Complex Tasks,"To train generalist robot policies, machine learning methods often require a
substantial amount of expert human teleoperation data. An ideal robot for
humans collecting data is one that closely mimics them: bimanual arms and
dexterous hands. However, creating such a bimanual teleoperation system with
over 50 DoF is a significant challenge. To address this, we introduce Bidex, an
extremely dexterous, low-cost, low-latency and portable bimanual dexterous
teleoperation system which relies on motion capture gloves and teacher arms. We
compare Bidex to a Vision Pro teleoperation system and a SteamVR system and
find Bidex to produce better quality data for more complex tasks at a faster
rate. Additionally, we show Bidex operating a mobile bimanual robot for in the
wild tasks. The robot hands (5k USD) and teleoperation system (7k USD) is
readily reproducible and can be used on many robot arms including two xArms
(16k USD). Website at https://bidex-teleop.github.io/","Kenneth Shaw, Yulong Li, Jiahui Yang, Mohan Kumar Srirama, Ray Liu, Haoyu Xiong, Russell Mendonca, Deepak Pathak",2024-11-20T19:53:35Z,2024-11-20T19:53:35Z,http://arxiv.org/abs/2411.13677v1,http://arxiv.org/pdf/2411.13677v1.pdf,all:robotic arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2024
$π_0$: A Vision-Language-Action Flow Model for General Robot Control,"Robot learning holds tremendous promise to unlock the full potential of
flexible, general, and dexterous robot systems, as well as to address some of
the deepest questions in artificial intelligence. However, bringing robot
learning to the level of generality required for effective real-world systems
faces major obstacles in terms of data, generalization, and robustness. In this
paper, we discuss how generalist robot policies (i.e., robot foundation models)
can address these challenges, and how we can design effective generalist robot
policies for complex and highly dexterous tasks. We propose a novel flow
matching architecture built on top of a pre-trained vision-language model (VLM)
to inherit Internet-scale semantic knowledge. We then discuss how this model
can be trained on a large and diverse dataset from multiple dexterous robot
platforms, including single-arm robots, dual-arm robots, and mobile
manipulators. We evaluate our model in terms of its ability to perform tasks in
zero shot after pre-training, follow language instructions from people and from
a high-level VLM policy, and its ability to acquire new skills via fine-tuning.
Our results cover a wide variety of tasks, such as laundry folding, table
cleaning, and assembling boxes.","Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, Ury Zhilinsky",2024-10-31T17:22:30Z,2024-11-13T17:30:10Z,http://arxiv.org/abs/2410.24164v3,http://arxiv.org/pdf/2410.24164v3.pdf,all:robotic arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2024
"Neurofeedback-Driven 6-DOF Robotic Arm: Integration of Brain-Computer
  Interface with Arduino for Advanced Control","Brain computer interface (BCI) applications in robotics are becoming more
famous and famous. People with disabilities are facing a real-time problem of
doing simple activities such as grasping, handshaking etc. in order to aid with
this problem, the use of brain signals to control actuators is showing a great
importance. The Emotive Insight, a Brain-Computer Interface (BCI) device, is
utilized in this project to collect brain signals and transform them into
commands for controlling a robotic arm using an Arduino controller. The Emotive
Insight captures brain signals, which are subsequently analyzed using Emotive
software and connected with Arduino code. The HITI Brain software integrates
these devices, allowing for smooth communication between brain activity and the
robotic arm. This system demonstrates how brain impulses may be utilized to
control external devices directly. The results showed that the system is
applicable efficiently to robotic arms and also for prosthetic arms with Multi
Degree of Freedom. In addition to that, the system can be used for other
actuators such as bikes, mobile robots, wheelchairs etc.","Ihab A. Satam, Róbert Szabolcsi",2024-10-29T12:55:04Z,2024-10-29T12:55:04Z,http://arxiv.org/abs/2410.22008v1,http://arxiv.org/pdf/2410.22008v1.pdf,all:robotic arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2024
SODA: a Soft Origami Dynamic utensil for Assisted feeding,"SODA aims to revolutionize assistive feeding systems by designing a
multi-purpose utensil using origami-inspired artificial muscles. Traditional
utensils, such as forks and spoons,are hard and stiff, causing discomfort and
fear among users, especially when operated by autonomous robotic arms.
Additionally, these systems require frequent utensil changes to handle
different food types. Our innovative utensil design addresses these issues by
offering a versatile, adaptive solution that can seamlessly transition between
gripping and scooping various foods without the need for manual intervention.
Utilizing the flexibility and strength of origami-inspired artificial muscles,
the utensil ensures safe and comfortable interactions, enhancing user
experience and efficiency. This approach not only simplifies the feeding
process but also promotes greater independence for individuals with limited
mobility, contributing to the advancement of soft robotics in healthcare
applications.","Yuxin Ray Song, Shufan Wang",2024-10-25T13:42:03Z,2024-10-25T13:42:03Z,http://arxiv.org/abs/2410.19558v1,http://arxiv.org/pdf/2410.19558v1.pdf,all:robotic arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2024
Technical Report of Mobile Manipulator Robot for Industrial Environments,"This paper describes Auriga's @Work team and their robot, developed at Shahid
Beheshti University Faculty of Electrical Engineering's Robotics and
Intelligent Automation Lab for RoboCup 2024 competitions. The robot is designed
for industrial tasks, optimizing efficiency in repetitive or hazardous
environments. It features a 4-wheel Mecanum system for omnidirectional movement
and a 5-degree-of-freedom manipulator arm with a 3D-printed gripper for object
handling and navigation. The electronics include custom boards with ESP32
microcontrollers and an Nvidia Jetson Nano for real-time control. Key software
components include Hector SLAM for mapping, A* path planning, and YOLO for
object detection, supported by integrated sensors for enhanced navigation and
collision avoidance.","Erfan Amoozad Khalili, Kiarash Ghasemzadeh, Hossein Gohari, Mohammadreza Jafari, Matin Jamshidi, Mahdi Khaksar, AmirReza AkramiFard, Mana Hatamzadeh, Saba Sadeghi, Mohammad Hossein Moaiyeri",2024-09-10T17:55:29Z,2024-10-27T09:29:07Z,http://arxiv.org/abs/2409.06693v2,http://arxiv.org/pdf/2409.06693v2.pdf,all:robotic arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2024
Vision-assisted Avocado Harvesting with Aerial Bimanual Manipulation,"Robotic fruit harvesting holds potential in precision agriculture to improve
harvesting efficiency. While ground mobile robots are mostly employed in fruit
harvesting, certain crops, like avocado trees, cannot be harvested efficiently
from the ground alone. This is because of unstructured ground and planting
arrangement and high-to-reach fruits. In such cases, aerial robots integrated
with manipulation capabilities can pave new ways in robotic harvesting. This
paper outlines the design and implementation of a bimanual UAV that employs
visual perception and learning to autonomously detect avocados, reach, and
harvest them. The dual-arm system comprises a gripper and a fixer arm, to
address a key challenge when harvesting avocados: once grasped, a rotational
motion is the most efficient way to detach the avocado from the peduncle;
however, the peduncle may store elastic energy preventing the avocado from
being harvested. The fixer arm aims to stabilize the peduncle, allowing the
gripper arm to harvest. The integrated visual perception process enables the
detection of avocados and the determination of their pose; the latter is then
used to determine target points for a bimanual manipulation planner. Several
experiments are conducted to assess the efficacy of each component, and
integrated experiments assess the effectiveness of the system.","Zhichao Liu, Jingzong Zhou, Caio Mucchiani, Konstantinos Karydis",2024-08-17T00:47:00Z,2024-08-17T00:47:00Z,http://arxiv.org/abs/2408.09058v1,http://arxiv.org/pdf/2408.09058v1.pdf,all:robotic arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2024
"Rico: extended TIAGo robot towards up-to-date social and assistive robot
  usage scenarios","Social and assistive robotics have vastly increased in popularity in recent
years. Due to the wide range of usage, robots executing such tasks must be
highly reliable and possess enough functions to satisfy multiple scenarios.
This article describes a mobile, artificial intelligence-driven, robotic
platform Rico. Its prior usage in similar scenarios, the number of its
capabilities, and the experiments it presented should qualify it as a proper
arm-less platform for social and assistive circumstances.","Tomasz Winiarski, Wojciech Dudek, Daniel Giełdowski",2024-07-31T07:43:27Z,2024-09-06T13:40:13Z,http://arxiv.org/abs/2407.21401v3,http://arxiv.org/pdf/2407.21401v3.pdf,all:robotic arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2024
Design and Control of a Novel Six-Degree-of-Freedom Hybrid Robotic Arm,"Robotic arms are key components in fruit-harvesting robots. In agricultural
settings, conventional serial or parallel robotic arms often fall short in
meeting the demands for a large workspace, rapid movement, enhanced capability
of obstacle avoidance and affordability. This study proposes a novel hybrid
six-degree-of-freedom (DoF) robotic arm that combines the advantages of
parallel and serial mechanisms. Inspired by yoga, we designed two sliders
capable of moving independently along a single rail, acting as two feet. These
sliders are interconnected with linkages and a meshed-gear set, allowing the
parallel mechanism to lower itself and perform a split to pass under obstacles.
This unique feature allows the arm to avoid obstacles such as pipes, tables and
beams typically found in greenhouses. Integrated with serially mounted joints,
the patented hybrid arm is able to maintain the end's pose even when it moves
with a mobile platform, facilitating fruit picking with the optimal pose in
dynamic conditions. Moreover, the hybrid arm's workspace is substantially
larger, being almost three times the volume of UR3 serial arms and fourteen
times that of the ABB IRB parallel arms. Experiments show that the
repeatability errors are 0.017 mm, 0.03 mm and 0.109 mm for the two sliders and
the arm's end, respectively, providing sufficient precision for agricultural
robots.","Yang Chen, Zhonghua Miao, Yuanyue Ge, Sen lin, Liping Chen, Ya Xiong",2024-07-29T09:23:30Z,2024-07-29T09:23:30Z,http://arxiv.org/abs/2407.19826v1,http://arxiv.org/pdf/2407.19826v1.pdf,all:robotic arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2024
"Robotically adjustable kinematics in a wrist-driven orthosis eases
  grasping across tasks","Without finger function, people with C5-7 spinal cord injury (SCI) regularly
utilize wrist extension to passively close the fingers and thumb together for
grasping. Wearable assistive grasping devices often focus on this familiar
wrist-driven technique to provide additional support and amplify grasp force.
Despite recent research advances in modernizing these tools, people with SCI
often abandon such wearable assistive devices in the long term. We suspect that
the wrist constraints imposed by such devices generate undesirable reach and
grasp kinematics. Here we show that using continuous robotic motor assistance
to give users more adaptability in their wrist posture prior to wrist-driven
grasping reduces task difficulty and perceived exertion. Our results
demonstrate that more free wrist mobility allows users to select comfortable
and natural postures depending on task needs, which improves the versatility of
the assistive grasping device for easier use across different hand poses in the
arm's workspace. This behavior holds the potential to improve ease of use and
desirability of future device designs through new modes of combining both
body-power and robotic automation.","Erin Y. Chang, Andrew I. W. McPherson, Hannah S. Stuart",2024-07-22T23:33:43Z,2024-07-22T23:33:43Z,http://arxiv.org/abs/2407.16095v1,http://arxiv.org/pdf/2407.16095v1.pdf,all:robotic arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2024
Autonomous Soil Collection in Environments With Heterogeneous Terrain,"To autonomously collect soil in uncultivated terrain, robotic arms must
distinguish between different amorphous materials and submerge themselves into
the correct material. We develop a prototype that collects soil in
heterogeneous terrain. If mounted to a mobile robot, it can be used to perform
soil collection and analysis without human intervention. Unique among soil
sampling robots, we use a general-purpose robotic arm rather than a soil core
sampler.","Andrew Dudash, Beyonce Andrades, Ryan Rubel, Mohammad Goli, Nathan Clark, William Ewald",2024-07-15T21:41:03Z,2024-07-15T21:41:03Z,http://arxiv.org/abs/2407.11251v1,http://arxiv.org/pdf/2407.11251v1.pdf,all:robotic arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2024
"Empowering Embodied Manipulation: A Bimanual-Mobile Robot Manipulation
  Dataset for Household Tasks","The advancements in embodied AI are increasingly enabling robots to tackle
complex real-world tasks, such as household manipulation. However, the
deployment of robots in these environments remains constrained by the lack of
comprehensive bimanual-mobile robot manipulation data that can be learned.
Existing datasets predominantly focus on single-arm manipulation tasks, while
the few dual-arm datasets available often lack mobility features, task
diversity, comprehensive sensor data, and robust evaluation metrics; they fail
to capture the intricate and dynamic nature of household manipulation tasks
that bimanual-mobile robots are expected to perform. To overcome these
limitations, we propose BRMData, a Bimanual-mobile Robot Manipulation Dataset
specifically designed for household applications. BRMData encompasses 10
diverse household tasks, including single-arm and dual-arm tasks, as well as
both tabletop and mobile manipulations, utilizing multi-view and depth-sensing
data information. Moreover, BRMData features tasks of increasing difficulty,
ranging from single-object to multi-object grasping, non-interactive to
human-robot interactive scenarios, and rigid-object to flexible-object
manipulation, closely simulating real-world household applications.
Additionally, we introduce a novel Manipulation Efficiency Score (MES) metric
to evaluate both the precision and efficiency of robot manipulation methods in
household tasks. We thoroughly evaluate and analyze the performance of advanced
robot manipulation learning methods using our BRMData, aiming to drive the
development of bimanual-mobile robot manipulation technologies. The dataset is
now open-sourced and available at https://embodiedrobot.github.io/.","Tianle Zhang, Dongjiang Li, Yihang Li, Zecui Zeng, Lin Zhao, Lei Sun, Yue Chen, Xuelong Wei, Yibing Zhan, Lusong Li, Xiaodong He",2024-05-29T08:15:56Z,2024-06-06T05:53:19Z,http://arxiv.org/abs/2405.18860v2,http://arxiv.org/pdf/2405.18860v2.pdf,all:robotic arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2024
A Weeding Robot for Seedling Removal,"Automatic weeding technologies have attained a lot of attention lately,
because of the harms and challenges weeds are causing for livestock farming, in
addition to that weeds reduce yields. We are targeting automatic and mechanical
Rumex weeding in open pasture fields using light weight mobile field robot
technologies. We describe a mobile weeding robot with GNSS navigation, 3D
computer vision for weed detection, and a robot arm with a mechanical weeding
tool. Our main contribution is showing the feasibility of light weight robot,
sensor, and tool technologies in mechanical removal of weed seedlings.","Jarkko Kotaniemi, Niko Känsäkoski, Tapio Heikkilä",2024-05-21T08:39:09Z,2024-05-21T08:39:09Z,http://arxiv.org/abs/2405.12596v1,http://arxiv.org/pdf/2405.12596v1.pdf,all:robotic arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2024
ACES: A Teleoperated Robotic Solution to Pipe Inspection from the Inside,"This paper presents the definition of a teleoperated robotic system for
non-destructive corrosion inspection of Steel Cylinder Concrete Pipes (SCCP)
from the inside. A general description of in-pipe environment and a state of
the art of in-pipe navigation solutions are exposed, with a zoom on the
characteristics of the SCCP case of interest (pipe dimensions, curves, slopes,
humidity, payload, etc.). Then, two specific steel corrosion measurement
techniques are described. In order to operate them, several possible
architectures of inspection system (mobile platform combined with a robotic
inspection manipulator) are presented, depending if the mobile platform is
self-centred or not and regarding the robotic manipulator type, namely a basic
cylindrical manipulator, a self centred one, or a force-controlled 6 degrees of
freedom (DoF) robotic arm. A suitable mechanical architecture is then selected
according to SCCP inspection needs. This includes relevant interfaces between
the robot, the corrosion measurement Non Destructive Testing (NDT) device and
the pipe. Finally, possible future adaptation of the chosen solution are
exposed.","Eric Lucet, Farès Kfoury",2024-05-16T09:26:41Z,2024-05-16T09:26:41Z,http://arxiv.org/abs/2405.09925v1,http://arxiv.org/pdf/2405.09925v1.pdf,all:robotic arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2024
Cybathlon -- Legged Mobile Assistance for Quadriplegics,"Assistance robots are the future for people who need daily care due to
limited mobility or being wheelchair-bound. Current solutions of attaching
robotic arms to motorized wheelchairs only provide limited additional mobility
at the cost of increased size. We present a mouth joystick control interface,
augmented with voice commands, for an independent quadrupedal assistance robot
with an arm. We validate and showcase our system in the Cybathlon Challenges
February 2024 Assistance Robot Race, where we solve four everyday tasks in
record time, winning first place. Our system remains generic and sets the basis
for a platform that could help and provide independence in the everyday lives
of people in wheelchairs.","Carmen Scheidemann, Andrei Cramariuc, Marco Hutter",2024-05-13T03:04:01Z,2024-05-13T03:04:01Z,http://arxiv.org/abs/2405.07445v1,http://arxiv.org/pdf/2405.07445v1.pdf,all:robotic arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2024
"Using Capability Maps Tailored to Arm Range of Motion in VR Exergames
  for Rehabilitation","Many neurological conditions, e.g., a stroke, can cause patients to
experience upper limb (UL) motor impairments that hinder their daily
activities. For such patients, while rehabilitation therapy is key for
regaining autonomy and restoring mobility, its long-term nature entails ongoing
time commitment and it is often not sufficiently engaging. Virtual reality (VR)
can transform rehabilitation therapy into engaging game-like tasks that can be
tailored to patient-specific activities, set goals, and provide rehabilitation
assessment. Yet, most VR systems lack built-in methods to track progress over
time and alter rehabilitation programs accordingly. We propose using arm
kinematic modeling and capability maps to allow a VR system to understand a
user's physical capability and limitation. Next, we suggest two use cases for
the VR system to utilize the user's capability map for tailoring rehabilitation
programs. Finally, for one use case, it is shown that the VR system can
emphasize and assess the use of specific UL joints.","Christian Lourido, Zaid Waghoo, Hassam Khan Wazir, Nishtha Bhagat, Vikram Kapila",2024-04-18T20:53:47Z,2024-04-18T20:53:47Z,http://arxiv.org/abs/2404.12504v1,http://arxiv.org/pdf/2404.12504v1.pdf,all:robotic arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2024
Learning Visual Quadrupedal Loco-Manipulation from Demonstrations,"Quadruped robots are progressively being integrated into human environments.
Despite the growing locomotion capabilities of quadrupedal robots, their
interaction with objects in realistic scenes is still limited. While additional
robotic arms on quadrupedal robots enable manipulating objects, they are
sometimes redundant given that a quadruped robot is essentially a mobile unit
equipped with four limbs, each possessing 3 degrees of freedom (DoFs). Hence,
we aim to empower a quadruped robot to execute real-world manipulation tasks
using only its legs. We decompose the loco-manipulation process into a
low-level reinforcement learning (RL)-based controller and a high-level
Behavior Cloning (BC)-based planner. By parameterizing the manipulation
trajectory, we synchronize the efforts of the upper and lower layers, thereby
leveraging the advantages of both RL and BC. Our approach is validated through
simulations and real-world experiments, demonstrating the robot's ability to
perform tasks that demand mobility and high precision, such as lifting a basket
from the ground while moving, closing a dishwasher, pressing a button, and
pushing a door. Project website: https://zhengmaohe.github.io/leg-manip","Zhengmao He, Kun Lei, Yanjie Ze, Koushil Sreenath, Zhongyu Li, Huazhe Xu",2024-03-29T17:59:05Z,2024-08-02T16:51:52Z,http://arxiv.org/abs/2403.20328v2,http://arxiv.org/pdf/2403.20328v2.pdf,all:robotic arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2024
"MindArm: Mechanized Intelligent Non-Invasive Neuro-Driven Prosthetic Arm
  System","Currently, individuals with arm mobility impairments (referred to as
""patients"") face limited technological solutions due to two key challenges: (1)
non-invasive prosthetic devices are often prohibitively expensive and costly to
maintain, and (2) invasive solutions require high-risk, costly brain surgery,
which can pose a health risk. Therefore, current technological solutions are
not accessible for all patients with different financial backgrounds. Toward
this, we propose a low-cost technological solution called MindArm, an
affordable, non-invasive neuro-driven prosthetic arm system. MindArm employs a
deep neural network (DNN) to translate brain signals, captured by low-cost
surface electroencephalogram (EEG) electrodes, into prosthetic arm movements.
Utilizing an Open Brain Computer Interface and UDP networking for signal
processing, the system seamlessly controls arm motion. In the compute module,
we run a trained DNN model to interpret filtered micro-voltage brain signals,
and then translate them into a prosthetic arm action via serial communication
seamlessly. Experimental results from a fully functional prototype show high
accuracy across three actions, with 91% for idle/stationary, 85% for handshake,
and 84% for cup pickup. The system costs approximately $500-550, including $400
for the EEG headset and $100-150 for motors, 3D printing, and assembly,
offering an affordable alternative for mind-controlled prosthetic devices.","Maha Nawaz, Abdul Basit, Muhammad Shafique",2024-03-29T06:09:24Z,2024-10-19T18:23:46Z,http://arxiv.org/abs/2403.19992v2,http://arxiv.org/pdf/2403.19992v2.pdf,all:robotic arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2024
"LocoMan: Advancing Versatile Quadrupedal Dexterity with Lightweight
  Loco-Manipulators","Quadrupedal robots have emerged as versatile agents capable of locomoting and
manipulating in complex environments. Traditional designs typically rely on the
robot's inherent body parts or incorporate top-mounted arms for manipulation
tasks. However, these configurations may limit the robot's operational
dexterity, efficiency and adaptability, particularly in cluttered or
constrained spaces. In this work, we present LocoMan, a dexterous quadrupedal
robot with a novel morphology to perform versatile manipulation in diverse
constrained environments. By equipping a Unitree Go1 robot with two low-cost
and lightweight modular 3-DoF loco-manipulators on its front calves, LocoMan
leverages the combined mobility and functionality of the legs and grippers for
complex manipulation tasks that require precise 6D positioning of the end
effector in a wide workspace. To harness the loco-manipulation capabilities of
LocoMan, we introduce a unified control framework that extends the whole-body
controller (WBC) to integrate the dynamics of loco-manipulators. Through
experiments, we validate that the proposed whole-body controller can accurately
and stably follow desired 6D trajectories of the end effector and torso, which,
when combined with the large workspace from our design, facilitates a diverse
set of challenging dexterous loco-manipulation tasks in confined spaces, such
as opening doors, plugging into sockets, picking objects in narrow and
low-lying spaces, and bimanual manipulation.","Changyi Lin, Xingyu Liu, Yuxiang Yang, Yaru Niu, Wenhao Yu, Tingnan Zhang, Jie Tan, Byron Boots, Ding Zhao",2024-03-27T02:13:24Z,2024-10-18T16:47:50Z,http://arxiv.org/abs/2403.18197v2,http://arxiv.org/pdf/2403.18197v2.pdf,all:robotic arm AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2024
Monitoring Electrostatic Adhesion Forces via Acoustic Pressure,"Electrostatic adhesion is widely used in mobile robotics, haptics, and
robotic end effectors for its adaptability to diverse substrates and low energy
consumption. Force sensing is important for feedback control, interaction, and
monitoring in the EA system. However, EA force monitoring often relies on bulky
and expensive sensors, increasing the complexity and weight of the entire
system. This paper presents an acoustic-pressure-based method to monitor EA
forces without contacting the adhesion pad. When the EA pad is driven by a
bipolar square-wave voltage to adhere a conductive object, periodic acoustic
pulses arise from the EA system. We employed a microphone to capture these
acoustic pressure signals and investigate the influence of peak pressure
values. Results show that the peak value of acoustic pressure increased with
the mass and contact area of the adhered object, as well as with the amplitude
and frequency of the driving voltage. We applied this technique to mass
estimation of various objects and simultaneous monitoring of two EA systems.
Then, we integrated this technique into an EA end effector that enables
monitoring the change of adhered object mass during transport. The proposed
technique offers a low-cost, non-contact, and multi-object monitoring solution
for EA end effectors in handling tasks.","Huacen Wang, Jiarui Zou, Zeju Zheng, Hongqiang Wang",2025-05-22T12:45:08Z,2025-05-22T12:45:08Z,http://arxiv.org/abs/2505.16609v1,http://arxiv.org/pdf/2505.16609v1.pdf,all:end effector AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2025
"A Combined Learning and Optimization Framework to Transfer Human
  Whole-body Loco-manipulation Skills to Mobile Manipulators","Humans' ability to smoothly switch between locomotion and manipulation is a
remarkable feature of sensorimotor coordination. Leaning and replication of
such human-like strategies can lead to the development of more sophisticated
robots capable of performing complex whole-body tasks in real-world
environments. To this end, this paper proposes a combined learning and
optimization framework for transferring human's loco-manipulation
soft-switching skills to mobile manipulators. The methodology departs from data
collection of human demonstrations for a locomotion-integrated manipulation
task through a vision system. Next, the wrist and pelvis motions are mapped to
mobile manipulators' End-Effector (EE) and mobile base. A kernelized movement
primitive algorithm learns the wrist and pelvis trajectories and generalizes to
new desired points according to task requirements. Next, the reference
trajectories are sent to a hierarchical quadratic programming controller, where
the EE and the mobile base reference trajectories are provided as the first and
second priority tasks, generating the feasible and optimal joint level
commands. A locomotion-integrated pick-and-place task is executed to validate
the proposed approach. After a human demonstrates the task, a mobile
manipulator executes the task with the same and new settings, grasping a bottle
at non-zero velocity. The results showed that the proposed approach
successfully transfers the human loco-manipulation skills to mobile
manipulators, even with different geometry.","Jianzhuang Zhao, Francesco Tassi, Yanlong Huang, Elena De Momi, Arash Ajoudani",2024-02-21T16:31:07Z,2024-02-21T16:31:07Z,http://arxiv.org/abs/2402.13915v1,http://arxiv.org/pdf/2402.13915v1.pdf,all:end effector AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2024
"Versatile Airborne Ultrasonic NDT Technologies via Active Omni-Sliding
  with Over-Actuated Aerial Vehicles","This paper presents the utilization of advanced methodologies in aerial
manipulation to address meaningful industrial applications and develop
versatile ultrasonic Non-Destructive Testing (NDT) technologies with aerial
robots. The primary objectives of this work are to enable multi-point
measurements through sliding without re-approaching the work surface, and
facilitate the representation of material thickness with B and C scans via
dynamic scanning in arbitrary directions (i.e. omnidirections). To accomplish
these objectives, a payload that can slide in omnidirections (here we call the
omni-sliding payload) is designed for an over-actuated aerial vehicle, ensuring
truly omnidirectional sliding mobility while exerting consistent forces in
contact with a flat work surface. The omni-sliding payload is equipped with an
omniwheel-based active end-effector and an Electro Magnetic Acoustic Transducer
(EMAT). Furthermore, to ensure successful development of the designed payload
and integration with the aerial vehicle, a comprehensive studying on contact
conditions and system dynamics during active sliding is presented, and the
derived system constraints are later used as guidelines for the hardware
development and control setting. The proposed methods are validated through
experiments, encompassing both the wall-sliding task and dynamic scanning for
Ultrasonic Testing (UT), employing the aerial platform - Voliro T.","Tong Hui, Florian Braun, Nicolas Scheidt, Marius Fehr, Matteo Fumagalli",2023-11-08T13:06:28Z,2023-11-08T13:06:28Z,http://arxiv.org/abs/2311.04662v1,http://arxiv.org/pdf/2311.04662v1.pdf,all:end effector AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2023
ORLA*: Mobile Manipulator-Based Object Rearrangement with Lazy A Star,"Effectively performing object rearrangement is an essential skill for mobile
manipulators, e.g., setting up a dinner table or organizing a desk. A key
challenge in such problems is deciding an appropriate manipulation order for
objects to effectively untangle dependencies between objects while considering
the necessary motions for realizing the manipulations (e.g., pick and place).
To our knowledge, computing time-optimal multi-object rearrangement solutions
for mobile manipulators remains a largely untapped research direction. In this
research, we propose ORLA*, which leverages delayed (lazy) evaluation in
searching for a high-quality object pick and place sequence that considers both
end-effector and mobile robot base travel. ORLA* also supports multi-layered
rearrangement tasks considering pile stability using machine learning.
Employing an optimal solver for finding temporary locations for displacing
objects, ORLA* can achieve global optimality. Through extensive simulation and
ablation study, we confirm the effectiveness of ORLA* delivering quality
solutions for challenging rearrangement instances. Supplementary materials are
available at: https://gaokai15.github.io/ORLA-Star/","Kai Gao,  Zhaxizhuoma, Yan Ding, Shiqi Zhang, Jingjin Yu",2023-09-24T17:40:19Z,2024-10-20T05:23:35Z,http://arxiv.org/abs/2309.13707v2,http://arxiv.org/pdf/2309.13707v2.pdf,all:end effector AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2023
"Planning Optimal Trajectories for Mobile Manipulators under End-effector
  Trajectory Continuity Constraint","Mobile manipulators have been employed in many applications that are
traditionally performed by either multiple fixed-base robots or a large robotic
system. This capability is enabled by the mobility of the mobile base. However,
the mobile base also brings redundancy to the system, which makes mobile
manipulator motion planning more challenging. In this paper, we tackle the
mobile manipulator motion planning problem under the end-effector trajectory
continuity constraint in which the end-effector is required to traverse a
continuous task-space trajectory (time-parametrized path), such as in mobile
printing or spraying applications. Our method decouples the problem into: (1)
planning an optimal base trajectory subject to geometric task constraints,
end-effector trajectory continuity constraint, collision avoidance, and base
velocity constraint; which ensures that (2) a manipulator trajectory is
computed subsequently based on the obtained base trajectory. To validate our
method, we propose a discrete optimal base trajectory planning algorithm to
solve several mobile printing tasks in hardware experiment and simulations.","Quang-Nam Nguyen, Quang-Cuong Pham",2023-09-21T16:52:29Z,2024-03-06T15:09:54Z,http://arxiv.org/abs/2309.12251v2,http://arxiv.org/pdf/2309.12251v2.pdf,all:end effector AND all:mobile base AND submittedDate:[202309062236 TO 202509052236],2023
"BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation
  for Everyday Household Activities","Real-world household tasks present significant challenges for mobile
manipulation robots. An analysis of existing robotics benchmarks reveals that
successful task performance hinges on three key whole-body control
capabilities: bimanual coordination, stable and precise navigation, and
extensive end-effector reachability. Achieving these capabilities requires
careful hardware design, but the resulting system complexity further
complicates visuomotor policy learning. To address these challenges, we
introduce the BEHAVIOR Robot Suite (BRS), a comprehensive framework for
whole-body manipulation in diverse household tasks. Built on a bimanual,
wheeled robot with a 4-DoF torso, BRS integrates a cost-effective whole-body
teleoperation interface for data collection and a novel algorithm for learning
whole-body visuomotor policies. We evaluate BRS on five challenging household
tasks that not only emphasize the three core capabilities but also introduce
additional complexities, such as long-range navigation, interaction with
articulated and deformable objects, and manipulation in confined spaces. We
believe that BRS's integrated robotic embodiment, data collection interface,
and learning framework mark a significant step toward enabling real-world
whole-body manipulation for everyday household tasks. BRS is open-sourced at
https://behavior-robot-suite.github.io/","Yunfan Jiang, Ruohan Zhang, Josiah Wong, Chen Wang, Yanjie Ze, Hang Yin, Cem Gokmen, Shuran Song, Jiajun Wu, Li Fei-Fei",2025-03-07T18:15:21Z,2025-08-24T20:43:32Z,http://arxiv.org/abs/2503.05652v2,http://arxiv.org/pdf/2503.05652v2.pdf,all:end effector AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
Inverse Kinematics on Guiding Vector Fields for Robot Path Following,"Inverse kinematics is a fundamental technique for motion and positioning
control in robotics, typically applied to end-effectors. In this paper, we
extend the concept of inverse kinematics to guiding vector fields for path
following in autonomous mobile robots. The desired path is defined by its
implicit equation, i.e., by a collection of points belonging to one or more
zero-level sets. These level sets serve as a reference to construct an error
signal that drives the guiding vector field toward the desired path, enabling
the robot to converge and travel along the path by following such a vector
field. We start with the formal exposition on how inverse kinematics can be
applied to guiding vector fields for single-integrator robots in an
m-dimensional Euclidean space. Then, we leverage inverse kinematics to ensure
that the level-set error signal behaves as a linear system, facilitating
control over the robot's transient motion toward the desired path and allowing
for the injection of feed-forward signals to induce precise motion behavior
along the path. We then propose solutions to the theoretical and practical
challenges of applying this technique to unicycles with constant speeds to
follow 2D paths with precise transient control. We finish by validating the
predicted theoretical results through real flights with fixed-wing drones.","Yu Zhou, Jesús Bautista, Weijia Yao, Héctor García de Marina",2025-02-24T16:50:01Z,2025-02-24T16:50:01Z,http://arxiv.org/abs/2502.17313v1,http://arxiv.org/pdf/2502.17313v1.pdf,all:end effector AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"A Training-Free Framework for Precise Mobile Manipulation of Small
  Everyday Objects","Many everyday mobile manipulation tasks require precise interaction with
small objects, such as grasping a knob to open a cabinet or pressing a light
switch. In this paper, we develop Servoing with Vision Models (SVM), a
closed-loop training-free framework that enables a mobile manipulator to tackle
such precise tasks involving the manipulation of small objects. SVM employs an
RGB-D wrist camera and uses visual servoing for control. Our novelty lies in
the use of state-of-the-art vision models to reliably compute 3D targets from
the wrist image for diverse tasks and under occlusion due to the end-effector.
To mitigate occlusion artifacts, we employ vision models to out-paint the
end-effector thereby significantly enhancing target localization. We
demonstrate that aided by out-painting methods, open-vocabulary object
detectors can serve as a drop-in module to identify semantic targets (e.g.
knobs) and point tracking methods can reliably track interaction sites
indicated by user clicks. This training-free method obtains an 85% zero-shot
success rate on manipulating unseen objects in novel environments in the real
world, outperforming an open-loop control method and an imitation learning
baseline trained on 1000+ demonstrations by an absolute success rate of 50%.","Arjun Gupta, Rishik Sathua, Saurabh Gupta",2025-02-19T18:59:17Z,2025-02-19T18:59:17Z,http://arxiv.org/abs/2502.13964v1,http://arxiv.org/pdf/2502.13964v1.pdf,all:end effector AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
Soft and Compliant Contact-Rich Hair Manipulation and Care,"Hair care robots can help address labor shortages in elderly care while
enabling those with limited mobility to maintain their hair-related identity.
We present MOE-Hair, a soft robot system that performs three hair-care tasks:
head patting, finger combing, and hair grasping. The system features a
tendon-driven soft robot end-effector (MOE) with a wrist-mounted RGBD camera,
leveraging both mechanical compliance for safety and visual force sensing
through deformation. In testing with a force-sensorized mannequin head, MOE
achieved comparable hair-grasping effectiveness while applying significantly
less force than rigid grippers. Our novel force estimation method combines
visual deformation data and tendon tensions from actuators to infer applied
forces, reducing sensing errors by up to 60.1% and 20.3% compared to actuator
current load-only and depth image-only baselines, respectively. A user study
with 12 participants demonstrated statistically significant preferences for
MOE-Hair over a baseline system in terms of comfort, effectiveness, and
appropriate force application. These results demonstrate the unique advantages
of soft robots in contact-rich hair-care tasks, while highlighting the
importance of precise force control despite the inherent compliance of the
system.","Uksang Yoo, Nathaniel Dennler, Eliot Xing, Maja Matarić, Stefanos Nikolaidis, Jeffrey Ichnowski, Jean Oh",2025-01-05T19:21:16Z,2025-01-05T19:21:16Z,http://arxiv.org/abs/2501.02630v1,http://arxiv.org/pdf/2501.02630v1.pdf,all:end effector AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2025
"Sinkage Study in Granular Material for Space Exploration Legged Robot
  Gripper","Wheeled rovers have been the primary choice for lunar exploration due to
their speed and efficiency. However, deeper areas, such as lunar caves and
craters, require the mobility of legged robots. To do so, appropriate end
effectors must be designed to enable climbing and walking on the granular
surface of the Moon. This paper investigates the behavior of an underactuated
soft gripper on deformable granular material when a legged robot is walking in
soft soil. A modular test bench and a simulation model were developed to
observe the gripper sinkage behavior under load. The gripper uses tendon-driven
fingers to match its target shape and grasp on the target surface using
multiple micro-spines. The sinkage of the gripper in silica sand was measured
by comparing the axial displacement of the gripper with the nominal load of the
robot mass. Multiple experiments were performed to observe the sinkage of the
gripper over a range of slope angles. A simulation model accounting for the
degrees of compliance of the gripper fingers was created using Altair
MotionSolve software and coupled to Altair EDEM to compute the gripper
interaction with particles utilizing the discrete element method. After
validation of the model, complementary simulations using Lunar gravity and a
regolith particle model were performed. The results show that a satisfactory
gripper model with accurate freedom of motion can be created in simulation
using the Altair simulation packages and expected sinkage under load in a
particle-filled environment can be estimated using this model. By computing the
sinkage of the end effector of legged robots, the results can be directly
integrated into the motion control algorithm and improve the accuracy of
mobility in a granular material environment.","Arthur Candalot, James Hurrell, Malik Manel Hashim, Brigid Hickey, Mickael Laine, Kazuya Yoshida",2024-11-08T14:34:09Z,2025-01-10T15:21:58Z,http://arxiv.org/abs/2411.07261v2,http://arxiv.org/pdf/2411.07261v2.pdf,all:end effector AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2024
"Generating and Optimizing Topologically Distinct Guesses for Mobile
  Manipulator Path Planning","Optimal path planning often suffers from getting stuck in a local optimum.
This is often the case for mobile manipulators due to nonconvexities induced by
obstacles and robot kinematics. This paper attempts to circumvent this issue by
proposing a pipeline to obtain multiple distinct local optima. By evaluating
and selecting the optimum among multiple distinct local optima, it is likely to
obtain a closer approximation of the global optimum. We demonstrate this
capability in optimal path planning of nonholonomic mobile manipulators in the
presence of obstacles and subject to end effector path constraints. The
nonholomicity, obstacles, and end effector path constraints often cause direct
optimal path planning approaches to get stuck in local optima. We demonstrate
that our pipeline is able to circumvent this issue and produce a final local
optimum that is close to the global optimum.","Rufus Cheuk Yin Wong, Mayank Sewlia, Adrian Wiltz, Dimos V. Dimarogonas",2024-10-27T23:48:08Z,2024-10-27T23:48:08Z,http://arxiv.org/abs/2410.20635v1,http://arxiv.org/pdf/2410.20635v1.pdf,all:end effector AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2024
Opening Articulated Structures in the Real World,"What does it take to build mobile manipulation systems that can competently
operate on previously unseen objects in previously unseen environments? This
work answers this question using opening of articulated structures as a mobile
manipulation testbed. Specifically, our focus is on the end-to-end performance
on this task without any privileged information, i.e. the robot starts at a
location with the novel target articulated object in view, and has to approach
the object and successfully open it. We first develop a system for this task,
and then conduct 100+ end-to-end system tests across 13 real world test sites.
Our large-scale study reveals a number of surprising findings: a) modular
systems outperform end-to-end learned systems for this task, even when the
end-to-end learned systems are trained on 1000+ demonstrations, b) perception,
and not precise end-effector control, is the primary bottleneck to task
success, and c) state-of-the-art articulation parameter estimation models
developed in isolation struggle when faced with robot-centric viewpoints.
Overall, our findings highlight the limitations of developing components of the
pipeline in isolation and underscore the need for system-level research,
providing a pragmatic roadmap for building generalizable mobile manipulation
systems. Videos, code, and models are available on the project website:
https://arjung128.github.io/opening-articulated-structures/","Arjun Gupta, Michelle Zhang, Rishik Sathua, Saurabh Gupta",2024-02-27T18:58:54Z,2025-05-07T03:38:59Z,http://arxiv.org/abs/2402.17767v3,http://arxiv.org/pdf/2402.17767v3.pdf,all:end effector AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2024
"RABBIT: A Robot-Assisted Bed Bathing System with Multimodal Perception
  and Integrated Compliance","This paper introduces RABBIT, a novel robot-assisted bed bathing system
designed to address the growing need for assistive technologies in personal
hygiene tasks. It combines multimodal perception and dual (software and
hardware) compliance to perform safe and comfortable physical human-robot
interaction. Using RGB and thermal imaging to segment dry, soapy, and wet skin
regions accurately, RABBIT can effectively execute washing, rinsing, and drying
tasks in line with expert caregiving practices. Our system includes
custom-designed motion primitives inspired by human caregiving techniques, and
a novel compliant end-effector called Scrubby, optimized for gentle and
effective interactions. We conducted a user study with 12 participants,
including one participant with severe mobility limitations, demonstrating the
system's effectiveness and perceived comfort. Supplementary material and videos
can be found on our website https://emprise.cs.cornell.edu/rabbit.","Rishabh Madan, Skyler Valdez, David Kim, Sujie Fang, Luoyan Zhong, Diego Virtue, Tapomayukh Bhattacharjee",2024-01-26T19:08:04Z,2025-04-23T21:58:09Z,http://arxiv.org/abs/2401.15159v2,http://arxiv.org/pdf/2401.15159v2.pdf,all:end effector AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2024
"Stretch with Stretch: Physical Therapy Exercise Games Led by a Mobile
  Manipulator","Physical therapy (PT) is a key component of many rehabilitation regimens,
such as treatments for Parkinson's disease (PD). However, there are shortages
of physical therapists and adherence to self-guided PT is low. Robots have the
potential to support physical therapists and increase adherence to self-guided
PT, but prior robotic systems have been large and immobile, which can be a
barrier to use in homes and clinics. We present Stretch with Stretch (SWS), a
novel robotic system for leading stretching exercise games for older adults
with PD. SWS consists of a compact and lightweight mobile manipulator (Hello
Robot Stretch RE1) that visually and verbally guides users through PT
exercises. The robot's soft end effector serves as a target that users
repetitively reach towards and press with a hand, foot, or knee. For each
exercise, target locations are customized for the individual via a visually
estimated kinematic model, a haptically estimated range of motion, and the
person's exercise performance. The system includes sound effects and verbal
feedback from the robot to keep users engaged throughout a session and augment
physical exercise with cognitive exercise. We conducted a user study for which
people with PD (n=10) performed 6 exercises with the system. Participants
perceived the SWS to be useful and easy to use. They also reported mild to
moderate perceived exertion (RPE).","Matthew Lamsey, You Liang Tan, Meredith D. Wells, Madeline Beatty, Zexuan Liu, Arjun Majumdar, Kendra Washington, Jerry Feldman, Naveen Kuppuswamy, Elizabeth Nguyen, Arielle Wallenstein, Madeleine E. Hackney, Charles C. Kemp",2023-12-20T18:57:20Z,2023-12-21T07:18:16Z,http://arxiv.org/abs/2312.13279v2,http://arxiv.org/pdf/2312.13279v2.pdf,all:end effector AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2023
ForceSight: Text-Guided Mobile Manipulation with Visual-Force Goals,"We present ForceSight, a system for text-guided mobile manipulation that
predicts visual-force goals using a deep neural network. Given a single RGBD
image combined with a text prompt, ForceSight determines a target end-effector
pose in the camera frame (kinematic goal) and the associated forces (force
goal). Together, these two components form a visual-force goal. Prior work has
demonstrated that deep models outputting human-interpretable kinematic goals
can enable dexterous manipulation by real robots. Forces are critical to
manipulation, yet have typically been relegated to lower-level execution in
these systems. When deployed on a mobile manipulator equipped with an
eye-in-hand RGBD camera, ForceSight performed tasks such as precision grasps,
drawer opening, and object handovers with an 81% success rate in unseen
environments with object instances that differed significantly from the
training data. In a separate experiment, relying exclusively on visual servoing
and ignoring force goals dropped the success rate from 90% to 45%,
demonstrating that force goals can significantly enhance performance. The
appendix, videos, code, and trained models are available at
https://force-sight.github.io/.","Jeremy A. Collins, Cody Houff, You Liang Tan, Charles C. Kemp",2023-09-21T17:59:50Z,2023-09-24T01:24:06Z,http://arxiv.org/abs/2309.12312v2,http://arxiv.org/pdf/2309.12312v2.pdf,all:end effector AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2023
"Mobile Manipulation Platform for Autonomous Indoor Inspections in
  Low-Clearance Areas","Mobile manipulators have been used for inspection, maintenance and repair
tasks over the years, but there are some key limitations. Stability concerns
typically require mobile platforms to be large in order to handle far-reaching
manipulators, or for the manipulators to have drastically reduced workspaces to
fit onto smaller mobile platforms. Therefore we propose a combination of two
widely-used robots, the Clearpath Jackal unmanned ground vehicle and the Kinova
Gen3 six degree-of-freedom manipulator. The Jackal has a small footprint and
works well in low-clearance indoor environments. Extensive testing of
localization, navigation and mapping using LiDAR sensors makes the Jackal a
well developed mobile platform suitable for mobile manipulation. The Gen3 has a
long reach with reasonable power consumption for manipulation tasks. A wrist
camera for RGB-D sensing and a customizable end effector interface makes the
Gen3 suitable for a myriad of manipulation tasks. Typically these features
would result in an unstable platform, however with a few minor hardware and
software modifications, we have produced a stable, high-performance mobile
manipulation platform with significant mobility, reach, sensing, and
maneuverability for indoor inspection tasks, without degradation of the
component robots' individual capabilities. These assertions were investigated
with hardware via semi-autonomous navigation to waypoints in a busy indoor
environment, and high-precision self-alignment alongside planar structures for
intervention tasks.","Erik Pearson, Paul Szenher, Christine Huang, Brendan Englot",2023-09-19T17:41:28Z,2023-09-19T17:41:28Z,http://arxiv.org/abs/2309.10794v1,http://arxiv.org/pdf/2309.10794v1.pdf,all:end effector AND all:mobile robot AND submittedDate:[202309062236 TO 202509052236],2023
